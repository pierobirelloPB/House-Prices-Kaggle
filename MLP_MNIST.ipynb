{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST digit recognition with MLPclassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display, Math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel774</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n",
       "0      1       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   pixel8  ...  pixel774  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n",
       "0       0  ...         0         0         0         0         0         0   \n",
       "\n",
       "   pixel780  pixel781  pixel782  pixel783  \n",
       "0         0         0         0         0  \n",
       "\n",
       "[1 rows x 785 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel774</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 784 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n",
       "0       0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   pixel9  ...  pixel774  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n",
       "0       0  ...         0         0         0         0         0         0   \n",
       "\n",
       "   pixel780  pixel781  pixel782  pixel783  \n",
       "0         0         0         0         0  \n",
       "\n",
       "[1 rows x 784 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 42000 entries, 0 to 41999\n",
      "Columns: 785 entries, label to pixel783\n",
      "dtypes: int64(785)\n",
      "memory usage: 251.5 MB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 28000 entries, 0 to 27999\n",
      "Columns: 784 entries, pixel0 to pixel783\n",
      "dtypes: int64(784)\n",
      "memory usage: 167.5 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "\n",
    "display(train_df.head(1))\n",
    "display(test_df.head(1))\n",
    "\n",
    "print(train_df.info())\n",
    "print(test_df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare X,y and plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle X \\in [0.0,1.0]$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle y \\in [0,9]$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 480x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 480x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 480x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = train_df.drop(columns=['label'],axis=1).values.astype('float32')/255\n",
    "y = train_df['label'].values.astype('int32')\n",
    "\n",
    "display(Math(f\"X \\\\in [{np.min(X)},{np.max(X)}]\"))\n",
    "display(Math(f\"y \\\\in [{np.min(y)},{np.max(y)}]\"))\n",
    "\n",
    "def plt_image(ex_idx):\n",
    "    ex_label = y[ex_idx]\n",
    "    ex_image = X[ex_idx,:].reshape(28,28)\n",
    "    plt.matshow(ex_image,cmap='binary')\n",
    "    plt.title(ex_label)\n",
    "    plt.show() \n",
    "\n",
    "ex_idxs = [20,40,60]\n",
    "for ex_idx in ex_idxs:\n",
    "    plt_image(ex_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
      "Iteration 1, loss = 0.37295633\n",
      "Iteration 2, loss = 0.17225538\n",
      "Iteration 3, loss = 0.12352998\n",
      "Iteration 4, loss = 0.09451591\n",
      "Iteration 5, loss = 0.07492742\n",
      "Iteration 6, loss = 0.06144740\n",
      "Iteration 7, loss = 0.04919556\n",
      "Iteration 8, loss = 0.04156113\n",
      "Iteration 9, loss = 0.03400888\n",
      "Iteration 10, loss = 0.02873998\n",
      "Iteration 11, loss = 0.02383651\n",
      "Iteration 12, loss = 0.01959767\n",
      "Iteration 13, loss = 0.01675616\n",
      "Iteration 14, loss = 0.01470996\n",
      "Iteration 15, loss = 0.01279907\n",
      "Iteration 16, loss = 0.01076655\n",
      "Iteration 17, loss = 0.00912821\n",
      "Iteration 18, loss = 0.00990122\n",
      "Iteration 19, loss = 0.01021190\n",
      "Iteration 20, loss = 0.00616851\n",
      "Iteration 21, loss = 0.01010955\n",
      "Iteration 22, loss = 0.00523257\n",
      "Iteration 23, loss = 0.00756719\n",
      "Iteration 24, loss = 0.00559569\n",
      "Iteration 25, loss = 0.00370070\n",
      "Iteration 26, loss = 0.00839332\n",
      "Iteration 27, loss = 0.00802532\n",
      "Iteration 28, loss = 0.00422385\n",
      "Iteration 29, loss = 0.00251014\n",
      "Iteration 30, loss = 0.00233373\n",
      "Iteration 31, loss = 0.00224437\n",
      "Iteration 32, loss = 0.01670889\n",
      "Iteration 33, loss = 0.00427145\n",
      "Iteration 34, loss = 0.00258129\n",
      "Iteration 35, loss = 0.00230691\n",
      "Iteration 36, loss = 0.00222492\n",
      "Iteration 37, loss = 0.00214725\n",
      "Iteration 38, loss = 0.00493380\n",
      "Iteration 39, loss = 0.01670707\n",
      "Iteration 40, loss = 0.00298345\n",
      "Iteration 41, loss = 0.00237988\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.0001, batch_size=50, hidden_layer_sizes=(100,), solver=adam; total time= 1.9min\n",
      "Iteration 1, loss = 0.36845534\n",
      "Iteration 2, loss = 0.17908494\n",
      "Iteration 3, loss = 0.13002346\n",
      "Iteration 4, loss = 0.10036950\n",
      "Iteration 5, loss = 0.08135792\n",
      "Iteration 6, loss = 0.06567127\n",
      "Iteration 7, loss = 0.05543762\n",
      "Iteration 8, loss = 0.04429322\n",
      "Iteration 9, loss = 0.03590797\n",
      "Iteration 10, loss = 0.03149436\n",
      "Iteration 11, loss = 0.02529800\n",
      "Iteration 12, loss = 0.02143004\n",
      "Iteration 13, loss = 0.01868280\n",
      "Iteration 14, loss = 0.01534600\n",
      "Iteration 15, loss = 0.01325567\n",
      "Iteration 16, loss = 0.01103628\n",
      "Iteration 17, loss = 0.01054138\n",
      "Iteration 18, loss = 0.01033023\n",
      "Iteration 19, loss = 0.00740873\n",
      "Iteration 20, loss = 0.01048800\n",
      "Iteration 21, loss = 0.00549340\n",
      "Iteration 22, loss = 0.00444346\n",
      "Iteration 23, loss = 0.00910378\n",
      "Iteration 24, loss = 0.00814929\n",
      "Iteration 25, loss = 0.00338078\n",
      "Iteration 26, loss = 0.00267107\n",
      "Iteration 27, loss = 0.00249401\n",
      "Iteration 28, loss = 0.00831247\n",
      "Iteration 29, loss = 0.01166150\n",
      "Iteration 30, loss = 0.00326621\n",
      "Iteration 31, loss = 0.00249995\n",
      "Iteration 32, loss = 0.00238987\n",
      "Iteration 33, loss = 0.00222494\n",
      "Iteration 34, loss = 0.00241356\n",
      "Iteration 35, loss = 0.02021491\n",
      "Iteration 36, loss = 0.00447748\n",
      "Iteration 37, loss = 0.00329786\n",
      "Iteration 38, loss = 0.00282772\n",
      "Iteration 39, loss = 0.00223802\n",
      "Iteration 40, loss = 0.00214048\n",
      "Iteration 41, loss = 0.00208565\n",
      "Iteration 42, loss = 0.00204317\n",
      "Iteration 43, loss = 0.00203889\n",
      "Iteration 44, loss = 0.01829133\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.0001, batch_size=50, hidden_layer_sizes=(100,), solver=adam; total time= 2.1min\n",
      "Iteration 1, loss = 0.37548970\n",
      "Iteration 2, loss = 0.17207385\n",
      "Iteration 3, loss = 0.12238261\n",
      "Iteration 4, loss = 0.09486701\n",
      "Iteration 5, loss = 0.07562759\n",
      "Iteration 6, loss = 0.06222422\n",
      "Iteration 7, loss = 0.05051560\n",
      "Iteration 8, loss = 0.04185238\n",
      "Iteration 9, loss = 0.03358200\n",
      "Iteration 10, loss = 0.02797236\n",
      "Iteration 11, loss = 0.02264529\n",
      "Iteration 12, loss = 0.01848714\n",
      "Iteration 13, loss = 0.01643231\n",
      "Iteration 14, loss = 0.01309189\n",
      "Iteration 15, loss = 0.01208803\n",
      "Iteration 16, loss = 0.01029446\n",
      "Iteration 17, loss = 0.00756012\n",
      "Iteration 18, loss = 0.00943079\n",
      "Iteration 19, loss = 0.00913762\n",
      "Iteration 20, loss = 0.00501664\n",
      "Iteration 21, loss = 0.00429053\n",
      "Iteration 22, loss = 0.00897615\n",
      "Iteration 23, loss = 0.00885984\n",
      "Iteration 24, loss = 0.00416486\n",
      "Iteration 25, loss = 0.00321219\n",
      "Iteration 26, loss = 0.00366144\n",
      "Iteration 27, loss = 0.01268790\n",
      "Iteration 28, loss = 0.00528261\n",
      "Iteration 29, loss = 0.00255768\n",
      "Iteration 30, loss = 0.00228404\n",
      "Iteration 31, loss = 0.00219020\n",
      "Iteration 32, loss = 0.00215605\n",
      "Iteration 33, loss = 0.01340734\n",
      "Iteration 34, loss = 0.00773985\n",
      "Iteration 35, loss = 0.00274362\n",
      "Iteration 36, loss = 0.00224403\n",
      "Iteration 37, loss = 0.00214053\n",
      "Iteration 38, loss = 0.00208639\n",
      "Iteration 39, loss = 0.00202662\n",
      "Iteration 40, loss = 0.01289924\n",
      "Iteration 41, loss = 0.00794185\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.0001, batch_size=50, hidden_layer_sizes=(100,), solver=adam; total time= 2.1min\n",
      "Iteration 1, loss = 0.37637776\n",
      "Iteration 2, loss = 0.17619156\n",
      "Iteration 3, loss = 0.12847290\n",
      "Iteration 4, loss = 0.09824400\n",
      "Iteration 5, loss = 0.07965945\n",
      "Iteration 6, loss = 0.06510473\n",
      "Iteration 7, loss = 0.05246796\n",
      "Iteration 8, loss = 0.04337090\n",
      "Iteration 9, loss = 0.03745083\n",
      "Iteration 10, loss = 0.03042607\n",
      "Iteration 11, loss = 0.02496851\n",
      "Iteration 12, loss = 0.02197059\n",
      "Iteration 13, loss = 0.01762720\n",
      "Iteration 14, loss = 0.01486079\n",
      "Iteration 15, loss = 0.01356821\n",
      "Iteration 16, loss = 0.01135003\n",
      "Iteration 17, loss = 0.01040056\n",
      "Iteration 18, loss = 0.00969465\n",
      "Iteration 19, loss = 0.00838326\n",
      "Iteration 20, loss = 0.00637268\n",
      "Iteration 21, loss = 0.00459817\n",
      "Iteration 22, loss = 0.01039496\n",
      "Iteration 23, loss = 0.00700389\n",
      "Iteration 24, loss = 0.00607588\n",
      "Iteration 25, loss = 0.00678333\n",
      "Iteration 26, loss = 0.00641498\n",
      "Iteration 27, loss = 0.00614569\n",
      "Iteration 28, loss = 0.00391248\n",
      "Iteration 29, loss = 0.00470837\n",
      "Iteration 30, loss = 0.00845859\n",
      "Iteration 31, loss = 0.00503279\n",
      "Iteration 32, loss = 0.00272116\n",
      "Iteration 33, loss = 0.00230307\n",
      "Iteration 34, loss = 0.00220328\n",
      "Iteration 35, loss = 0.00839265\n",
      "Iteration 36, loss = 0.01142300\n",
      "Iteration 37, loss = 0.00362553\n",
      "Iteration 38, loss = 0.00240155\n",
      "Iteration 39, loss = 0.00224229\n",
      "Iteration 40, loss = 0.00217280\n",
      "Iteration 41, loss = 0.00210366\n",
      "Iteration 42, loss = 0.00207887\n",
      "Iteration 43, loss = 0.01883805\n",
      "Iteration 44, loss = 0.00395901\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.0001, batch_size=50, hidden_layer_sizes=(100,), solver=adam; total time= 2.1min\n",
      "Iteration 1, loss = 0.37768348\n",
      "Iteration 2, loss = 0.17769165\n",
      "Iteration 3, loss = 0.12581792\n",
      "Iteration 4, loss = 0.09864253\n",
      "Iteration 5, loss = 0.07849566\n",
      "Iteration 6, loss = 0.06329099\n",
      "Iteration 7, loss = 0.05265278\n",
      "Iteration 8, loss = 0.04345847\n",
      "Iteration 9, loss = 0.03599917\n",
      "Iteration 10, loss = 0.03016421\n",
      "Iteration 11, loss = 0.02499133\n",
      "Iteration 12, loss = 0.02024296\n",
      "Iteration 13, loss = 0.01738902\n",
      "Iteration 14, loss = 0.01528957\n",
      "Iteration 15, loss = 0.01376066\n",
      "Iteration 16, loss = 0.01060204\n",
      "Iteration 17, loss = 0.00952569\n",
      "Iteration 18, loss = 0.00789079\n",
      "Iteration 19, loss = 0.00847429\n",
      "Iteration 20, loss = 0.00976355\n",
      "Iteration 21, loss = 0.00793740\n",
      "Iteration 22, loss = 0.00433170\n",
      "Iteration 23, loss = 0.00549452\n",
      "Iteration 24, loss = 0.00959305\n",
      "Iteration 25, loss = 0.00656790\n",
      "Iteration 26, loss = 0.00330210\n",
      "Iteration 27, loss = 0.00274653\n",
      "Iteration 28, loss = 0.00243936\n",
      "Iteration 29, loss = 0.00337881\n",
      "Iteration 30, loss = 0.01820976\n",
      "Iteration 31, loss = 0.00393825\n",
      "Iteration 32, loss = 0.00282238\n",
      "Iteration 33, loss = 0.00232836\n",
      "Iteration 34, loss = 0.00222441\n",
      "Iteration 35, loss = 0.00215331\n",
      "Iteration 36, loss = 0.00923404\n",
      "Iteration 37, loss = 0.01205817\n",
      "Iteration 38, loss = 0.00472788\n",
      "Iteration 39, loss = 0.00249197\n",
      "Iteration 40, loss = 0.00224841\n",
      "Iteration 41, loss = 0.00216870\n",
      "Iteration 42, loss = 0.00211025\n",
      "Iteration 43, loss = 0.00207561\n",
      "Iteration 44, loss = 0.01833900\n",
      "Iteration 45, loss = 0.00381108\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.0001, batch_size=50, hidden_layer_sizes=(100,), solver=adam; total time= 1.9min\n",
      "Iteration 1, loss = 1.06438531\n",
      "Iteration 2, loss = 0.47441312\n",
      "Iteration 3, loss = 0.38674597\n",
      "Iteration 4, loss = 0.34644669\n",
      "Iteration 5, loss = 0.32052708\n",
      "Iteration 6, loss = 0.30116400\n",
      "Iteration 7, loss = 0.28582406\n",
      "Iteration 8, loss = 0.27276877\n",
      "Iteration 9, loss = 0.26137875\n",
      "Iteration 10, loss = 0.25088870\n",
      "Iteration 11, loss = 0.24142362\n",
      "Iteration 12, loss = 0.23257836\n",
      "Iteration 13, loss = 0.22475204\n",
      "Iteration 14, loss = 0.21730593\n",
      "Iteration 15, loss = 0.21057466\n",
      "Iteration 16, loss = 0.20431421\n",
      "Iteration 17, loss = 0.19838502\n",
      "Iteration 18, loss = 0.19262936\n",
      "Iteration 19, loss = 0.18746886\n",
      "Iteration 20, loss = 0.18239485\n",
      "Iteration 21, loss = 0.17779183\n",
      "Iteration 22, loss = 0.17328466\n",
      "Iteration 23, loss = 0.16901191\n",
      "Iteration 24, loss = 0.16519324\n",
      "Iteration 25, loss = 0.16117511\n",
      "Iteration 26, loss = 0.15773329\n",
      "Iteration 27, loss = 0.15405582\n",
      "Iteration 28, loss = 0.15088058\n",
      "Iteration 29, loss = 0.14775068\n",
      "Iteration 30, loss = 0.14465789\n",
      "Iteration 31, loss = 0.14157642\n",
      "Iteration 32, loss = 0.13880126\n",
      "Iteration 33, loss = 0.13620323\n",
      "Iteration 34, loss = 0.13364992\n",
      "Iteration 35, loss = 0.13117764\n",
      "Iteration 36, loss = 0.12873192\n",
      "Iteration 37, loss = 0.12643413\n",
      "Iteration 38, loss = 0.12426687\n",
      "Iteration 39, loss = 0.12218159\n",
      "Iteration 40, loss = 0.11992022\n",
      "Iteration 41, loss = 0.11785457\n",
      "Iteration 42, loss = 0.11586390\n",
      "Iteration 43, loss = 0.11418669\n",
      "Iteration 44, loss = 0.11231989\n",
      "Iteration 45, loss = 0.11035032\n",
      "Iteration 46, loss = 0.10867519\n",
      "Iteration 47, loss = 0.10694888\n",
      "Iteration 48, loss = 0.10511972\n",
      "Iteration 49, loss = 0.10353237\n",
      "Iteration 50, loss = 0.10203159\n",
      "Iteration 51, loss = 0.10040352\n",
      "Iteration 52, loss = 0.09897333\n",
      "Iteration 53, loss = 0.09757552\n",
      "Iteration 54, loss = 0.09609131\n",
      "Iteration 55, loss = 0.09485619\n",
      "Iteration 56, loss = 0.09335795\n",
      "Iteration 57, loss = 0.09206498\n",
      "Iteration 58, loss = 0.09080373\n",
      "Iteration 59, loss = 0.08945801\n",
      "Iteration 60, loss = 0.08802234\n",
      "Iteration 61, loss = 0.08681592\n",
      "Iteration 62, loss = 0.08575840\n",
      "Iteration 63, loss = 0.08445947\n",
      "Iteration 64, loss = 0.08335886\n",
      "Iteration 65, loss = 0.08232937\n",
      "Iteration 66, loss = 0.08113839\n",
      "Iteration 67, loss = 0.08013222\n",
      "Iteration 68, loss = 0.07892713\n",
      "Iteration 69, loss = 0.07792238\n",
      "Iteration 70, loss = 0.07694286\n",
      "Iteration 71, loss = 0.07599481\n",
      "Iteration 72, loss = 0.07506240\n",
      "Iteration 73, loss = 0.07397375\n",
      "Iteration 74, loss = 0.07313986\n",
      "Iteration 75, loss = 0.07214221\n",
      "Iteration 76, loss = 0.07133055\n",
      "Iteration 77, loss = 0.07033810\n",
      "Iteration 78, loss = 0.06943143\n",
      "Iteration 79, loss = 0.06864529\n",
      "Iteration 80, loss = 0.06773174\n",
      "Iteration 81, loss = 0.06684491\n",
      "Iteration 82, loss = 0.06606744\n",
      "Iteration 83, loss = 0.06520982\n",
      "Iteration 84, loss = 0.06458815\n",
      "Iteration 85, loss = 0.06364834\n",
      "Iteration 86, loss = 0.06300390\n",
      "Iteration 87, loss = 0.06223829\n",
      "Iteration 88, loss = 0.06148629\n",
      "Iteration 89, loss = 0.06071988\n",
      "Iteration 90, loss = 0.06000218\n",
      "Iteration 91, loss = 0.05943245\n",
      "Iteration 92, loss = 0.05875719\n",
      "Iteration 93, loss = 0.05803440\n",
      "Iteration 94, loss = 0.05738377\n",
      "Iteration 95, loss = 0.05674652\n",
      "Iteration 96, loss = 0.05608470\n",
      "Iteration 97, loss = 0.05536345\n",
      "Iteration 98, loss = 0.05489191\n",
      "Iteration 99, loss = 0.05418622\n",
      "Iteration 100, loss = 0.05355150\n",
      "Iteration 101, loss = 0.05304397\n",
      "Iteration 102, loss = 0.05249472\n",
      "Iteration 103, loss = 0.05190829\n",
      "Iteration 104, loss = 0.05125536\n",
      "Iteration 105, loss = 0.05081122\n",
      "Iteration 106, loss = 0.05020181\n",
      "Iteration 107, loss = 0.04968459\n",
      "Iteration 108, loss = 0.04919063\n",
      "Iteration 109, loss = 0.04858697\n",
      "Iteration 110, loss = 0.04813829\n",
      "Iteration 111, loss = 0.04759792\n",
      "Iteration 112, loss = 0.04711704\n",
      "Iteration 113, loss = 0.04655362\n",
      "Iteration 114, loss = 0.04610036\n",
      "Iteration 115, loss = 0.04560292\n",
      "Iteration 116, loss = 0.04516537\n",
      "Iteration 117, loss = 0.04465559\n",
      "Iteration 118, loss = 0.04426191\n",
      "Iteration 119, loss = 0.04381398\n",
      "Iteration 120, loss = 0.04339399\n",
      "Iteration 121, loss = 0.04287042\n",
      "Iteration 122, loss = 0.04244490\n",
      "Iteration 123, loss = 0.04211686\n",
      "Iteration 124, loss = 0.04164524\n",
      "Iteration 125, loss = 0.04134124\n",
      "Iteration 126, loss = 0.04083963\n",
      "Iteration 127, loss = 0.04040098\n",
      "Iteration 128, loss = 0.04002457\n",
      "Iteration 129, loss = 0.03971086\n",
      "Iteration 130, loss = 0.03925200\n",
      "Iteration 131, loss = 0.03889226\n",
      "Iteration 132, loss = 0.03851307\n",
      "Iteration 133, loss = 0.03818401\n",
      "Iteration 134, loss = 0.03772374\n",
      "Iteration 135, loss = 0.03736094\n",
      "Iteration 136, loss = 0.03705259\n",
      "Iteration 137, loss = 0.03673070\n",
      "Iteration 138, loss = 0.03632308\n",
      "Iteration 139, loss = 0.03602684\n",
      "Iteration 140, loss = 0.03557555\n",
      "Iteration 141, loss = 0.03531192\n",
      "Iteration 142, loss = 0.03504436\n",
      "Iteration 143, loss = 0.03465142\n",
      "Iteration 144, loss = 0.03440471\n",
      "Iteration 145, loss = 0.03402885\n",
      "Iteration 146, loss = 0.03365123\n",
      "Iteration 147, loss = 0.03341834\n",
      "Iteration 148, loss = 0.03314604\n",
      "Iteration 149, loss = 0.03282857\n",
      "Iteration 150, loss = 0.03255792\n",
      "Iteration 151, loss = 0.03218916\n",
      "Iteration 152, loss = 0.03189720\n",
      "Iteration 153, loss = 0.03162168\n",
      "Iteration 154, loss = 0.03137909\n",
      "Iteration 155, loss = 0.03107630\n",
      "Iteration 156, loss = 0.03080452\n",
      "Iteration 157, loss = 0.03055180\n",
      "Iteration 158, loss = 0.03027495\n",
      "Iteration 159, loss = 0.02990842\n",
      "Iteration 160, loss = 0.02968350\n",
      "Iteration 161, loss = 0.02950754\n",
      "Iteration 162, loss = 0.02915755\n",
      "Iteration 163, loss = 0.02898140\n",
      "Iteration 164, loss = 0.02870308\n",
      "Iteration 165, loss = 0.02844477\n",
      "Iteration 166, loss = 0.02821850\n",
      "Iteration 167, loss = 0.02801567\n",
      "Iteration 168, loss = 0.02767675\n",
      "Iteration 169, loss = 0.02750856\n",
      "Iteration 170, loss = 0.02724707\n",
      "Iteration 171, loss = 0.02706509\n",
      "Iteration 172, loss = 0.02682387\n",
      "Iteration 173, loss = 0.02658296\n",
      "Iteration 174, loss = 0.02632063\n",
      "Iteration 175, loss = 0.02611366\n",
      "Iteration 176, loss = 0.02590870\n",
      "Iteration 177, loss = 0.02568003\n",
      "Iteration 178, loss = 0.02544248\n",
      "Iteration 179, loss = 0.02528294\n",
      "Iteration 180, loss = 0.02509620\n",
      "Iteration 181, loss = 0.02488540\n",
      "Iteration 182, loss = 0.02466253\n",
      "Iteration 183, loss = 0.02441799\n",
      "Iteration 184, loss = 0.02424033\n",
      "Iteration 185, loss = 0.02404846\n",
      "Iteration 186, loss = 0.02391280\n",
      "Iteration 187, loss = 0.02371683\n",
      "Iteration 188, loss = 0.02352081\n",
      "Iteration 189, loss = 0.02329302\n",
      "Iteration 190, loss = 0.02311403\n",
      "Iteration 191, loss = 0.02291410\n",
      "Iteration 192, loss = 0.02276124\n",
      "Iteration 193, loss = 0.02257973\n",
      "Iteration 194, loss = 0.02237242\n",
      "Iteration 195, loss = 0.02216875\n",
      "Iteration 196, loss = 0.02204834\n",
      "Iteration 197, loss = 0.02186442\n",
      "Iteration 198, loss = 0.02169346\n",
      "Iteration 199, loss = 0.02150118\n",
      "Iteration 200, loss = 0.02136704\n",
      "[CV] END activation=relu, alpha=0.0001, batch_size=50, hidden_layer_sizes=(100,), solver=sgd; total time= 5.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.01879221\n",
      "Iteration 2, loss = 0.46352547\n",
      "Iteration 3, loss = 0.38024529\n",
      "Iteration 4, loss = 0.34152610\n",
      "Iteration 5, loss = 0.31664204\n",
      "Iteration 6, loss = 0.29830634\n",
      "Iteration 7, loss = 0.28361104\n",
      "Iteration 8, loss = 0.27136751\n",
      "Iteration 9, loss = 0.26092070\n",
      "Iteration 10, loss = 0.25127391\n",
      "Iteration 11, loss = 0.24257422\n",
      "Iteration 12, loss = 0.23459725\n",
      "Iteration 13, loss = 0.22738057\n",
      "Iteration 14, loss = 0.22022156\n",
      "Iteration 15, loss = 0.21401159\n",
      "Iteration 16, loss = 0.20786282\n",
      "Iteration 17, loss = 0.20195432\n",
      "Iteration 18, loss = 0.19663253\n",
      "Iteration 19, loss = 0.19127855\n",
      "Iteration 20, loss = 0.18656981\n",
      "Iteration 21, loss = 0.18155105\n",
      "Iteration 22, loss = 0.17708608\n",
      "Iteration 23, loss = 0.17307143\n",
      "Iteration 24, loss = 0.16870236\n",
      "Iteration 25, loss = 0.16501996\n",
      "Iteration 26, loss = 0.16121898\n",
      "Iteration 27, loss = 0.15776495\n",
      "Iteration 28, loss = 0.15424575\n",
      "Iteration 29, loss = 0.15109390\n",
      "Iteration 30, loss = 0.14779712\n",
      "Iteration 31, loss = 0.14476143\n",
      "Iteration 32, loss = 0.14186948\n",
      "Iteration 33, loss = 0.13906312\n",
      "Iteration 34, loss = 0.13626372\n",
      "Iteration 35, loss = 0.13359948\n",
      "Iteration 36, loss = 0.13095720\n",
      "Iteration 37, loss = 0.12850967\n",
      "Iteration 38, loss = 0.12604419\n",
      "Iteration 39, loss = 0.12361920\n",
      "Iteration 40, loss = 0.12154744\n",
      "Iteration 41, loss = 0.11917670\n",
      "Iteration 42, loss = 0.11722893\n",
      "Iteration 43, loss = 0.11489988\n",
      "Iteration 44, loss = 0.11330416\n",
      "Iteration 45, loss = 0.11123291\n",
      "Iteration 46, loss = 0.10952294\n",
      "Iteration 47, loss = 0.10747342\n",
      "Iteration 48, loss = 0.10576913\n",
      "Iteration 49, loss = 0.10410241\n",
      "Iteration 50, loss = 0.10236018\n",
      "Iteration 51, loss = 0.10074210\n",
      "Iteration 52, loss = 0.09919469\n",
      "Iteration 53, loss = 0.09749093\n",
      "Iteration 54, loss = 0.09596696\n",
      "Iteration 55, loss = 0.09454247\n",
      "Iteration 56, loss = 0.09319049\n",
      "Iteration 57, loss = 0.09175935\n",
      "Iteration 58, loss = 0.09037539\n",
      "Iteration 59, loss = 0.08900705\n",
      "Iteration 60, loss = 0.08773820\n",
      "Iteration 61, loss = 0.08642962\n",
      "Iteration 62, loss = 0.08530152\n",
      "Iteration 63, loss = 0.08402585\n",
      "Iteration 64, loss = 0.08292508\n",
      "Iteration 65, loss = 0.08177704\n",
      "Iteration 66, loss = 0.08054221\n",
      "Iteration 67, loss = 0.07946251\n",
      "Iteration 68, loss = 0.07830191\n",
      "Iteration 69, loss = 0.07740578\n",
      "Iteration 70, loss = 0.07638417\n",
      "Iteration 71, loss = 0.07533767\n",
      "Iteration 72, loss = 0.07444765\n",
      "Iteration 73, loss = 0.07343476\n",
      "Iteration 74, loss = 0.07247951\n",
      "Iteration 75, loss = 0.07151286\n",
      "Iteration 76, loss = 0.07068841\n",
      "Iteration 77, loss = 0.06963387\n",
      "Iteration 78, loss = 0.06873040\n",
      "Iteration 79, loss = 0.06791267\n",
      "Iteration 80, loss = 0.06713425\n",
      "Iteration 81, loss = 0.06635941\n",
      "Iteration 82, loss = 0.06549476\n",
      "Iteration 83, loss = 0.06462631\n",
      "Iteration 84, loss = 0.06386953\n",
      "Iteration 85, loss = 0.06319819\n",
      "Iteration 86, loss = 0.06237620\n",
      "Iteration 87, loss = 0.06166912\n",
      "Iteration 88, loss = 0.06097441\n",
      "Iteration 89, loss = 0.06021845\n",
      "Iteration 90, loss = 0.05955508\n",
      "Iteration 91, loss = 0.05881140\n",
      "Iteration 92, loss = 0.05809536\n",
      "Iteration 93, loss = 0.05740843\n",
      "Iteration 94, loss = 0.05695572\n",
      "Iteration 95, loss = 0.05619139\n",
      "Iteration 96, loss = 0.05555472\n",
      "Iteration 97, loss = 0.05494351\n",
      "Iteration 98, loss = 0.05424369\n",
      "Iteration 99, loss = 0.05376356\n",
      "Iteration 100, loss = 0.05312807\n",
      "Iteration 101, loss = 0.05261252\n",
      "Iteration 102, loss = 0.05197066\n",
      "Iteration 103, loss = 0.05137269\n",
      "Iteration 104, loss = 0.05083868\n",
      "Iteration 105, loss = 0.05025272\n",
      "Iteration 106, loss = 0.04976925\n",
      "Iteration 107, loss = 0.04921860\n",
      "Iteration 108, loss = 0.04875817\n",
      "Iteration 109, loss = 0.04820134\n",
      "Iteration 110, loss = 0.04763613\n",
      "Iteration 111, loss = 0.04713994\n",
      "Iteration 112, loss = 0.04668462\n",
      "Iteration 113, loss = 0.04614290\n",
      "Iteration 114, loss = 0.04577615\n",
      "Iteration 115, loss = 0.04526094\n",
      "Iteration 116, loss = 0.04479858\n",
      "Iteration 117, loss = 0.04430633\n",
      "Iteration 118, loss = 0.04385750\n",
      "Iteration 119, loss = 0.04337436\n",
      "Iteration 120, loss = 0.04291972\n",
      "Iteration 121, loss = 0.04252645\n",
      "Iteration 122, loss = 0.04210910\n",
      "Iteration 123, loss = 0.04160877\n",
      "Iteration 124, loss = 0.04137992\n",
      "Iteration 125, loss = 0.04087424\n",
      "Iteration 126, loss = 0.04050694\n",
      "Iteration 127, loss = 0.03999546\n",
      "Iteration 128, loss = 0.03973754\n",
      "Iteration 129, loss = 0.03929752\n",
      "Iteration 130, loss = 0.03890784\n",
      "Iteration 131, loss = 0.03848070\n",
      "Iteration 132, loss = 0.03822330\n",
      "Iteration 133, loss = 0.03775596\n",
      "Iteration 134, loss = 0.03748409\n",
      "Iteration 135, loss = 0.03707672\n",
      "Iteration 136, loss = 0.03672745\n",
      "Iteration 137, loss = 0.03640207\n",
      "Iteration 138, loss = 0.03607562\n",
      "Iteration 139, loss = 0.03576403\n",
      "Iteration 140, loss = 0.03540212\n",
      "Iteration 141, loss = 0.03507611\n",
      "Iteration 142, loss = 0.03478498\n",
      "Iteration 143, loss = 0.03437245\n",
      "Iteration 144, loss = 0.03406390\n",
      "Iteration 145, loss = 0.03381814\n",
      "Iteration 146, loss = 0.03349167\n",
      "Iteration 147, loss = 0.03322444\n",
      "Iteration 148, loss = 0.03287504\n",
      "Iteration 149, loss = 0.03257952\n",
      "Iteration 150, loss = 0.03230992\n",
      "Iteration 151, loss = 0.03200802\n",
      "Iteration 152, loss = 0.03172304\n",
      "Iteration 153, loss = 0.03141046\n",
      "Iteration 154, loss = 0.03116927\n",
      "Iteration 155, loss = 0.03090922\n",
      "Iteration 156, loss = 0.03058766\n",
      "Iteration 157, loss = 0.03030119\n",
      "Iteration 158, loss = 0.02999965\n",
      "Iteration 159, loss = 0.02977473\n",
      "Iteration 160, loss = 0.02952900\n",
      "Iteration 161, loss = 0.02922767\n",
      "Iteration 162, loss = 0.02901268\n",
      "Iteration 163, loss = 0.02880366\n",
      "Iteration 164, loss = 0.02855025\n",
      "Iteration 165, loss = 0.02828210\n",
      "Iteration 166, loss = 0.02798832\n",
      "Iteration 167, loss = 0.02784952\n",
      "Iteration 168, loss = 0.02751103\n",
      "Iteration 169, loss = 0.02736213\n",
      "Iteration 170, loss = 0.02708512\n",
      "Iteration 171, loss = 0.02680497\n",
      "Iteration 172, loss = 0.02666734\n",
      "Iteration 173, loss = 0.02644633\n",
      "Iteration 174, loss = 0.02620997\n",
      "Iteration 175, loss = 0.02596497\n",
      "Iteration 176, loss = 0.02579616\n",
      "Iteration 177, loss = 0.02559568\n",
      "Iteration 178, loss = 0.02533927\n",
      "Iteration 179, loss = 0.02513250\n",
      "Iteration 180, loss = 0.02490963\n",
      "Iteration 181, loss = 0.02473762\n",
      "Iteration 182, loss = 0.02458449\n",
      "Iteration 183, loss = 0.02439317\n",
      "Iteration 184, loss = 0.02418184\n",
      "Iteration 185, loss = 0.02393253\n",
      "Iteration 186, loss = 0.02374771\n",
      "Iteration 187, loss = 0.02357381\n",
      "Iteration 188, loss = 0.02338409\n",
      "Iteration 189, loss = 0.02323144\n",
      "Iteration 190, loss = 0.02301099\n",
      "Iteration 191, loss = 0.02285499\n",
      "Iteration 192, loss = 0.02267963\n",
      "Iteration 193, loss = 0.02247878\n",
      "Iteration 194, loss = 0.02230198\n",
      "Iteration 195, loss = 0.02208518\n",
      "Iteration 196, loss = 0.02199796\n",
      "Iteration 197, loss = 0.02177463\n",
      "Iteration 198, loss = 0.02161425\n",
      "Iteration 199, loss = 0.02144710\n",
      "Iteration 200, loss = 0.02124394\n",
      "[CV] END activation=relu, alpha=0.0001, batch_size=50, hidden_layer_sizes=(100,), solver=sgd; total time= 5.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.01914783\n",
      "Iteration 2, loss = 0.46478022\n",
      "Iteration 3, loss = 0.38160881\n",
      "Iteration 4, loss = 0.34294989\n",
      "Iteration 5, loss = 0.31882726\n",
      "Iteration 6, loss = 0.30100215\n",
      "Iteration 7, loss = 0.28718149\n",
      "Iteration 8, loss = 0.27521223\n",
      "Iteration 9, loss = 0.26489546\n",
      "Iteration 10, loss = 0.25566354\n",
      "Iteration 11, loss = 0.24708449\n",
      "Iteration 12, loss = 0.23931154\n",
      "Iteration 13, loss = 0.23209221\n",
      "Iteration 14, loss = 0.22516013\n",
      "Iteration 15, loss = 0.21896410\n",
      "Iteration 16, loss = 0.21280376\n",
      "Iteration 17, loss = 0.20710239\n",
      "Iteration 18, loss = 0.20140805\n",
      "Iteration 19, loss = 0.19623415\n",
      "Iteration 20, loss = 0.19130491\n",
      "Iteration 21, loss = 0.18635776\n",
      "Iteration 22, loss = 0.18199281\n",
      "Iteration 23, loss = 0.17756432\n",
      "Iteration 24, loss = 0.17334862\n",
      "Iteration 25, loss = 0.16946251\n",
      "Iteration 26, loss = 0.16543644\n",
      "Iteration 27, loss = 0.16193135\n",
      "Iteration 28, loss = 0.15822725\n",
      "Iteration 29, loss = 0.15474531\n",
      "Iteration 30, loss = 0.15154201\n",
      "Iteration 31, loss = 0.14825931\n",
      "Iteration 32, loss = 0.14520695\n",
      "Iteration 33, loss = 0.14224116\n",
      "Iteration 34, loss = 0.13932322\n",
      "Iteration 35, loss = 0.13650300\n",
      "Iteration 36, loss = 0.13381600\n",
      "Iteration 37, loss = 0.13134171\n",
      "Iteration 38, loss = 0.12869002\n",
      "Iteration 39, loss = 0.12644450\n",
      "Iteration 40, loss = 0.12410621\n",
      "Iteration 41, loss = 0.12164175\n",
      "Iteration 42, loss = 0.11974553\n",
      "Iteration 43, loss = 0.11762515\n",
      "Iteration 44, loss = 0.11544672\n",
      "Iteration 45, loss = 0.11349080\n",
      "Iteration 46, loss = 0.11154619\n",
      "Iteration 47, loss = 0.10972745\n",
      "Iteration 48, loss = 0.10784668\n",
      "Iteration 49, loss = 0.10610184\n",
      "Iteration 50, loss = 0.10433317\n",
      "Iteration 51, loss = 0.10246087\n",
      "Iteration 52, loss = 0.10112872\n",
      "Iteration 53, loss = 0.09927128\n",
      "Iteration 54, loss = 0.09777639\n",
      "Iteration 55, loss = 0.09625206\n",
      "Iteration 56, loss = 0.09483279\n",
      "Iteration 57, loss = 0.09314171\n",
      "Iteration 58, loss = 0.09198608\n",
      "Iteration 59, loss = 0.09054119\n",
      "Iteration 60, loss = 0.08919888\n",
      "Iteration 61, loss = 0.08802097\n",
      "Iteration 62, loss = 0.08673378\n",
      "Iteration 63, loss = 0.08549225\n",
      "Iteration 64, loss = 0.08425639\n",
      "Iteration 65, loss = 0.08309057\n",
      "Iteration 66, loss = 0.08181190\n",
      "Iteration 67, loss = 0.08060594\n",
      "Iteration 68, loss = 0.07965296\n",
      "Iteration 69, loss = 0.07863255\n",
      "Iteration 70, loss = 0.07750597\n",
      "Iteration 71, loss = 0.07628730\n",
      "Iteration 72, loss = 0.07548711\n",
      "Iteration 73, loss = 0.07453230\n",
      "Iteration 74, loss = 0.07349649\n",
      "Iteration 75, loss = 0.07242968\n",
      "Iteration 76, loss = 0.07161192\n",
      "Iteration 77, loss = 0.07055042\n",
      "Iteration 78, loss = 0.06974502\n",
      "Iteration 79, loss = 0.06898998\n",
      "Iteration 80, loss = 0.06812967\n",
      "Iteration 81, loss = 0.06715022\n",
      "Iteration 82, loss = 0.06620437\n",
      "Iteration 83, loss = 0.06554404\n",
      "Iteration 84, loss = 0.06471067\n",
      "Iteration 85, loss = 0.06393406\n",
      "Iteration 86, loss = 0.06311969\n",
      "Iteration 87, loss = 0.06229140\n",
      "Iteration 88, loss = 0.06161963\n",
      "Iteration 89, loss = 0.06092482\n",
      "Iteration 90, loss = 0.06028362\n",
      "Iteration 91, loss = 0.05945213\n",
      "Iteration 92, loss = 0.05879897\n",
      "Iteration 93, loss = 0.05809353\n",
      "Iteration 94, loss = 0.05741737\n",
      "Iteration 95, loss = 0.05673889\n",
      "Iteration 96, loss = 0.05609553\n",
      "Iteration 97, loss = 0.05547367\n",
      "Iteration 98, loss = 0.05490409\n",
      "Iteration 99, loss = 0.05424800\n",
      "Iteration 100, loss = 0.05361932\n",
      "Iteration 101, loss = 0.05307359\n",
      "Iteration 102, loss = 0.05252917\n",
      "Iteration 103, loss = 0.05189137\n",
      "Iteration 104, loss = 0.05125915\n",
      "Iteration 105, loss = 0.05064058\n",
      "Iteration 106, loss = 0.05015739\n",
      "Iteration 107, loss = 0.04973287\n",
      "Iteration 108, loss = 0.04910766\n",
      "Iteration 109, loss = 0.04860089\n",
      "Iteration 110, loss = 0.04804458\n",
      "Iteration 111, loss = 0.04758257\n",
      "Iteration 112, loss = 0.04700413\n",
      "Iteration 113, loss = 0.04653029\n",
      "Iteration 114, loss = 0.04603983\n",
      "Iteration 115, loss = 0.04560177\n",
      "Iteration 116, loss = 0.04498707\n",
      "Iteration 117, loss = 0.04450093\n",
      "Iteration 118, loss = 0.04409561\n",
      "Iteration 119, loss = 0.04370979\n",
      "Iteration 120, loss = 0.04317882\n",
      "Iteration 121, loss = 0.04280287\n",
      "Iteration 122, loss = 0.04223129\n",
      "Iteration 123, loss = 0.04190454\n",
      "Iteration 124, loss = 0.04138580\n",
      "Iteration 125, loss = 0.04099178\n",
      "Iteration 126, loss = 0.04063396\n",
      "Iteration 127, loss = 0.04031139\n",
      "Iteration 128, loss = 0.03973176\n",
      "Iteration 129, loss = 0.03936107\n",
      "Iteration 130, loss = 0.03899029\n",
      "Iteration 131, loss = 0.03863026\n",
      "Iteration 132, loss = 0.03817601\n",
      "Iteration 133, loss = 0.03794680\n",
      "Iteration 134, loss = 0.03750231\n",
      "Iteration 135, loss = 0.03719240\n",
      "Iteration 136, loss = 0.03674841\n",
      "Iteration 137, loss = 0.03636204\n",
      "Iteration 138, loss = 0.03600651\n",
      "Iteration 139, loss = 0.03573112\n",
      "Iteration 140, loss = 0.03531032\n",
      "Iteration 141, loss = 0.03502397\n",
      "Iteration 142, loss = 0.03463086\n",
      "Iteration 143, loss = 0.03433289\n",
      "Iteration 144, loss = 0.03403497\n",
      "Iteration 145, loss = 0.03370614\n",
      "Iteration 146, loss = 0.03340899\n",
      "Iteration 147, loss = 0.03308574\n",
      "Iteration 148, loss = 0.03278657\n",
      "Iteration 149, loss = 0.03242956\n",
      "Iteration 150, loss = 0.03212167\n",
      "Iteration 151, loss = 0.03185115\n",
      "Iteration 152, loss = 0.03152981\n",
      "Iteration 153, loss = 0.03125831\n",
      "Iteration 154, loss = 0.03099246\n",
      "Iteration 155, loss = 0.03070838\n",
      "Iteration 156, loss = 0.03047520\n",
      "Iteration 157, loss = 0.03008404\n",
      "Iteration 158, loss = 0.02986322\n",
      "Iteration 159, loss = 0.02962674\n",
      "Iteration 160, loss = 0.02931074\n",
      "Iteration 161, loss = 0.02904621\n",
      "Iteration 162, loss = 0.02884217\n",
      "Iteration 163, loss = 0.02852831\n",
      "Iteration 164, loss = 0.02825943\n",
      "Iteration 165, loss = 0.02801496\n",
      "Iteration 166, loss = 0.02772681\n",
      "Iteration 167, loss = 0.02751423\n",
      "Iteration 168, loss = 0.02736862\n",
      "Iteration 169, loss = 0.02705818\n",
      "Iteration 170, loss = 0.02685541\n",
      "Iteration 171, loss = 0.02659997\n",
      "Iteration 172, loss = 0.02639740\n",
      "Iteration 173, loss = 0.02615024\n",
      "Iteration 174, loss = 0.02595043\n",
      "Iteration 175, loss = 0.02576398\n",
      "Iteration 176, loss = 0.02544711\n",
      "Iteration 177, loss = 0.02527726\n",
      "Iteration 178, loss = 0.02502825\n",
      "Iteration 179, loss = 0.02483294\n",
      "Iteration 180, loss = 0.02464172\n",
      "Iteration 181, loss = 0.02442383\n",
      "Iteration 182, loss = 0.02422693\n",
      "Iteration 183, loss = 0.02406996\n",
      "Iteration 184, loss = 0.02388063\n",
      "Iteration 185, loss = 0.02364387\n",
      "Iteration 186, loss = 0.02343302\n",
      "Iteration 187, loss = 0.02328676\n",
      "Iteration 188, loss = 0.02308452\n",
      "Iteration 189, loss = 0.02295586\n",
      "Iteration 190, loss = 0.02269747\n",
      "Iteration 191, loss = 0.02251887\n",
      "Iteration 192, loss = 0.02235358\n",
      "Iteration 193, loss = 0.02220343\n",
      "Iteration 194, loss = 0.02199867\n",
      "Iteration 195, loss = 0.02183376\n",
      "Iteration 196, loss = 0.02160349\n",
      "Iteration 197, loss = 0.02146990\n",
      "Iteration 198, loss = 0.02134057\n",
      "Iteration 199, loss = 0.02112670\n",
      "Iteration 200, loss = 0.02101115\n",
      "[CV] END activation=relu, alpha=0.0001, batch_size=50, hidden_layer_sizes=(100,), solver=sgd; total time= 5.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.00808834\n",
      "Iteration 2, loss = 0.47363595\n",
      "Iteration 3, loss = 0.38984075\n",
      "Iteration 4, loss = 0.34981513\n",
      "Iteration 5, loss = 0.32453110\n",
      "Iteration 6, loss = 0.30566829\n",
      "Iteration 7, loss = 0.29028066\n",
      "Iteration 8, loss = 0.27735504\n",
      "Iteration 9, loss = 0.26573283\n",
      "Iteration 10, loss = 0.25553004\n",
      "Iteration 11, loss = 0.24603939\n",
      "Iteration 12, loss = 0.23760593\n",
      "Iteration 13, loss = 0.22942408\n",
      "Iteration 14, loss = 0.22206706\n",
      "Iteration 15, loss = 0.21485925\n",
      "Iteration 16, loss = 0.20840290\n",
      "Iteration 17, loss = 0.20210076\n",
      "Iteration 18, loss = 0.19633029\n",
      "Iteration 19, loss = 0.19095009\n",
      "Iteration 20, loss = 0.18579467\n",
      "Iteration 21, loss = 0.18087767\n",
      "Iteration 22, loss = 0.17615651\n",
      "Iteration 23, loss = 0.17174416\n",
      "Iteration 24, loss = 0.16729361\n",
      "Iteration 25, loss = 0.16319466\n",
      "Iteration 26, loss = 0.15952623\n",
      "Iteration 27, loss = 0.15585624\n",
      "Iteration 28, loss = 0.15228840\n",
      "Iteration 29, loss = 0.14872829\n",
      "Iteration 30, loss = 0.14560369\n",
      "Iteration 31, loss = 0.14255274\n",
      "Iteration 32, loss = 0.13957326\n",
      "Iteration 33, loss = 0.13653633\n",
      "Iteration 34, loss = 0.13403453\n",
      "Iteration 35, loss = 0.13140450\n",
      "Iteration 36, loss = 0.12889980\n",
      "Iteration 37, loss = 0.12637873\n",
      "Iteration 38, loss = 0.12401754\n",
      "Iteration 39, loss = 0.12168445\n",
      "Iteration 40, loss = 0.11978440\n",
      "Iteration 41, loss = 0.11757335\n",
      "Iteration 42, loss = 0.11555205\n",
      "Iteration 43, loss = 0.11343163\n",
      "Iteration 44, loss = 0.11156661\n",
      "Iteration 45, loss = 0.10982237\n",
      "Iteration 46, loss = 0.10793064\n",
      "Iteration 47, loss = 0.10616027\n",
      "Iteration 48, loss = 0.10450227\n",
      "Iteration 49, loss = 0.10282004\n",
      "Iteration 50, loss = 0.10126380\n",
      "Iteration 51, loss = 0.09977626\n",
      "Iteration 52, loss = 0.09819402\n",
      "Iteration 53, loss = 0.09664818\n",
      "Iteration 54, loss = 0.09529936\n",
      "Iteration 55, loss = 0.09374807\n",
      "Iteration 56, loss = 0.09258910\n",
      "Iteration 57, loss = 0.09132502\n",
      "Iteration 58, loss = 0.08986792\n",
      "Iteration 59, loss = 0.08868643\n",
      "Iteration 60, loss = 0.08744977\n",
      "Iteration 61, loss = 0.08621717\n",
      "Iteration 62, loss = 0.08490337\n",
      "Iteration 63, loss = 0.08382943\n",
      "Iteration 64, loss = 0.08268299\n",
      "Iteration 65, loss = 0.08149353\n",
      "Iteration 66, loss = 0.08047808\n",
      "Iteration 67, loss = 0.07930736\n",
      "Iteration 68, loss = 0.07834961\n",
      "Iteration 69, loss = 0.07739199\n",
      "Iteration 70, loss = 0.07629818\n",
      "Iteration 71, loss = 0.07538529\n",
      "Iteration 72, loss = 0.07436920\n",
      "Iteration 73, loss = 0.07347118\n",
      "Iteration 74, loss = 0.07250424\n",
      "Iteration 75, loss = 0.07158414\n",
      "Iteration 76, loss = 0.07072352\n",
      "Iteration 77, loss = 0.06999749\n",
      "Iteration 78, loss = 0.06901013\n",
      "Iteration 79, loss = 0.06815235\n",
      "Iteration 80, loss = 0.06734582\n",
      "Iteration 81, loss = 0.06662045\n",
      "Iteration 82, loss = 0.06570375\n",
      "Iteration 83, loss = 0.06490005\n",
      "Iteration 84, loss = 0.06409869\n",
      "Iteration 85, loss = 0.06337760\n",
      "Iteration 86, loss = 0.06266228\n",
      "Iteration 87, loss = 0.06196301\n",
      "Iteration 88, loss = 0.06121345\n",
      "Iteration 89, loss = 0.06038652\n",
      "Iteration 90, loss = 0.05979768\n",
      "Iteration 91, loss = 0.05903155\n",
      "Iteration 92, loss = 0.05841275\n",
      "Iteration 93, loss = 0.05774373\n",
      "Iteration 94, loss = 0.05711011\n",
      "Iteration 95, loss = 0.05643116\n",
      "Iteration 96, loss = 0.05585261\n",
      "Iteration 97, loss = 0.05528292\n",
      "Iteration 98, loss = 0.05450144\n",
      "Iteration 99, loss = 0.05409080\n",
      "Iteration 100, loss = 0.05345051\n",
      "Iteration 101, loss = 0.05292210\n",
      "Iteration 102, loss = 0.05229611\n",
      "Iteration 103, loss = 0.05174195\n",
      "Iteration 104, loss = 0.05125019\n",
      "Iteration 105, loss = 0.05068402\n",
      "Iteration 106, loss = 0.05009295\n",
      "Iteration 107, loss = 0.04947823\n",
      "Iteration 108, loss = 0.04902361\n",
      "Iteration 109, loss = 0.04857604\n",
      "Iteration 110, loss = 0.04800742\n",
      "Iteration 111, loss = 0.04750169\n",
      "Iteration 112, loss = 0.04694639\n",
      "Iteration 113, loss = 0.04653331\n",
      "Iteration 114, loss = 0.04600204\n",
      "Iteration 115, loss = 0.04554007\n",
      "Iteration 116, loss = 0.04509951\n",
      "Iteration 117, loss = 0.04456922\n",
      "Iteration 118, loss = 0.04414838\n",
      "Iteration 119, loss = 0.04366577\n",
      "Iteration 120, loss = 0.04328599\n",
      "Iteration 121, loss = 0.04291202\n",
      "Iteration 122, loss = 0.04243081\n",
      "Iteration 123, loss = 0.04207557\n",
      "Iteration 124, loss = 0.04163429\n",
      "Iteration 125, loss = 0.04118445\n",
      "Iteration 126, loss = 0.04075852\n",
      "Iteration 127, loss = 0.04041130\n",
      "Iteration 128, loss = 0.03998525\n",
      "Iteration 129, loss = 0.03961395\n",
      "Iteration 130, loss = 0.03922761\n",
      "Iteration 131, loss = 0.03887492\n",
      "Iteration 132, loss = 0.03832196\n",
      "Iteration 133, loss = 0.03814826\n",
      "Iteration 134, loss = 0.03769050\n",
      "Iteration 135, loss = 0.03733743\n",
      "Iteration 136, loss = 0.03706078\n",
      "Iteration 137, loss = 0.03665819\n",
      "Iteration 138, loss = 0.03635507\n",
      "Iteration 139, loss = 0.03598746\n",
      "Iteration 140, loss = 0.03571077\n",
      "Iteration 141, loss = 0.03538196\n",
      "Iteration 142, loss = 0.03506052\n",
      "Iteration 143, loss = 0.03475412\n",
      "Iteration 144, loss = 0.03438490\n",
      "Iteration 145, loss = 0.03409042\n",
      "Iteration 146, loss = 0.03374305\n",
      "Iteration 147, loss = 0.03350395\n",
      "Iteration 148, loss = 0.03322122\n",
      "Iteration 149, loss = 0.03281030\n",
      "Iteration 150, loss = 0.03254777\n",
      "Iteration 151, loss = 0.03226447\n",
      "Iteration 152, loss = 0.03199443\n",
      "Iteration 153, loss = 0.03169404\n",
      "Iteration 154, loss = 0.03136783\n",
      "Iteration 155, loss = 0.03116747\n",
      "Iteration 156, loss = 0.03090689\n",
      "Iteration 157, loss = 0.03055375\n",
      "Iteration 158, loss = 0.03031978\n",
      "Iteration 159, loss = 0.03006644\n",
      "Iteration 160, loss = 0.02976395\n",
      "Iteration 161, loss = 0.02959683\n",
      "Iteration 162, loss = 0.02925304\n",
      "Iteration 163, loss = 0.02904110\n",
      "Iteration 164, loss = 0.02881464\n",
      "Iteration 165, loss = 0.02850950\n",
      "Iteration 166, loss = 0.02833304\n",
      "Iteration 167, loss = 0.02805164\n",
      "Iteration 168, loss = 0.02778218\n",
      "Iteration 169, loss = 0.02759100\n",
      "Iteration 170, loss = 0.02738759\n",
      "Iteration 171, loss = 0.02717313\n",
      "Iteration 172, loss = 0.02688185\n",
      "Iteration 173, loss = 0.02669450\n",
      "Iteration 174, loss = 0.02639882\n",
      "Iteration 175, loss = 0.02625709\n",
      "Iteration 176, loss = 0.02607074\n",
      "Iteration 177, loss = 0.02579210\n",
      "Iteration 178, loss = 0.02563392\n",
      "Iteration 179, loss = 0.02535536\n",
      "Iteration 180, loss = 0.02519221\n",
      "Iteration 181, loss = 0.02500582\n",
      "Iteration 182, loss = 0.02479275\n",
      "Iteration 183, loss = 0.02459173\n",
      "Iteration 184, loss = 0.02441840\n",
      "Iteration 185, loss = 0.02423218\n",
      "Iteration 186, loss = 0.02403524\n",
      "Iteration 187, loss = 0.02381110\n",
      "Iteration 188, loss = 0.02362929\n",
      "Iteration 189, loss = 0.02347035\n",
      "Iteration 190, loss = 0.02325052\n",
      "Iteration 191, loss = 0.02307660\n",
      "Iteration 192, loss = 0.02286687\n",
      "Iteration 193, loss = 0.02272920\n",
      "Iteration 194, loss = 0.02253381\n",
      "Iteration 195, loss = 0.02230738\n",
      "Iteration 196, loss = 0.02217719\n",
      "Iteration 197, loss = 0.02202856\n",
      "Iteration 198, loss = 0.02184194\n",
      "Iteration 199, loss = 0.02165756\n",
      "Iteration 200, loss = 0.02147057\n",
      "[CV] END activation=relu, alpha=0.0001, batch_size=50, hidden_layer_sizes=(100,), solver=sgd; total time= 5.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.08255641\n",
      "Iteration 2, loss = 0.48498479\n",
      "Iteration 3, loss = 0.39576217\n",
      "Iteration 4, loss = 0.35495437\n",
      "Iteration 5, loss = 0.32910796\n",
      "Iteration 6, loss = 0.31032244\n",
      "Iteration 7, loss = 0.29534115\n",
      "Iteration 8, loss = 0.28254044\n",
      "Iteration 9, loss = 0.27142761\n",
      "Iteration 10, loss = 0.26127236\n",
      "Iteration 11, loss = 0.25227248\n",
      "Iteration 12, loss = 0.24412073\n",
      "Iteration 13, loss = 0.23617877\n",
      "Iteration 14, loss = 0.22929353\n",
      "Iteration 15, loss = 0.22249200\n",
      "Iteration 16, loss = 0.21610071\n",
      "Iteration 17, loss = 0.21014315\n",
      "Iteration 18, loss = 0.20452533\n",
      "Iteration 19, loss = 0.19920538\n",
      "Iteration 20, loss = 0.19423980\n",
      "Iteration 21, loss = 0.18942405\n",
      "Iteration 22, loss = 0.18465749\n",
      "Iteration 23, loss = 0.18046142\n",
      "Iteration 24, loss = 0.17637708\n",
      "Iteration 25, loss = 0.17228640\n",
      "Iteration 26, loss = 0.16850275\n",
      "Iteration 27, loss = 0.16469452\n",
      "Iteration 28, loss = 0.16108086\n",
      "Iteration 29, loss = 0.15759546\n",
      "Iteration 30, loss = 0.15412926\n",
      "Iteration 31, loss = 0.15110277\n",
      "Iteration 32, loss = 0.14758488\n",
      "Iteration 33, loss = 0.14488918\n",
      "Iteration 34, loss = 0.14196721\n",
      "Iteration 35, loss = 0.13924073\n",
      "Iteration 36, loss = 0.13631643\n",
      "Iteration 37, loss = 0.13370328\n",
      "Iteration 38, loss = 0.13129778\n",
      "Iteration 39, loss = 0.12888368\n",
      "Iteration 40, loss = 0.12636000\n",
      "Iteration 41, loss = 0.12413113\n",
      "Iteration 42, loss = 0.12175308\n",
      "Iteration 43, loss = 0.11964448\n",
      "Iteration 44, loss = 0.11759549\n",
      "Iteration 45, loss = 0.11566073\n",
      "Iteration 46, loss = 0.11364135\n",
      "Iteration 47, loss = 0.11170038\n",
      "Iteration 48, loss = 0.10981490\n",
      "Iteration 49, loss = 0.10800719\n",
      "Iteration 50, loss = 0.10634660\n",
      "Iteration 51, loss = 0.10455512\n",
      "Iteration 52, loss = 0.10296505\n",
      "Iteration 53, loss = 0.10135385\n",
      "Iteration 54, loss = 0.09967386\n",
      "Iteration 55, loss = 0.09822526\n",
      "Iteration 56, loss = 0.09680861\n",
      "Iteration 57, loss = 0.09536845\n",
      "Iteration 58, loss = 0.09394147\n",
      "Iteration 59, loss = 0.09249930\n",
      "Iteration 60, loss = 0.09128433\n",
      "Iteration 61, loss = 0.08984914\n",
      "Iteration 62, loss = 0.08852005\n",
      "Iteration 63, loss = 0.08742018\n",
      "Iteration 64, loss = 0.08623799\n",
      "Iteration 65, loss = 0.08498671\n",
      "Iteration 66, loss = 0.08376735\n",
      "Iteration 67, loss = 0.08259505\n",
      "Iteration 68, loss = 0.08163827\n",
      "Iteration 69, loss = 0.08044128\n",
      "Iteration 70, loss = 0.07924423\n",
      "Iteration 71, loss = 0.07839659\n",
      "Iteration 72, loss = 0.07742546\n",
      "Iteration 73, loss = 0.07644572\n",
      "Iteration 74, loss = 0.07546386\n",
      "Iteration 75, loss = 0.07456664\n",
      "Iteration 76, loss = 0.07343094\n",
      "Iteration 77, loss = 0.07259372\n",
      "Iteration 78, loss = 0.07170414\n",
      "Iteration 79, loss = 0.07081234\n",
      "Iteration 80, loss = 0.07000518\n",
      "Iteration 81, loss = 0.06890532\n",
      "Iteration 82, loss = 0.06830442\n",
      "Iteration 83, loss = 0.06741096\n",
      "Iteration 84, loss = 0.06656920\n",
      "Iteration 85, loss = 0.06580649\n",
      "Iteration 86, loss = 0.06510158\n",
      "Iteration 87, loss = 0.06423034\n",
      "Iteration 88, loss = 0.06343512\n",
      "Iteration 89, loss = 0.06275614\n",
      "Iteration 90, loss = 0.06203527\n",
      "Iteration 91, loss = 0.06130066\n",
      "Iteration 92, loss = 0.06059258\n",
      "Iteration 93, loss = 0.05990419\n",
      "Iteration 94, loss = 0.05926699\n",
      "Iteration 95, loss = 0.05857472\n",
      "Iteration 96, loss = 0.05788053\n",
      "Iteration 97, loss = 0.05733658\n",
      "Iteration 98, loss = 0.05671186\n",
      "Iteration 99, loss = 0.05601447\n",
      "Iteration 100, loss = 0.05551437\n",
      "Iteration 101, loss = 0.05492703\n",
      "Iteration 102, loss = 0.05422493\n",
      "Iteration 103, loss = 0.05370841\n",
      "Iteration 104, loss = 0.05321606\n",
      "Iteration 105, loss = 0.05262259\n",
      "Iteration 106, loss = 0.05195072\n",
      "Iteration 107, loss = 0.05148417\n",
      "Iteration 108, loss = 0.05094966\n",
      "Iteration 109, loss = 0.05044516\n",
      "Iteration 110, loss = 0.04989440\n",
      "Iteration 111, loss = 0.04922689\n",
      "Iteration 112, loss = 0.04889430\n",
      "Iteration 113, loss = 0.04832230\n",
      "Iteration 114, loss = 0.04792062\n",
      "Iteration 115, loss = 0.04740411\n",
      "Iteration 116, loss = 0.04692740\n",
      "Iteration 117, loss = 0.04639334\n",
      "Iteration 118, loss = 0.04590379\n",
      "Iteration 119, loss = 0.04557913\n",
      "Iteration 120, loss = 0.04509354\n",
      "Iteration 121, loss = 0.04458869\n",
      "Iteration 122, loss = 0.04418341\n",
      "Iteration 123, loss = 0.04365771\n",
      "Iteration 124, loss = 0.04330086\n",
      "Iteration 125, loss = 0.04289289\n",
      "Iteration 126, loss = 0.04241360\n",
      "Iteration 127, loss = 0.04207000\n",
      "Iteration 128, loss = 0.04164451\n",
      "Iteration 129, loss = 0.04124142\n",
      "Iteration 130, loss = 0.04081102\n",
      "Iteration 131, loss = 0.04033916\n",
      "Iteration 132, loss = 0.04003456\n",
      "Iteration 133, loss = 0.03964148\n",
      "Iteration 134, loss = 0.03929636\n",
      "Iteration 135, loss = 0.03890458\n",
      "Iteration 136, loss = 0.03848868\n",
      "Iteration 137, loss = 0.03814223\n",
      "Iteration 138, loss = 0.03786837\n",
      "Iteration 139, loss = 0.03750803\n",
      "Iteration 140, loss = 0.03712032\n",
      "Iteration 141, loss = 0.03682889\n",
      "Iteration 142, loss = 0.03650751\n",
      "Iteration 143, loss = 0.03613091\n",
      "Iteration 144, loss = 0.03578342\n",
      "Iteration 145, loss = 0.03539983\n",
      "Iteration 146, loss = 0.03514798\n",
      "Iteration 147, loss = 0.03480413\n",
      "Iteration 148, loss = 0.03447475\n",
      "Iteration 149, loss = 0.03417227\n",
      "Iteration 150, loss = 0.03384862\n",
      "Iteration 151, loss = 0.03358977\n",
      "Iteration 152, loss = 0.03327033\n",
      "Iteration 153, loss = 0.03297133\n",
      "Iteration 154, loss = 0.03264048\n",
      "Iteration 155, loss = 0.03236967\n",
      "Iteration 156, loss = 0.03211717\n",
      "Iteration 157, loss = 0.03186364\n",
      "Iteration 158, loss = 0.03152420\n",
      "Iteration 159, loss = 0.03122377\n",
      "Iteration 160, loss = 0.03097366\n",
      "Iteration 161, loss = 0.03068594\n",
      "Iteration 162, loss = 0.03042362\n",
      "Iteration 163, loss = 0.03022443\n",
      "Iteration 164, loss = 0.02995477\n",
      "Iteration 165, loss = 0.02969155\n",
      "Iteration 166, loss = 0.02940832\n",
      "Iteration 167, loss = 0.02920922\n",
      "Iteration 168, loss = 0.02890930\n",
      "Iteration 169, loss = 0.02871328\n",
      "Iteration 170, loss = 0.02836852\n",
      "Iteration 171, loss = 0.02817665\n",
      "Iteration 172, loss = 0.02800650\n",
      "Iteration 173, loss = 0.02775704\n",
      "Iteration 174, loss = 0.02753097\n",
      "Iteration 175, loss = 0.02734011\n",
      "Iteration 176, loss = 0.02707389\n",
      "Iteration 177, loss = 0.02687076\n",
      "Iteration 178, loss = 0.02668247\n",
      "Iteration 179, loss = 0.02643761\n",
      "Iteration 180, loss = 0.02619105\n",
      "Iteration 181, loss = 0.02594236\n",
      "Iteration 182, loss = 0.02581190\n",
      "Iteration 183, loss = 0.02554787\n",
      "Iteration 184, loss = 0.02538293\n",
      "Iteration 185, loss = 0.02513834\n",
      "Iteration 186, loss = 0.02489916\n",
      "Iteration 187, loss = 0.02473068\n",
      "Iteration 188, loss = 0.02453889\n",
      "Iteration 189, loss = 0.02436665\n",
      "Iteration 190, loss = 0.02419002\n",
      "Iteration 191, loss = 0.02394214\n",
      "Iteration 192, loss = 0.02376341\n",
      "Iteration 193, loss = 0.02360355\n",
      "Iteration 194, loss = 0.02343357\n",
      "Iteration 195, loss = 0.02323019\n",
      "Iteration 196, loss = 0.02301477\n",
      "Iteration 197, loss = 0.02288604\n",
      "Iteration 198, loss = 0.02265574\n",
      "Iteration 199, loss = 0.02251306\n",
      "Iteration 200, loss = 0.02231322\n",
      "[CV] END activation=relu, alpha=0.0001, batch_size=50, hidden_layer_sizes=(100,), solver=sgd; total time= 4.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.26978195\n",
      "Iteration 2, loss = 0.10497960\n",
      "Iteration 3, loss = 0.06729771\n",
      "Iteration 4, loss = 0.04929030\n",
      "Iteration 5, loss = 0.03370663\n",
      "Iteration 6, loss = 0.02517883\n",
      "Iteration 7, loss = 0.02175793\n",
      "Iteration 8, loss = 0.02249092\n",
      "Iteration 9, loss = 0.01931223\n",
      "Iteration 10, loss = 0.01641332\n",
      "Iteration 11, loss = 0.01693130\n",
      "Iteration 12, loss = 0.01336040\n",
      "Iteration 13, loss = 0.01274018\n",
      "Iteration 14, loss = 0.01710945\n",
      "Iteration 15, loss = 0.01089729\n",
      "Iteration 16, loss = 0.01218402\n",
      "Iteration 17, loss = 0.01583548\n",
      "Iteration 18, loss = 0.01019031\n",
      "Iteration 19, loss = 0.00606815\n",
      "Iteration 20, loss = 0.00999694\n",
      "Iteration 21, loss = 0.01940810\n",
      "Iteration 22, loss = 0.01006982\n",
      "Iteration 23, loss = 0.00983979\n",
      "Iteration 24, loss = 0.01031591\n",
      "Iteration 25, loss = 0.00975740\n",
      "Iteration 26, loss = 0.01277033\n",
      "Iteration 27, loss = 0.01102770\n",
      "Iteration 28, loss = 0.00562669\n",
      "Iteration 29, loss = 0.00458546\n",
      "Iteration 30, loss = 0.01571113\n",
      "Iteration 31, loss = 0.01005285\n",
      "Iteration 32, loss = 0.00887960\n",
      "Iteration 33, loss = 0.00785640\n",
      "Iteration 34, loss = 0.00643351\n",
      "Iteration 35, loss = 0.01630157\n",
      "Iteration 36, loss = 0.00897422\n",
      "Iteration 37, loss = 0.00525969\n",
      "Iteration 38, loss = 0.00396790\n",
      "Iteration 39, loss = 0.00372106\n",
      "Iteration 40, loss = 0.00357057\n",
      "Iteration 41, loss = 0.00343966\n",
      "Iteration 42, loss = 0.00328695\n",
      "Iteration 43, loss = 0.00310829\n",
      "Iteration 44, loss = 0.00303061\n",
      "Iteration 45, loss = 0.03440479\n",
      "Iteration 46, loss = 0.00944373\n",
      "Iteration 47, loss = 0.00696914\n",
      "Iteration 48, loss = 0.00956236\n",
      "Iteration 49, loss = 0.00814531\n",
      "Iteration 50, loss = 0.00741581\n",
      "Iteration 51, loss = 0.00728532\n",
      "Iteration 52, loss = 0.01490188\n",
      "Iteration 53, loss = 0.00464456\n",
      "Iteration 54, loss = 0.00488283\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.0001, batch_size=50, hidden_layer_sizes=(400, 100), solver=adam; total time= 9.5min\n",
      "Iteration 1, loss = 0.26636525\n",
      "Iteration 2, loss = 0.10263342\n",
      "Iteration 3, loss = 0.06853401\n",
      "Iteration 4, loss = 0.04558451\n",
      "Iteration 5, loss = 0.03737927\n",
      "Iteration 6, loss = 0.02662930\n",
      "Iteration 7, loss = 0.02325462\n",
      "Iteration 8, loss = 0.02204483\n",
      "Iteration 9, loss = 0.01920756\n",
      "Iteration 10, loss = 0.01718378\n",
      "Iteration 11, loss = 0.01420301\n",
      "Iteration 12, loss = 0.01785821\n",
      "Iteration 13, loss = 0.01486214\n",
      "Iteration 14, loss = 0.00876554\n",
      "Iteration 15, loss = 0.01847373\n",
      "Iteration 16, loss = 0.01267134\n",
      "Iteration 17, loss = 0.01372155\n",
      "Iteration 18, loss = 0.01394594\n",
      "Iteration 19, loss = 0.01058907\n",
      "Iteration 20, loss = 0.00961931\n",
      "Iteration 21, loss = 0.01650923\n",
      "Iteration 22, loss = 0.01120136\n",
      "Iteration 23, loss = 0.00735737\n",
      "Iteration 24, loss = 0.01276432\n",
      "Iteration 25, loss = 0.01194626\n",
      "Iteration 26, loss = 0.00723368\n",
      "Iteration 27, loss = 0.01155918\n",
      "Iteration 28, loss = 0.01107362\n",
      "Iteration 29, loss = 0.00797425\n",
      "Iteration 30, loss = 0.01342776\n",
      "Iteration 31, loss = 0.01068834\n",
      "Iteration 32, loss = 0.00660029\n",
      "Iteration 33, loss = 0.00570528\n",
      "Iteration 34, loss = 0.01685173\n",
      "Iteration 35, loss = 0.00803684\n",
      "Iteration 36, loss = 0.00760120\n",
      "Iteration 37, loss = 0.01064515\n",
      "Iteration 38, loss = 0.00760247\n",
      "Iteration 39, loss = 0.01136909\n",
      "Iteration 40, loss = 0.00864365\n",
      "Iteration 41, loss = 0.01202830\n",
      "Iteration 42, loss = 0.00823264\n",
      "Iteration 43, loss = 0.00670800\n",
      "Iteration 44, loss = 0.01070744\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.0001, batch_size=50, hidden_layer_sizes=(400, 100), solver=adam; total time= 6.7min\n",
      "Iteration 1, loss = 0.26764523\n",
      "Iteration 2, loss = 0.10669143\n",
      "Iteration 3, loss = 0.06788031\n",
      "Iteration 4, loss = 0.04794510\n",
      "Iteration 5, loss = 0.03581108\n",
      "Iteration 6, loss = 0.02782869\n",
      "Iteration 7, loss = 0.02460732\n",
      "Iteration 8, loss = 0.02420460\n",
      "Iteration 9, loss = 0.01468534\n",
      "Iteration 10, loss = 0.01854226\n",
      "Iteration 11, loss = 0.01828612\n",
      "Iteration 12, loss = 0.01138952\n",
      "Iteration 13, loss = 0.01683020\n",
      "Iteration 14, loss = 0.01134750\n",
      "Iteration 15, loss = 0.01469915\n",
      "Iteration 16, loss = 0.00886764\n",
      "Iteration 17, loss = 0.01375903\n",
      "Iteration 18, loss = 0.01093651\n",
      "Iteration 19, loss = 0.01417666\n",
      "Iteration 20, loss = 0.00862797\n",
      "Iteration 21, loss = 0.01313461\n",
      "Iteration 22, loss = 0.01337931\n",
      "Iteration 23, loss = 0.00959867\n",
      "Iteration 24, loss = 0.01030746\n",
      "Iteration 25, loss = 0.01013937\n",
      "Iteration 26, loss = 0.01390127\n",
      "Iteration 27, loss = 0.00903827\n",
      "Iteration 28, loss = 0.00968143\n",
      "Iteration 29, loss = 0.00948806\n",
      "Iteration 30, loss = 0.00982339\n",
      "Iteration 31, loss = 0.01432834\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.0001, batch_size=50, hidden_layer_sizes=(400, 100), solver=adam; total time=57.1min\n",
      "Iteration 1, loss = 0.26616637\n",
      "Iteration 2, loss = 0.10252201\n",
      "Iteration 3, loss = 0.06668975\n",
      "Iteration 4, loss = 0.04740830\n",
      "Iteration 5, loss = 0.03422496\n",
      "Iteration 6, loss = 0.03060020\n",
      "Iteration 7, loss = 0.02320999\n",
      "Iteration 8, loss = 0.02250413\n",
      "Iteration 9, loss = 0.01751594\n",
      "Iteration 10, loss = 0.01700811\n",
      "Iteration 11, loss = 0.01609877\n",
      "Iteration 12, loss = 0.01564803\n",
      "Iteration 13, loss = 0.01670008\n",
      "Iteration 14, loss = 0.01252713\n",
      "Iteration 15, loss = 0.01127187\n",
      "Iteration 16, loss = 0.01214962\n",
      "Iteration 17, loss = 0.01537508\n",
      "Iteration 18, loss = 0.00798047\n",
      "Iteration 19, loss = 0.01578574\n",
      "Iteration 20, loss = 0.01232160\n",
      "Iteration 21, loss = 0.00915991\n",
      "Iteration 22, loss = 0.00658524\n",
      "Iteration 23, loss = 0.00833946\n",
      "Iteration 24, loss = 0.01905457\n",
      "Iteration 25, loss = 0.00618585\n",
      "Iteration 26, loss = 0.00658482\n",
      "Iteration 27, loss = 0.01441116\n",
      "Iteration 28, loss = 0.01161078\n",
      "Iteration 29, loss = 0.00738768\n",
      "Iteration 30, loss = 0.00925262\n",
      "Iteration 31, loss = 0.01179253\n",
      "Iteration 32, loss = 0.01106404\n",
      "Iteration 33, loss = 0.01016691\n",
      "Iteration 34, loss = 0.00807867\n",
      "Iteration 35, loss = 0.00915160\n",
      "Iteration 36, loss = 0.00994475\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.0001, batch_size=50, hidden_layer_sizes=(400, 100), solver=adam; total time= 5.4min\n",
      "Iteration 1, loss = 0.26409366\n",
      "Iteration 2, loss = 0.10461519\n",
      "Iteration 3, loss = 0.06844919\n",
      "Iteration 4, loss = 0.04568378\n",
      "Iteration 5, loss = 0.03579191\n",
      "Iteration 6, loss = 0.02722622\n",
      "Iteration 7, loss = 0.02428447\n",
      "Iteration 8, loss = 0.01840428\n",
      "Iteration 9, loss = 0.02019110\n",
      "Iteration 10, loss = 0.01434527\n",
      "Iteration 11, loss = 0.01890743\n",
      "Iteration 12, loss = 0.01397983\n",
      "Iteration 13, loss = 0.01516068\n",
      "Iteration 14, loss = 0.00945997\n",
      "Iteration 15, loss = 0.01280287\n",
      "Iteration 16, loss = 0.01698894\n",
      "Iteration 17, loss = 0.01245683\n",
      "Iteration 18, loss = 0.00677051\n",
      "Iteration 19, loss = 0.01324833\n",
      "Iteration 20, loss = 0.01765847\n",
      "Iteration 21, loss = 0.00706254\n",
      "Iteration 22, loss = 0.01119544\n",
      "Iteration 23, loss = 0.01036819\n",
      "Iteration 24, loss = 0.01679053\n",
      "Iteration 25, loss = 0.00962605\n",
      "Iteration 26, loss = 0.00910783\n",
      "Iteration 27, loss = 0.01349036\n",
      "Iteration 28, loss = 0.00836314\n",
      "Iteration 29, loss = 0.00639613\n",
      "Iteration 30, loss = 0.01319944\n",
      "Iteration 31, loss = 0.00835775\n",
      "Iteration 32, loss = 0.00975442\n",
      "Iteration 33, loss = 0.00956126\n",
      "Iteration 34, loss = 0.01419970\n",
      "Iteration 35, loss = 0.01041359\n",
      "Iteration 36, loss = 0.00645821\n",
      "Iteration 37, loss = 0.00791514\n",
      "Iteration 38, loss = 0.00884056\n",
      "Iteration 39, loss = 0.01108303\n",
      "Iteration 40, loss = 0.00949853\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.0001, batch_size=50, hidden_layer_sizes=(400, 100), solver=adam; total time= 5.6min\n",
      "Iteration 1, loss = 0.99211192\n",
      "Iteration 2, loss = 0.38622592\n",
      "Iteration 3, loss = 0.31612471\n",
      "Iteration 4, loss = 0.27963895\n",
      "Iteration 5, loss = 0.25449795\n",
      "Iteration 6, loss = 0.23378768\n",
      "Iteration 7, loss = 0.21642721\n",
      "Iteration 8, loss = 0.20171353\n",
      "Iteration 9, loss = 0.18808970\n",
      "Iteration 10, loss = 0.17652339\n",
      "Iteration 11, loss = 0.16551050\n",
      "Iteration 12, loss = 0.15604614\n",
      "Iteration 13, loss = 0.14757180\n",
      "Iteration 14, loss = 0.13940538\n",
      "Iteration 15, loss = 0.13221858\n",
      "Iteration 16, loss = 0.12546377\n",
      "Iteration 17, loss = 0.11935436\n",
      "Iteration 18, loss = 0.11356792\n",
      "Iteration 19, loss = 0.10807829\n",
      "Iteration 20, loss = 0.10328997\n",
      "Iteration 21, loss = 0.09860012\n",
      "Iteration 22, loss = 0.09482088\n",
      "Iteration 23, loss = 0.09028023\n",
      "Iteration 24, loss = 0.08658862\n",
      "Iteration 25, loss = 0.08266444\n",
      "Iteration 26, loss = 0.07942698\n",
      "Iteration 27, loss = 0.07625275\n",
      "Iteration 28, loss = 0.07290266\n",
      "Iteration 29, loss = 0.07030084\n",
      "Iteration 30, loss = 0.06743512\n",
      "Iteration 31, loss = 0.06437586\n",
      "Iteration 32, loss = 0.06254691\n",
      "Iteration 33, loss = 0.05986777\n",
      "Iteration 34, loss = 0.05758834\n",
      "Iteration 35, loss = 0.05534904\n",
      "Iteration 36, loss = 0.05363221\n",
      "Iteration 37, loss = 0.05159396\n",
      "Iteration 38, loss = 0.04949196\n",
      "Iteration 39, loss = 0.04763688\n",
      "Iteration 40, loss = 0.04592899\n",
      "Iteration 41, loss = 0.04434019\n",
      "Iteration 42, loss = 0.04277088\n",
      "Iteration 43, loss = 0.04106868\n",
      "Iteration 44, loss = 0.03986275\n",
      "Iteration 45, loss = 0.03838203\n",
      "Iteration 46, loss = 0.03712104\n",
      "Iteration 47, loss = 0.03599355\n",
      "Iteration 48, loss = 0.03457543\n",
      "Iteration 49, loss = 0.03329769\n",
      "Iteration 50, loss = 0.03215089\n",
      "Iteration 51, loss = 0.03121309\n",
      "Iteration 52, loss = 0.03000881\n",
      "Iteration 53, loss = 0.02916563\n",
      "Iteration 54, loss = 0.02814761\n",
      "Iteration 55, loss = 0.02733280\n",
      "Iteration 56, loss = 0.02631577\n",
      "Iteration 57, loss = 0.02547268\n",
      "Iteration 58, loss = 0.02469905\n",
      "Iteration 59, loss = 0.02377223\n",
      "Iteration 60, loss = 0.02305495\n",
      "Iteration 61, loss = 0.02244820\n",
      "Iteration 62, loss = 0.02184079\n",
      "Iteration 63, loss = 0.02094862\n",
      "Iteration 64, loss = 0.02045086\n",
      "Iteration 65, loss = 0.01982005\n",
      "Iteration 66, loss = 0.01921563\n",
      "Iteration 67, loss = 0.01869179\n",
      "Iteration 68, loss = 0.01812196\n",
      "Iteration 69, loss = 0.01759535\n",
      "Iteration 70, loss = 0.01706836\n",
      "Iteration 71, loss = 0.01658582\n",
      "Iteration 72, loss = 0.01608343\n",
      "Iteration 73, loss = 0.01569298\n",
      "Iteration 74, loss = 0.01513216\n",
      "Iteration 75, loss = 0.01484624\n",
      "Iteration 76, loss = 0.01437715\n",
      "Iteration 77, loss = 0.01399920\n",
      "Iteration 78, loss = 0.01367069\n",
      "Iteration 79, loss = 0.01331898\n",
      "Iteration 80, loss = 0.01293581\n",
      "Iteration 81, loss = 0.01261260\n",
      "Iteration 82, loss = 0.01228471\n",
      "Iteration 83, loss = 0.01197855\n",
      "Iteration 84, loss = 0.01175584\n",
      "Iteration 85, loss = 0.01142465\n",
      "Iteration 86, loss = 0.01112848\n",
      "Iteration 87, loss = 0.01092805\n",
      "Iteration 88, loss = 0.01056836\n",
      "Iteration 89, loss = 0.01040408\n",
      "Iteration 90, loss = 0.01010765\n",
      "Iteration 91, loss = 0.00996011\n",
      "Iteration 92, loss = 0.00972869\n",
      "Iteration 93, loss = 0.00946184\n",
      "Iteration 94, loss = 0.00928612\n",
      "Iteration 95, loss = 0.00908479\n",
      "Iteration 96, loss = 0.00887767\n",
      "Iteration 97, loss = 0.00870807\n",
      "Iteration 98, loss = 0.00852336\n",
      "Iteration 99, loss = 0.00833530\n",
      "Iteration 100, loss = 0.00815404\n",
      "Iteration 101, loss = 0.00803626\n",
      "Iteration 102, loss = 0.00783638\n",
      "Iteration 103, loss = 0.00772408\n",
      "Iteration 104, loss = 0.00757210\n",
      "Iteration 105, loss = 0.00740422\n",
      "Iteration 106, loss = 0.00727882\n",
      "Iteration 107, loss = 0.00715950\n",
      "Iteration 108, loss = 0.00700057\n",
      "Iteration 109, loss = 0.00687061\n",
      "Iteration 110, loss = 0.00678230\n",
      "Iteration 111, loss = 0.00666424\n",
      "Iteration 112, loss = 0.00652014\n",
      "Iteration 113, loss = 0.00641865\n",
      "Iteration 114, loss = 0.00632499\n",
      "Iteration 115, loss = 0.00623598\n",
      "Iteration 116, loss = 0.00611812\n",
      "Iteration 117, loss = 0.00602246\n",
      "Iteration 118, loss = 0.00595355\n",
      "Iteration 119, loss = 0.00584302\n",
      "Iteration 120, loss = 0.00574950\n",
      "Iteration 121, loss = 0.00567656\n",
      "Iteration 122, loss = 0.00558141\n",
      "Iteration 123, loss = 0.00552992\n",
      "Iteration 124, loss = 0.00540915\n",
      "Iteration 125, loss = 0.00534642\n",
      "Iteration 126, loss = 0.00529238\n",
      "Iteration 127, loss = 0.00520933\n",
      "Iteration 128, loss = 0.00512755\n",
      "Iteration 129, loss = 0.00506469\n",
      "Iteration 130, loss = 0.00501701\n",
      "Iteration 131, loss = 0.00494319\n",
      "Iteration 132, loss = 0.00487509\n",
      "Iteration 133, loss = 0.00481652\n",
      "Iteration 134, loss = 0.00475935\n",
      "Iteration 135, loss = 0.00470307\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.0001, batch_size=50, hidden_layer_sizes=(400, 100), solver=sgd; total time=11.3min\n",
      "Iteration 1, loss = 0.98007416\n",
      "Iteration 2, loss = 0.38076398\n",
      "Iteration 3, loss = 0.31278714\n",
      "Iteration 4, loss = 0.27834914\n",
      "Iteration 5, loss = 0.25369405\n",
      "Iteration 6, loss = 0.23392759\n",
      "Iteration 7, loss = 0.21750056\n",
      "Iteration 8, loss = 0.20279976\n",
      "Iteration 9, loss = 0.19044906\n",
      "Iteration 10, loss = 0.17907956\n",
      "Iteration 11, loss = 0.16922505\n",
      "Iteration 12, loss = 0.15996300\n",
      "Iteration 13, loss = 0.15095595\n",
      "Iteration 14, loss = 0.14340773\n",
      "Iteration 15, loss = 0.13654992\n",
      "Iteration 16, loss = 0.12963897\n",
      "Iteration 17, loss = 0.12340790\n",
      "Iteration 18, loss = 0.11789935\n",
      "Iteration 19, loss = 0.11253882\n",
      "Iteration 20, loss = 0.10750679\n",
      "Iteration 21, loss = 0.10282928\n",
      "Iteration 22, loss = 0.09828033\n",
      "Iteration 23, loss = 0.09403900\n",
      "Iteration 24, loss = 0.09063402\n",
      "Iteration 25, loss = 0.08638190\n",
      "Iteration 26, loss = 0.08312410\n",
      "Iteration 27, loss = 0.07982612\n",
      "Iteration 28, loss = 0.07643626\n",
      "Iteration 29, loss = 0.07353990\n",
      "Iteration 30, loss = 0.07083479\n",
      "Iteration 31, loss = 0.06792543\n",
      "Iteration 32, loss = 0.06551991\n",
      "Iteration 33, loss = 0.06282584\n",
      "Iteration 34, loss = 0.06037859\n",
      "Iteration 35, loss = 0.05818614\n",
      "Iteration 36, loss = 0.05601931\n",
      "Iteration 37, loss = 0.05408193\n",
      "Iteration 38, loss = 0.05201552\n",
      "Iteration 39, loss = 0.05041610\n",
      "Iteration 40, loss = 0.04843534\n",
      "Iteration 41, loss = 0.04687802\n",
      "Iteration 42, loss = 0.04496035\n",
      "Iteration 43, loss = 0.04363350\n",
      "Iteration 44, loss = 0.04190952\n",
      "Iteration 45, loss = 0.04063800\n",
      "Iteration 46, loss = 0.03917127\n",
      "Iteration 47, loss = 0.03792797\n",
      "Iteration 48, loss = 0.03660933\n",
      "Iteration 49, loss = 0.03533529\n",
      "Iteration 50, loss = 0.03409622\n",
      "Iteration 51, loss = 0.03297795\n",
      "Iteration 52, loss = 0.03183957\n",
      "Iteration 53, loss = 0.03090988\n",
      "Iteration 54, loss = 0.02985646\n",
      "Iteration 55, loss = 0.02891456\n",
      "Iteration 56, loss = 0.02796479\n",
      "Iteration 57, loss = 0.02712712\n",
      "Iteration 58, loss = 0.02617927\n",
      "Iteration 59, loss = 0.02548897\n",
      "Iteration 60, loss = 0.02465241\n",
      "Iteration 61, loss = 0.02389438\n",
      "Iteration 62, loss = 0.02302031\n",
      "Iteration 63, loss = 0.02226317\n",
      "Iteration 64, loss = 0.02179750\n",
      "Iteration 65, loss = 0.02106765\n",
      "Iteration 66, loss = 0.02049716\n",
      "Iteration 67, loss = 0.01979074\n",
      "Iteration 68, loss = 0.01919660\n",
      "Iteration 69, loss = 0.01869847\n",
      "Iteration 70, loss = 0.01804791\n",
      "Iteration 71, loss = 0.01762264\n",
      "Iteration 72, loss = 0.01710897\n",
      "Iteration 73, loss = 0.01658057\n",
      "Iteration 74, loss = 0.01629621\n",
      "Iteration 75, loss = 0.01577382\n",
      "Iteration 76, loss = 0.01536173\n",
      "Iteration 77, loss = 0.01485082\n",
      "Iteration 78, loss = 0.01450144\n",
      "Iteration 79, loss = 0.01408232\n",
      "Iteration 80, loss = 0.01377326\n",
      "Iteration 81, loss = 0.01338966\n",
      "Iteration 82, loss = 0.01307149\n",
      "Iteration 83, loss = 0.01270371\n",
      "Iteration 84, loss = 0.01237144\n",
      "Iteration 85, loss = 0.01214919\n",
      "Iteration 86, loss = 0.01187232\n",
      "Iteration 87, loss = 0.01152986\n",
      "Iteration 88, loss = 0.01130026\n",
      "Iteration 89, loss = 0.01102416\n",
      "Iteration 90, loss = 0.01077725\n",
      "Iteration 91, loss = 0.01053062\n",
      "Iteration 92, loss = 0.01032942\n",
      "Iteration 93, loss = 0.01000002\n",
      "Iteration 94, loss = 0.00980718\n",
      "Iteration 95, loss = 0.00965738\n",
      "Iteration 96, loss = 0.00945968\n",
      "Iteration 97, loss = 0.00922719\n",
      "Iteration 98, loss = 0.00902327\n",
      "Iteration 99, loss = 0.00885947\n",
      "Iteration 100, loss = 0.00861406\n",
      "Iteration 101, loss = 0.00852976\n",
      "Iteration 102, loss = 0.00829594\n",
      "Iteration 103, loss = 0.00819045\n",
      "Iteration 104, loss = 0.00795294\n",
      "Iteration 105, loss = 0.00790944\n",
      "Iteration 106, loss = 0.00770697\n",
      "Iteration 107, loss = 0.00756994\n",
      "Iteration 108, loss = 0.00743315\n",
      "Iteration 109, loss = 0.00728672\n",
      "Iteration 110, loss = 0.00715520\n",
      "Iteration 111, loss = 0.00705238\n",
      "Iteration 112, loss = 0.00694356\n",
      "Iteration 113, loss = 0.00679124\n",
      "Iteration 114, loss = 0.00673645\n",
      "Iteration 115, loss = 0.00659967\n",
      "Iteration 116, loss = 0.00647204\n",
      "Iteration 117, loss = 0.00638427\n",
      "Iteration 118, loss = 0.00627270\n",
      "Iteration 119, loss = 0.00620290\n",
      "Iteration 120, loss = 0.00608770\n",
      "Iteration 121, loss = 0.00603877\n",
      "Iteration 122, loss = 0.00592957\n",
      "Iteration 123, loss = 0.00585719\n",
      "Iteration 124, loss = 0.00575597\n",
      "Iteration 125, loss = 0.00569818\n",
      "Iteration 126, loss = 0.00559242\n",
      "Iteration 127, loss = 0.00553034\n",
      "Iteration 128, loss = 0.00545110\n",
      "Iteration 129, loss = 0.00537395\n",
      "Iteration 130, loss = 0.00532281\n",
      "Iteration 131, loss = 0.00524573\n",
      "Iteration 132, loss = 0.00517719\n",
      "Iteration 133, loss = 0.00510564\n",
      "Iteration 134, loss = 0.00504480\n",
      "Iteration 135, loss = 0.00497850\n",
      "Iteration 136, loss = 0.00492185\n",
      "Iteration 137, loss = 0.00487595\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.0001, batch_size=50, hidden_layer_sizes=(400, 100), solver=sgd; total time=11.1min\n",
      "Iteration 1, loss = 0.97248370\n",
      "Iteration 2, loss = 0.38211150\n",
      "Iteration 3, loss = 0.30953134\n",
      "Iteration 4, loss = 0.27236552\n",
      "Iteration 5, loss = 0.24710197\n",
      "Iteration 6, loss = 0.22651363\n",
      "Iteration 7, loss = 0.20990656\n",
      "Iteration 8, loss = 0.19563604\n",
      "Iteration 9, loss = 0.18343997\n",
      "Iteration 10, loss = 0.17219337\n",
      "Iteration 11, loss = 0.16206146\n",
      "Iteration 12, loss = 0.15290116\n",
      "Iteration 13, loss = 0.14492934\n",
      "Iteration 14, loss = 0.13738223\n",
      "Iteration 15, loss = 0.13043230\n",
      "Iteration 16, loss = 0.12397669\n",
      "Iteration 17, loss = 0.11805567\n",
      "Iteration 18, loss = 0.11246546\n",
      "Iteration 19, loss = 0.10759046\n",
      "Iteration 20, loss = 0.10292336\n",
      "Iteration 21, loss = 0.09814169\n",
      "Iteration 22, loss = 0.09423459\n",
      "Iteration 23, loss = 0.09028197\n",
      "Iteration 24, loss = 0.08642888\n",
      "Iteration 25, loss = 0.08274007\n",
      "Iteration 26, loss = 0.07957981\n",
      "Iteration 27, loss = 0.07626331\n",
      "Iteration 28, loss = 0.07349658\n",
      "Iteration 29, loss = 0.07049864\n",
      "Iteration 30, loss = 0.06769112\n",
      "Iteration 31, loss = 0.06542604\n",
      "Iteration 32, loss = 0.06272316\n",
      "Iteration 33, loss = 0.06041667\n",
      "Iteration 34, loss = 0.05831705\n",
      "Iteration 35, loss = 0.05606167\n",
      "Iteration 36, loss = 0.05390976\n",
      "Iteration 37, loss = 0.05192515\n",
      "Iteration 38, loss = 0.05001597\n",
      "Iteration 39, loss = 0.04848763\n",
      "Iteration 40, loss = 0.04700035\n",
      "Iteration 41, loss = 0.04516423\n",
      "Iteration 42, loss = 0.04359105\n",
      "Iteration 43, loss = 0.04223449\n",
      "Iteration 44, loss = 0.04067016\n",
      "Iteration 45, loss = 0.03937117\n",
      "Iteration 46, loss = 0.03788688\n",
      "Iteration 47, loss = 0.03667441\n",
      "Iteration 48, loss = 0.03564094\n",
      "Iteration 49, loss = 0.03447495\n",
      "Iteration 50, loss = 0.03324721\n",
      "Iteration 51, loss = 0.03208360\n",
      "Iteration 52, loss = 0.03113613\n",
      "Iteration 53, loss = 0.03008009\n",
      "Iteration 54, loss = 0.02918232\n",
      "Iteration 55, loss = 0.02820705\n",
      "Iteration 56, loss = 0.02735538\n",
      "Iteration 57, loss = 0.02648486\n",
      "Iteration 58, loss = 0.02562892\n",
      "Iteration 59, loss = 0.02493136\n",
      "Iteration 60, loss = 0.02407149\n",
      "Iteration 61, loss = 0.02334321\n",
      "Iteration 62, loss = 0.02263403\n",
      "Iteration 63, loss = 0.02188950\n",
      "Iteration 64, loss = 0.02126552\n",
      "Iteration 65, loss = 0.02049846\n",
      "Iteration 66, loss = 0.02019802\n",
      "Iteration 67, loss = 0.01950937\n",
      "Iteration 68, loss = 0.01892950\n",
      "Iteration 69, loss = 0.01840195\n",
      "Iteration 70, loss = 0.01783184\n",
      "Iteration 71, loss = 0.01740154\n",
      "Iteration 72, loss = 0.01687752\n",
      "Iteration 73, loss = 0.01640977\n",
      "Iteration 74, loss = 0.01595766\n",
      "Iteration 75, loss = 0.01554218\n",
      "Iteration 76, loss = 0.01518300\n",
      "Iteration 77, loss = 0.01477834\n",
      "Iteration 78, loss = 0.01430674\n",
      "Iteration 79, loss = 0.01403540\n",
      "Iteration 80, loss = 0.01361664\n",
      "Iteration 81, loss = 0.01328763\n",
      "Iteration 82, loss = 0.01297234\n",
      "Iteration 83, loss = 0.01265633\n",
      "Iteration 84, loss = 0.01238051\n",
      "Iteration 85, loss = 0.01209549\n",
      "Iteration 86, loss = 0.01174645\n",
      "Iteration 87, loss = 0.01150637\n",
      "Iteration 88, loss = 0.01123278\n",
      "Iteration 89, loss = 0.01096259\n",
      "Iteration 90, loss = 0.01075699\n",
      "Iteration 91, loss = 0.01046448\n",
      "Iteration 92, loss = 0.01028765\n",
      "Iteration 93, loss = 0.00999405\n",
      "Iteration 94, loss = 0.00981800\n",
      "Iteration 95, loss = 0.00960365\n",
      "Iteration 96, loss = 0.00937243\n",
      "Iteration 97, loss = 0.00921480\n",
      "Iteration 98, loss = 0.00907157\n",
      "Iteration 99, loss = 0.00882542\n",
      "Iteration 100, loss = 0.00859907\n",
      "Iteration 101, loss = 0.00848586\n",
      "Iteration 102, loss = 0.00831883\n",
      "Iteration 103, loss = 0.00813819\n",
      "Iteration 104, loss = 0.00798585\n",
      "Iteration 105, loss = 0.00783218\n",
      "Iteration 106, loss = 0.00772233\n",
      "Iteration 107, loss = 0.00760512\n",
      "Iteration 108, loss = 0.00745680\n",
      "Iteration 109, loss = 0.00728697\n",
      "Iteration 110, loss = 0.00718572\n",
      "Iteration 111, loss = 0.00704185\n",
      "Iteration 112, loss = 0.00693759\n",
      "Iteration 113, loss = 0.00679675\n",
      "Iteration 114, loss = 0.00669852\n",
      "Iteration 115, loss = 0.00656031\n",
      "Iteration 116, loss = 0.00647283\n",
      "Iteration 117, loss = 0.00639155\n",
      "Iteration 118, loss = 0.00625697\n",
      "Iteration 119, loss = 0.00616998\n",
      "Iteration 120, loss = 0.00604676\n",
      "Iteration 121, loss = 0.00599252\n",
      "Iteration 122, loss = 0.00589445\n",
      "Iteration 123, loss = 0.00582138\n",
      "Iteration 124, loss = 0.00575510\n",
      "Iteration 125, loss = 0.00563061\n",
      "Iteration 126, loss = 0.00558530\n",
      "Iteration 127, loss = 0.00550108\n",
      "Iteration 128, loss = 0.00542138\n",
      "Iteration 129, loss = 0.00535295\n",
      "Iteration 130, loss = 0.00528309\n",
      "Iteration 131, loss = 0.00520993\n",
      "Iteration 132, loss = 0.00513516\n",
      "Iteration 133, loss = 0.00508045\n",
      "Iteration 134, loss = 0.00500358\n",
      "Iteration 135, loss = 0.00495340\n",
      "Iteration 136, loss = 0.00489820\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.0001, batch_size=50, hidden_layer_sizes=(400, 100), solver=sgd; total time=10.7min\n",
      "Iteration 1, loss = 0.92549579\n",
      "Iteration 2, loss = 0.38127710\n",
      "Iteration 3, loss = 0.31145980\n",
      "Iteration 4, loss = 0.27652117\n",
      "Iteration 5, loss = 0.25152231\n",
      "Iteration 6, loss = 0.23147096\n",
      "Iteration 7, loss = 0.21426722\n",
      "Iteration 8, loss = 0.19994560\n",
      "Iteration 9, loss = 0.18695836\n",
      "Iteration 10, loss = 0.17552891\n",
      "Iteration 11, loss = 0.16523877\n",
      "Iteration 12, loss = 0.15578813\n",
      "Iteration 13, loss = 0.14740418\n",
      "Iteration 14, loss = 0.13974522\n",
      "Iteration 15, loss = 0.13270726\n",
      "Iteration 16, loss = 0.12607370\n",
      "Iteration 17, loss = 0.12018938\n",
      "Iteration 18, loss = 0.11456687\n",
      "Iteration 19, loss = 0.10931368\n",
      "Iteration 20, loss = 0.10445019\n",
      "Iteration 21, loss = 0.09979915\n",
      "Iteration 22, loss = 0.09536527\n",
      "Iteration 23, loss = 0.09136335\n",
      "Iteration 24, loss = 0.08770985\n",
      "Iteration 25, loss = 0.08419495\n",
      "Iteration 26, loss = 0.08060169\n",
      "Iteration 27, loss = 0.07747433\n",
      "Iteration 28, loss = 0.07443195\n",
      "Iteration 29, loss = 0.07167347\n",
      "Iteration 30, loss = 0.06879433\n",
      "Iteration 31, loss = 0.06606986\n",
      "Iteration 32, loss = 0.06345104\n",
      "Iteration 33, loss = 0.06117200\n",
      "Iteration 34, loss = 0.05897678\n",
      "Iteration 35, loss = 0.05686961\n",
      "Iteration 36, loss = 0.05482044\n",
      "Iteration 37, loss = 0.05278837\n",
      "Iteration 38, loss = 0.05090791\n",
      "Iteration 39, loss = 0.04896111\n",
      "Iteration 40, loss = 0.04717879\n",
      "Iteration 41, loss = 0.04547156\n",
      "Iteration 42, loss = 0.04431668\n",
      "Iteration 43, loss = 0.04279742\n",
      "Iteration 44, loss = 0.04114808\n",
      "Iteration 45, loss = 0.03987514\n",
      "Iteration 46, loss = 0.03837092\n",
      "Iteration 47, loss = 0.03711386\n",
      "Iteration 48, loss = 0.03581670\n",
      "Iteration 49, loss = 0.03454365\n",
      "Iteration 50, loss = 0.03352519\n",
      "Iteration 51, loss = 0.03260016\n",
      "Iteration 52, loss = 0.03138216\n",
      "Iteration 53, loss = 0.03028565\n",
      "Iteration 54, loss = 0.02931657\n",
      "Iteration 55, loss = 0.02835607\n",
      "Iteration 56, loss = 0.02748669\n",
      "Iteration 57, loss = 0.02655980\n",
      "Iteration 58, loss = 0.02566823\n",
      "Iteration 59, loss = 0.02497554\n",
      "Iteration 60, loss = 0.02417730\n",
      "Iteration 61, loss = 0.02337361\n",
      "Iteration 62, loss = 0.02276720\n",
      "Iteration 63, loss = 0.02215666\n",
      "Iteration 64, loss = 0.02135849\n",
      "Iteration 65, loss = 0.02076073\n",
      "Iteration 66, loss = 0.02012536\n",
      "Iteration 67, loss = 0.01956328\n",
      "Iteration 68, loss = 0.01905816\n",
      "Iteration 69, loss = 0.01837826\n",
      "Iteration 70, loss = 0.01796540\n",
      "Iteration 71, loss = 0.01743094\n",
      "Iteration 72, loss = 0.01703516\n",
      "Iteration 73, loss = 0.01648529\n",
      "Iteration 74, loss = 0.01611352\n",
      "Iteration 75, loss = 0.01554802\n",
      "Iteration 76, loss = 0.01520368\n",
      "Iteration 77, loss = 0.01478133\n",
      "Iteration 78, loss = 0.01438504\n",
      "Iteration 79, loss = 0.01405346\n",
      "Iteration 80, loss = 0.01365409\n",
      "Iteration 81, loss = 0.01340838\n",
      "Iteration 82, loss = 0.01290129\n",
      "Iteration 83, loss = 0.01261109\n",
      "Iteration 84, loss = 0.01235901\n",
      "Iteration 85, loss = 0.01205453\n",
      "Iteration 86, loss = 0.01172076\n",
      "Iteration 87, loss = 0.01141638\n",
      "Iteration 88, loss = 0.01125053\n",
      "Iteration 89, loss = 0.01097454\n",
      "Iteration 90, loss = 0.01074056\n",
      "Iteration 91, loss = 0.01046072\n",
      "Iteration 92, loss = 0.01024886\n",
      "Iteration 93, loss = 0.01000431\n",
      "Iteration 94, loss = 0.00983913\n",
      "Iteration 95, loss = 0.00959849\n",
      "Iteration 96, loss = 0.00937918\n",
      "Iteration 97, loss = 0.00918228\n",
      "Iteration 98, loss = 0.00895080\n",
      "Iteration 99, loss = 0.00883716\n",
      "Iteration 100, loss = 0.00860244\n",
      "Iteration 101, loss = 0.00851675\n",
      "Iteration 102, loss = 0.00832547\n",
      "Iteration 103, loss = 0.00812234\n",
      "Iteration 104, loss = 0.00798202\n",
      "Iteration 105, loss = 0.00782348\n",
      "Iteration 106, loss = 0.00767145\n",
      "Iteration 107, loss = 0.00751705\n",
      "Iteration 108, loss = 0.00738602\n",
      "Iteration 109, loss = 0.00726470\n",
      "Iteration 110, loss = 0.00714412\n",
      "Iteration 111, loss = 0.00701979\n",
      "Iteration 112, loss = 0.00686963\n",
      "Iteration 113, loss = 0.00677237\n",
      "Iteration 114, loss = 0.00666580\n",
      "Iteration 115, loss = 0.00653529\n",
      "Iteration 116, loss = 0.00645809\n",
      "Iteration 117, loss = 0.00635335\n",
      "Iteration 118, loss = 0.00624355\n",
      "Iteration 119, loss = 0.00617236\n",
      "Iteration 120, loss = 0.00606024\n",
      "Iteration 121, loss = 0.00598159\n",
      "Iteration 122, loss = 0.00588941\n",
      "Iteration 123, loss = 0.00578318\n",
      "Iteration 124, loss = 0.00570646\n",
      "Iteration 125, loss = 0.00562490\n",
      "Iteration 126, loss = 0.00555188\n",
      "Iteration 127, loss = 0.00546813\n",
      "Iteration 128, loss = 0.00540390\n",
      "Iteration 129, loss = 0.00531858\n",
      "Iteration 130, loss = 0.00526085\n",
      "Iteration 131, loss = 0.00518554\n",
      "Iteration 132, loss = 0.00512592\n",
      "Iteration 133, loss = 0.00505806\n",
      "Iteration 134, loss = 0.00499170\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.0001, batch_size=50, hidden_layer_sizes=(400, 100), solver=sgd; total time=10.8min\n",
      "Iteration 1, loss = 0.96263956\n",
      "Iteration 2, loss = 0.38303151\n",
      "Iteration 3, loss = 0.30977459\n",
      "Iteration 4, loss = 0.27289257\n",
      "Iteration 5, loss = 0.24700098\n",
      "Iteration 6, loss = 0.22727849\n",
      "Iteration 7, loss = 0.21136224\n",
      "Iteration 8, loss = 0.19705714\n",
      "Iteration 9, loss = 0.18485234\n",
      "Iteration 10, loss = 0.17363696\n",
      "Iteration 11, loss = 0.16382410\n",
      "Iteration 12, loss = 0.15491521\n",
      "Iteration 13, loss = 0.14687217\n",
      "Iteration 14, loss = 0.13896787\n",
      "Iteration 15, loss = 0.13219648\n",
      "Iteration 16, loss = 0.12623622\n",
      "Iteration 17, loss = 0.11987726\n",
      "Iteration 18, loss = 0.11407722\n",
      "Iteration 19, loss = 0.10885786\n",
      "Iteration 20, loss = 0.10439399\n",
      "Iteration 21, loss = 0.09969699\n",
      "Iteration 22, loss = 0.09545199\n",
      "Iteration 23, loss = 0.09124484\n",
      "Iteration 24, loss = 0.08766290\n",
      "Iteration 25, loss = 0.08382690\n",
      "Iteration 26, loss = 0.08056852\n",
      "Iteration 27, loss = 0.07733270\n",
      "Iteration 28, loss = 0.07419444\n",
      "Iteration 29, loss = 0.07142726\n",
      "Iteration 30, loss = 0.06843016\n",
      "Iteration 31, loss = 0.06590051\n",
      "Iteration 32, loss = 0.06331688\n",
      "Iteration 33, loss = 0.06126770\n",
      "Iteration 34, loss = 0.05861469\n",
      "Iteration 35, loss = 0.05675117\n",
      "Iteration 36, loss = 0.05450881\n",
      "Iteration 37, loss = 0.05259063\n",
      "Iteration 38, loss = 0.05075520\n",
      "Iteration 39, loss = 0.04886181\n",
      "Iteration 40, loss = 0.04707147\n",
      "Iteration 41, loss = 0.04539097\n",
      "Iteration 42, loss = 0.04400720\n",
      "Iteration 43, loss = 0.04228972\n",
      "Iteration 44, loss = 0.04100250\n",
      "Iteration 45, loss = 0.03950948\n",
      "Iteration 46, loss = 0.03796619\n",
      "Iteration 47, loss = 0.03705809\n",
      "Iteration 48, loss = 0.03568980\n",
      "Iteration 49, loss = 0.03451487\n",
      "Iteration 50, loss = 0.03345276\n",
      "Iteration 51, loss = 0.03231303\n",
      "Iteration 52, loss = 0.03128619\n",
      "Iteration 53, loss = 0.03007440\n",
      "Iteration 54, loss = 0.02921789\n",
      "Iteration 55, loss = 0.02814844\n",
      "Iteration 56, loss = 0.02727677\n",
      "Iteration 57, loss = 0.02639363\n",
      "Iteration 58, loss = 0.02558003\n",
      "Iteration 59, loss = 0.02489234\n",
      "Iteration 60, loss = 0.02415717\n",
      "Iteration 61, loss = 0.02330404\n",
      "Iteration 62, loss = 0.02264606\n",
      "Iteration 63, loss = 0.02186494\n",
      "Iteration 64, loss = 0.02120081\n",
      "Iteration 65, loss = 0.02067229\n",
      "Iteration 66, loss = 0.02011166\n",
      "Iteration 67, loss = 0.01942574\n",
      "Iteration 68, loss = 0.01892027\n",
      "Iteration 69, loss = 0.01844302\n",
      "Iteration 70, loss = 0.01791067\n",
      "Iteration 71, loss = 0.01734499\n",
      "Iteration 72, loss = 0.01693762\n",
      "Iteration 73, loss = 0.01655278\n",
      "Iteration 74, loss = 0.01604135\n",
      "Iteration 75, loss = 0.01566363\n",
      "Iteration 76, loss = 0.01516629\n",
      "Iteration 77, loss = 0.01477737\n",
      "Iteration 78, loss = 0.01438183\n",
      "Iteration 79, loss = 0.01401475\n",
      "Iteration 80, loss = 0.01370871\n",
      "Iteration 81, loss = 0.01326402\n",
      "Iteration 82, loss = 0.01304036\n",
      "Iteration 83, loss = 0.01269752\n",
      "Iteration 84, loss = 0.01240209\n",
      "Iteration 85, loss = 0.01205810\n",
      "Iteration 86, loss = 0.01180173\n",
      "Iteration 87, loss = 0.01158973\n",
      "Iteration 88, loss = 0.01127140\n",
      "Iteration 89, loss = 0.01097804\n",
      "Iteration 90, loss = 0.01077320\n",
      "Iteration 91, loss = 0.01055403\n",
      "Iteration 92, loss = 0.01031820\n",
      "Iteration 93, loss = 0.01008560\n",
      "Iteration 94, loss = 0.00989426\n",
      "Iteration 95, loss = 0.00966835\n",
      "Iteration 96, loss = 0.00947691\n",
      "Iteration 97, loss = 0.00928758\n",
      "Iteration 98, loss = 0.00910611\n",
      "Iteration 99, loss = 0.00890126\n",
      "Iteration 100, loss = 0.00873575\n",
      "Iteration 101, loss = 0.00859565\n",
      "Iteration 102, loss = 0.00841359\n",
      "Iteration 103, loss = 0.00821845\n",
      "Iteration 104, loss = 0.00811680\n",
      "Iteration 105, loss = 0.00796877\n",
      "Iteration 106, loss = 0.00779250\n",
      "Iteration 107, loss = 0.00767013\n",
      "Iteration 108, loss = 0.00752323\n",
      "Iteration 109, loss = 0.00736624\n",
      "Iteration 110, loss = 0.00728727\n",
      "Iteration 111, loss = 0.00710435\n",
      "Iteration 112, loss = 0.00702033\n",
      "Iteration 113, loss = 0.00688403\n",
      "Iteration 114, loss = 0.00678133\n",
      "Iteration 115, loss = 0.00666852\n",
      "Iteration 116, loss = 0.00656490\n",
      "Iteration 117, loss = 0.00643953\n",
      "Iteration 118, loss = 0.00633359\n",
      "Iteration 119, loss = 0.00625405\n",
      "Iteration 120, loss = 0.00615094\n",
      "Iteration 121, loss = 0.00607362\n",
      "Iteration 122, loss = 0.00597835\n",
      "Iteration 123, loss = 0.00587088\n",
      "Iteration 124, loss = 0.00580088\n",
      "Iteration 125, loss = 0.00569847\n",
      "Iteration 126, loss = 0.00560714\n",
      "Iteration 127, loss = 0.00556602\n",
      "Iteration 128, loss = 0.00549935\n",
      "Iteration 129, loss = 0.00540123\n",
      "Iteration 130, loss = 0.00531470\n",
      "Iteration 131, loss = 0.00525681\n",
      "Iteration 132, loss = 0.00518643\n",
      "Iteration 133, loss = 0.00512479\n",
      "Iteration 134, loss = 0.00505145\n",
      "Iteration 135, loss = 0.00497966\n",
      "Iteration 136, loss = 0.00492176\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.0001, batch_size=50, hidden_layer_sizes=(400, 100), solver=sgd; total time=11.5min\n",
      "Iteration 1, loss = 0.34676479\n",
      "Iteration 2, loss = 0.13666170\n",
      "Iteration 3, loss = 0.09257791\n",
      "Iteration 4, loss = 0.07212665\n",
      "Iteration 5, loss = 0.05709980\n",
      "Iteration 6, loss = 0.04631277\n",
      "Iteration 7, loss = 0.03808452\n",
      "Iteration 8, loss = 0.02776980\n",
      "Iteration 9, loss = 0.02933125\n",
      "Iteration 10, loss = 0.02505807\n",
      "Iteration 11, loss = 0.02111656\n",
      "Iteration 12, loss = 0.01938255\n",
      "Iteration 13, loss = 0.01984808\n",
      "Iteration 14, loss = 0.01737453\n",
      "Iteration 15, loss = 0.01557945\n",
      "Iteration 16, loss = 0.01981182\n",
      "Iteration 17, loss = 0.01933687\n",
      "Iteration 18, loss = 0.01337174\n",
      "Iteration 19, loss = 0.00867866\n",
      "Iteration 20, loss = 0.01568255\n",
      "Iteration 21, loss = 0.01533814\n",
      "Iteration 22, loss = 0.01508889\n",
      "Iteration 23, loss = 0.01048502\n",
      "Iteration 24, loss = 0.01015269\n",
      "Iteration 25, loss = 0.01286977\n",
      "Iteration 26, loss = 0.01486451\n",
      "Iteration 27, loss = 0.00917003\n",
      "Iteration 28, loss = 0.01226436\n",
      "Iteration 29, loss = 0.00998681\n",
      "Iteration 30, loss = 0.01142799\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.0001, batch_size=50, hidden_layer_sizes=(100, 100, 100), solver=adam; total time= 2.3min\n",
      "Iteration 1, loss = 0.33578862\n",
      "Iteration 2, loss = 0.13374544\n",
      "Iteration 3, loss = 0.09297674\n",
      "Iteration 4, loss = 0.07303175\n",
      "Iteration 5, loss = 0.05463128\n",
      "Iteration 6, loss = 0.04573016\n",
      "Iteration 7, loss = 0.03648169\n",
      "Iteration 8, loss = 0.03425515\n",
      "Iteration 9, loss = 0.02723392\n",
      "Iteration 10, loss = 0.02470000\n",
      "Iteration 11, loss = 0.02405109\n",
      "Iteration 12, loss = 0.02277610\n",
      "Iteration 13, loss = 0.01796261\n",
      "Iteration 14, loss = 0.01670092\n",
      "Iteration 15, loss = 0.02245407\n",
      "Iteration 16, loss = 0.01622920\n",
      "Iteration 17, loss = 0.01279973\n",
      "Iteration 18, loss = 0.01687150\n",
      "Iteration 19, loss = 0.01486671\n",
      "Iteration 20, loss = 0.01330439\n",
      "Iteration 21, loss = 0.01662292\n",
      "Iteration 22, loss = 0.00924808\n",
      "Iteration 23, loss = 0.01217137\n",
      "Iteration 24, loss = 0.01454004\n",
      "Iteration 25, loss = 0.01848166\n",
      "Iteration 26, loss = 0.01267624\n",
      "Iteration 27, loss = 0.00667579\n",
      "Iteration 28, loss = 0.00756094\n",
      "Iteration 29, loss = 0.01931183\n",
      "Iteration 30, loss = 0.00857341\n",
      "Iteration 31, loss = 0.00781487\n",
      "Iteration 32, loss = 0.01151695\n",
      "Iteration 33, loss = 0.01255859\n",
      "Iteration 34, loss = 0.01101517\n",
      "Iteration 35, loss = 0.00788983\n",
      "Iteration 36, loss = 0.00846035\n",
      "Iteration 37, loss = 0.01344071\n",
      "Iteration 38, loss = 0.01644839\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.0001, batch_size=50, hidden_layer_sizes=(100, 100, 100), solver=adam; total time= 3.4min\n",
      "Iteration 1, loss = 0.32671581\n",
      "Iteration 2, loss = 0.12927968\n",
      "Iteration 3, loss = 0.09072798\n",
      "Iteration 4, loss = 0.06626920\n",
      "Iteration 5, loss = 0.05483941\n",
      "Iteration 6, loss = 0.04285246\n",
      "Iteration 7, loss = 0.03530026\n",
      "Iteration 8, loss = 0.02964012\n",
      "Iteration 9, loss = 0.02704344\n",
      "Iteration 10, loss = 0.02814429\n",
      "Iteration 11, loss = 0.01822600\n",
      "Iteration 12, loss = 0.02461494\n",
      "Iteration 13, loss = 0.01903780\n",
      "Iteration 14, loss = 0.01419207\n",
      "Iteration 15, loss = 0.01678319\n",
      "Iteration 16, loss = 0.01888292\n",
      "Iteration 17, loss = 0.01344680\n",
      "Iteration 18, loss = 0.01944777\n",
      "Iteration 19, loss = 0.00994504\n",
      "Iteration 20, loss = 0.01573801\n",
      "Iteration 21, loss = 0.01448646\n",
      "Iteration 22, loss = 0.01290396\n",
      "Iteration 23, loss = 0.00752748\n",
      "Iteration 24, loss = 0.01550393\n",
      "Iteration 25, loss = 0.01138519\n",
      "Iteration 26, loss = 0.01116660\n",
      "Iteration 27, loss = 0.01118333\n",
      "Iteration 28, loss = 0.01247409\n",
      "Iteration 29, loss = 0.00932422\n",
      "Iteration 30, loss = 0.00897019\n",
      "Iteration 31, loss = 0.01265629\n",
      "Iteration 32, loss = 0.01287932\n",
      "Iteration 33, loss = 0.00988703\n",
      "Iteration 34, loss = 0.00867541\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.0001, batch_size=50, hidden_layer_sizes=(100, 100, 100), solver=adam; total time= 2.6min\n",
      "Iteration 1, loss = 0.33864061\n",
      "Iteration 2, loss = 0.13884913\n",
      "Iteration 3, loss = 0.09791002\n",
      "Iteration 4, loss = 0.07225527\n",
      "Iteration 5, loss = 0.05896107\n",
      "Iteration 6, loss = 0.04509972\n",
      "Iteration 7, loss = 0.04003557\n",
      "Iteration 8, loss = 0.03531453\n",
      "Iteration 9, loss = 0.02511758\n",
      "Iteration 10, loss = 0.02486795\n",
      "Iteration 11, loss = 0.02401233\n",
      "Iteration 12, loss = 0.02448524\n",
      "Iteration 13, loss = 0.01715225\n",
      "Iteration 14, loss = 0.01763599\n",
      "Iteration 15, loss = 0.01656382\n",
      "Iteration 16, loss = 0.01909937\n",
      "Iteration 17, loss = 0.01875747\n",
      "Iteration 18, loss = 0.01541759\n",
      "Iteration 19, loss = 0.01126261\n",
      "Iteration 20, loss = 0.01765735\n",
      "Iteration 21, loss = 0.01489690\n",
      "Iteration 22, loss = 0.00775765\n",
      "Iteration 23, loss = 0.01290856\n",
      "Iteration 24, loss = 0.01313667\n",
      "Iteration 25, loss = 0.01616909\n",
      "Iteration 26, loss = 0.01049391\n",
      "Iteration 27, loss = 0.01074805\n",
      "Iteration 28, loss = 0.01097891\n",
      "Iteration 29, loss = 0.00865661\n",
      "Iteration 30, loss = 0.01701036\n",
      "Iteration 31, loss = 0.00875176\n",
      "Iteration 32, loss = 0.01406150\n",
      "Iteration 33, loss = 0.00699763\n",
      "Iteration 34, loss = 0.00909828\n",
      "Iteration 35, loss = 0.01481927\n",
      "Iteration 36, loss = 0.00861658\n",
      "Iteration 37, loss = 0.00900577\n",
      "Iteration 38, loss = 0.00602858\n",
      "Iteration 39, loss = 0.01556394\n",
      "Iteration 40, loss = 0.00543478\n",
      "Iteration 41, loss = 0.00701714\n",
      "Iteration 42, loss = 0.01388216\n",
      "Iteration 43, loss = 0.00663405\n",
      "Iteration 44, loss = 0.01370407\n",
      "Iteration 45, loss = 0.00716902\n",
      "Iteration 46, loss = 0.00720067\n",
      "Iteration 47, loss = 0.00763142\n",
      "Iteration 48, loss = 0.00962773\n",
      "Iteration 49, loss = 0.01001819\n",
      "Iteration 50, loss = 0.00817232\n",
      "Iteration 51, loss = 0.00746845\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.0001, batch_size=50, hidden_layer_sizes=(100, 100, 100), solver=adam; total time= 3.8min\n",
      "Iteration 1, loss = 0.33080633\n",
      "Iteration 2, loss = 0.13816176\n",
      "Iteration 3, loss = 0.09336652\n",
      "Iteration 4, loss = 0.07146696\n",
      "Iteration 5, loss = 0.05563926\n",
      "Iteration 6, loss = 0.04745657\n",
      "Iteration 7, loss = 0.03970284\n",
      "Iteration 8, loss = 0.03638389\n",
      "Iteration 9, loss = 0.02444080\n",
      "Iteration 10, loss = 0.02621209\n",
      "Iteration 11, loss = 0.02496810\n",
      "Iteration 12, loss = 0.02048436\n",
      "Iteration 13, loss = 0.02123610\n",
      "Iteration 14, loss = 0.01788244\n",
      "Iteration 15, loss = 0.01734156\n",
      "Iteration 16, loss = 0.01798571\n",
      "Iteration 17, loss = 0.01594540\n",
      "Iteration 18, loss = 0.01697713\n",
      "Iteration 19, loss = 0.01525188\n",
      "Iteration 20, loss = 0.01430587\n",
      "Iteration 21, loss = 0.01039695\n",
      "Iteration 22, loss = 0.01492330\n",
      "Iteration 23, loss = 0.01655584\n",
      "Iteration 24, loss = 0.01147184\n",
      "Iteration 25, loss = 0.00910067\n",
      "Iteration 26, loss = 0.01863975\n",
      "Iteration 27, loss = 0.01355033\n",
      "Iteration 28, loss = 0.01080085\n",
      "Iteration 29, loss = 0.01057963\n",
      "Iteration 30, loss = 0.01032968\n",
      "Iteration 31, loss = 0.01050474\n",
      "Iteration 32, loss = 0.01209048\n",
      "Iteration 33, loss = 0.01223458\n",
      "Iteration 34, loss = 0.01142961\n",
      "Iteration 35, loss = 0.00758306\n",
      "Iteration 36, loss = 0.00821218\n",
      "Iteration 37, loss = 0.00820428\n",
      "Iteration 38, loss = 0.01262591\n",
      "Iteration 39, loss = 0.01567052\n",
      "Iteration 40, loss = 0.00629377\n",
      "Iteration 41, loss = 0.00559109\n",
      "Iteration 42, loss = 0.01290039\n",
      "Iteration 43, loss = 0.01093538\n",
      "Iteration 44, loss = 0.00719869\n",
      "Iteration 45, loss = 0.00812893\n",
      "Iteration 46, loss = 0.01094320\n",
      "Iteration 47, loss = 0.01012998\n",
      "Iteration 48, loss = 0.00482128\n",
      "Iteration 49, loss = 0.01085150\n",
      "Iteration 50, loss = 0.01216879\n",
      "Iteration 51, loss = 0.00414309\n",
      "Iteration 52, loss = 0.01024794\n",
      "Iteration 53, loss = 0.01217063\n",
      "Iteration 54, loss = 0.00831121\n",
      "Iteration 55, loss = 0.01060938\n",
      "Iteration 56, loss = 0.00841263\n",
      "Iteration 57, loss = 0.00777855\n",
      "Iteration 58, loss = 0.00737797\n",
      "Iteration 59, loss = 0.00730336\n",
      "Iteration 60, loss = 0.01001733\n",
      "Iteration 61, loss = 0.00841720\n",
      "Iteration 62, loss = 0.00704236\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.0001, batch_size=50, hidden_layer_sizes=(100, 100, 100), solver=adam; total time= 5.8min\n",
      "Iteration 1, loss = 1.28157407\n",
      "Iteration 2, loss = 0.43122498\n",
      "Iteration 3, loss = 0.32761979\n",
      "Iteration 4, loss = 0.28361706\n",
      "Iteration 5, loss = 0.25427381\n",
      "Iteration 6, loss = 0.23207722\n",
      "Iteration 7, loss = 0.21343583\n",
      "Iteration 8, loss = 0.19674253\n",
      "Iteration 9, loss = 0.18246047\n",
      "Iteration 10, loss = 0.17103865\n",
      "Iteration 11, loss = 0.15982016\n",
      "Iteration 12, loss = 0.14968651\n",
      "Iteration 13, loss = 0.14110560\n",
      "Iteration 14, loss = 0.13386447\n",
      "Iteration 15, loss = 0.12675426\n",
      "Iteration 16, loss = 0.12019155\n",
      "Iteration 17, loss = 0.11339173\n",
      "Iteration 18, loss = 0.10776074\n",
      "Iteration 19, loss = 0.10317137\n",
      "Iteration 20, loss = 0.09818598\n",
      "Iteration 21, loss = 0.09349430\n",
      "Iteration 22, loss = 0.08942022\n",
      "Iteration 23, loss = 0.08496106\n",
      "Iteration 24, loss = 0.08182020\n",
      "Iteration 25, loss = 0.07776412\n",
      "Iteration 26, loss = 0.07437340\n",
      "Iteration 27, loss = 0.07122667\n",
      "Iteration 28, loss = 0.06785143\n",
      "Iteration 29, loss = 0.06528479\n",
      "Iteration 30, loss = 0.06253802\n",
      "Iteration 31, loss = 0.06012333\n",
      "Iteration 32, loss = 0.05733624\n",
      "Iteration 33, loss = 0.05453071\n",
      "Iteration 34, loss = 0.05274544\n",
      "Iteration 35, loss = 0.05027119\n",
      "Iteration 36, loss = 0.04835922\n",
      "Iteration 37, loss = 0.04612863\n",
      "Iteration 38, loss = 0.04450380\n",
      "Iteration 39, loss = 0.04232723\n",
      "Iteration 40, loss = 0.04083720\n",
      "Iteration 41, loss = 0.03892307\n",
      "Iteration 42, loss = 0.03731598\n",
      "Iteration 43, loss = 0.03565537\n",
      "Iteration 44, loss = 0.03426145\n",
      "Iteration 45, loss = 0.03300851\n",
      "Iteration 46, loss = 0.03156717\n",
      "Iteration 47, loss = 0.03005010\n",
      "Iteration 48, loss = 0.02907013\n",
      "Iteration 49, loss = 0.02803547\n",
      "Iteration 50, loss = 0.02674208\n",
      "Iteration 51, loss = 0.02571757\n",
      "Iteration 52, loss = 0.02469342\n",
      "Iteration 53, loss = 0.02354274\n",
      "Iteration 54, loss = 0.02233225\n",
      "Iteration 55, loss = 0.02148058\n",
      "Iteration 56, loss = 0.02072032\n",
      "Iteration 57, loss = 0.01988892\n",
      "Iteration 58, loss = 0.01919590\n",
      "Iteration 59, loss = 0.01833160\n",
      "Iteration 60, loss = 0.01742113\n",
      "Iteration 61, loss = 0.01690253\n",
      "Iteration 62, loss = 0.01613212\n",
      "Iteration 63, loss = 0.01576481\n",
      "Iteration 64, loss = 0.01513753\n",
      "Iteration 65, loss = 0.01464219\n",
      "Iteration 66, loss = 0.01410683\n",
      "Iteration 67, loss = 0.01334898\n",
      "Iteration 68, loss = 0.01296937\n",
      "Iteration 69, loss = 0.01231658\n",
      "Iteration 70, loss = 0.01173919\n",
      "Iteration 71, loss = 0.01149648\n",
      "Iteration 72, loss = 0.01093329\n",
      "Iteration 73, loss = 0.01054040\n",
      "Iteration 74, loss = 0.01017810\n",
      "Iteration 75, loss = 0.00978986\n",
      "Iteration 76, loss = 0.00943519\n",
      "Iteration 77, loss = 0.00915955\n",
      "Iteration 78, loss = 0.00891985\n",
      "Iteration 79, loss = 0.00851077\n",
      "Iteration 80, loss = 0.00814064\n",
      "Iteration 81, loss = 0.00784111\n",
      "Iteration 82, loss = 0.00758856\n",
      "Iteration 83, loss = 0.00733636\n",
      "Iteration 84, loss = 0.00709421\n",
      "Iteration 85, loss = 0.00690298\n",
      "Iteration 86, loss = 0.00676173\n",
      "Iteration 87, loss = 0.00643645\n",
      "Iteration 88, loss = 0.00626096\n",
      "Iteration 89, loss = 0.00610458\n",
      "Iteration 90, loss = 0.00593631\n",
      "Iteration 91, loss = 0.00574314\n",
      "Iteration 92, loss = 0.00549364\n",
      "Iteration 93, loss = 0.00535974\n",
      "Iteration 94, loss = 0.00525026\n",
      "Iteration 95, loss = 0.00509562\n",
      "Iteration 96, loss = 0.00492956\n",
      "Iteration 97, loss = 0.00484565\n",
      "Iteration 98, loss = 0.00469310\n",
      "Iteration 99, loss = 0.00469591\n",
      "Iteration 100, loss = 0.00445339\n",
      "Iteration 101, loss = 0.00433304\n",
      "Iteration 102, loss = 0.00426008\n",
      "Iteration 103, loss = 0.00419125\n",
      "Iteration 104, loss = 0.00406367\n",
      "Iteration 105, loss = 0.00395724\n",
      "Iteration 106, loss = 0.00387353\n",
      "Iteration 107, loss = 0.00382557\n",
      "Iteration 108, loss = 0.00373781\n",
      "Iteration 109, loss = 0.00365241\n",
      "Iteration 110, loss = 0.00357437\n",
      "Iteration 111, loss = 0.00352845\n",
      "Iteration 112, loss = 0.00344124\n",
      "Iteration 113, loss = 0.00340331\n",
      "Iteration 114, loss = 0.00333829\n",
      "Iteration 115, loss = 0.00326750\n",
      "Iteration 116, loss = 0.00317626\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.0001, batch_size=50, hidden_layer_sizes=(100, 100, 100), solver=sgd; total time= 7.4min\n",
      "Iteration 1, loss = 1.25553971\n",
      "Iteration 2, loss = 0.40606155\n",
      "Iteration 3, loss = 0.31755854\n",
      "Iteration 4, loss = 0.27632653\n",
      "Iteration 5, loss = 0.24802869\n",
      "Iteration 6, loss = 0.22726749\n",
      "Iteration 7, loss = 0.20961523\n",
      "Iteration 8, loss = 0.19437968\n",
      "Iteration 9, loss = 0.18177770\n",
      "Iteration 10, loss = 0.17151735\n",
      "Iteration 11, loss = 0.16081898\n",
      "Iteration 12, loss = 0.15255224\n",
      "Iteration 13, loss = 0.14407517\n",
      "Iteration 14, loss = 0.13534971\n",
      "Iteration 15, loss = 0.12881525\n",
      "Iteration 16, loss = 0.12196943\n",
      "Iteration 17, loss = 0.11574892\n",
      "Iteration 18, loss = 0.10939541\n",
      "Iteration 19, loss = 0.10373932\n",
      "Iteration 20, loss = 0.09886158\n",
      "Iteration 21, loss = 0.09390724\n",
      "Iteration 22, loss = 0.09008591\n",
      "Iteration 23, loss = 0.08554516\n",
      "Iteration 24, loss = 0.08158091\n",
      "Iteration 25, loss = 0.07751492\n",
      "Iteration 26, loss = 0.07390718\n",
      "Iteration 27, loss = 0.07038419\n",
      "Iteration 28, loss = 0.06681851\n",
      "Iteration 29, loss = 0.06394225\n",
      "Iteration 30, loss = 0.06113920\n",
      "Iteration 31, loss = 0.05836523\n",
      "Iteration 32, loss = 0.05602521\n",
      "Iteration 33, loss = 0.05347552\n",
      "Iteration 34, loss = 0.05110642\n",
      "Iteration 35, loss = 0.04891950\n",
      "Iteration 36, loss = 0.04702635\n",
      "Iteration 37, loss = 0.04471069\n",
      "Iteration 38, loss = 0.04334976\n",
      "Iteration 39, loss = 0.04139800\n",
      "Iteration 40, loss = 0.03918417\n",
      "Iteration 41, loss = 0.03755706\n",
      "Iteration 42, loss = 0.03633641\n",
      "Iteration 43, loss = 0.03453395\n",
      "Iteration 44, loss = 0.03289875\n",
      "Iteration 45, loss = 0.03195835\n",
      "Iteration 46, loss = 0.03009946\n",
      "Iteration 47, loss = 0.02918222\n",
      "Iteration 48, loss = 0.02776598\n",
      "Iteration 49, loss = 0.02672161\n",
      "Iteration 50, loss = 0.02556263\n",
      "Iteration 51, loss = 0.02430832\n",
      "Iteration 52, loss = 0.02339757\n",
      "Iteration 53, loss = 0.02231601\n",
      "Iteration 54, loss = 0.02138448\n",
      "Iteration 55, loss = 0.02038308\n",
      "Iteration 56, loss = 0.01963873\n",
      "Iteration 57, loss = 0.01867447\n",
      "Iteration 58, loss = 0.01771802\n",
      "Iteration 59, loss = 0.01699138\n",
      "Iteration 60, loss = 0.01627724\n",
      "Iteration 61, loss = 0.01556002\n",
      "Iteration 62, loss = 0.01500958\n",
      "Iteration 63, loss = 0.01430546\n",
      "Iteration 64, loss = 0.01395227\n",
      "Iteration 65, loss = 0.01316180\n",
      "Iteration 66, loss = 0.01263671\n",
      "Iteration 67, loss = 0.01197347\n",
      "Iteration 68, loss = 0.01168959\n",
      "Iteration 69, loss = 0.01108002\n",
      "Iteration 70, loss = 0.01076107\n",
      "Iteration 71, loss = 0.01028503\n",
      "Iteration 72, loss = 0.00980812\n",
      "Iteration 73, loss = 0.00948230\n",
      "Iteration 74, loss = 0.00910616\n",
      "Iteration 75, loss = 0.00877122\n",
      "Iteration 76, loss = 0.00834782\n",
      "Iteration 77, loss = 0.00822128\n",
      "Iteration 78, loss = 0.00781005\n",
      "Iteration 79, loss = 0.00752202\n",
      "Iteration 80, loss = 0.00728073\n",
      "Iteration 81, loss = 0.00711952\n",
      "Iteration 82, loss = 0.00681627\n",
      "Iteration 83, loss = 0.00652534\n",
      "Iteration 84, loss = 0.00622817\n",
      "Iteration 85, loss = 0.00608640\n",
      "Iteration 86, loss = 0.00592932\n",
      "Iteration 87, loss = 0.00577379\n",
      "Iteration 88, loss = 0.00556243\n",
      "Iteration 89, loss = 0.00532185\n",
      "Iteration 90, loss = 0.00523085\n",
      "Iteration 91, loss = 0.00509313\n",
      "Iteration 92, loss = 0.00490274\n",
      "Iteration 93, loss = 0.00479510\n",
      "Iteration 94, loss = 0.00456165\n",
      "Iteration 95, loss = 0.00446734\n",
      "Iteration 96, loss = 0.00439835\n",
      "Iteration 97, loss = 0.00419689\n",
      "Iteration 98, loss = 0.00416612\n",
      "Iteration 99, loss = 0.00403716\n",
      "Iteration 100, loss = 0.00391868\n",
      "Iteration 101, loss = 0.00387308\n",
      "Iteration 102, loss = 0.00376989\n",
      "Iteration 103, loss = 0.00366811\n",
      "Iteration 104, loss = 0.00361726\n",
      "Iteration 105, loss = 0.00352203\n",
      "Iteration 106, loss = 0.00347086\n",
      "Iteration 107, loss = 0.00343288\n",
      "Iteration 108, loss = 0.00331723\n",
      "Iteration 109, loss = 0.00323275\n",
      "Iteration 110, loss = 0.00318492\n",
      "Iteration 111, loss = 0.00314475\n",
      "Iteration 112, loss = 0.00312422\n",
      "Iteration 113, loss = 0.00302149\n",
      "Iteration 114, loss = 0.00296184\n",
      "Iteration 115, loss = 0.00290215\n",
      "Iteration 116, loss = 0.00286188\n",
      "Iteration 117, loss = 0.00285250\n",
      "Iteration 118, loss = 0.00277578\n",
      "Iteration 119, loss = 0.00273203\n",
      "Iteration 120, loss = 0.00270783\n",
      "Iteration 121, loss = 0.00265122\n",
      "Iteration 122, loss = 0.00261025\n",
      "Iteration 123, loss = 0.00258133\n",
      "Iteration 124, loss = 0.00251735\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.0001, batch_size=50, hidden_layer_sizes=(100, 100, 100), solver=sgd; total time= 6.0min\n",
      "Iteration 1, loss = 1.23038708\n",
      "Iteration 2, loss = 0.39509015\n",
      "Iteration 3, loss = 0.30884063\n",
      "Iteration 4, loss = 0.26914670\n",
      "Iteration 5, loss = 0.24195160\n",
      "Iteration 6, loss = 0.21942017\n",
      "Iteration 7, loss = 0.20205089\n",
      "Iteration 8, loss = 0.18587180\n",
      "Iteration 9, loss = 0.17177014\n",
      "Iteration 10, loss = 0.15982549\n",
      "Iteration 11, loss = 0.14937159\n",
      "Iteration 12, loss = 0.13981811\n",
      "Iteration 13, loss = 0.13098156\n",
      "Iteration 14, loss = 0.12315510\n",
      "Iteration 15, loss = 0.11638126\n",
      "Iteration 16, loss = 0.11014867\n",
      "Iteration 17, loss = 0.10356476\n",
      "Iteration 18, loss = 0.09798746\n",
      "Iteration 19, loss = 0.09241399\n",
      "Iteration 20, loss = 0.08788865\n",
      "Iteration 21, loss = 0.08380740\n",
      "Iteration 22, loss = 0.07933302\n",
      "Iteration 23, loss = 0.07520273\n",
      "Iteration 24, loss = 0.07189078\n",
      "Iteration 25, loss = 0.06792568\n",
      "Iteration 26, loss = 0.06513281\n",
      "Iteration 27, loss = 0.06137526\n",
      "Iteration 28, loss = 0.05876193\n",
      "Iteration 29, loss = 0.05556263\n",
      "Iteration 30, loss = 0.05357493\n",
      "Iteration 31, loss = 0.05058976\n",
      "Iteration 32, loss = 0.04857046\n",
      "Iteration 33, loss = 0.04598123\n",
      "Iteration 34, loss = 0.04366114\n",
      "Iteration 35, loss = 0.04172802\n",
      "Iteration 36, loss = 0.03982246\n",
      "Iteration 37, loss = 0.03802755\n",
      "Iteration 38, loss = 0.03590828\n",
      "Iteration 39, loss = 0.03423785\n",
      "Iteration 40, loss = 0.03253401\n",
      "Iteration 41, loss = 0.03089606\n",
      "Iteration 42, loss = 0.02967721\n",
      "Iteration 43, loss = 0.02819677\n",
      "Iteration 44, loss = 0.02660626\n",
      "Iteration 45, loss = 0.02551532\n",
      "Iteration 46, loss = 0.02440631\n",
      "Iteration 47, loss = 0.02331693\n",
      "Iteration 48, loss = 0.02200169\n",
      "Iteration 49, loss = 0.02082448\n",
      "Iteration 50, loss = 0.01992292\n",
      "Iteration 51, loss = 0.01873389\n",
      "Iteration 52, loss = 0.01819293\n",
      "Iteration 53, loss = 0.01722144\n",
      "Iteration 54, loss = 0.01643615\n",
      "Iteration 55, loss = 0.01573275\n",
      "Iteration 56, loss = 0.01489874\n",
      "Iteration 57, loss = 0.01426371\n",
      "Iteration 58, loss = 0.01356894\n",
      "Iteration 59, loss = 0.01274287\n",
      "Iteration 60, loss = 0.01264525\n",
      "Iteration 61, loss = 0.01187555\n",
      "Iteration 62, loss = 0.01127995\n",
      "Iteration 63, loss = 0.01078174\n",
      "Iteration 64, loss = 0.01043715\n",
      "Iteration 65, loss = 0.00996466\n",
      "Iteration 66, loss = 0.00951337\n",
      "Iteration 67, loss = 0.00915093\n",
      "Iteration 68, loss = 0.00880787\n",
      "Iteration 69, loss = 0.00843018\n",
      "Iteration 70, loss = 0.00809925\n",
      "Iteration 71, loss = 0.00774479\n",
      "Iteration 72, loss = 0.00754288\n",
      "Iteration 73, loss = 0.00723168\n",
      "Iteration 74, loss = 0.00700624\n",
      "Iteration 75, loss = 0.00661050\n",
      "Iteration 76, loss = 0.00651349\n",
      "Iteration 77, loss = 0.00622705\n",
      "Iteration 78, loss = 0.00602992\n",
      "Iteration 79, loss = 0.00590852\n",
      "Iteration 80, loss = 0.00565604\n",
      "Iteration 81, loss = 0.00545374\n",
      "Iteration 82, loss = 0.00531288\n",
      "Iteration 83, loss = 0.00512697\n",
      "Iteration 84, loss = 0.00495072\n",
      "Iteration 85, loss = 0.00483850\n",
      "Iteration 86, loss = 0.00468592\n",
      "Iteration 87, loss = 0.00458626\n",
      "Iteration 88, loss = 0.00445609\n",
      "Iteration 89, loss = 0.00432458\n",
      "Iteration 90, loss = 0.00419507\n",
      "Iteration 91, loss = 0.00410985\n",
      "Iteration 92, loss = 0.00400368\n",
      "Iteration 93, loss = 0.00391730\n",
      "Iteration 94, loss = 0.00381902\n",
      "Iteration 95, loss = 0.00373228\n",
      "Iteration 96, loss = 0.00364886\n",
      "Iteration 97, loss = 0.00355836\n",
      "Iteration 98, loss = 0.00348497\n",
      "Iteration 99, loss = 0.00342033\n",
      "Iteration 100, loss = 0.00333927\n",
      "Iteration 101, loss = 0.00328500\n",
      "Iteration 102, loss = 0.00320144\n",
      "Iteration 103, loss = 0.00315827\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.0001, batch_size=50, hidden_layer_sizes=(100, 100, 100), solver=sgd; total time= 3.9min\n",
      "Iteration 1, loss = 1.26882776\n",
      "Iteration 2, loss = 0.41104338\n",
      "Iteration 3, loss = 0.31608666\n",
      "Iteration 4, loss = 0.27473488\n",
      "Iteration 5, loss = 0.24741201\n",
      "Iteration 6, loss = 0.22501247\n",
      "Iteration 7, loss = 0.20676943\n",
      "Iteration 8, loss = 0.19182230\n",
      "Iteration 9, loss = 0.17673136\n",
      "Iteration 10, loss = 0.16498612\n",
      "Iteration 11, loss = 0.15471031\n",
      "Iteration 12, loss = 0.14481333\n",
      "Iteration 13, loss = 0.13651593\n",
      "Iteration 14, loss = 0.12855191\n",
      "Iteration 15, loss = 0.12143932\n",
      "Iteration 16, loss = 0.11559191\n",
      "Iteration 17, loss = 0.10878910\n",
      "Iteration 18, loss = 0.10350301\n",
      "Iteration 19, loss = 0.09861847\n",
      "Iteration 20, loss = 0.09367517\n",
      "Iteration 21, loss = 0.08950051\n",
      "Iteration 22, loss = 0.08508974\n",
      "Iteration 23, loss = 0.08083091\n",
      "Iteration 24, loss = 0.07727380\n",
      "Iteration 25, loss = 0.07327480\n",
      "Iteration 26, loss = 0.07055870\n",
      "Iteration 27, loss = 0.06692426\n",
      "Iteration 28, loss = 0.06421047\n",
      "Iteration 29, loss = 0.06125288\n",
      "Iteration 30, loss = 0.05880809\n",
      "Iteration 31, loss = 0.05578640\n",
      "Iteration 32, loss = 0.05311897\n",
      "Iteration 33, loss = 0.05119879\n",
      "Iteration 34, loss = 0.04918034\n",
      "Iteration 35, loss = 0.04665534\n",
      "Iteration 36, loss = 0.04485719\n",
      "Iteration 37, loss = 0.04280463\n",
      "Iteration 38, loss = 0.04060350\n",
      "Iteration 39, loss = 0.03921818\n",
      "Iteration 40, loss = 0.03764252\n",
      "Iteration 41, loss = 0.03606127\n",
      "Iteration 42, loss = 0.03421750\n",
      "Iteration 43, loss = 0.03292709\n",
      "Iteration 44, loss = 0.03145057\n",
      "Iteration 45, loss = 0.03003550\n",
      "Iteration 46, loss = 0.02862434\n",
      "Iteration 47, loss = 0.02748497\n",
      "Iteration 48, loss = 0.02611920\n",
      "Iteration 49, loss = 0.02519344\n",
      "Iteration 50, loss = 0.02365150\n",
      "Iteration 51, loss = 0.02247965\n",
      "Iteration 52, loss = 0.02201320\n",
      "Iteration 53, loss = 0.02092719\n",
      "Iteration 54, loss = 0.02010689\n",
      "Iteration 55, loss = 0.01907546\n",
      "Iteration 56, loss = 0.01828656\n",
      "Iteration 57, loss = 0.01755948\n",
      "Iteration 58, loss = 0.01676072\n",
      "Iteration 59, loss = 0.01628853\n",
      "Iteration 60, loss = 0.01544334\n",
      "Iteration 61, loss = 0.01483821\n",
      "Iteration 62, loss = 0.01419813\n",
      "Iteration 63, loss = 0.01361604\n",
      "Iteration 64, loss = 0.01312166\n",
      "Iteration 65, loss = 0.01263727\n",
      "Iteration 66, loss = 0.01213678\n",
      "Iteration 67, loss = 0.01156553\n",
      "Iteration 68, loss = 0.01121087\n",
      "Iteration 69, loss = 0.01076781\n",
      "Iteration 70, loss = 0.01018063\n",
      "Iteration 71, loss = 0.00995274\n",
      "Iteration 72, loss = 0.00961356\n",
      "Iteration 73, loss = 0.00932397\n",
      "Iteration 74, loss = 0.00894781\n",
      "Iteration 75, loss = 0.00865772\n",
      "Iteration 76, loss = 0.00828221\n",
      "Iteration 77, loss = 0.00800431\n",
      "Iteration 78, loss = 0.00777458\n",
      "Iteration 79, loss = 0.00738734\n",
      "Iteration 80, loss = 0.00724008\n",
      "Iteration 81, loss = 0.00704438\n",
      "Iteration 82, loss = 0.00678548\n",
      "Iteration 83, loss = 0.00662704\n",
      "Iteration 84, loss = 0.00629476\n",
      "Iteration 85, loss = 0.00610743\n",
      "Iteration 86, loss = 0.00601234\n",
      "Iteration 87, loss = 0.00580727\n",
      "Iteration 88, loss = 0.00553212\n",
      "Iteration 89, loss = 0.00540233\n",
      "Iteration 90, loss = 0.00517073\n",
      "Iteration 91, loss = 0.00505343\n",
      "Iteration 92, loss = 0.00491737\n",
      "Iteration 93, loss = 0.00474568\n",
      "Iteration 94, loss = 0.00469573\n",
      "Iteration 95, loss = 0.00460585\n",
      "Iteration 96, loss = 0.00445975\n",
      "Iteration 97, loss = 0.00431620\n",
      "Iteration 98, loss = 0.00419300\n",
      "Iteration 99, loss = 0.00407313\n",
      "Iteration 100, loss = 0.00396747\n",
      "Iteration 101, loss = 0.00391183\n",
      "Iteration 102, loss = 0.00380887\n",
      "Iteration 103, loss = 0.00372226\n",
      "Iteration 104, loss = 0.00362054\n",
      "Iteration 105, loss = 0.00353342\n",
      "Iteration 106, loss = 0.00348408\n",
      "Iteration 107, loss = 0.00339529\n",
      "Iteration 108, loss = 0.00333990\n",
      "Iteration 109, loss = 0.00325639\n",
      "Iteration 110, loss = 0.00325078\n",
      "Iteration 111, loss = 0.00315605\n",
      "Iteration 112, loss = 0.00309460\n",
      "Iteration 113, loss = 0.00302357\n",
      "Iteration 114, loss = 0.00298652\n",
      "Iteration 115, loss = 0.00291544\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.0001, batch_size=50, hidden_layer_sizes=(100, 100, 100), solver=sgd; total time= 5.9min\n",
      "Iteration 1, loss = 1.23423327\n",
      "Iteration 2, loss = 0.40537748\n",
      "Iteration 3, loss = 0.32425941\n",
      "Iteration 4, loss = 0.28426940\n",
      "Iteration 5, loss = 0.25728653\n",
      "Iteration 6, loss = 0.23492463\n",
      "Iteration 7, loss = 0.21699679\n",
      "Iteration 8, loss = 0.20112250\n",
      "Iteration 9, loss = 0.18607102\n",
      "Iteration 10, loss = 0.17350406\n",
      "Iteration 11, loss = 0.16255486\n",
      "Iteration 12, loss = 0.15251465\n",
      "Iteration 13, loss = 0.14270310\n",
      "Iteration 14, loss = 0.13525819\n",
      "Iteration 15, loss = 0.12684647\n",
      "Iteration 16, loss = 0.12035467\n",
      "Iteration 17, loss = 0.11435630\n",
      "Iteration 18, loss = 0.10892496\n",
      "Iteration 19, loss = 0.10272031\n",
      "Iteration 20, loss = 0.09832751\n",
      "Iteration 21, loss = 0.09366852\n",
      "Iteration 22, loss = 0.08908710\n",
      "Iteration 23, loss = 0.08537821\n",
      "Iteration 24, loss = 0.08126750\n",
      "Iteration 25, loss = 0.07766081\n",
      "Iteration 26, loss = 0.07431124\n",
      "Iteration 27, loss = 0.07100011\n",
      "Iteration 28, loss = 0.06782410\n",
      "Iteration 29, loss = 0.06532293\n",
      "Iteration 30, loss = 0.06207833\n",
      "Iteration 31, loss = 0.05958097\n",
      "Iteration 32, loss = 0.05719650\n",
      "Iteration 33, loss = 0.05493088\n",
      "Iteration 34, loss = 0.05237782\n",
      "Iteration 35, loss = 0.05017575\n",
      "Iteration 36, loss = 0.04800837\n",
      "Iteration 37, loss = 0.04626619\n",
      "Iteration 38, loss = 0.04426690\n",
      "Iteration 39, loss = 0.04251939\n",
      "Iteration 40, loss = 0.04016649\n",
      "Iteration 41, loss = 0.03860944\n",
      "Iteration 42, loss = 0.03719281\n",
      "Iteration 43, loss = 0.03538677\n",
      "Iteration 44, loss = 0.03406590\n",
      "Iteration 45, loss = 0.03273477\n",
      "Iteration 46, loss = 0.03132939\n",
      "Iteration 47, loss = 0.02967627\n",
      "Iteration 48, loss = 0.02836865\n",
      "Iteration 49, loss = 0.02732409\n",
      "Iteration 50, loss = 0.02660090\n",
      "Iteration 51, loss = 0.02509019\n",
      "Iteration 52, loss = 0.02416875\n",
      "Iteration 53, loss = 0.02310409\n",
      "Iteration 54, loss = 0.02217198\n",
      "Iteration 55, loss = 0.02100320\n",
      "Iteration 56, loss = 0.01997801\n",
      "Iteration 57, loss = 0.01962212\n",
      "Iteration 58, loss = 0.01854636\n",
      "Iteration 59, loss = 0.01773391\n",
      "Iteration 60, loss = 0.01704449\n",
      "Iteration 61, loss = 0.01636411\n",
      "Iteration 62, loss = 0.01593518\n",
      "Iteration 63, loss = 0.01500433\n",
      "Iteration 64, loss = 0.01469766\n",
      "Iteration 65, loss = 0.01383371\n",
      "Iteration 66, loss = 0.01344407\n",
      "Iteration 67, loss = 0.01285858\n",
      "Iteration 68, loss = 0.01238506\n",
      "Iteration 69, loss = 0.01164510\n",
      "Iteration 70, loss = 0.01132466\n",
      "Iteration 71, loss = 0.01087241\n",
      "Iteration 72, loss = 0.01035994\n",
      "Iteration 73, loss = 0.01009765\n",
      "Iteration 74, loss = 0.00960788\n",
      "Iteration 75, loss = 0.00923935\n",
      "Iteration 76, loss = 0.00875230\n",
      "Iteration 77, loss = 0.00852709\n",
      "Iteration 78, loss = 0.00821855\n",
      "Iteration 79, loss = 0.00786769\n",
      "Iteration 80, loss = 0.00755817\n",
      "Iteration 81, loss = 0.00739925\n",
      "Iteration 82, loss = 0.00716019\n",
      "Iteration 83, loss = 0.00694596\n",
      "Iteration 84, loss = 0.00669319\n",
      "Iteration 85, loss = 0.00645834\n",
      "Iteration 86, loss = 0.00634367\n",
      "Iteration 87, loss = 0.00612868\n",
      "Iteration 88, loss = 0.00595257\n",
      "Iteration 89, loss = 0.00572712\n",
      "Iteration 90, loss = 0.00551695\n",
      "Iteration 91, loss = 0.00539175\n",
      "Iteration 92, loss = 0.00523591\n",
      "Iteration 93, loss = 0.00510538\n",
      "Iteration 94, loss = 0.00490719\n",
      "Iteration 95, loss = 0.00483645\n",
      "Iteration 96, loss = 0.00465123\n",
      "Iteration 97, loss = 0.00458987\n",
      "Iteration 98, loss = 0.00443050\n",
      "Iteration 99, loss = 0.00436010\n",
      "Iteration 100, loss = 0.00415666\n",
      "Iteration 101, loss = 0.00409276\n",
      "Iteration 102, loss = 0.00393107\n",
      "Iteration 103, loss = 0.00387434\n",
      "Iteration 104, loss = 0.00381177\n",
      "Iteration 105, loss = 0.00370158\n",
      "Iteration 106, loss = 0.00361527\n",
      "Iteration 107, loss = 0.00356636\n",
      "Iteration 108, loss = 0.00348115\n",
      "Iteration 109, loss = 0.00342054\n",
      "Iteration 110, loss = 0.00333511\n",
      "Iteration 111, loss = 0.00328556\n",
      "Iteration 112, loss = 0.00321920\n",
      "Iteration 113, loss = 0.00314924\n",
      "Iteration 114, loss = 0.00312864\n",
      "Iteration 115, loss = 0.00302598\n",
      "Iteration 116, loss = 0.00302763\n",
      "Iteration 117, loss = 0.00292117\n",
      "Iteration 118, loss = 0.00288517\n",
      "Iteration 119, loss = 0.00283169\n",
      "Iteration 120, loss = 0.00277964\n",
      "Iteration 121, loss = 0.00272972\n",
      "Iteration 122, loss = 0.00272778\n",
      "Iteration 123, loss = 0.00265594\n",
      "Iteration 124, loss = 0.00261898\n",
      "Iteration 125, loss = 0.00259186\n",
      "Iteration 126, loss = 0.00254425\n",
      "Iteration 127, loss = 0.00250776\n",
      "Iteration 128, loss = 0.00246530\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.0001, batch_size=50, hidden_layer_sizes=(100, 100, 100), solver=sgd; total time= 6.4min\n",
      "Iteration 1, loss = 0.44010045\n",
      "Iteration 2, loss = 0.21290851\n",
      "Iteration 3, loss = 0.15695511\n",
      "Iteration 4, loss = 0.12619645\n",
      "Iteration 5, loss = 0.10179730\n",
      "Iteration 6, loss = 0.08348774\n",
      "Iteration 7, loss = 0.07139945\n",
      "Iteration 8, loss = 0.05993107\n",
      "Iteration 9, loss = 0.05234071\n",
      "Iteration 10, loss = 0.04513456\n",
      "Iteration 11, loss = 0.03785297\n",
      "Iteration 12, loss = 0.03278165\n",
      "Iteration 13, loss = 0.02776231\n",
      "Iteration 14, loss = 0.02447871\n",
      "Iteration 15, loss = 0.02057772\n",
      "Iteration 16, loss = 0.01769944\n",
      "Iteration 17, loss = 0.01615115\n",
      "Iteration 18, loss = 0.01345415\n",
      "Iteration 19, loss = 0.01179593\n",
      "Iteration 20, loss = 0.01107783\n",
      "Iteration 21, loss = 0.00881482\n",
      "Iteration 22, loss = 0.00816393\n",
      "Iteration 23, loss = 0.00713906\n",
      "Iteration 24, loss = 0.00736363\n",
      "Iteration 25, loss = 0.00513567\n",
      "Iteration 26, loss = 0.00409706\n",
      "Iteration 27, loss = 0.00500954\n",
      "Iteration 28, loss = 0.00649008\n",
      "Iteration 29, loss = 0.00635702\n",
      "Iteration 30, loss = 0.00372607\n",
      "Iteration 31, loss = 0.00255472\n",
      "Iteration 32, loss = 0.00205216\n",
      "Iteration 33, loss = 0.00194743\n",
      "Iteration 34, loss = 0.00191568\n",
      "Iteration 35, loss = 0.00298622\n",
      "Iteration 36, loss = 0.01405495\n",
      "Iteration 37, loss = 0.00457409\n",
      "Iteration 38, loss = 0.00204045\n",
      "Iteration 39, loss = 0.00160417\n",
      "Iteration 40, loss = 0.00150480\n",
      "Iteration 41, loss = 0.00142047\n",
      "Iteration 42, loss = 0.00137385\n",
      "Iteration 43, loss = 0.00133071\n",
      "Iteration 44, loss = 0.00129297\n",
      "Iteration 45, loss = 0.00126476\n",
      "Iteration 46, loss = 0.00124911\n",
      "Iteration 47, loss = 0.00379008\n",
      "Iteration 48, loss = 0.01852104\n",
      "Iteration 49, loss = 0.00323990\n",
      "Iteration 50, loss = 0.00147399\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.0001, batch_size=100, hidden_layer_sizes=(100,), solver=adam; total time= 1.5min\n",
      "Iteration 1, loss = 0.43490978\n",
      "Iteration 2, loss = 0.21196406\n",
      "Iteration 3, loss = 0.16044910\n",
      "Iteration 4, loss = 0.12684723\n",
      "Iteration 5, loss = 0.10507885\n",
      "Iteration 6, loss = 0.08681819\n",
      "Iteration 7, loss = 0.07471760\n",
      "Iteration 8, loss = 0.06373639\n",
      "Iteration 9, loss = 0.05465775\n",
      "Iteration 10, loss = 0.04810062\n",
      "Iteration 11, loss = 0.04071237\n",
      "Iteration 12, loss = 0.03580880\n",
      "Iteration 13, loss = 0.03106343\n",
      "Iteration 14, loss = 0.02716761\n",
      "Iteration 15, loss = 0.02370926\n",
      "Iteration 16, loss = 0.01958226\n",
      "Iteration 17, loss = 0.01779913\n",
      "Iteration 18, loss = 0.01525878\n",
      "Iteration 19, loss = 0.01362448\n",
      "Iteration 20, loss = 0.01235357\n",
      "Iteration 21, loss = 0.01073253\n",
      "Iteration 22, loss = 0.00884173\n",
      "Iteration 23, loss = 0.00765665\n",
      "Iteration 24, loss = 0.00636891\n",
      "Iteration 25, loss = 0.00609768\n",
      "Iteration 26, loss = 0.00524763\n",
      "Iteration 27, loss = 0.00454240\n",
      "Iteration 28, loss = 0.00877091\n",
      "Iteration 29, loss = 0.00509150\n",
      "Iteration 30, loss = 0.00365188\n",
      "Iteration 31, loss = 0.00326888\n",
      "Iteration 32, loss = 0.00262750\n",
      "Iteration 33, loss = 0.00208761\n",
      "Iteration 34, loss = 0.00204967\n",
      "Iteration 35, loss = 0.00194898\n",
      "Iteration 36, loss = 0.01418927\n",
      "Iteration 37, loss = 0.00580718\n",
      "Iteration 38, loss = 0.00230503\n",
      "Iteration 39, loss = 0.00166321\n",
      "Iteration 40, loss = 0.00154291\n",
      "Iteration 41, loss = 0.00148096\n",
      "Iteration 42, loss = 0.00141684\n",
      "Iteration 43, loss = 0.00137146\n",
      "Iteration 44, loss = 0.00133590\n",
      "Iteration 45, loss = 0.00194750\n",
      "Iteration 46, loss = 0.01781273\n",
      "Iteration 47, loss = 0.00378540\n",
      "Iteration 48, loss = 0.00211526\n",
      "Iteration 49, loss = 0.00154699\n",
      "Iteration 50, loss = 0.00135583\n",
      "Iteration 51, loss = 0.00130718\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.0001, batch_size=100, hidden_layer_sizes=(100,), solver=adam; total time= 1.5min\n",
      "Iteration 1, loss = 0.43796279\n",
      "Iteration 2, loss = 0.21245118\n",
      "Iteration 3, loss = 0.15759914\n",
      "Iteration 4, loss = 0.12593899\n",
      "Iteration 5, loss = 0.10329592\n",
      "Iteration 6, loss = 0.08645899\n",
      "Iteration 7, loss = 0.07334566\n",
      "Iteration 8, loss = 0.06189502\n",
      "Iteration 9, loss = 0.05386098\n",
      "Iteration 10, loss = 0.04642747\n",
      "Iteration 11, loss = 0.03977950\n",
      "Iteration 12, loss = 0.03348164\n",
      "Iteration 13, loss = 0.02978229\n",
      "Iteration 14, loss = 0.02510944\n",
      "Iteration 15, loss = 0.02080380\n",
      "Iteration 16, loss = 0.01887662\n",
      "Iteration 17, loss = 0.01631596\n",
      "Iteration 18, loss = 0.01342144\n",
      "Iteration 19, loss = 0.01149419\n",
      "Iteration 20, loss = 0.00976237\n",
      "Iteration 21, loss = 0.00850215\n",
      "Iteration 22, loss = 0.00740316\n",
      "Iteration 23, loss = 0.00746640\n",
      "Iteration 24, loss = 0.00544623\n",
      "Iteration 25, loss = 0.00435477\n",
      "Iteration 26, loss = 0.00586601\n",
      "Iteration 27, loss = 0.01038162\n",
      "Iteration 28, loss = 0.00666331\n",
      "Iteration 29, loss = 0.00379025\n",
      "Iteration 30, loss = 0.00275475\n",
      "Iteration 31, loss = 0.00224299\n",
      "Iteration 32, loss = 0.00220225\n",
      "Iteration 33, loss = 0.00842791\n",
      "Iteration 34, loss = 0.00581723\n",
      "Iteration 35, loss = 0.00237949\n",
      "Iteration 36, loss = 0.00178660\n",
      "Iteration 37, loss = 0.00160251\n",
      "Iteration 38, loss = 0.00152978\n",
      "Iteration 39, loss = 0.00145901\n",
      "Iteration 40, loss = 0.00139642\n",
      "Iteration 41, loss = 0.00139184\n",
      "Iteration 42, loss = 0.01080623\n",
      "Iteration 43, loss = 0.00895128\n",
      "Iteration 44, loss = 0.00303771\n",
      "Iteration 45, loss = 0.00165460\n",
      "Iteration 46, loss = 0.00142189\n",
      "Iteration 47, loss = 0.00134139\n",
      "Iteration 48, loss = 0.00130101\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.0001, batch_size=100, hidden_layer_sizes=(100,), solver=adam; total time= 1.3min\n",
      "Iteration 1, loss = 0.46155120\n",
      "Iteration 2, loss = 0.21520017\n",
      "Iteration 3, loss = 0.15995140\n",
      "Iteration 4, loss = 0.12743668\n",
      "Iteration 5, loss = 0.10446111\n",
      "Iteration 6, loss = 0.08734975\n",
      "Iteration 7, loss = 0.07348428\n",
      "Iteration 8, loss = 0.06301125\n",
      "Iteration 9, loss = 0.05332057\n",
      "Iteration 10, loss = 0.04615684\n",
      "Iteration 11, loss = 0.04020404\n",
      "Iteration 12, loss = 0.03484545\n",
      "Iteration 13, loss = 0.02992325\n",
      "Iteration 14, loss = 0.02539874\n",
      "Iteration 15, loss = 0.02225844\n",
      "Iteration 16, loss = 0.01979809\n",
      "Iteration 17, loss = 0.01641803\n",
      "Iteration 18, loss = 0.01396286\n",
      "Iteration 19, loss = 0.01362818\n",
      "Iteration 20, loss = 0.01121324\n",
      "Iteration 21, loss = 0.00881738\n",
      "Iteration 22, loss = 0.00801504\n",
      "Iteration 23, loss = 0.00871319\n",
      "Iteration 24, loss = 0.00676263\n",
      "Iteration 25, loss = 0.00475273\n",
      "Iteration 26, loss = 0.00411617\n",
      "Iteration 27, loss = 0.00475016\n",
      "Iteration 28, loss = 0.00473145\n",
      "Iteration 29, loss = 0.00863258\n",
      "Iteration 30, loss = 0.00592620\n",
      "Iteration 31, loss = 0.00268832\n",
      "Iteration 32, loss = 0.00218215\n",
      "Iteration 33, loss = 0.00192099\n",
      "Iteration 34, loss = 0.00184415\n",
      "Iteration 35, loss = 0.00186632\n",
      "Iteration 36, loss = 0.00172304\n",
      "Iteration 37, loss = 0.00159813\n",
      "Iteration 38, loss = 0.00251723\n",
      "Iteration 39, loss = 0.02122252\n",
      "Iteration 40, loss = 0.00500193\n",
      "Iteration 41, loss = 0.00220647\n",
      "Iteration 42, loss = 0.00155894\n",
      "Iteration 43, loss = 0.00145938\n",
      "Iteration 44, loss = 0.00139771\n",
      "Iteration 45, loss = 0.00135767\n",
      "Iteration 46, loss = 0.00131689\n",
      "Iteration 47, loss = 0.00128235\n",
      "Iteration 48, loss = 0.00125869\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.0001, batch_size=100, hidden_layer_sizes=(100,), solver=adam; total time= 1.3min\n",
      "Iteration 1, loss = 0.45008473\n",
      "Iteration 2, loss = 0.20689167\n",
      "Iteration 3, loss = 0.15479705\n",
      "Iteration 4, loss = 0.12299223\n",
      "Iteration 5, loss = 0.10080063\n",
      "Iteration 6, loss = 0.08491419\n",
      "Iteration 7, loss = 0.07083993\n",
      "Iteration 8, loss = 0.05990751\n",
      "Iteration 9, loss = 0.05187219\n",
      "Iteration 10, loss = 0.04443376\n",
      "Iteration 11, loss = 0.03742322\n",
      "Iteration 12, loss = 0.03214887\n",
      "Iteration 13, loss = 0.02788542\n",
      "Iteration 14, loss = 0.02417102\n",
      "Iteration 15, loss = 0.02211762\n",
      "Iteration 16, loss = 0.01854196\n",
      "Iteration 17, loss = 0.01650112\n",
      "Iteration 18, loss = 0.01301447\n",
      "Iteration 19, loss = 0.01126862\n",
      "Iteration 20, loss = 0.01045092\n",
      "Iteration 21, loss = 0.00793166\n",
      "Iteration 22, loss = 0.00795430\n",
      "Iteration 23, loss = 0.00655094\n",
      "Iteration 24, loss = 0.00604740\n",
      "Iteration 25, loss = 0.00473502\n",
      "Iteration 26, loss = 0.00456054\n",
      "Iteration 27, loss = 0.00570370\n",
      "Iteration 28, loss = 0.00468696\n",
      "Iteration 29, loss = 0.00639745\n",
      "Iteration 30, loss = 0.00345174\n",
      "Iteration 31, loss = 0.00284665\n",
      "Iteration 32, loss = 0.00206782\n",
      "Iteration 33, loss = 0.00799147\n",
      "Iteration 34, loss = 0.00531457\n",
      "Iteration 35, loss = 0.00243423\n",
      "Iteration 36, loss = 0.00172601\n",
      "Iteration 37, loss = 0.00154491\n",
      "Iteration 38, loss = 0.00152842\n",
      "Iteration 39, loss = 0.00143766\n",
      "Iteration 40, loss = 0.00139906\n",
      "Iteration 41, loss = 0.00142716\n",
      "Iteration 42, loss = 0.00138521\n",
      "Iteration 43, loss = 0.01464852\n",
      "Iteration 44, loss = 0.00708602\n",
      "Iteration 45, loss = 0.00196768\n",
      "Iteration 46, loss = 0.00144331\n",
      "Iteration 47, loss = 0.00133556\n",
      "Iteration 48, loss = 0.00128569\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.0001, batch_size=100, hidden_layer_sizes=(100,), solver=adam; total time= 1.7min\n",
      "Iteration 1, loss = 1.41588085\n",
      "Iteration 2, loss = 0.68140217\n",
      "Iteration 3, loss = 0.51420183\n",
      "Iteration 4, loss = 0.44238952\n",
      "Iteration 5, loss = 0.40144805\n",
      "Iteration 6, loss = 0.37414189\n",
      "Iteration 7, loss = 0.35433170\n",
      "Iteration 8, loss = 0.33888690\n",
      "Iteration 9, loss = 0.32624398\n",
      "Iteration 10, loss = 0.31570016\n",
      "Iteration 11, loss = 0.30642464\n",
      "Iteration 12, loss = 0.29824157\n",
      "Iteration 13, loss = 0.29084027\n",
      "Iteration 14, loss = 0.28414615\n",
      "Iteration 15, loss = 0.27787876\n",
      "Iteration 16, loss = 0.27196004\n",
      "Iteration 17, loss = 0.26676943\n",
      "Iteration 18, loss = 0.26161888\n",
      "Iteration 19, loss = 0.25681383\n",
      "Iteration 20, loss = 0.25213473\n",
      "Iteration 21, loss = 0.24789311\n",
      "Iteration 22, loss = 0.24379573\n",
      "Iteration 23, loss = 0.23969155\n",
      "Iteration 24, loss = 0.23579133\n",
      "Iteration 25, loss = 0.23226475\n",
      "Iteration 26, loss = 0.22855627\n",
      "Iteration 27, loss = 0.22521963\n",
      "Iteration 28, loss = 0.22183337\n",
      "Iteration 29, loss = 0.21850635\n",
      "Iteration 30, loss = 0.21534024\n",
      "Iteration 31, loss = 0.21236974\n",
      "Iteration 32, loss = 0.20946349\n",
      "Iteration 33, loss = 0.20644672\n",
      "Iteration 34, loss = 0.20352768\n",
      "Iteration 35, loss = 0.20111810\n",
      "Iteration 36, loss = 0.19825162\n",
      "Iteration 37, loss = 0.19565836\n",
      "Iteration 38, loss = 0.19309529\n",
      "Iteration 39, loss = 0.19059605\n",
      "Iteration 40, loss = 0.18814111\n",
      "Iteration 41, loss = 0.18585842\n",
      "Iteration 42, loss = 0.18356605\n",
      "Iteration 43, loss = 0.18127410\n",
      "Iteration 44, loss = 0.17914933\n",
      "Iteration 45, loss = 0.17694082\n",
      "Iteration 46, loss = 0.17487158\n",
      "Iteration 47, loss = 0.17302551\n",
      "Iteration 48, loss = 0.17081849\n",
      "Iteration 49, loss = 0.16887567\n",
      "Iteration 50, loss = 0.16694738\n",
      "Iteration 51, loss = 0.16498872\n",
      "Iteration 52, loss = 0.16327174\n",
      "Iteration 53, loss = 0.16119577\n",
      "Iteration 54, loss = 0.15962276\n",
      "Iteration 55, loss = 0.15790222\n",
      "Iteration 56, loss = 0.15613437\n",
      "Iteration 57, loss = 0.15443110\n",
      "Iteration 58, loss = 0.15269006\n",
      "Iteration 59, loss = 0.15110363\n",
      "Iteration 60, loss = 0.14958204\n",
      "Iteration 61, loss = 0.14787219\n",
      "Iteration 62, loss = 0.14652866\n",
      "Iteration 63, loss = 0.14504876\n",
      "Iteration 64, loss = 0.14360706\n",
      "Iteration 65, loss = 0.14208116\n",
      "Iteration 66, loss = 0.14053965\n",
      "Iteration 67, loss = 0.13923826\n",
      "Iteration 68, loss = 0.13788760\n",
      "Iteration 69, loss = 0.13652932\n",
      "Iteration 70, loss = 0.13518443\n",
      "Iteration 71, loss = 0.13381188\n",
      "Iteration 72, loss = 0.13251470\n",
      "Iteration 73, loss = 0.13132255\n",
      "Iteration 74, loss = 0.12997545\n",
      "Iteration 75, loss = 0.12888914\n",
      "Iteration 76, loss = 0.12760116\n",
      "Iteration 77, loss = 0.12644288\n",
      "Iteration 78, loss = 0.12523966\n",
      "Iteration 79, loss = 0.12396890\n",
      "Iteration 80, loss = 0.12307209\n",
      "Iteration 81, loss = 0.12183449\n",
      "Iteration 82, loss = 0.12061771\n",
      "Iteration 83, loss = 0.11973173\n",
      "Iteration 84, loss = 0.11871083\n",
      "Iteration 85, loss = 0.11762395\n",
      "Iteration 86, loss = 0.11657828\n",
      "Iteration 87, loss = 0.11551664\n",
      "Iteration 88, loss = 0.11439498\n",
      "Iteration 89, loss = 0.11350824\n",
      "Iteration 90, loss = 0.11264784\n",
      "Iteration 91, loss = 0.11166909\n",
      "Iteration 92, loss = 0.11059968\n",
      "Iteration 93, loss = 0.10979753\n",
      "Iteration 94, loss = 0.10882320\n",
      "Iteration 95, loss = 0.10795525\n",
      "Iteration 96, loss = 0.10704083\n",
      "Iteration 97, loss = 0.10625861\n",
      "Iteration 98, loss = 0.10532668\n",
      "Iteration 99, loss = 0.10440283\n",
      "Iteration 100, loss = 0.10354960\n",
      "Iteration 101, loss = 0.10282494\n",
      "Iteration 102, loss = 0.10198649\n",
      "Iteration 103, loss = 0.10106140\n",
      "Iteration 104, loss = 0.10044030\n",
      "Iteration 105, loss = 0.09955635\n",
      "Iteration 106, loss = 0.09877138\n",
      "Iteration 107, loss = 0.09806045\n",
      "Iteration 108, loss = 0.09721081\n",
      "Iteration 109, loss = 0.09657259\n",
      "Iteration 110, loss = 0.09580939\n",
      "Iteration 111, loss = 0.09509589\n",
      "Iteration 112, loss = 0.09432869\n",
      "Iteration 113, loss = 0.09364102\n",
      "Iteration 114, loss = 0.09286555\n",
      "Iteration 115, loss = 0.09223255\n",
      "Iteration 116, loss = 0.09148905\n",
      "Iteration 117, loss = 0.09088282\n",
      "Iteration 118, loss = 0.09015672\n",
      "Iteration 119, loss = 0.08952902\n",
      "Iteration 120, loss = 0.08879885\n",
      "Iteration 121, loss = 0.08816326\n",
      "Iteration 122, loss = 0.08756216\n",
      "Iteration 123, loss = 0.08683585\n",
      "Iteration 124, loss = 0.08633270\n",
      "Iteration 125, loss = 0.08568485\n",
      "Iteration 126, loss = 0.08506743\n",
      "Iteration 127, loss = 0.08444153\n",
      "Iteration 128, loss = 0.08388651\n",
      "Iteration 129, loss = 0.08328064\n",
      "Iteration 130, loss = 0.08267248\n",
      "Iteration 131, loss = 0.08212841\n",
      "Iteration 132, loss = 0.08154621\n",
      "Iteration 133, loss = 0.08098613\n",
      "Iteration 134, loss = 0.08044342\n",
      "Iteration 135, loss = 0.07986671\n",
      "Iteration 136, loss = 0.07931007\n",
      "Iteration 137, loss = 0.07872118\n",
      "Iteration 138, loss = 0.07820050\n",
      "Iteration 139, loss = 0.07768379\n",
      "Iteration 140, loss = 0.07721055\n",
      "Iteration 141, loss = 0.07670714\n",
      "Iteration 142, loss = 0.07615780\n",
      "Iteration 143, loss = 0.07559747\n",
      "Iteration 144, loss = 0.07507816\n",
      "Iteration 145, loss = 0.07466550\n",
      "Iteration 146, loss = 0.07418212\n",
      "Iteration 147, loss = 0.07368940\n",
      "Iteration 148, loss = 0.07317121\n",
      "Iteration 149, loss = 0.07266396\n",
      "Iteration 150, loss = 0.07232745\n",
      "Iteration 151, loss = 0.07174391\n",
      "Iteration 152, loss = 0.07125832\n",
      "Iteration 153, loss = 0.07087034\n",
      "Iteration 154, loss = 0.07044597\n",
      "Iteration 155, loss = 0.07000685\n",
      "Iteration 156, loss = 0.06960581\n",
      "Iteration 157, loss = 0.06905940\n",
      "Iteration 158, loss = 0.06871401\n",
      "Iteration 159, loss = 0.06825509\n",
      "Iteration 160, loss = 0.06786849\n",
      "Iteration 161, loss = 0.06740098\n",
      "Iteration 162, loss = 0.06696710\n",
      "Iteration 163, loss = 0.06655823\n",
      "Iteration 164, loss = 0.06619954\n",
      "Iteration 165, loss = 0.06575030\n",
      "Iteration 166, loss = 0.06546258\n",
      "Iteration 167, loss = 0.06488902\n",
      "Iteration 168, loss = 0.06455749\n",
      "Iteration 169, loss = 0.06416991\n",
      "Iteration 170, loss = 0.06384942\n",
      "Iteration 171, loss = 0.06345168\n",
      "Iteration 172, loss = 0.06307689\n",
      "Iteration 173, loss = 0.06274771\n",
      "Iteration 174, loss = 0.06225580\n",
      "Iteration 175, loss = 0.06195186\n",
      "Iteration 176, loss = 0.06154323\n",
      "Iteration 177, loss = 0.06122719\n",
      "Iteration 178, loss = 0.06084560\n",
      "Iteration 179, loss = 0.06043423\n",
      "Iteration 180, loss = 0.06014074\n",
      "Iteration 181, loss = 0.05989674\n",
      "Iteration 182, loss = 0.05946526\n",
      "Iteration 183, loss = 0.05911474\n",
      "Iteration 184, loss = 0.05878453\n",
      "Iteration 185, loss = 0.05844206\n",
      "Iteration 186, loss = 0.05802342\n",
      "Iteration 187, loss = 0.05779718\n",
      "Iteration 188, loss = 0.05738304\n",
      "Iteration 189, loss = 0.05709729\n",
      "Iteration 190, loss = 0.05679400\n",
      "Iteration 191, loss = 0.05640196\n",
      "Iteration 192, loss = 0.05612205\n",
      "Iteration 193, loss = 0.05575583\n",
      "Iteration 194, loss = 0.05555362\n",
      "Iteration 195, loss = 0.05520375\n",
      "Iteration 196, loss = 0.05492649\n",
      "Iteration 197, loss = 0.05458454\n",
      "Iteration 198, loss = 0.05433539\n",
      "Iteration 199, loss = 0.05398425\n",
      "Iteration 200, loss = 0.05366850\n",
      "[CV] END activation=relu, alpha=0.0001, batch_size=100, hidden_layer_sizes=(100,), solver=sgd; total time= 4.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.40075408\n",
      "Iteration 2, loss = 0.66020750\n",
      "Iteration 3, loss = 0.50244213\n",
      "Iteration 4, loss = 0.43584958\n",
      "Iteration 5, loss = 0.39768679\n",
      "Iteration 6, loss = 0.37178343\n",
      "Iteration 7, loss = 0.35293999\n",
      "Iteration 8, loss = 0.33783943\n",
      "Iteration 9, loss = 0.32563913\n",
      "Iteration 10, loss = 0.31512441\n",
      "Iteration 11, loss = 0.30631140\n",
      "Iteration 12, loss = 0.29816891\n",
      "Iteration 13, loss = 0.29126391\n",
      "Iteration 14, loss = 0.28458803\n",
      "Iteration 15, loss = 0.27816810\n",
      "Iteration 16, loss = 0.27273791\n",
      "Iteration 17, loss = 0.26735926\n",
      "Iteration 18, loss = 0.26238358\n",
      "Iteration 19, loss = 0.25750340\n",
      "Iteration 20, loss = 0.25298789\n",
      "Iteration 21, loss = 0.24855893\n",
      "Iteration 22, loss = 0.24436228\n",
      "Iteration 23, loss = 0.24017962\n",
      "Iteration 24, loss = 0.23639047\n",
      "Iteration 25, loss = 0.23262235\n",
      "Iteration 26, loss = 0.22897143\n",
      "Iteration 27, loss = 0.22537816\n",
      "Iteration 28, loss = 0.22184043\n",
      "Iteration 29, loss = 0.21865250\n",
      "Iteration 30, loss = 0.21534095\n",
      "Iteration 31, loss = 0.21229077\n",
      "Iteration 32, loss = 0.20916230\n",
      "Iteration 33, loss = 0.20615283\n",
      "Iteration 34, loss = 0.20346929\n",
      "Iteration 35, loss = 0.20043418\n",
      "Iteration 36, loss = 0.19763624\n",
      "Iteration 37, loss = 0.19504201\n",
      "Iteration 38, loss = 0.19245781\n",
      "Iteration 39, loss = 0.18981799\n",
      "Iteration 40, loss = 0.18737219\n",
      "Iteration 41, loss = 0.18489656\n",
      "Iteration 42, loss = 0.18255023\n",
      "Iteration 43, loss = 0.18024228\n",
      "Iteration 44, loss = 0.17798958\n",
      "Iteration 45, loss = 0.17574606\n",
      "Iteration 46, loss = 0.17362943\n",
      "Iteration 47, loss = 0.17143445\n",
      "Iteration 48, loss = 0.16936617\n",
      "Iteration 49, loss = 0.16727116\n",
      "Iteration 50, loss = 0.16530244\n",
      "Iteration 51, loss = 0.16344881\n",
      "Iteration 52, loss = 0.16150599\n",
      "Iteration 53, loss = 0.15960910\n",
      "Iteration 54, loss = 0.15783812\n",
      "Iteration 55, loss = 0.15602178\n",
      "Iteration 56, loss = 0.15433785\n",
      "Iteration 57, loss = 0.15256320\n",
      "Iteration 58, loss = 0.15096661\n",
      "Iteration 59, loss = 0.14919836\n",
      "Iteration 60, loss = 0.14772986\n",
      "Iteration 61, loss = 0.14610433\n",
      "Iteration 62, loss = 0.14450007\n",
      "Iteration 63, loss = 0.14296404\n",
      "Iteration 64, loss = 0.14144195\n",
      "Iteration 65, loss = 0.14004736\n",
      "Iteration 66, loss = 0.13864939\n",
      "Iteration 67, loss = 0.13719083\n",
      "Iteration 68, loss = 0.13578172\n",
      "Iteration 69, loss = 0.13451077\n",
      "Iteration 70, loss = 0.13303170\n",
      "Iteration 71, loss = 0.13167552\n",
      "Iteration 72, loss = 0.13045580\n",
      "Iteration 73, loss = 0.12909075\n",
      "Iteration 74, loss = 0.12792792\n",
      "Iteration 75, loss = 0.12663808\n",
      "Iteration 76, loss = 0.12544768\n",
      "Iteration 77, loss = 0.12417421\n",
      "Iteration 78, loss = 0.12307493\n",
      "Iteration 79, loss = 0.12177371\n",
      "Iteration 80, loss = 0.12078902\n",
      "Iteration 81, loss = 0.11950804\n",
      "Iteration 82, loss = 0.11849776\n",
      "Iteration 83, loss = 0.11736473\n",
      "Iteration 84, loss = 0.11628908\n",
      "Iteration 85, loss = 0.11523039\n",
      "Iteration 86, loss = 0.11422273\n",
      "Iteration 87, loss = 0.11321575\n",
      "Iteration 88, loss = 0.11213399\n",
      "Iteration 89, loss = 0.11104142\n",
      "Iteration 90, loss = 0.11013061\n",
      "Iteration 91, loss = 0.10916648\n",
      "Iteration 92, loss = 0.10821729\n",
      "Iteration 93, loss = 0.10722419\n",
      "Iteration 94, loss = 0.10641631\n",
      "Iteration 95, loss = 0.10534449\n",
      "Iteration 96, loss = 0.10453156\n",
      "Iteration 97, loss = 0.10361905\n",
      "Iteration 98, loss = 0.10280935\n",
      "Iteration 99, loss = 0.10183425\n",
      "Iteration 100, loss = 0.10099775\n",
      "Iteration 101, loss = 0.10017962\n",
      "Iteration 102, loss = 0.09936773\n",
      "Iteration 103, loss = 0.09842121\n",
      "Iteration 104, loss = 0.09771438\n",
      "Iteration 105, loss = 0.09691905\n",
      "Iteration 106, loss = 0.09611611\n",
      "Iteration 107, loss = 0.09531786\n",
      "Iteration 108, loss = 0.09464682\n",
      "Iteration 109, loss = 0.09379531\n",
      "Iteration 110, loss = 0.09309570\n",
      "Iteration 111, loss = 0.09232258\n",
      "Iteration 112, loss = 0.09162528\n",
      "Iteration 113, loss = 0.09091437\n",
      "Iteration 114, loss = 0.09017266\n",
      "Iteration 115, loss = 0.08941504\n",
      "Iteration 116, loss = 0.08868976\n",
      "Iteration 117, loss = 0.08810500\n",
      "Iteration 118, loss = 0.08748702\n",
      "Iteration 119, loss = 0.08680642\n",
      "Iteration 120, loss = 0.08608990\n",
      "Iteration 121, loss = 0.08544101\n",
      "Iteration 122, loss = 0.08472666\n",
      "Iteration 123, loss = 0.08416426\n",
      "Iteration 124, loss = 0.08356722\n",
      "Iteration 125, loss = 0.08286919\n",
      "Iteration 126, loss = 0.08227263\n",
      "Iteration 127, loss = 0.08170419\n",
      "Iteration 128, loss = 0.08104689\n",
      "Iteration 129, loss = 0.08046494\n",
      "Iteration 130, loss = 0.07989052\n",
      "Iteration 131, loss = 0.07928385\n",
      "Iteration 132, loss = 0.07872001\n",
      "Iteration 133, loss = 0.07820356\n",
      "Iteration 134, loss = 0.07756763\n",
      "Iteration 135, loss = 0.07714440\n",
      "Iteration 136, loss = 0.07648654\n",
      "Iteration 137, loss = 0.07604006\n",
      "Iteration 138, loss = 0.07545448\n",
      "Iteration 139, loss = 0.07491029\n",
      "Iteration 140, loss = 0.07443098\n",
      "Iteration 141, loss = 0.07389823\n",
      "Iteration 142, loss = 0.07337471\n",
      "Iteration 143, loss = 0.07288761\n",
      "Iteration 144, loss = 0.07236818\n",
      "Iteration 145, loss = 0.07183354\n",
      "Iteration 146, loss = 0.07141114\n",
      "Iteration 147, loss = 0.07089945\n",
      "Iteration 148, loss = 0.07043165\n",
      "Iteration 149, loss = 0.06997820\n",
      "Iteration 150, loss = 0.06955088\n",
      "Iteration 151, loss = 0.06903529\n",
      "Iteration 152, loss = 0.06854195\n",
      "Iteration 153, loss = 0.06819627\n",
      "Iteration 154, loss = 0.06760116\n",
      "Iteration 155, loss = 0.06724955\n",
      "Iteration 156, loss = 0.06677166\n",
      "Iteration 157, loss = 0.06631872\n",
      "Iteration 158, loss = 0.06590908\n",
      "Iteration 159, loss = 0.06552851\n",
      "Iteration 160, loss = 0.06509541\n",
      "Iteration 161, loss = 0.06469919\n",
      "Iteration 162, loss = 0.06431194\n",
      "Iteration 163, loss = 0.06386031\n",
      "Iteration 164, loss = 0.06350292\n",
      "Iteration 165, loss = 0.06309382\n",
      "Iteration 166, loss = 0.06264039\n",
      "Iteration 167, loss = 0.06229589\n",
      "Iteration 168, loss = 0.06186074\n",
      "Iteration 169, loss = 0.06153748\n",
      "Iteration 170, loss = 0.06117621\n",
      "Iteration 171, loss = 0.06078611\n",
      "Iteration 172, loss = 0.06042564\n",
      "Iteration 173, loss = 0.06001022\n",
      "Iteration 174, loss = 0.05968444\n",
      "Iteration 175, loss = 0.05924491\n",
      "Iteration 176, loss = 0.05894508\n",
      "Iteration 177, loss = 0.05851935\n",
      "Iteration 178, loss = 0.05814391\n",
      "Iteration 179, loss = 0.05788184\n",
      "Iteration 180, loss = 0.05751165\n",
      "Iteration 181, loss = 0.05714306\n",
      "Iteration 182, loss = 0.05679828\n",
      "Iteration 183, loss = 0.05647403\n",
      "Iteration 184, loss = 0.05614629\n",
      "Iteration 185, loss = 0.05586817\n",
      "Iteration 186, loss = 0.05550883\n",
      "Iteration 187, loss = 0.05515450\n",
      "Iteration 188, loss = 0.05487455\n",
      "Iteration 189, loss = 0.05451367\n",
      "Iteration 190, loss = 0.05419302\n",
      "Iteration 191, loss = 0.05390203\n",
      "Iteration 192, loss = 0.05367498\n",
      "Iteration 193, loss = 0.05333839\n",
      "Iteration 194, loss = 0.05301168\n",
      "Iteration 195, loss = 0.05267326\n",
      "Iteration 196, loss = 0.05236575\n",
      "Iteration 197, loss = 0.05213919\n",
      "Iteration 198, loss = 0.05179050\n",
      "Iteration 199, loss = 0.05148763\n",
      "Iteration 200, loss = 0.05122182\n",
      "[CV] END activation=relu, alpha=0.0001, batch_size=100, hidden_layer_sizes=(100,), solver=sgd; total time= 4.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.40972031\n",
      "Iteration 2, loss = 0.66925119\n",
      "Iteration 3, loss = 0.50647514\n",
      "Iteration 4, loss = 0.43640069\n",
      "Iteration 5, loss = 0.39616077\n",
      "Iteration 6, loss = 0.36967205\n",
      "Iteration 7, loss = 0.35019842\n",
      "Iteration 8, loss = 0.33505932\n",
      "Iteration 9, loss = 0.32209361\n",
      "Iteration 10, loss = 0.31153444\n",
      "Iteration 11, loss = 0.30222897\n",
      "Iteration 12, loss = 0.29386567\n",
      "Iteration 13, loss = 0.28638057\n",
      "Iteration 14, loss = 0.27925410\n",
      "Iteration 15, loss = 0.27283821\n",
      "Iteration 16, loss = 0.26685494\n",
      "Iteration 17, loss = 0.26107134\n",
      "Iteration 18, loss = 0.25577100\n",
      "Iteration 19, loss = 0.25067534\n",
      "Iteration 20, loss = 0.24580964\n",
      "Iteration 21, loss = 0.24136311\n",
      "Iteration 22, loss = 0.23700711\n",
      "Iteration 23, loss = 0.23280719\n",
      "Iteration 24, loss = 0.22887256\n",
      "Iteration 25, loss = 0.22501182\n",
      "Iteration 26, loss = 0.22131290\n",
      "Iteration 27, loss = 0.21775701\n",
      "Iteration 28, loss = 0.21447910\n",
      "Iteration 29, loss = 0.21125974\n",
      "Iteration 30, loss = 0.20808565\n",
      "Iteration 31, loss = 0.20505237\n",
      "Iteration 32, loss = 0.20226100\n",
      "Iteration 33, loss = 0.19939416\n",
      "Iteration 34, loss = 0.19641106\n",
      "Iteration 35, loss = 0.19393565\n",
      "Iteration 36, loss = 0.19114943\n",
      "Iteration 37, loss = 0.18872340\n",
      "Iteration 38, loss = 0.18623685\n",
      "Iteration 39, loss = 0.18371190\n",
      "Iteration 40, loss = 0.18146651\n",
      "Iteration 41, loss = 0.17910067\n",
      "Iteration 42, loss = 0.17693177\n",
      "Iteration 43, loss = 0.17474073\n",
      "Iteration 44, loss = 0.17260857\n",
      "Iteration 45, loss = 0.17047447\n",
      "Iteration 46, loss = 0.16849290\n",
      "Iteration 47, loss = 0.16652522\n",
      "Iteration 48, loss = 0.16455822\n",
      "Iteration 49, loss = 0.16264760\n",
      "Iteration 50, loss = 0.16064340\n",
      "Iteration 51, loss = 0.15897403\n",
      "Iteration 52, loss = 0.15709550\n",
      "Iteration 53, loss = 0.15542380\n",
      "Iteration 54, loss = 0.15367462\n",
      "Iteration 55, loss = 0.15201212\n",
      "Iteration 56, loss = 0.15032647\n",
      "Iteration 57, loss = 0.14880898\n",
      "Iteration 58, loss = 0.14713322\n",
      "Iteration 59, loss = 0.14579949\n",
      "Iteration 60, loss = 0.14408729\n",
      "Iteration 61, loss = 0.14268489\n",
      "Iteration 62, loss = 0.14128333\n",
      "Iteration 63, loss = 0.13976942\n",
      "Iteration 64, loss = 0.13843880\n",
      "Iteration 65, loss = 0.13704658\n",
      "Iteration 66, loss = 0.13560043\n",
      "Iteration 67, loss = 0.13448651\n",
      "Iteration 68, loss = 0.13299946\n",
      "Iteration 69, loss = 0.13175618\n",
      "Iteration 70, loss = 0.13047746\n",
      "Iteration 71, loss = 0.12927295\n",
      "Iteration 72, loss = 0.12795490\n",
      "Iteration 73, loss = 0.12688950\n",
      "Iteration 74, loss = 0.12560225\n",
      "Iteration 75, loss = 0.12446663\n",
      "Iteration 76, loss = 0.12327662\n",
      "Iteration 77, loss = 0.12211172\n",
      "Iteration 78, loss = 0.12109206\n",
      "Iteration 79, loss = 0.11994062\n",
      "Iteration 80, loss = 0.11887636\n",
      "Iteration 81, loss = 0.11784728\n",
      "Iteration 82, loss = 0.11673876\n",
      "Iteration 83, loss = 0.11581090\n",
      "Iteration 84, loss = 0.11470926\n",
      "Iteration 85, loss = 0.11368265\n",
      "Iteration 86, loss = 0.11274810\n",
      "Iteration 87, loss = 0.11161769\n",
      "Iteration 88, loss = 0.11080026\n",
      "Iteration 89, loss = 0.10973986\n",
      "Iteration 90, loss = 0.10885701\n",
      "Iteration 91, loss = 0.10797920\n",
      "Iteration 92, loss = 0.10703876\n",
      "Iteration 93, loss = 0.10616987\n",
      "Iteration 94, loss = 0.10526962\n",
      "Iteration 95, loss = 0.10447738\n",
      "Iteration 96, loss = 0.10361141\n",
      "Iteration 97, loss = 0.10276701\n",
      "Iteration 98, loss = 0.10189098\n",
      "Iteration 99, loss = 0.10102075\n",
      "Iteration 100, loss = 0.10023215\n",
      "Iteration 101, loss = 0.09949672\n",
      "Iteration 102, loss = 0.09866947\n",
      "Iteration 103, loss = 0.09791894\n",
      "Iteration 104, loss = 0.09710544\n",
      "Iteration 105, loss = 0.09630433\n",
      "Iteration 106, loss = 0.09564701\n",
      "Iteration 107, loss = 0.09481016\n",
      "Iteration 108, loss = 0.09411337\n",
      "Iteration 109, loss = 0.09342514\n",
      "Iteration 110, loss = 0.09270256\n",
      "Iteration 111, loss = 0.09203857\n",
      "Iteration 112, loss = 0.09127358\n",
      "Iteration 113, loss = 0.09068373\n",
      "Iteration 114, loss = 0.08986274\n",
      "Iteration 115, loss = 0.08929210\n",
      "Iteration 116, loss = 0.08847489\n",
      "Iteration 117, loss = 0.08796614\n",
      "Iteration 118, loss = 0.08726825\n",
      "Iteration 119, loss = 0.08671180\n",
      "Iteration 120, loss = 0.08598333\n",
      "Iteration 121, loss = 0.08542149\n",
      "Iteration 122, loss = 0.08473602\n",
      "Iteration 123, loss = 0.08418274\n",
      "Iteration 124, loss = 0.08351470\n",
      "Iteration 125, loss = 0.08286431\n",
      "Iteration 126, loss = 0.08240614\n",
      "Iteration 127, loss = 0.08169666\n",
      "Iteration 128, loss = 0.08115326\n",
      "Iteration 129, loss = 0.08056937\n",
      "Iteration 130, loss = 0.08009713\n",
      "Iteration 131, loss = 0.07946911\n",
      "Iteration 132, loss = 0.07893113\n",
      "Iteration 133, loss = 0.07837398\n",
      "Iteration 134, loss = 0.07784603\n",
      "Iteration 135, loss = 0.07725974\n",
      "Iteration 136, loss = 0.07674032\n",
      "Iteration 137, loss = 0.07622872\n",
      "Iteration 138, loss = 0.07574089\n",
      "Iteration 139, loss = 0.07514069\n",
      "Iteration 140, loss = 0.07471060\n",
      "Iteration 141, loss = 0.07415931\n",
      "Iteration 142, loss = 0.07374883\n",
      "Iteration 143, loss = 0.07317495\n",
      "Iteration 144, loss = 0.07267870\n",
      "Iteration 145, loss = 0.07216758\n",
      "Iteration 146, loss = 0.07171558\n",
      "Iteration 147, loss = 0.07128420\n",
      "Iteration 148, loss = 0.07073408\n",
      "Iteration 149, loss = 0.07030001\n",
      "Iteration 150, loss = 0.06992218\n",
      "Iteration 151, loss = 0.06944324\n",
      "Iteration 152, loss = 0.06893767\n",
      "Iteration 153, loss = 0.06849130\n",
      "Iteration 154, loss = 0.06817767\n",
      "Iteration 155, loss = 0.06761795\n",
      "Iteration 156, loss = 0.06719931\n",
      "Iteration 157, loss = 0.06671974\n",
      "Iteration 158, loss = 0.06635955\n",
      "Iteration 159, loss = 0.06590003\n",
      "Iteration 160, loss = 0.06555161\n",
      "Iteration 161, loss = 0.06506635\n",
      "Iteration 162, loss = 0.06467116\n",
      "Iteration 163, loss = 0.06425390\n",
      "Iteration 164, loss = 0.06381580\n",
      "Iteration 165, loss = 0.06340962\n",
      "Iteration 166, loss = 0.06306029\n",
      "Iteration 167, loss = 0.06273662\n",
      "Iteration 168, loss = 0.06230667\n",
      "Iteration 169, loss = 0.06197787\n",
      "Iteration 170, loss = 0.06146477\n",
      "Iteration 171, loss = 0.06114961\n",
      "Iteration 172, loss = 0.06071039\n",
      "Iteration 173, loss = 0.06047877\n",
      "Iteration 174, loss = 0.06007949\n",
      "Iteration 175, loss = 0.05972308\n",
      "Iteration 176, loss = 0.05933900\n",
      "Iteration 177, loss = 0.05898086\n",
      "Iteration 178, loss = 0.05860352\n",
      "Iteration 179, loss = 0.05830745\n",
      "Iteration 180, loss = 0.05797054\n",
      "Iteration 181, loss = 0.05754990\n",
      "Iteration 182, loss = 0.05722975\n",
      "Iteration 183, loss = 0.05697001\n",
      "Iteration 184, loss = 0.05648933\n",
      "Iteration 185, loss = 0.05618628\n",
      "Iteration 186, loss = 0.05588127\n",
      "Iteration 187, loss = 0.05565955\n",
      "Iteration 188, loss = 0.05523687\n",
      "Iteration 189, loss = 0.05489715\n",
      "Iteration 190, loss = 0.05455142\n",
      "Iteration 191, loss = 0.05429765\n",
      "Iteration 192, loss = 0.05397412\n",
      "Iteration 193, loss = 0.05366185\n",
      "Iteration 194, loss = 0.05333904\n",
      "Iteration 195, loss = 0.05304999\n",
      "Iteration 196, loss = 0.05274930\n",
      "Iteration 197, loss = 0.05249364\n",
      "Iteration 198, loss = 0.05213041\n",
      "Iteration 199, loss = 0.05184649\n",
      "Iteration 200, loss = 0.05150729\n",
      "[CV] END activation=relu, alpha=0.0001, batch_size=100, hidden_layer_sizes=(100,), solver=sgd; total time= 4.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.41666679\n",
      "Iteration 2, loss = 0.65327597\n",
      "Iteration 3, loss = 0.49773266\n",
      "Iteration 4, loss = 0.43201693\n",
      "Iteration 5, loss = 0.39412196\n",
      "Iteration 6, loss = 0.36841783\n",
      "Iteration 7, loss = 0.34949694\n",
      "Iteration 8, loss = 0.33471877\n",
      "Iteration 9, loss = 0.32242132\n",
      "Iteration 10, loss = 0.31190641\n",
      "Iteration 11, loss = 0.30291434\n",
      "Iteration 12, loss = 0.29468810\n",
      "Iteration 13, loss = 0.28719431\n",
      "Iteration 14, loss = 0.28042899\n",
      "Iteration 15, loss = 0.27412885\n",
      "Iteration 16, loss = 0.26829476\n",
      "Iteration 17, loss = 0.26274665\n",
      "Iteration 18, loss = 0.25747495\n",
      "Iteration 19, loss = 0.25264094\n",
      "Iteration 20, loss = 0.24789572\n",
      "Iteration 21, loss = 0.24355349\n",
      "Iteration 22, loss = 0.23930321\n",
      "Iteration 23, loss = 0.23512232\n",
      "Iteration 24, loss = 0.23134912\n",
      "Iteration 25, loss = 0.22761495\n",
      "Iteration 26, loss = 0.22384090\n",
      "Iteration 27, loss = 0.22025562\n",
      "Iteration 28, loss = 0.21695393\n",
      "Iteration 29, loss = 0.21380941\n",
      "Iteration 30, loss = 0.21037715\n",
      "Iteration 31, loss = 0.20732371\n",
      "Iteration 32, loss = 0.20426836\n",
      "Iteration 33, loss = 0.20143533\n",
      "Iteration 34, loss = 0.19857290\n",
      "Iteration 35, loss = 0.19566664\n",
      "Iteration 36, loss = 0.19301400\n",
      "Iteration 37, loss = 0.19033071\n",
      "Iteration 38, loss = 0.18778529\n",
      "Iteration 39, loss = 0.18526165\n",
      "Iteration 40, loss = 0.18290928\n",
      "Iteration 41, loss = 0.18067726\n",
      "Iteration 42, loss = 0.17827541\n",
      "Iteration 43, loss = 0.17593308\n",
      "Iteration 44, loss = 0.17395753\n",
      "Iteration 45, loss = 0.17170071\n",
      "Iteration 46, loss = 0.16957971\n",
      "Iteration 47, loss = 0.16738275\n",
      "Iteration 48, loss = 0.16556693\n",
      "Iteration 49, loss = 0.16356742\n",
      "Iteration 50, loss = 0.16172876\n",
      "Iteration 51, loss = 0.15979698\n",
      "Iteration 52, loss = 0.15789891\n",
      "Iteration 53, loss = 0.15607840\n",
      "Iteration 54, loss = 0.15442466\n",
      "Iteration 55, loss = 0.15262390\n",
      "Iteration 56, loss = 0.15100216\n",
      "Iteration 57, loss = 0.14932443\n",
      "Iteration 58, loss = 0.14770075\n",
      "Iteration 59, loss = 0.14609507\n",
      "Iteration 60, loss = 0.14446224\n",
      "Iteration 61, loss = 0.14307677\n",
      "Iteration 62, loss = 0.14161070\n",
      "Iteration 63, loss = 0.14015510\n",
      "Iteration 64, loss = 0.13875513\n",
      "Iteration 65, loss = 0.13734821\n",
      "Iteration 66, loss = 0.13592440\n",
      "Iteration 67, loss = 0.13461512\n",
      "Iteration 68, loss = 0.13311564\n",
      "Iteration 69, loss = 0.13191081\n",
      "Iteration 70, loss = 0.13060127\n",
      "Iteration 71, loss = 0.12939575\n",
      "Iteration 72, loss = 0.12818752\n",
      "Iteration 73, loss = 0.12696300\n",
      "Iteration 74, loss = 0.12577778\n",
      "Iteration 75, loss = 0.12456097\n",
      "Iteration 76, loss = 0.12347282\n",
      "Iteration 77, loss = 0.12232950\n",
      "Iteration 78, loss = 0.12118357\n",
      "Iteration 79, loss = 0.12014442\n",
      "Iteration 80, loss = 0.11906314\n",
      "Iteration 81, loss = 0.11801019\n",
      "Iteration 82, loss = 0.11703143\n",
      "Iteration 83, loss = 0.11596389\n",
      "Iteration 84, loss = 0.11492292\n",
      "Iteration 85, loss = 0.11386030\n",
      "Iteration 86, loss = 0.11293805\n",
      "Iteration 87, loss = 0.11201640\n",
      "Iteration 88, loss = 0.11107835\n",
      "Iteration 89, loss = 0.11006815\n",
      "Iteration 90, loss = 0.10916848\n",
      "Iteration 91, loss = 0.10832969\n",
      "Iteration 92, loss = 0.10737045\n",
      "Iteration 93, loss = 0.10637656\n",
      "Iteration 94, loss = 0.10574005\n",
      "Iteration 95, loss = 0.10481989\n",
      "Iteration 96, loss = 0.10397730\n",
      "Iteration 97, loss = 0.10315630\n",
      "Iteration 98, loss = 0.10231748\n",
      "Iteration 99, loss = 0.10153006\n",
      "Iteration 100, loss = 0.10077421\n",
      "Iteration 101, loss = 0.10000662\n",
      "Iteration 102, loss = 0.09918643\n",
      "Iteration 103, loss = 0.09842131\n",
      "Iteration 104, loss = 0.09764536\n",
      "Iteration 105, loss = 0.09691567\n",
      "Iteration 106, loss = 0.09618877\n",
      "Iteration 107, loss = 0.09539123\n",
      "Iteration 108, loss = 0.09477963\n",
      "Iteration 109, loss = 0.09404842\n",
      "Iteration 110, loss = 0.09329533\n",
      "Iteration 111, loss = 0.09276087\n",
      "Iteration 112, loss = 0.09193141\n",
      "Iteration 113, loss = 0.09133004\n",
      "Iteration 114, loss = 0.09065042\n",
      "Iteration 115, loss = 0.08999231\n",
      "Iteration 116, loss = 0.08936219\n",
      "Iteration 117, loss = 0.08873449\n",
      "Iteration 118, loss = 0.08812007\n",
      "Iteration 119, loss = 0.08746756\n",
      "Iteration 120, loss = 0.08691988\n",
      "Iteration 121, loss = 0.08616474\n",
      "Iteration 122, loss = 0.08566893\n",
      "Iteration 123, loss = 0.08507264\n",
      "Iteration 124, loss = 0.08444019\n",
      "Iteration 125, loss = 0.08387755\n",
      "Iteration 126, loss = 0.08327500\n",
      "Iteration 127, loss = 0.08272847\n",
      "Iteration 128, loss = 0.08218700\n",
      "Iteration 129, loss = 0.08164116\n",
      "Iteration 130, loss = 0.08110123\n",
      "Iteration 131, loss = 0.08053330\n",
      "Iteration 132, loss = 0.07997096\n",
      "Iteration 133, loss = 0.07947790\n",
      "Iteration 134, loss = 0.07892100\n",
      "Iteration 135, loss = 0.07844198\n",
      "Iteration 136, loss = 0.07784248\n",
      "Iteration 137, loss = 0.07737703\n",
      "Iteration 138, loss = 0.07687011\n",
      "Iteration 139, loss = 0.07635798\n",
      "Iteration 140, loss = 0.07587476\n",
      "Iteration 141, loss = 0.07536227\n",
      "Iteration 142, loss = 0.07489234\n",
      "Iteration 143, loss = 0.07440506\n",
      "Iteration 144, loss = 0.07396051\n",
      "Iteration 145, loss = 0.07340166\n",
      "Iteration 146, loss = 0.07293728\n",
      "Iteration 147, loss = 0.07256517\n",
      "Iteration 148, loss = 0.07206087\n",
      "Iteration 149, loss = 0.07161780\n",
      "Iteration 150, loss = 0.07121796\n",
      "Iteration 151, loss = 0.07075182\n",
      "Iteration 152, loss = 0.07028965\n",
      "Iteration 153, loss = 0.06991374\n",
      "Iteration 154, loss = 0.06932830\n",
      "Iteration 155, loss = 0.06888512\n",
      "Iteration 156, loss = 0.06856224\n",
      "Iteration 157, loss = 0.06807013\n",
      "Iteration 158, loss = 0.06776103\n",
      "Iteration 159, loss = 0.06728348\n",
      "Iteration 160, loss = 0.06688631\n",
      "Iteration 161, loss = 0.06644789\n",
      "Iteration 162, loss = 0.06604529\n",
      "Iteration 163, loss = 0.06574866\n",
      "Iteration 164, loss = 0.06530085\n",
      "Iteration 165, loss = 0.06482406\n",
      "Iteration 166, loss = 0.06450889\n",
      "Iteration 167, loss = 0.06417163\n",
      "Iteration 168, loss = 0.06371838\n",
      "Iteration 169, loss = 0.06332712\n",
      "Iteration 170, loss = 0.06299107\n",
      "Iteration 171, loss = 0.06264035\n",
      "Iteration 172, loss = 0.06225361\n",
      "Iteration 173, loss = 0.06186073\n",
      "Iteration 174, loss = 0.06145065\n",
      "Iteration 175, loss = 0.06112441\n",
      "Iteration 176, loss = 0.06076695\n",
      "Iteration 177, loss = 0.06046832\n",
      "Iteration 178, loss = 0.06011896\n",
      "Iteration 179, loss = 0.05976390\n",
      "Iteration 180, loss = 0.05942959\n",
      "Iteration 181, loss = 0.05904782\n",
      "Iteration 182, loss = 0.05872176\n",
      "Iteration 183, loss = 0.05838911\n",
      "Iteration 184, loss = 0.05808502\n",
      "Iteration 185, loss = 0.05774267\n",
      "Iteration 186, loss = 0.05737815\n",
      "Iteration 187, loss = 0.05707517\n",
      "Iteration 188, loss = 0.05680602\n",
      "Iteration 189, loss = 0.05642423\n",
      "Iteration 190, loss = 0.05612795\n",
      "Iteration 191, loss = 0.05581633\n",
      "Iteration 192, loss = 0.05547055\n",
      "Iteration 193, loss = 0.05514425\n",
      "Iteration 194, loss = 0.05484188\n",
      "Iteration 195, loss = 0.05454148\n",
      "Iteration 196, loss = 0.05429853\n",
      "Iteration 197, loss = 0.05396378\n",
      "Iteration 198, loss = 0.05365115\n",
      "Iteration 199, loss = 0.05339202\n",
      "Iteration 200, loss = 0.05312586\n",
      "[CV] END activation=relu, alpha=0.0001, batch_size=100, hidden_layer_sizes=(100,), solver=sgd; total time= 3.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.46182066\n",
      "Iteration 2, loss = 0.69639922\n",
      "Iteration 3, loss = 0.52209974\n",
      "Iteration 4, loss = 0.44890389\n",
      "Iteration 5, loss = 0.40733805\n",
      "Iteration 6, loss = 0.37946880\n",
      "Iteration 7, loss = 0.35910981\n",
      "Iteration 8, loss = 0.34314907\n",
      "Iteration 9, loss = 0.33012146\n",
      "Iteration 10, loss = 0.31900938\n",
      "Iteration 11, loss = 0.30915197\n",
      "Iteration 12, loss = 0.30067067\n",
      "Iteration 13, loss = 0.29286609\n",
      "Iteration 14, loss = 0.28583957\n",
      "Iteration 15, loss = 0.27920358\n",
      "Iteration 16, loss = 0.27304898\n",
      "Iteration 17, loss = 0.26738113\n",
      "Iteration 18, loss = 0.26216285\n",
      "Iteration 19, loss = 0.25690972\n",
      "Iteration 20, loss = 0.25216114\n",
      "Iteration 21, loss = 0.24744068\n",
      "Iteration 22, loss = 0.24311349\n",
      "Iteration 23, loss = 0.23882365\n",
      "Iteration 24, loss = 0.23474988\n",
      "Iteration 25, loss = 0.23092752\n",
      "Iteration 26, loss = 0.22721414\n",
      "Iteration 27, loss = 0.22356504\n",
      "Iteration 28, loss = 0.22006242\n",
      "Iteration 29, loss = 0.21676763\n",
      "Iteration 30, loss = 0.21374782\n",
      "Iteration 31, loss = 0.21043143\n",
      "Iteration 32, loss = 0.20735744\n",
      "Iteration 33, loss = 0.20442120\n",
      "Iteration 34, loss = 0.20168942\n",
      "Iteration 35, loss = 0.19888010\n",
      "Iteration 36, loss = 0.19621893\n",
      "Iteration 37, loss = 0.19358529\n",
      "Iteration 38, loss = 0.19100033\n",
      "Iteration 39, loss = 0.18838420\n",
      "Iteration 40, loss = 0.18601102\n",
      "Iteration 41, loss = 0.18348360\n",
      "Iteration 42, loss = 0.18116255\n",
      "Iteration 43, loss = 0.17900903\n",
      "Iteration 44, loss = 0.17682834\n",
      "Iteration 45, loss = 0.17465092\n",
      "Iteration 46, loss = 0.17254126\n",
      "Iteration 47, loss = 0.17040437\n",
      "Iteration 48, loss = 0.16850505\n",
      "Iteration 49, loss = 0.16655776\n",
      "Iteration 50, loss = 0.16458591\n",
      "Iteration 51, loss = 0.16273205\n",
      "Iteration 52, loss = 0.16080892\n",
      "Iteration 53, loss = 0.15902470\n",
      "Iteration 54, loss = 0.15744210\n",
      "Iteration 55, loss = 0.15562228\n",
      "Iteration 56, loss = 0.15394812\n",
      "Iteration 57, loss = 0.15224823\n",
      "Iteration 58, loss = 0.15072838\n",
      "Iteration 59, loss = 0.14905618\n",
      "Iteration 60, loss = 0.14744586\n",
      "Iteration 61, loss = 0.14614748\n",
      "Iteration 62, loss = 0.14461704\n",
      "Iteration 63, loss = 0.14310854\n",
      "Iteration 64, loss = 0.14168743\n",
      "Iteration 65, loss = 0.14018849\n",
      "Iteration 66, loss = 0.13877874\n",
      "Iteration 67, loss = 0.13757496\n",
      "Iteration 68, loss = 0.13613282\n",
      "Iteration 69, loss = 0.13485766\n",
      "Iteration 70, loss = 0.13348771\n",
      "Iteration 71, loss = 0.13237330\n",
      "Iteration 72, loss = 0.13118347\n",
      "Iteration 73, loss = 0.12981290\n",
      "Iteration 74, loss = 0.12865476\n",
      "Iteration 75, loss = 0.12751079\n",
      "Iteration 76, loss = 0.12636715\n",
      "Iteration 77, loss = 0.12512187\n",
      "Iteration 78, loss = 0.12410350\n",
      "Iteration 79, loss = 0.12290536\n",
      "Iteration 80, loss = 0.12184984\n",
      "Iteration 81, loss = 0.12074681\n",
      "Iteration 82, loss = 0.11973825\n",
      "Iteration 83, loss = 0.11867290\n",
      "Iteration 84, loss = 0.11771657\n",
      "Iteration 85, loss = 0.11672630\n",
      "Iteration 86, loss = 0.11570991\n",
      "Iteration 87, loss = 0.11473723\n",
      "Iteration 88, loss = 0.11372297\n",
      "Iteration 89, loss = 0.11276890\n",
      "Iteration 90, loss = 0.11199431\n",
      "Iteration 91, loss = 0.11106115\n",
      "Iteration 92, loss = 0.11010231\n",
      "Iteration 93, loss = 0.10910141\n",
      "Iteration 94, loss = 0.10838662\n",
      "Iteration 95, loss = 0.10748146\n",
      "Iteration 96, loss = 0.10658718\n",
      "Iteration 97, loss = 0.10568114\n",
      "Iteration 98, loss = 0.10491910\n",
      "Iteration 99, loss = 0.10406678\n",
      "Iteration 100, loss = 0.10330967\n",
      "Iteration 101, loss = 0.10255875\n",
      "Iteration 102, loss = 0.10167474\n",
      "Iteration 103, loss = 0.10095100\n",
      "Iteration 104, loss = 0.10024529\n",
      "Iteration 105, loss = 0.09937344\n",
      "Iteration 106, loss = 0.09867171\n",
      "Iteration 107, loss = 0.09788770\n",
      "Iteration 108, loss = 0.09721005\n",
      "Iteration 109, loss = 0.09649682\n",
      "Iteration 110, loss = 0.09577637\n",
      "Iteration 111, loss = 0.09508077\n",
      "Iteration 112, loss = 0.09436490\n",
      "Iteration 113, loss = 0.09368990\n",
      "Iteration 114, loss = 0.09301058\n",
      "Iteration 115, loss = 0.09226154\n",
      "Iteration 116, loss = 0.09163059\n",
      "Iteration 117, loss = 0.09094824\n",
      "Iteration 118, loss = 0.09034960\n",
      "Iteration 119, loss = 0.08968484\n",
      "Iteration 120, loss = 0.08904799\n",
      "Iteration 121, loss = 0.08844489\n",
      "Iteration 122, loss = 0.08772333\n",
      "Iteration 123, loss = 0.08715873\n",
      "Iteration 124, loss = 0.08660589\n",
      "Iteration 125, loss = 0.08599881\n",
      "Iteration 126, loss = 0.08540307\n",
      "Iteration 127, loss = 0.08490877\n",
      "Iteration 128, loss = 0.08425571\n",
      "Iteration 129, loss = 0.08365621\n",
      "Iteration 130, loss = 0.08303325\n",
      "Iteration 131, loss = 0.08253362\n",
      "Iteration 132, loss = 0.08190863\n",
      "Iteration 133, loss = 0.08137090\n",
      "Iteration 134, loss = 0.08084315\n",
      "Iteration 135, loss = 0.08027784\n",
      "Iteration 136, loss = 0.07976678\n",
      "Iteration 137, loss = 0.07931455\n",
      "Iteration 138, loss = 0.07874126\n",
      "Iteration 139, loss = 0.07823539\n",
      "Iteration 140, loss = 0.07768674\n",
      "Iteration 141, loss = 0.07723515\n",
      "Iteration 142, loss = 0.07668299\n",
      "Iteration 143, loss = 0.07614533\n",
      "Iteration 144, loss = 0.07569735\n",
      "Iteration 145, loss = 0.07515670\n",
      "Iteration 146, loss = 0.07481115\n",
      "Iteration 147, loss = 0.07428554\n",
      "Iteration 148, loss = 0.07385379\n",
      "Iteration 149, loss = 0.07326154\n",
      "Iteration 150, loss = 0.07284086\n",
      "Iteration 151, loss = 0.07235499\n",
      "Iteration 152, loss = 0.07194560\n",
      "Iteration 153, loss = 0.07145081\n",
      "Iteration 154, loss = 0.07101650\n",
      "Iteration 155, loss = 0.07055915\n",
      "Iteration 156, loss = 0.07015214\n",
      "Iteration 157, loss = 0.06959158\n",
      "Iteration 158, loss = 0.06934051\n",
      "Iteration 159, loss = 0.06882640\n",
      "Iteration 160, loss = 0.06838891\n",
      "Iteration 161, loss = 0.06799836\n",
      "Iteration 162, loss = 0.06759841\n",
      "Iteration 163, loss = 0.06714971\n",
      "Iteration 164, loss = 0.06667547\n",
      "Iteration 165, loss = 0.06632245\n",
      "Iteration 166, loss = 0.06602547\n",
      "Iteration 167, loss = 0.06557337\n",
      "Iteration 168, loss = 0.06516802\n",
      "Iteration 169, loss = 0.06473680\n",
      "Iteration 170, loss = 0.06437254\n",
      "Iteration 171, loss = 0.06399054\n",
      "Iteration 172, loss = 0.06358516\n",
      "Iteration 173, loss = 0.06326065\n",
      "Iteration 174, loss = 0.06283956\n",
      "Iteration 175, loss = 0.06249234\n",
      "Iteration 176, loss = 0.06209451\n",
      "Iteration 177, loss = 0.06175361\n",
      "Iteration 178, loss = 0.06144196\n",
      "Iteration 179, loss = 0.06098583\n",
      "Iteration 180, loss = 0.06064965\n",
      "Iteration 181, loss = 0.06025678\n",
      "Iteration 182, loss = 0.05994469\n",
      "Iteration 183, loss = 0.05958022\n",
      "Iteration 184, loss = 0.05927597\n",
      "Iteration 185, loss = 0.05890773\n",
      "Iteration 186, loss = 0.05853420\n",
      "Iteration 187, loss = 0.05825399\n",
      "Iteration 188, loss = 0.05793846\n",
      "Iteration 189, loss = 0.05757527\n",
      "Iteration 190, loss = 0.05723471\n",
      "Iteration 191, loss = 0.05695548\n",
      "Iteration 192, loss = 0.05661172\n",
      "Iteration 193, loss = 0.05623992\n",
      "Iteration 194, loss = 0.05597838\n",
      "Iteration 195, loss = 0.05567937\n",
      "Iteration 196, loss = 0.05533677\n",
      "Iteration 197, loss = 0.05491050\n",
      "Iteration 198, loss = 0.05472095\n",
      "Iteration 199, loss = 0.05439879\n",
      "Iteration 200, loss = 0.05410980\n",
      "[CV] END activation=relu, alpha=0.0001, batch_size=100, hidden_layer_sizes=(100,), solver=sgd; total time= 3.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.31652241\n",
      "Iteration 2, loss = 0.12217755\n",
      "Iteration 3, loss = 0.07610389\n",
      "Iteration 4, loss = 0.05094367\n",
      "Iteration 5, loss = 0.03575384\n",
      "Iteration 6, loss = 0.02309515\n",
      "Iteration 7, loss = 0.02108098\n",
      "Iteration 8, loss = 0.01710485\n",
      "Iteration 9, loss = 0.01471461\n",
      "Iteration 10, loss = 0.01208376\n",
      "Iteration 11, loss = 0.01401170\n",
      "Iteration 12, loss = 0.00729946\n",
      "Iteration 13, loss = 0.01060889\n",
      "Iteration 14, loss = 0.01434919\n",
      "Iteration 15, loss = 0.01290746\n",
      "Iteration 16, loss = 0.00861585\n",
      "Iteration 17, loss = 0.01593334\n",
      "Iteration 18, loss = 0.00420798\n",
      "Iteration 19, loss = 0.00831782\n",
      "Iteration 20, loss = 0.00960795\n",
      "Iteration 21, loss = 0.00570079\n",
      "Iteration 22, loss = 0.00598692\n",
      "Iteration 23, loss = 0.01203859\n",
      "Iteration 24, loss = 0.00247504\n",
      "Iteration 25, loss = 0.00184621\n",
      "Iteration 26, loss = 0.00132289\n",
      "Iteration 27, loss = 0.00122772\n",
      "Iteration 28, loss = 0.00119910\n",
      "Iteration 29, loss = 0.00117716\n",
      "Iteration 30, loss = 0.00115590\n",
      "Iteration 31, loss = 0.00113477\n",
      "Iteration 32, loss = 0.00111283\n",
      "Iteration 33, loss = 0.00109050\n",
      "Iteration 34, loss = 0.00106732\n",
      "Iteration 35, loss = 0.00104264\n",
      "Iteration 36, loss = 0.00101664\n",
      "Iteration 37, loss = 0.00098959\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.0001, batch_size=100, hidden_layer_sizes=(400, 100), solver=adam; total time= 2.9min\n",
      "Iteration 1, loss = 0.31194998\n",
      "Iteration 2, loss = 0.11612882\n",
      "Iteration 3, loss = 0.07234573\n",
      "Iteration 4, loss = 0.05104211\n",
      "Iteration 5, loss = 0.03647435\n",
      "Iteration 6, loss = 0.02532390\n",
      "Iteration 7, loss = 0.02179986\n",
      "Iteration 8, loss = 0.01386650\n",
      "Iteration 9, loss = 0.01105706\n",
      "Iteration 10, loss = 0.01553657\n",
      "Iteration 11, loss = 0.00725894\n",
      "Iteration 12, loss = 0.00880457\n",
      "Iteration 13, loss = 0.01568699\n",
      "Iteration 14, loss = 0.01284661\n",
      "Iteration 15, loss = 0.00961393\n",
      "Iteration 16, loss = 0.00888486\n",
      "Iteration 17, loss = 0.00576785\n",
      "Iteration 18, loss = 0.00973255\n",
      "Iteration 19, loss = 0.01066978\n",
      "Iteration 20, loss = 0.01027953\n",
      "Iteration 21, loss = 0.00646689\n",
      "Iteration 22, loss = 0.00496018\n",
      "Iteration 23, loss = 0.00544313\n",
      "Iteration 24, loss = 0.01233468\n",
      "Iteration 25, loss = 0.00694208\n",
      "Iteration 26, loss = 0.00292966\n",
      "Iteration 27, loss = 0.00987095\n",
      "Iteration 28, loss = 0.01026922\n",
      "Iteration 29, loss = 0.00532729\n",
      "Iteration 30, loss = 0.00169134\n",
      "Iteration 31, loss = 0.00137126\n",
      "Iteration 32, loss = 0.00133693\n",
      "Iteration 33, loss = 0.00131399\n",
      "Iteration 34, loss = 0.00129274\n",
      "Iteration 35, loss = 0.00127176\n",
      "Iteration 36, loss = 0.00125034\n",
      "Iteration 37, loss = 0.00122806\n",
      "Iteration 38, loss = 0.00120471\n",
      "Iteration 39, loss = 0.00118007\n",
      "Iteration 40, loss = 0.00115382\n",
      "Iteration 41, loss = 0.00112612\n",
      "Iteration 42, loss = 0.00109653\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.0001, batch_size=100, hidden_layer_sizes=(400, 100), solver=adam; total time= 3.3min\n",
      "Iteration 1, loss = 0.30955910\n",
      "Iteration 2, loss = 0.11659021\n",
      "Iteration 3, loss = 0.07116987\n",
      "Iteration 4, loss = 0.05020547\n",
      "Iteration 5, loss = 0.03452070\n",
      "Iteration 6, loss = 0.02462004\n",
      "Iteration 7, loss = 0.02118354\n",
      "Iteration 8, loss = 0.01731142\n",
      "Iteration 9, loss = 0.01278717\n",
      "Iteration 10, loss = 0.00977056\n",
      "Iteration 11, loss = 0.01091589\n",
      "Iteration 12, loss = 0.01434545\n",
      "Iteration 13, loss = 0.01148402\n",
      "Iteration 14, loss = 0.00680298\n",
      "Iteration 15, loss = 0.00864542\n",
      "Iteration 16, loss = 0.01464106\n",
      "Iteration 17, loss = 0.00575213\n",
      "Iteration 18, loss = 0.00910261\n",
      "Iteration 19, loss = 0.00787422\n",
      "Iteration 20, loss = 0.00992492\n",
      "Iteration 21, loss = 0.00577888\n",
      "Iteration 22, loss = 0.00576759\n",
      "Iteration 23, loss = 0.01312270\n",
      "Iteration 24, loss = 0.01091599\n",
      "Iteration 25, loss = 0.00748842\n",
      "Iteration 26, loss = 0.00342512\n",
      "Iteration 27, loss = 0.00209807\n",
      "Iteration 28, loss = 0.00202082\n",
      "Iteration 29, loss = 0.00242113\n",
      "Iteration 30, loss = 0.00129179\n",
      "Iteration 31, loss = 0.00122575\n",
      "Iteration 32, loss = 0.00120238\n",
      "Iteration 33, loss = 0.00118022\n",
      "Iteration 34, loss = 0.00115824\n",
      "Iteration 35, loss = 0.00113566\n",
      "Iteration 36, loss = 0.00111222\n",
      "Iteration 37, loss = 0.00108762\n",
      "Iteration 38, loss = 0.00106184\n",
      "Iteration 39, loss = 0.00103446\n",
      "Iteration 40, loss = 0.00100580\n",
      "Iteration 41, loss = 0.00097550\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.0001, batch_size=100, hidden_layer_sizes=(400, 100), solver=adam; total time= 3.2min\n",
      "Iteration 1, loss = 0.30969134\n",
      "Iteration 2, loss = 0.12104508\n",
      "Iteration 3, loss = 0.07488317\n",
      "Iteration 4, loss = 0.04983338\n",
      "Iteration 5, loss = 0.03613940\n",
      "Iteration 6, loss = 0.02557045\n",
      "Iteration 7, loss = 0.01769623\n",
      "Iteration 8, loss = 0.01876087\n",
      "Iteration 9, loss = 0.01360903\n",
      "Iteration 10, loss = 0.01490847\n",
      "Iteration 11, loss = 0.01283286\n",
      "Iteration 12, loss = 0.00967713\n",
      "Iteration 13, loss = 0.01156374\n",
      "Iteration 14, loss = 0.01115242\n",
      "Iteration 15, loss = 0.01314866\n",
      "Iteration 16, loss = 0.00609965\n",
      "Iteration 17, loss = 0.00984694\n",
      "Iteration 18, loss = 0.00976075\n",
      "Iteration 19, loss = 0.01003243\n",
      "Iteration 20, loss = 0.00446623\n",
      "Iteration 21, loss = 0.00324312\n",
      "Iteration 22, loss = 0.00381404\n",
      "Iteration 23, loss = 0.01322095\n",
      "Iteration 24, loss = 0.01203148\n",
      "Iteration 25, loss = 0.00782545\n",
      "Iteration 26, loss = 0.00249602\n",
      "Iteration 27, loss = 0.00175044\n",
      "Iteration 28, loss = 0.00129031\n",
      "Iteration 29, loss = 0.00124208\n",
      "Iteration 30, loss = 0.00121797\n",
      "Iteration 31, loss = 0.00119692\n",
      "Iteration 32, loss = 0.00117627\n",
      "Iteration 33, loss = 0.00115558\n",
      "Iteration 34, loss = 0.00113402\n",
      "Iteration 35, loss = 0.00111153\n",
      "Iteration 36, loss = 0.00108789\n",
      "Iteration 37, loss = 0.00106313\n",
      "Iteration 38, loss = 0.00103683\n",
      "Iteration 39, loss = 0.00100922\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.0001, batch_size=100, hidden_layer_sizes=(400, 100), solver=adam; total time= 3.0min\n",
      "Iteration 1, loss = 0.31033040\n",
      "Iteration 2, loss = 0.11976654\n",
      "Iteration 3, loss = 0.07158449\n",
      "Iteration 4, loss = 0.05222479\n",
      "Iteration 5, loss = 0.03423234\n",
      "Iteration 6, loss = 0.02585482\n",
      "Iteration 7, loss = 0.01944022\n",
      "Iteration 8, loss = 0.01830734\n",
      "Iteration 9, loss = 0.01162893\n",
      "Iteration 10, loss = 0.01032692\n",
      "Iteration 11, loss = 0.01675839\n",
      "Iteration 12, loss = 0.01428875\n",
      "Iteration 13, loss = 0.01139385\n",
      "Iteration 14, loss = 0.00750883\n",
      "Iteration 15, loss = 0.01164134\n",
      "Iteration 16, loss = 0.01016241\n",
      "Iteration 17, loss = 0.00601883\n",
      "Iteration 18, loss = 0.00974924\n",
      "Iteration 19, loss = 0.00294130\n",
      "Iteration 20, loss = 0.01519448\n",
      "Iteration 21, loss = 0.01021518\n",
      "Iteration 22, loss = 0.00368693\n",
      "Iteration 23, loss = 0.00150615\n",
      "Iteration 24, loss = 0.00124548\n",
      "Iteration 25, loss = 0.00118100\n",
      "Iteration 26, loss = 0.00115548\n",
      "Iteration 27, loss = 0.00113378\n",
      "Iteration 28, loss = 0.00111301\n",
      "Iteration 29, loss = 0.00109257\n",
      "Iteration 30, loss = 0.00107175\n",
      "Iteration 31, loss = 0.00105033\n",
      "Iteration 32, loss = 0.00102809\n",
      "Iteration 33, loss = 0.00100521\n",
      "Iteration 34, loss = 0.00098099\n",
      "Iteration 35, loss = 0.00095598\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.0001, batch_size=100, hidden_layer_sizes=(400, 100), solver=adam; total time= 2.2min\n",
      "Iteration 1, loss = 1.32597337\n",
      "Iteration 2, loss = 0.53028151\n",
      "Iteration 3, loss = 0.40229831\n",
      "Iteration 4, loss = 0.34991590\n",
      "Iteration 5, loss = 0.31863314\n",
      "Iteration 6, loss = 0.29609155\n",
      "Iteration 7, loss = 0.27864539\n",
      "Iteration 8, loss = 0.26400209\n",
      "Iteration 9, loss = 0.25154843\n",
      "Iteration 10, loss = 0.24051355\n",
      "Iteration 11, loss = 0.23045733\n",
      "Iteration 12, loss = 0.22176768\n",
      "Iteration 13, loss = 0.21274729\n",
      "Iteration 14, loss = 0.20540379\n",
      "Iteration 15, loss = 0.19826111\n",
      "Iteration 16, loss = 0.19159908\n",
      "Iteration 17, loss = 0.18558808\n",
      "Iteration 18, loss = 0.17949964\n",
      "Iteration 19, loss = 0.17404722\n",
      "Iteration 20, loss = 0.16888763\n",
      "Iteration 21, loss = 0.16375187\n",
      "Iteration 22, loss = 0.15897252\n",
      "Iteration 23, loss = 0.15448979\n",
      "Iteration 24, loss = 0.15010187\n",
      "Iteration 25, loss = 0.14628102\n",
      "Iteration 26, loss = 0.14191319\n",
      "Iteration 27, loss = 0.13820781\n",
      "Iteration 28, loss = 0.13471276\n",
      "Iteration 29, loss = 0.13104481\n",
      "Iteration 30, loss = 0.12800091\n",
      "Iteration 31, loss = 0.12465705\n",
      "Iteration 32, loss = 0.12166154\n",
      "Iteration 33, loss = 0.11872616\n",
      "Iteration 34, loss = 0.11559715\n",
      "Iteration 35, loss = 0.11316745\n",
      "Iteration 36, loss = 0.11041768\n",
      "Iteration 37, loss = 0.10790130\n",
      "Iteration 38, loss = 0.10520279\n",
      "Iteration 39, loss = 0.10272813\n",
      "Iteration 40, loss = 0.10071081\n",
      "Iteration 41, loss = 0.09809108\n",
      "Iteration 42, loss = 0.09595602\n",
      "Iteration 43, loss = 0.09396390\n",
      "Iteration 44, loss = 0.09178105\n",
      "Iteration 45, loss = 0.08989273\n",
      "Iteration 46, loss = 0.08789370\n",
      "Iteration 47, loss = 0.08613745\n",
      "Iteration 48, loss = 0.08424813\n",
      "Iteration 49, loss = 0.08233832\n",
      "Iteration 50, loss = 0.08079239\n",
      "Iteration 51, loss = 0.07894309\n",
      "Iteration 52, loss = 0.07729920\n",
      "Iteration 53, loss = 0.07567116\n",
      "Iteration 54, loss = 0.07418460\n",
      "Iteration 55, loss = 0.07284731\n",
      "Iteration 56, loss = 0.07124360\n",
      "Iteration 57, loss = 0.06973697\n",
      "Iteration 58, loss = 0.06838516\n",
      "Iteration 59, loss = 0.06688154\n",
      "Iteration 60, loss = 0.06576829\n",
      "Iteration 61, loss = 0.06449352\n",
      "Iteration 62, loss = 0.06300322\n",
      "Iteration 63, loss = 0.06184794\n",
      "Iteration 64, loss = 0.06065279\n",
      "Iteration 65, loss = 0.05949145\n",
      "Iteration 66, loss = 0.05824501\n",
      "Iteration 67, loss = 0.05717159\n",
      "Iteration 68, loss = 0.05595998\n",
      "Iteration 69, loss = 0.05505152\n",
      "Iteration 70, loss = 0.05398484\n",
      "Iteration 71, loss = 0.05298365\n",
      "Iteration 72, loss = 0.05202483\n",
      "Iteration 73, loss = 0.05096595\n",
      "Iteration 74, loss = 0.04999132\n",
      "Iteration 75, loss = 0.04913430\n",
      "Iteration 76, loss = 0.04811390\n",
      "Iteration 77, loss = 0.04743007\n",
      "Iteration 78, loss = 0.04638922\n",
      "Iteration 79, loss = 0.04556182\n",
      "Iteration 80, loss = 0.04471241\n",
      "Iteration 81, loss = 0.04385155\n",
      "Iteration 82, loss = 0.04328818\n",
      "Iteration 83, loss = 0.04232601\n",
      "Iteration 84, loss = 0.04158506\n",
      "Iteration 85, loss = 0.04079414\n",
      "Iteration 86, loss = 0.04012817\n",
      "Iteration 87, loss = 0.03937344\n",
      "Iteration 88, loss = 0.03877534\n",
      "Iteration 89, loss = 0.03798202\n",
      "Iteration 90, loss = 0.03723377\n",
      "Iteration 91, loss = 0.03670405\n",
      "Iteration 92, loss = 0.03606831\n",
      "Iteration 93, loss = 0.03534437\n",
      "Iteration 94, loss = 0.03467843\n",
      "Iteration 95, loss = 0.03429397\n",
      "Iteration 96, loss = 0.03354405\n",
      "Iteration 97, loss = 0.03302345\n",
      "Iteration 98, loss = 0.03245908\n",
      "Iteration 99, loss = 0.03191861\n",
      "Iteration 100, loss = 0.03139585\n",
      "Iteration 101, loss = 0.03080130\n",
      "Iteration 102, loss = 0.03025811\n",
      "Iteration 103, loss = 0.02974412\n",
      "Iteration 104, loss = 0.02927430\n",
      "Iteration 105, loss = 0.02883852\n",
      "Iteration 106, loss = 0.02834851\n",
      "Iteration 107, loss = 0.02784933\n",
      "Iteration 108, loss = 0.02739349\n",
      "Iteration 109, loss = 0.02694850\n",
      "Iteration 110, loss = 0.02645331\n",
      "Iteration 111, loss = 0.02608617\n",
      "Iteration 112, loss = 0.02556344\n",
      "Iteration 113, loss = 0.02521693\n",
      "Iteration 114, loss = 0.02483548\n",
      "Iteration 115, loss = 0.02435418\n",
      "Iteration 116, loss = 0.02405112\n",
      "Iteration 117, loss = 0.02361088\n",
      "Iteration 118, loss = 0.02330570\n",
      "Iteration 119, loss = 0.02281702\n",
      "Iteration 120, loss = 0.02252674\n",
      "Iteration 121, loss = 0.02210848\n",
      "Iteration 122, loss = 0.02183616\n",
      "Iteration 123, loss = 0.02144006\n",
      "Iteration 124, loss = 0.02124296\n",
      "Iteration 125, loss = 0.02084371\n",
      "Iteration 126, loss = 0.02056102\n",
      "Iteration 127, loss = 0.02020508\n",
      "Iteration 128, loss = 0.01978780\n",
      "Iteration 129, loss = 0.01958631\n",
      "Iteration 130, loss = 0.01923436\n",
      "Iteration 131, loss = 0.01899127\n",
      "Iteration 132, loss = 0.01870682\n",
      "Iteration 133, loss = 0.01847302\n",
      "Iteration 134, loss = 0.01812556\n",
      "Iteration 135, loss = 0.01791433\n",
      "Iteration 136, loss = 0.01766252\n",
      "Iteration 137, loss = 0.01741080\n",
      "Iteration 138, loss = 0.01712773\n",
      "Iteration 139, loss = 0.01686771\n",
      "Iteration 140, loss = 0.01655061\n",
      "Iteration 141, loss = 0.01636498\n",
      "Iteration 142, loss = 0.01612524\n",
      "Iteration 143, loss = 0.01592007\n",
      "Iteration 144, loss = 0.01572370\n",
      "Iteration 145, loss = 0.01549962\n",
      "Iteration 146, loss = 0.01526550\n",
      "Iteration 147, loss = 0.01504542\n",
      "Iteration 148, loss = 0.01481249\n",
      "Iteration 149, loss = 0.01465384\n",
      "Iteration 150, loss = 0.01436360\n",
      "Iteration 151, loss = 0.01416398\n",
      "Iteration 152, loss = 0.01401868\n",
      "Iteration 153, loss = 0.01387407\n",
      "Iteration 154, loss = 0.01362050\n",
      "Iteration 155, loss = 0.01342564\n",
      "Iteration 156, loss = 0.01325690\n",
      "Iteration 157, loss = 0.01308249\n",
      "Iteration 158, loss = 0.01293014\n",
      "Iteration 159, loss = 0.01276311\n",
      "Iteration 160, loss = 0.01257849\n",
      "Iteration 161, loss = 0.01242247\n",
      "Iteration 162, loss = 0.01226053\n",
      "Iteration 163, loss = 0.01209247\n",
      "Iteration 164, loss = 0.01193438\n",
      "Iteration 165, loss = 0.01180411\n",
      "Iteration 166, loss = 0.01164534\n",
      "Iteration 167, loss = 0.01150591\n",
      "Iteration 168, loss = 0.01136335\n",
      "Iteration 169, loss = 0.01116979\n",
      "Iteration 170, loss = 0.01105643\n",
      "Iteration 171, loss = 0.01091322\n",
      "Iteration 172, loss = 0.01073640\n",
      "Iteration 173, loss = 0.01066995\n",
      "Iteration 174, loss = 0.01051956\n",
      "Iteration 175, loss = 0.01041725\n",
      "Iteration 176, loss = 0.01028329\n",
      "Iteration 177, loss = 0.01017268\n",
      "Iteration 178, loss = 0.01003366\n",
      "Iteration 179, loss = 0.00989545\n",
      "Iteration 180, loss = 0.00979730\n",
      "Iteration 181, loss = 0.00967865\n",
      "Iteration 182, loss = 0.00956114\n",
      "Iteration 183, loss = 0.00943436\n",
      "Iteration 184, loss = 0.00932435\n",
      "Iteration 185, loss = 0.00921826\n",
      "Iteration 186, loss = 0.00912295\n",
      "Iteration 187, loss = 0.00904312\n",
      "Iteration 188, loss = 0.00892411\n",
      "Iteration 189, loss = 0.00884407\n",
      "Iteration 190, loss = 0.00873367\n",
      "Iteration 191, loss = 0.00863317\n",
      "Iteration 192, loss = 0.00856706\n",
      "Iteration 193, loss = 0.00841600\n",
      "Iteration 194, loss = 0.00836276\n",
      "Iteration 195, loss = 0.00829232\n",
      "Iteration 196, loss = 0.00817677\n",
      "Iteration 197, loss = 0.00810725\n",
      "Iteration 198, loss = 0.00799542\n",
      "Iteration 199, loss = 0.00790446\n",
      "Iteration 200, loss = 0.00784657\n",
      "[CV] END activation=relu, alpha=0.0001, batch_size=100, hidden_layer_sizes=(400, 100), solver=sgd; total time= 7.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.36704264\n",
      "Iteration 2, loss = 0.54398146\n",
      "Iteration 3, loss = 0.40729282\n",
      "Iteration 4, loss = 0.35264815\n",
      "Iteration 5, loss = 0.32054222\n",
      "Iteration 6, loss = 0.29806032\n",
      "Iteration 7, loss = 0.28009283\n",
      "Iteration 8, loss = 0.26521822\n",
      "Iteration 9, loss = 0.25266231\n",
      "Iteration 10, loss = 0.24161724\n",
      "Iteration 11, loss = 0.23138371\n",
      "Iteration 12, loss = 0.22234667\n",
      "Iteration 13, loss = 0.21396862\n",
      "Iteration 14, loss = 0.20621375\n",
      "Iteration 15, loss = 0.19886776\n",
      "Iteration 16, loss = 0.19199961\n",
      "Iteration 17, loss = 0.18577378\n",
      "Iteration 18, loss = 0.17943882\n",
      "Iteration 19, loss = 0.17417660\n",
      "Iteration 20, loss = 0.16889837\n",
      "Iteration 21, loss = 0.16377460\n",
      "Iteration 22, loss = 0.15910794\n",
      "Iteration 23, loss = 0.15448346\n",
      "Iteration 24, loss = 0.15007885\n",
      "Iteration 25, loss = 0.14615555\n",
      "Iteration 26, loss = 0.14199379\n",
      "Iteration 27, loss = 0.13830383\n",
      "Iteration 28, loss = 0.13471654\n",
      "Iteration 29, loss = 0.13125900\n",
      "Iteration 30, loss = 0.12781903\n",
      "Iteration 31, loss = 0.12464039\n",
      "Iteration 32, loss = 0.12160741\n",
      "Iteration 33, loss = 0.11875937\n",
      "Iteration 34, loss = 0.11565348\n",
      "Iteration 35, loss = 0.11300939\n",
      "Iteration 36, loss = 0.11008754\n",
      "Iteration 37, loss = 0.10794873\n",
      "Iteration 38, loss = 0.10522234\n",
      "Iteration 39, loss = 0.10275664\n",
      "Iteration 40, loss = 0.10066520\n",
      "Iteration 41, loss = 0.09813563\n",
      "Iteration 42, loss = 0.09604038\n",
      "Iteration 43, loss = 0.09396486\n",
      "Iteration 44, loss = 0.09195241\n",
      "Iteration 45, loss = 0.08962908\n",
      "Iteration 46, loss = 0.08767614\n",
      "Iteration 47, loss = 0.08601502\n",
      "Iteration 48, loss = 0.08416665\n",
      "Iteration 49, loss = 0.08220914\n",
      "Iteration 50, loss = 0.08060599\n",
      "Iteration 51, loss = 0.07896910\n",
      "Iteration 52, loss = 0.07708412\n",
      "Iteration 53, loss = 0.07540206\n",
      "Iteration 54, loss = 0.07395300\n",
      "Iteration 55, loss = 0.07256534\n",
      "Iteration 56, loss = 0.07111914\n",
      "Iteration 57, loss = 0.06955319\n",
      "Iteration 58, loss = 0.06822519\n",
      "Iteration 59, loss = 0.06692608\n",
      "Iteration 60, loss = 0.06554817\n",
      "Iteration 61, loss = 0.06427967\n",
      "Iteration 62, loss = 0.06285314\n",
      "Iteration 63, loss = 0.06180891\n",
      "Iteration 64, loss = 0.06063578\n",
      "Iteration 65, loss = 0.05936335\n",
      "Iteration 66, loss = 0.05813827\n",
      "Iteration 67, loss = 0.05715363\n",
      "Iteration 68, loss = 0.05597096\n",
      "Iteration 69, loss = 0.05490291\n",
      "Iteration 70, loss = 0.05395166\n",
      "Iteration 71, loss = 0.05291057\n",
      "Iteration 72, loss = 0.05178512\n",
      "Iteration 73, loss = 0.05082224\n",
      "Iteration 74, loss = 0.04994179\n",
      "Iteration 75, loss = 0.04901992\n",
      "Iteration 76, loss = 0.04793524\n",
      "Iteration 77, loss = 0.04717972\n",
      "Iteration 78, loss = 0.04621895\n",
      "Iteration 79, loss = 0.04559989\n",
      "Iteration 80, loss = 0.04467367\n",
      "Iteration 81, loss = 0.04383457\n",
      "Iteration 82, loss = 0.04294374\n",
      "Iteration 83, loss = 0.04230083\n",
      "Iteration 84, loss = 0.04147168\n",
      "Iteration 85, loss = 0.04063092\n",
      "Iteration 86, loss = 0.03994568\n",
      "Iteration 87, loss = 0.03921248\n",
      "Iteration 88, loss = 0.03865047\n",
      "Iteration 89, loss = 0.03782354\n",
      "Iteration 90, loss = 0.03717967\n",
      "Iteration 91, loss = 0.03647849\n",
      "Iteration 92, loss = 0.03590830\n",
      "Iteration 93, loss = 0.03521896\n",
      "Iteration 94, loss = 0.03467700\n",
      "Iteration 95, loss = 0.03404269\n",
      "Iteration 96, loss = 0.03348214\n",
      "Iteration 97, loss = 0.03290044\n",
      "Iteration 98, loss = 0.03230195\n",
      "Iteration 99, loss = 0.03173352\n",
      "Iteration 100, loss = 0.03114456\n",
      "Iteration 101, loss = 0.03056897\n",
      "Iteration 102, loss = 0.03011486\n",
      "Iteration 103, loss = 0.02966841\n",
      "Iteration 104, loss = 0.02908662\n",
      "Iteration 105, loss = 0.02860517\n",
      "Iteration 106, loss = 0.02808299\n",
      "Iteration 107, loss = 0.02751712\n",
      "Iteration 108, loss = 0.02716890\n",
      "Iteration 109, loss = 0.02664619\n",
      "Iteration 110, loss = 0.02623091\n",
      "Iteration 111, loss = 0.02579130\n",
      "Iteration 112, loss = 0.02545932\n",
      "Iteration 113, loss = 0.02501740\n",
      "Iteration 114, loss = 0.02455825\n",
      "Iteration 115, loss = 0.02418989\n",
      "Iteration 116, loss = 0.02381637\n",
      "Iteration 117, loss = 0.02337415\n",
      "Iteration 118, loss = 0.02307806\n",
      "Iteration 119, loss = 0.02256597\n",
      "Iteration 120, loss = 0.02219163\n",
      "Iteration 121, loss = 0.02193258\n",
      "Iteration 122, loss = 0.02152575\n",
      "Iteration 123, loss = 0.02120127\n",
      "Iteration 124, loss = 0.02088913\n",
      "Iteration 125, loss = 0.02056250\n",
      "Iteration 126, loss = 0.02032132\n",
      "Iteration 127, loss = 0.01993241\n",
      "Iteration 128, loss = 0.01952221\n",
      "Iteration 129, loss = 0.01930018\n",
      "Iteration 130, loss = 0.01909935\n",
      "Iteration 131, loss = 0.01875612\n",
      "Iteration 132, loss = 0.01839696\n",
      "Iteration 133, loss = 0.01815413\n",
      "Iteration 134, loss = 0.01787056\n",
      "Iteration 135, loss = 0.01761819\n",
      "Iteration 136, loss = 0.01734997\n",
      "Iteration 137, loss = 0.01712151\n",
      "Iteration 138, loss = 0.01683918\n",
      "Iteration 139, loss = 0.01659125\n",
      "Iteration 140, loss = 0.01634755\n",
      "Iteration 141, loss = 0.01605016\n",
      "Iteration 142, loss = 0.01590884\n",
      "Iteration 143, loss = 0.01565093\n",
      "Iteration 144, loss = 0.01541510\n",
      "Iteration 145, loss = 0.01522128\n",
      "Iteration 146, loss = 0.01497712\n",
      "Iteration 147, loss = 0.01475670\n",
      "Iteration 148, loss = 0.01453694\n",
      "Iteration 149, loss = 0.01436308\n",
      "Iteration 150, loss = 0.01425470\n",
      "Iteration 151, loss = 0.01395800\n",
      "Iteration 152, loss = 0.01377939\n",
      "Iteration 153, loss = 0.01363120\n",
      "Iteration 154, loss = 0.01336548\n",
      "Iteration 155, loss = 0.01322035\n",
      "Iteration 156, loss = 0.01308156\n",
      "Iteration 157, loss = 0.01290956\n",
      "Iteration 158, loss = 0.01265744\n",
      "Iteration 159, loss = 0.01253292\n",
      "Iteration 160, loss = 0.01234581\n",
      "Iteration 161, loss = 0.01220063\n",
      "Iteration 162, loss = 0.01206107\n",
      "Iteration 163, loss = 0.01188314\n",
      "Iteration 164, loss = 0.01176713\n",
      "Iteration 165, loss = 0.01159555\n",
      "Iteration 166, loss = 0.01147079\n",
      "Iteration 167, loss = 0.01129983\n",
      "Iteration 168, loss = 0.01116484\n",
      "Iteration 169, loss = 0.01102882\n",
      "Iteration 170, loss = 0.01090582\n",
      "Iteration 171, loss = 0.01077451\n",
      "Iteration 172, loss = 0.01060825\n",
      "Iteration 173, loss = 0.01047388\n",
      "Iteration 174, loss = 0.01036125\n",
      "Iteration 175, loss = 0.01022100\n",
      "Iteration 176, loss = 0.01010594\n",
      "Iteration 177, loss = 0.00995524\n",
      "Iteration 178, loss = 0.00987192\n",
      "Iteration 179, loss = 0.00972218\n",
      "Iteration 180, loss = 0.00961438\n",
      "Iteration 181, loss = 0.00953983\n",
      "Iteration 182, loss = 0.00940202\n",
      "Iteration 183, loss = 0.00931413\n",
      "Iteration 184, loss = 0.00921786\n",
      "Iteration 185, loss = 0.00911225\n",
      "Iteration 186, loss = 0.00898914\n",
      "Iteration 187, loss = 0.00891288\n",
      "Iteration 188, loss = 0.00878506\n",
      "Iteration 189, loss = 0.00871007\n",
      "Iteration 190, loss = 0.00858632\n",
      "Iteration 191, loss = 0.00850643\n",
      "Iteration 192, loss = 0.00840601\n",
      "Iteration 193, loss = 0.00833481\n",
      "Iteration 194, loss = 0.00824739\n",
      "Iteration 195, loss = 0.00813171\n",
      "Iteration 196, loss = 0.00805354\n",
      "Iteration 197, loss = 0.00798340\n",
      "Iteration 198, loss = 0.00789305\n",
      "Iteration 199, loss = 0.00775609\n",
      "Iteration 200, loss = 0.00774226\n",
      "[CV] END activation=relu, alpha=0.0001, batch_size=100, hidden_layer_sizes=(400, 100), solver=sgd; total time= 6.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.35825147\n",
      "Iteration 2, loss = 0.54859186\n",
      "Iteration 3, loss = 0.40951654\n",
      "Iteration 4, loss = 0.35315604\n",
      "Iteration 5, loss = 0.31955768\n",
      "Iteration 6, loss = 0.29672433\n",
      "Iteration 7, loss = 0.27847086\n",
      "Iteration 8, loss = 0.26322164\n",
      "Iteration 9, loss = 0.25011701\n",
      "Iteration 10, loss = 0.23888309\n",
      "Iteration 11, loss = 0.22854846\n",
      "Iteration 12, loss = 0.21950304\n",
      "Iteration 13, loss = 0.21073856\n",
      "Iteration 14, loss = 0.20295830\n",
      "Iteration 15, loss = 0.19514833\n",
      "Iteration 16, loss = 0.18856596\n",
      "Iteration 17, loss = 0.18187870\n",
      "Iteration 18, loss = 0.17593130\n",
      "Iteration 19, loss = 0.16982556\n",
      "Iteration 20, loss = 0.16470751\n",
      "Iteration 21, loss = 0.15926829\n",
      "Iteration 22, loss = 0.15473755\n",
      "Iteration 23, loss = 0.15000986\n",
      "Iteration 24, loss = 0.14559228\n",
      "Iteration 25, loss = 0.14125835\n",
      "Iteration 26, loss = 0.13745440\n",
      "Iteration 27, loss = 0.13352134\n",
      "Iteration 28, loss = 0.12999541\n",
      "Iteration 29, loss = 0.12660163\n",
      "Iteration 30, loss = 0.12316136\n",
      "Iteration 31, loss = 0.12010730\n",
      "Iteration 32, loss = 0.11678039\n",
      "Iteration 33, loss = 0.11381212\n",
      "Iteration 34, loss = 0.11110261\n",
      "Iteration 35, loss = 0.10834300\n",
      "Iteration 36, loss = 0.10579096\n",
      "Iteration 37, loss = 0.10325518\n",
      "Iteration 38, loss = 0.10074282\n",
      "Iteration 39, loss = 0.09804781\n",
      "Iteration 40, loss = 0.09617253\n",
      "Iteration 41, loss = 0.09370135\n",
      "Iteration 42, loss = 0.09184124\n",
      "Iteration 43, loss = 0.08947278\n",
      "Iteration 44, loss = 0.08758141\n",
      "Iteration 45, loss = 0.08549123\n",
      "Iteration 46, loss = 0.08367793\n",
      "Iteration 47, loss = 0.08175782\n",
      "Iteration 48, loss = 0.08008732\n",
      "Iteration 49, loss = 0.07819928\n",
      "Iteration 50, loss = 0.07667136\n",
      "Iteration 51, loss = 0.07493868\n",
      "Iteration 52, loss = 0.07326843\n",
      "Iteration 53, loss = 0.07187027\n",
      "Iteration 54, loss = 0.07020541\n",
      "Iteration 55, loss = 0.06872265\n",
      "Iteration 56, loss = 0.06727271\n",
      "Iteration 57, loss = 0.06591664\n",
      "Iteration 58, loss = 0.06464397\n",
      "Iteration 59, loss = 0.06334938\n",
      "Iteration 60, loss = 0.06183093\n",
      "Iteration 61, loss = 0.06078920\n",
      "Iteration 62, loss = 0.05941148\n",
      "Iteration 63, loss = 0.05832069\n",
      "Iteration 64, loss = 0.05707702\n",
      "Iteration 65, loss = 0.05605340\n",
      "Iteration 66, loss = 0.05493630\n",
      "Iteration 67, loss = 0.05382751\n",
      "Iteration 68, loss = 0.05266027\n",
      "Iteration 69, loss = 0.05161890\n",
      "Iteration 70, loss = 0.05057903\n",
      "Iteration 71, loss = 0.04984273\n",
      "Iteration 72, loss = 0.04892389\n",
      "Iteration 73, loss = 0.04785476\n",
      "Iteration 74, loss = 0.04684797\n",
      "Iteration 75, loss = 0.04592874\n",
      "Iteration 76, loss = 0.04511470\n",
      "Iteration 77, loss = 0.04429589\n",
      "Iteration 78, loss = 0.04333893\n",
      "Iteration 79, loss = 0.04255201\n",
      "Iteration 80, loss = 0.04177120\n",
      "Iteration 81, loss = 0.04102796\n",
      "Iteration 82, loss = 0.04020892\n",
      "Iteration 83, loss = 0.03943705\n",
      "Iteration 84, loss = 0.03877509\n",
      "Iteration 85, loss = 0.03808171\n",
      "Iteration 86, loss = 0.03733559\n",
      "Iteration 87, loss = 0.03674533\n",
      "Iteration 88, loss = 0.03605577\n",
      "Iteration 89, loss = 0.03526710\n",
      "Iteration 90, loss = 0.03474171\n",
      "Iteration 91, loss = 0.03397858\n",
      "Iteration 92, loss = 0.03344900\n",
      "Iteration 93, loss = 0.03284573\n",
      "Iteration 94, loss = 0.03223326\n",
      "Iteration 95, loss = 0.03170843\n",
      "Iteration 96, loss = 0.03116569\n",
      "Iteration 97, loss = 0.03061998\n",
      "Iteration 98, loss = 0.03013537\n",
      "Iteration 99, loss = 0.02947233\n",
      "Iteration 100, loss = 0.02906247\n",
      "Iteration 101, loss = 0.02850559\n",
      "Iteration 102, loss = 0.02802716\n",
      "Iteration 103, loss = 0.02747014\n",
      "Iteration 104, loss = 0.02700343\n",
      "Iteration 105, loss = 0.02663103\n",
      "Iteration 106, loss = 0.02616921\n",
      "Iteration 107, loss = 0.02566985\n",
      "Iteration 108, loss = 0.02532192\n",
      "Iteration 109, loss = 0.02483402\n",
      "Iteration 110, loss = 0.02445381\n",
      "Iteration 111, loss = 0.02408838\n",
      "Iteration 112, loss = 0.02361041\n",
      "Iteration 113, loss = 0.02321519\n",
      "Iteration 114, loss = 0.02285034\n",
      "Iteration 115, loss = 0.02244977\n",
      "Iteration 116, loss = 0.02221967\n",
      "Iteration 117, loss = 0.02178855\n",
      "Iteration 118, loss = 0.02143706\n",
      "Iteration 119, loss = 0.02110675\n",
      "Iteration 120, loss = 0.02074397\n",
      "Iteration 121, loss = 0.02040818\n",
      "Iteration 122, loss = 0.02004869\n",
      "Iteration 123, loss = 0.01981386\n",
      "Iteration 124, loss = 0.01942042\n",
      "Iteration 125, loss = 0.01913982\n",
      "Iteration 126, loss = 0.01892925\n",
      "Iteration 127, loss = 0.01856964\n",
      "Iteration 128, loss = 0.01833099\n",
      "Iteration 129, loss = 0.01798215\n",
      "Iteration 130, loss = 0.01769778\n",
      "Iteration 131, loss = 0.01751207\n",
      "Iteration 132, loss = 0.01719467\n",
      "Iteration 133, loss = 0.01695053\n",
      "Iteration 134, loss = 0.01668928\n",
      "Iteration 135, loss = 0.01652395\n",
      "Iteration 136, loss = 0.01620200\n",
      "Iteration 137, loss = 0.01600745\n",
      "Iteration 138, loss = 0.01569772\n",
      "Iteration 139, loss = 0.01553200\n",
      "Iteration 140, loss = 0.01533322\n",
      "Iteration 141, loss = 0.01512153\n",
      "Iteration 142, loss = 0.01486379\n",
      "Iteration 143, loss = 0.01466650\n",
      "Iteration 144, loss = 0.01441008\n",
      "Iteration 145, loss = 0.01424344\n",
      "Iteration 146, loss = 0.01405808\n",
      "Iteration 147, loss = 0.01385114\n",
      "Iteration 148, loss = 0.01365540\n",
      "Iteration 149, loss = 0.01349112\n",
      "Iteration 150, loss = 0.01333062\n",
      "Iteration 151, loss = 0.01313039\n",
      "Iteration 152, loss = 0.01296641\n",
      "Iteration 153, loss = 0.01276499\n",
      "Iteration 154, loss = 0.01261734\n",
      "Iteration 155, loss = 0.01241017\n",
      "Iteration 156, loss = 0.01231711\n",
      "Iteration 157, loss = 0.01211458\n",
      "Iteration 158, loss = 0.01196759\n",
      "Iteration 159, loss = 0.01186291\n",
      "Iteration 160, loss = 0.01166967\n",
      "Iteration 161, loss = 0.01151255\n",
      "Iteration 162, loss = 0.01137937\n",
      "Iteration 163, loss = 0.01123348\n",
      "Iteration 164, loss = 0.01106697\n",
      "Iteration 165, loss = 0.01096980\n",
      "Iteration 166, loss = 0.01082547\n",
      "Iteration 167, loss = 0.01067892\n",
      "Iteration 168, loss = 0.01057597\n",
      "Iteration 169, loss = 0.01043506\n",
      "Iteration 170, loss = 0.01031386\n",
      "Iteration 171, loss = 0.01019457\n",
      "Iteration 172, loss = 0.01005618\n",
      "Iteration 173, loss = 0.00994025\n",
      "Iteration 174, loss = 0.00980142\n",
      "Iteration 175, loss = 0.00970783\n",
      "Iteration 176, loss = 0.00960476\n",
      "Iteration 177, loss = 0.00946453\n",
      "Iteration 178, loss = 0.00935313\n",
      "Iteration 179, loss = 0.00926125\n",
      "Iteration 180, loss = 0.00917029\n",
      "Iteration 181, loss = 0.00905679\n",
      "Iteration 182, loss = 0.00896758\n",
      "Iteration 183, loss = 0.00883105\n",
      "Iteration 184, loss = 0.00876337\n",
      "Iteration 185, loss = 0.00866539\n",
      "Iteration 186, loss = 0.00856731\n",
      "Iteration 187, loss = 0.00849149\n",
      "Iteration 188, loss = 0.00838775\n",
      "Iteration 189, loss = 0.00829483\n",
      "Iteration 190, loss = 0.00820023\n",
      "Iteration 191, loss = 0.00810981\n",
      "Iteration 192, loss = 0.00802207\n",
      "Iteration 193, loss = 0.00796040\n",
      "Iteration 194, loss = 0.00784119\n",
      "Iteration 195, loss = 0.00778522\n",
      "Iteration 196, loss = 0.00768973\n",
      "Iteration 197, loss = 0.00761742\n",
      "Iteration 198, loss = 0.00752566\n",
      "Iteration 199, loss = 0.00746075\n",
      "Iteration 200, loss = 0.00739814\n",
      "[CV] END activation=relu, alpha=0.0001, batch_size=100, hidden_layer_sizes=(400, 100), solver=sgd; total time= 6.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.41973127\n",
      "Iteration 2, loss = 0.57852613\n",
      "Iteration 3, loss = 0.42432515\n",
      "Iteration 4, loss = 0.36343165\n",
      "Iteration 5, loss = 0.32860997\n",
      "Iteration 6, loss = 0.30507791\n",
      "Iteration 7, loss = 0.28685849\n",
      "Iteration 8, loss = 0.27189603\n",
      "Iteration 9, loss = 0.25932380\n",
      "Iteration 10, loss = 0.24847543\n",
      "Iteration 11, loss = 0.23841186\n",
      "Iteration 12, loss = 0.22905796\n",
      "Iteration 13, loss = 0.22067728\n",
      "Iteration 14, loss = 0.21255349\n",
      "Iteration 15, loss = 0.20586168\n",
      "Iteration 16, loss = 0.19837251\n",
      "Iteration 17, loss = 0.19228199\n",
      "Iteration 18, loss = 0.18608675\n",
      "Iteration 19, loss = 0.18005206\n",
      "Iteration 20, loss = 0.17491454\n",
      "Iteration 21, loss = 0.16970375\n",
      "Iteration 22, loss = 0.16455308\n",
      "Iteration 23, loss = 0.15948349\n",
      "Iteration 24, loss = 0.15514860\n",
      "Iteration 25, loss = 0.15045499\n",
      "Iteration 26, loss = 0.14645044\n",
      "Iteration 27, loss = 0.14224948\n",
      "Iteration 28, loss = 0.13856859\n",
      "Iteration 29, loss = 0.13461353\n",
      "Iteration 30, loss = 0.13101151\n",
      "Iteration 31, loss = 0.12748843\n",
      "Iteration 32, loss = 0.12445308\n",
      "Iteration 33, loss = 0.12132356\n",
      "Iteration 34, loss = 0.11825049\n",
      "Iteration 35, loss = 0.11498899\n",
      "Iteration 36, loss = 0.11264261\n",
      "Iteration 37, loss = 0.10957928\n",
      "Iteration 38, loss = 0.10713009\n",
      "Iteration 39, loss = 0.10441730\n",
      "Iteration 40, loss = 0.10205627\n",
      "Iteration 41, loss = 0.09973246\n",
      "Iteration 42, loss = 0.09770777\n",
      "Iteration 43, loss = 0.09531357\n",
      "Iteration 44, loss = 0.09300526\n",
      "Iteration 45, loss = 0.09110682\n",
      "Iteration 46, loss = 0.08899460\n",
      "Iteration 47, loss = 0.08705124\n",
      "Iteration 48, loss = 0.08538685\n",
      "Iteration 49, loss = 0.08339419\n",
      "Iteration 50, loss = 0.08138665\n",
      "Iteration 51, loss = 0.07972613\n",
      "Iteration 52, loss = 0.07820041\n",
      "Iteration 53, loss = 0.07661468\n",
      "Iteration 54, loss = 0.07480291\n",
      "Iteration 55, loss = 0.07333316\n",
      "Iteration 56, loss = 0.07190025\n",
      "Iteration 57, loss = 0.07040319\n",
      "Iteration 58, loss = 0.06900923\n",
      "Iteration 59, loss = 0.06768119\n",
      "Iteration 60, loss = 0.06626903\n",
      "Iteration 61, loss = 0.06483952\n",
      "Iteration 62, loss = 0.06376545\n",
      "Iteration 63, loss = 0.06237621\n",
      "Iteration 64, loss = 0.06111359\n",
      "Iteration 65, loss = 0.05999649\n",
      "Iteration 66, loss = 0.05863623\n",
      "Iteration 67, loss = 0.05765000\n",
      "Iteration 68, loss = 0.05641877\n",
      "Iteration 69, loss = 0.05540243\n",
      "Iteration 70, loss = 0.05445735\n",
      "Iteration 71, loss = 0.05323523\n",
      "Iteration 72, loss = 0.05241913\n",
      "Iteration 73, loss = 0.05133185\n",
      "Iteration 74, loss = 0.05043982\n",
      "Iteration 75, loss = 0.04953272\n",
      "Iteration 76, loss = 0.04844731\n",
      "Iteration 77, loss = 0.04754128\n",
      "Iteration 78, loss = 0.04680529\n",
      "Iteration 79, loss = 0.04568082\n",
      "Iteration 80, loss = 0.04506498\n",
      "Iteration 81, loss = 0.04428075\n",
      "Iteration 82, loss = 0.04337151\n",
      "Iteration 83, loss = 0.04268776\n",
      "Iteration 84, loss = 0.04182497\n",
      "Iteration 85, loss = 0.04105578\n",
      "Iteration 86, loss = 0.04016581\n",
      "Iteration 87, loss = 0.03953990\n",
      "Iteration 88, loss = 0.03877735\n",
      "Iteration 89, loss = 0.03816641\n",
      "Iteration 90, loss = 0.03746034\n",
      "Iteration 91, loss = 0.03675759\n",
      "Iteration 92, loss = 0.03618831\n",
      "Iteration 93, loss = 0.03553922\n",
      "Iteration 94, loss = 0.03479309\n",
      "Iteration 95, loss = 0.03419907\n",
      "Iteration 96, loss = 0.03361891\n",
      "Iteration 97, loss = 0.03304691\n",
      "Iteration 98, loss = 0.03240507\n",
      "Iteration 99, loss = 0.03189197\n",
      "Iteration 100, loss = 0.03137950\n",
      "Iteration 101, loss = 0.03073692\n",
      "Iteration 102, loss = 0.03031767\n",
      "Iteration 103, loss = 0.02983837\n",
      "Iteration 104, loss = 0.02923479\n",
      "Iteration 105, loss = 0.02873474\n",
      "Iteration 106, loss = 0.02827493\n",
      "Iteration 107, loss = 0.02778417\n",
      "Iteration 108, loss = 0.02723032\n",
      "Iteration 109, loss = 0.02680790\n",
      "Iteration 110, loss = 0.02637701\n",
      "Iteration 111, loss = 0.02597166\n",
      "Iteration 112, loss = 0.02535758\n",
      "Iteration 113, loss = 0.02510743\n",
      "Iteration 114, loss = 0.02464413\n",
      "Iteration 115, loss = 0.02423196\n",
      "Iteration 116, loss = 0.02386523\n",
      "Iteration 117, loss = 0.02345926\n",
      "Iteration 118, loss = 0.02309698\n",
      "Iteration 119, loss = 0.02269624\n",
      "Iteration 120, loss = 0.02229835\n",
      "Iteration 121, loss = 0.02192981\n",
      "Iteration 122, loss = 0.02155135\n",
      "Iteration 123, loss = 0.02127784\n",
      "Iteration 124, loss = 0.02099729\n",
      "Iteration 125, loss = 0.02056758\n",
      "Iteration 126, loss = 0.02024639\n",
      "Iteration 127, loss = 0.01992941\n",
      "Iteration 128, loss = 0.01967969\n",
      "Iteration 129, loss = 0.01929414\n",
      "Iteration 130, loss = 0.01900190\n",
      "Iteration 131, loss = 0.01871645\n",
      "Iteration 132, loss = 0.01847832\n",
      "Iteration 133, loss = 0.01812639\n",
      "Iteration 134, loss = 0.01781622\n",
      "Iteration 135, loss = 0.01767237\n",
      "Iteration 136, loss = 0.01730974\n",
      "Iteration 137, loss = 0.01709259\n",
      "Iteration 138, loss = 0.01687398\n",
      "Iteration 139, loss = 0.01658159\n",
      "Iteration 140, loss = 0.01627169\n",
      "Iteration 141, loss = 0.01611610\n",
      "Iteration 142, loss = 0.01581753\n",
      "Iteration 143, loss = 0.01561615\n",
      "Iteration 144, loss = 0.01546028\n",
      "Iteration 145, loss = 0.01518948\n",
      "Iteration 146, loss = 0.01491659\n",
      "Iteration 147, loss = 0.01475424\n",
      "Iteration 148, loss = 0.01451598\n",
      "Iteration 149, loss = 0.01435972\n",
      "Iteration 150, loss = 0.01417783\n",
      "Iteration 151, loss = 0.01394561\n",
      "Iteration 152, loss = 0.01375722\n",
      "Iteration 153, loss = 0.01355751\n",
      "Iteration 154, loss = 0.01337679\n",
      "Iteration 155, loss = 0.01315169\n",
      "Iteration 156, loss = 0.01300313\n",
      "Iteration 157, loss = 0.01284222\n",
      "Iteration 158, loss = 0.01261563\n",
      "Iteration 159, loss = 0.01250668\n",
      "Iteration 160, loss = 0.01232398\n",
      "Iteration 161, loss = 0.01214285\n",
      "Iteration 162, loss = 0.01205941\n",
      "Iteration 163, loss = 0.01185312\n",
      "Iteration 164, loss = 0.01167885\n",
      "Iteration 165, loss = 0.01152311\n",
      "Iteration 166, loss = 0.01139549\n",
      "Iteration 167, loss = 0.01122753\n",
      "Iteration 168, loss = 0.01110321\n",
      "Iteration 169, loss = 0.01097326\n",
      "Iteration 170, loss = 0.01083307\n",
      "Iteration 171, loss = 0.01069752\n",
      "Iteration 172, loss = 0.01057963\n",
      "Iteration 173, loss = 0.01042509\n",
      "Iteration 174, loss = 0.01030725\n",
      "Iteration 175, loss = 0.01017001\n",
      "Iteration 176, loss = 0.01003964\n",
      "Iteration 177, loss = 0.00991033\n",
      "Iteration 178, loss = 0.00984964\n",
      "Iteration 179, loss = 0.00970936\n",
      "Iteration 180, loss = 0.00961462\n",
      "Iteration 181, loss = 0.00945496\n",
      "Iteration 182, loss = 0.00936828\n",
      "Iteration 183, loss = 0.00924275\n",
      "Iteration 184, loss = 0.00918740\n",
      "Iteration 185, loss = 0.00906964\n",
      "Iteration 186, loss = 0.00895575\n",
      "Iteration 187, loss = 0.00885760\n",
      "Iteration 188, loss = 0.00875892\n",
      "Iteration 189, loss = 0.00864918\n",
      "Iteration 190, loss = 0.00855219\n",
      "Iteration 191, loss = 0.00849212\n",
      "Iteration 192, loss = 0.00837200\n",
      "Iteration 193, loss = 0.00827126\n",
      "Iteration 194, loss = 0.00819457\n",
      "Iteration 195, loss = 0.00810195\n",
      "Iteration 196, loss = 0.00804249\n",
      "Iteration 197, loss = 0.00791319\n",
      "Iteration 198, loss = 0.00783672\n",
      "Iteration 199, loss = 0.00776730\n",
      "Iteration 200, loss = 0.00768688\n",
      "[CV] END activation=relu, alpha=0.0001, batch_size=100, hidden_layer_sizes=(400, 100), solver=sgd; total time= 8.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.37376578\n",
      "Iteration 2, loss = 0.55063190\n",
      "Iteration 3, loss = 0.41297196\n",
      "Iteration 4, loss = 0.35805798\n",
      "Iteration 5, loss = 0.32643702\n",
      "Iteration 6, loss = 0.30364581\n",
      "Iteration 7, loss = 0.28615583\n",
      "Iteration 8, loss = 0.27125444\n",
      "Iteration 9, loss = 0.25898934\n",
      "Iteration 10, loss = 0.24775068\n",
      "Iteration 11, loss = 0.23794656\n",
      "Iteration 12, loss = 0.22859294\n",
      "Iteration 13, loss = 0.22039963\n",
      "Iteration 14, loss = 0.21286480\n",
      "Iteration 15, loss = 0.20539321\n",
      "Iteration 16, loss = 0.19844159\n",
      "Iteration 17, loss = 0.19228941\n",
      "Iteration 18, loss = 0.18597943\n",
      "Iteration 19, loss = 0.18041383\n",
      "Iteration 20, loss = 0.17471463\n",
      "Iteration 21, loss = 0.16952747\n",
      "Iteration 22, loss = 0.16470207\n",
      "Iteration 23, loss = 0.15974099\n",
      "Iteration 24, loss = 0.15490367\n",
      "Iteration 25, loss = 0.15073509\n",
      "Iteration 26, loss = 0.14676733\n",
      "Iteration 27, loss = 0.14274870\n",
      "Iteration 28, loss = 0.13893944\n",
      "Iteration 29, loss = 0.13501139\n",
      "Iteration 30, loss = 0.13178134\n",
      "Iteration 31, loss = 0.12832201\n",
      "Iteration 32, loss = 0.12486052\n",
      "Iteration 33, loss = 0.12199825\n",
      "Iteration 34, loss = 0.11900044\n",
      "Iteration 35, loss = 0.11585221\n",
      "Iteration 36, loss = 0.11308860\n",
      "Iteration 37, loss = 0.11056928\n",
      "Iteration 38, loss = 0.10772624\n",
      "Iteration 39, loss = 0.10519959\n",
      "Iteration 40, loss = 0.10296212\n",
      "Iteration 41, loss = 0.10042958\n",
      "Iteration 42, loss = 0.09807444\n",
      "Iteration 43, loss = 0.09588207\n",
      "Iteration 44, loss = 0.09378254\n",
      "Iteration 45, loss = 0.09171551\n",
      "Iteration 46, loss = 0.08967028\n",
      "Iteration 47, loss = 0.08768179\n",
      "Iteration 48, loss = 0.08586320\n",
      "Iteration 49, loss = 0.08410822\n",
      "Iteration 50, loss = 0.08228280\n",
      "Iteration 51, loss = 0.08050822\n",
      "Iteration 52, loss = 0.07884015\n",
      "Iteration 53, loss = 0.07713108\n",
      "Iteration 54, loss = 0.07568943\n",
      "Iteration 55, loss = 0.07383496\n",
      "Iteration 56, loss = 0.07249268\n",
      "Iteration 57, loss = 0.07095617\n",
      "Iteration 58, loss = 0.06950178\n",
      "Iteration 59, loss = 0.06815306\n",
      "Iteration 60, loss = 0.06679163\n",
      "Iteration 61, loss = 0.06544557\n",
      "Iteration 62, loss = 0.06419335\n",
      "Iteration 63, loss = 0.06299225\n",
      "Iteration 64, loss = 0.06169164\n",
      "Iteration 65, loss = 0.06040927\n",
      "Iteration 66, loss = 0.05928610\n",
      "Iteration 67, loss = 0.05822515\n",
      "Iteration 68, loss = 0.05692675\n",
      "Iteration 69, loss = 0.05588779\n",
      "Iteration 70, loss = 0.05469160\n",
      "Iteration 71, loss = 0.05376166\n",
      "Iteration 72, loss = 0.05288516\n",
      "Iteration 73, loss = 0.05168288\n",
      "Iteration 74, loss = 0.05086053\n",
      "Iteration 75, loss = 0.04977897\n",
      "Iteration 76, loss = 0.04879798\n",
      "Iteration 77, loss = 0.04785605\n",
      "Iteration 78, loss = 0.04703010\n",
      "Iteration 79, loss = 0.04619196\n",
      "Iteration 80, loss = 0.04541493\n",
      "Iteration 81, loss = 0.04435364\n",
      "Iteration 82, loss = 0.04368797\n",
      "Iteration 83, loss = 0.04287263\n",
      "Iteration 84, loss = 0.04213060\n",
      "Iteration 85, loss = 0.04133124\n",
      "Iteration 86, loss = 0.04057220\n",
      "Iteration 87, loss = 0.03984290\n",
      "Iteration 88, loss = 0.03914133\n",
      "Iteration 89, loss = 0.03855475\n",
      "Iteration 90, loss = 0.03774690\n",
      "Iteration 91, loss = 0.03707577\n",
      "Iteration 92, loss = 0.03646731\n",
      "Iteration 93, loss = 0.03574629\n",
      "Iteration 94, loss = 0.03508019\n",
      "Iteration 95, loss = 0.03437863\n",
      "Iteration 96, loss = 0.03383520\n",
      "Iteration 97, loss = 0.03324126\n",
      "Iteration 98, loss = 0.03261446\n",
      "Iteration 99, loss = 0.03205338\n",
      "Iteration 100, loss = 0.03158580\n",
      "Iteration 101, loss = 0.03106842\n",
      "Iteration 102, loss = 0.03045163\n",
      "Iteration 103, loss = 0.02996220\n",
      "Iteration 104, loss = 0.02946742\n",
      "Iteration 105, loss = 0.02902623\n",
      "Iteration 106, loss = 0.02837611\n",
      "Iteration 107, loss = 0.02788817\n",
      "Iteration 108, loss = 0.02739213\n",
      "Iteration 109, loss = 0.02698060\n",
      "Iteration 110, loss = 0.02663052\n",
      "Iteration 111, loss = 0.02616660\n",
      "Iteration 112, loss = 0.02570797\n",
      "Iteration 113, loss = 0.02535896\n",
      "Iteration 114, loss = 0.02478093\n",
      "Iteration 115, loss = 0.02440301\n",
      "Iteration 116, loss = 0.02399342\n",
      "Iteration 117, loss = 0.02360727\n",
      "Iteration 118, loss = 0.02329566\n",
      "Iteration 119, loss = 0.02285196\n",
      "Iteration 120, loss = 0.02253127\n",
      "Iteration 121, loss = 0.02208804\n",
      "Iteration 122, loss = 0.02180620\n",
      "Iteration 123, loss = 0.02134450\n",
      "Iteration 124, loss = 0.02108803\n",
      "Iteration 125, loss = 0.02078660\n",
      "Iteration 126, loss = 0.02041167\n",
      "Iteration 127, loss = 0.02011859\n",
      "Iteration 128, loss = 0.01978847\n",
      "Iteration 129, loss = 0.01944308\n",
      "Iteration 130, loss = 0.01925408\n",
      "Iteration 131, loss = 0.01887142\n",
      "Iteration 132, loss = 0.01863592\n",
      "Iteration 133, loss = 0.01830422\n",
      "Iteration 134, loss = 0.01798295\n",
      "Iteration 135, loss = 0.01780157\n",
      "Iteration 136, loss = 0.01749091\n",
      "Iteration 137, loss = 0.01723675\n",
      "Iteration 138, loss = 0.01700614\n",
      "Iteration 139, loss = 0.01671258\n",
      "Iteration 140, loss = 0.01650891\n",
      "Iteration 141, loss = 0.01619594\n",
      "Iteration 142, loss = 0.01600543\n",
      "Iteration 143, loss = 0.01577241\n",
      "Iteration 144, loss = 0.01550547\n",
      "Iteration 145, loss = 0.01533248\n",
      "Iteration 146, loss = 0.01511065\n",
      "Iteration 147, loss = 0.01488698\n",
      "Iteration 148, loss = 0.01469804\n",
      "Iteration 149, loss = 0.01436142\n",
      "Iteration 150, loss = 0.01425557\n",
      "Iteration 151, loss = 0.01411082\n",
      "Iteration 152, loss = 0.01386995\n",
      "Iteration 153, loss = 0.01366534\n",
      "Iteration 154, loss = 0.01349708\n",
      "Iteration 155, loss = 0.01331622\n",
      "Iteration 156, loss = 0.01314382\n",
      "Iteration 157, loss = 0.01294738\n",
      "Iteration 158, loss = 0.01277739\n",
      "Iteration 159, loss = 0.01260060\n",
      "Iteration 160, loss = 0.01245122\n",
      "Iteration 161, loss = 0.01227687\n",
      "Iteration 162, loss = 0.01209047\n",
      "Iteration 163, loss = 0.01198380\n",
      "Iteration 164, loss = 0.01178568\n",
      "Iteration 165, loss = 0.01165296\n",
      "Iteration 166, loss = 0.01150855\n",
      "Iteration 167, loss = 0.01140869\n",
      "Iteration 168, loss = 0.01121838\n",
      "Iteration 169, loss = 0.01108358\n",
      "Iteration 170, loss = 0.01096621\n",
      "Iteration 171, loss = 0.01080641\n",
      "Iteration 172, loss = 0.01067982\n",
      "Iteration 173, loss = 0.01052844\n",
      "Iteration 174, loss = 0.01039458\n",
      "Iteration 175, loss = 0.01028656\n",
      "Iteration 176, loss = 0.01016709\n",
      "Iteration 177, loss = 0.01005350\n",
      "Iteration 178, loss = 0.00992447\n",
      "Iteration 179, loss = 0.00981046\n",
      "Iteration 180, loss = 0.00969506\n",
      "Iteration 181, loss = 0.00959137\n",
      "Iteration 182, loss = 0.00945207\n",
      "Iteration 183, loss = 0.00936744\n",
      "Iteration 184, loss = 0.00925205\n",
      "Iteration 185, loss = 0.00910061\n",
      "Iteration 186, loss = 0.00904714\n",
      "Iteration 187, loss = 0.00892634\n",
      "Iteration 188, loss = 0.00883447\n",
      "Iteration 189, loss = 0.00871267\n",
      "Iteration 190, loss = 0.00867086\n",
      "Iteration 191, loss = 0.00852641\n",
      "Iteration 192, loss = 0.00845022\n",
      "Iteration 193, loss = 0.00835536\n",
      "Iteration 194, loss = 0.00824956\n",
      "Iteration 195, loss = 0.00817151\n",
      "Iteration 196, loss = 0.00808569\n",
      "Iteration 197, loss = 0.00797761\n",
      "Iteration 198, loss = 0.00787653\n",
      "Iteration 199, loss = 0.00783933\n",
      "Iteration 200, loss = 0.00772377\n",
      "[CV] END activation=relu, alpha=0.0001, batch_size=100, hidden_layer_sizes=(400, 100), solver=sgd; total time= 6.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.40408029\n",
      "Iteration 2, loss = 0.15809320\n",
      "Iteration 3, loss = 0.10539299\n",
      "Iteration 4, loss = 0.07997723\n",
      "Iteration 5, loss = 0.05915609\n",
      "Iteration 6, loss = 0.04687960\n",
      "Iteration 7, loss = 0.03750140\n",
      "Iteration 8, loss = 0.03197810\n",
      "Iteration 9, loss = 0.02720507\n",
      "Iteration 10, loss = 0.02112453\n",
      "Iteration 11, loss = 0.02005264\n",
      "Iteration 12, loss = 0.01488737\n",
      "Iteration 13, loss = 0.01731260\n",
      "Iteration 14, loss = 0.02140952\n",
      "Iteration 15, loss = 0.01462453\n",
      "Iteration 16, loss = 0.01113501\n",
      "Iteration 17, loss = 0.01486563\n",
      "Iteration 18, loss = 0.01405725\n",
      "Iteration 19, loss = 0.00691302\n",
      "Iteration 20, loss = 0.00830687\n",
      "Iteration 21, loss = 0.00832213\n",
      "Iteration 22, loss = 0.01991589\n",
      "Iteration 23, loss = 0.01103749\n",
      "Iteration 24, loss = 0.00443311\n",
      "Iteration 25, loss = 0.00147525\n",
      "Iteration 26, loss = 0.01010989\n",
      "Iteration 27, loss = 0.01605642\n",
      "Iteration 28, loss = 0.01183395\n",
      "Iteration 29, loss = 0.00769113\n",
      "Iteration 30, loss = 0.00498168\n",
      "Iteration 31, loss = 0.01101722\n",
      "Iteration 32, loss = 0.01180752\n",
      "Iteration 33, loss = 0.00538668\n",
      "Iteration 34, loss = 0.00482696\n",
      "Iteration 35, loss = 0.00642662\n",
      "Iteration 36, loss = 0.01455899\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.0001, batch_size=100, hidden_layer_sizes=(100, 100, 100), solver=adam; total time=  54.4s\n",
      "Iteration 1, loss = 0.40271797\n",
      "Iteration 2, loss = 0.15613174\n",
      "Iteration 3, loss = 0.10803538\n",
      "Iteration 4, loss = 0.08341905\n",
      "Iteration 5, loss = 0.06444637\n",
      "Iteration 6, loss = 0.05309530\n",
      "Iteration 7, loss = 0.04232366\n",
      "Iteration 8, loss = 0.03378937\n",
      "Iteration 9, loss = 0.03019653\n",
      "Iteration 10, loss = 0.02860880\n",
      "Iteration 11, loss = 0.02102773\n",
      "Iteration 12, loss = 0.01921179\n",
      "Iteration 13, loss = 0.01877815\n",
      "Iteration 14, loss = 0.01651192\n",
      "Iteration 15, loss = 0.01814286\n",
      "Iteration 16, loss = 0.01315887\n",
      "Iteration 17, loss = 0.00848571\n",
      "Iteration 18, loss = 0.01196816\n",
      "Iteration 19, loss = 0.01546867\n",
      "Iteration 20, loss = 0.01568365\n",
      "Iteration 21, loss = 0.01165235\n",
      "Iteration 22, loss = 0.01162550\n",
      "Iteration 23, loss = 0.01019719\n",
      "Iteration 24, loss = 0.00593631\n",
      "Iteration 25, loss = 0.01378772\n",
      "Iteration 26, loss = 0.01116407\n",
      "Iteration 27, loss = 0.01177663\n",
      "Iteration 28, loss = 0.00438864\n",
      "Iteration 29, loss = 0.00455211\n",
      "Iteration 30, loss = 0.00916689\n",
      "Iteration 31, loss = 0.01534430\n",
      "Iteration 32, loss = 0.00582240\n",
      "Iteration 33, loss = 0.01209988\n",
      "Iteration 34, loss = 0.00803760\n",
      "Iteration 35, loss = 0.00596976\n",
      "Iteration 36, loss = 0.00793535\n",
      "Iteration 37, loss = 0.00753570\n",
      "Iteration 38, loss = 0.01241465\n",
      "Iteration 39, loss = 0.00933342\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.0001, batch_size=100, hidden_layer_sizes=(100, 100, 100), solver=adam; total time=  58.4s\n",
      "Iteration 1, loss = 0.41249734\n",
      "Iteration 2, loss = 0.15030155\n",
      "Iteration 3, loss = 0.10759475\n",
      "Iteration 4, loss = 0.08171115\n",
      "Iteration 5, loss = 0.05970599\n",
      "Iteration 6, loss = 0.04844234\n",
      "Iteration 7, loss = 0.03861122\n",
      "Iteration 8, loss = 0.03618495\n",
      "Iteration 9, loss = 0.02472440\n",
      "Iteration 10, loss = 0.02276707\n",
      "Iteration 11, loss = 0.02079525\n",
      "Iteration 12, loss = 0.02014448\n",
      "Iteration 13, loss = 0.01899761\n",
      "Iteration 14, loss = 0.01079597\n",
      "Iteration 15, loss = 0.01386855\n",
      "Iteration 16, loss = 0.01926343\n",
      "Iteration 17, loss = 0.01348560\n",
      "Iteration 18, loss = 0.01032317\n",
      "Iteration 19, loss = 0.01146501\n",
      "Iteration 20, loss = 0.01739087\n",
      "Iteration 21, loss = 0.00916707\n",
      "Iteration 22, loss = 0.00904164\n",
      "Iteration 23, loss = 0.00925749\n",
      "Iteration 24, loss = 0.00463030\n",
      "Iteration 25, loss = 0.00787854\n",
      "Iteration 26, loss = 0.01679714\n",
      "Iteration 27, loss = 0.01166321\n",
      "Iteration 28, loss = 0.00458953\n",
      "Iteration 29, loss = 0.00756430\n",
      "Iteration 30, loss = 0.01024957\n",
      "Iteration 31, loss = 0.00846879\n",
      "Iteration 32, loss = 0.01232772\n",
      "Iteration 33, loss = 0.00852991\n",
      "Iteration 34, loss = 0.00238048\n",
      "Iteration 35, loss = 0.00085849\n",
      "Iteration 36, loss = 0.00076118\n",
      "Iteration 37, loss = 0.00073634\n",
      "Iteration 38, loss = 0.00072199\n",
      "Iteration 39, loss = 0.00071086\n",
      "Iteration 40, loss = 0.00070109\n",
      "Iteration 41, loss = 0.00069198\n",
      "Iteration 42, loss = 0.00068299\n",
      "Iteration 43, loss = 0.00067472\n",
      "Iteration 44, loss = 0.00066593\n",
      "Iteration 45, loss = 0.00065707\n",
      "Iteration 46, loss = 0.00064794\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.0001, batch_size=100, hidden_layer_sizes=(100, 100, 100), solver=adam; total time= 1.2min\n",
      "Iteration 1, loss = 0.40104452\n",
      "Iteration 2, loss = 0.15580869\n",
      "Iteration 3, loss = 0.10935411\n",
      "Iteration 4, loss = 0.07841113\n",
      "Iteration 5, loss = 0.06240961\n",
      "Iteration 6, loss = 0.04803877\n",
      "Iteration 7, loss = 0.04263404\n",
      "Iteration 8, loss = 0.03168621\n",
      "Iteration 9, loss = 0.02643013\n",
      "Iteration 10, loss = 0.02186783\n",
      "Iteration 11, loss = 0.02022064\n",
      "Iteration 12, loss = 0.02065795\n",
      "Iteration 13, loss = 0.01718031\n",
      "Iteration 14, loss = 0.01480601\n",
      "Iteration 15, loss = 0.01477872\n",
      "Iteration 16, loss = 0.01196254\n",
      "Iteration 17, loss = 0.02046972\n",
      "Iteration 18, loss = 0.00860485\n",
      "Iteration 19, loss = 0.01161142\n",
      "Iteration 20, loss = 0.01258175\n",
      "Iteration 21, loss = 0.00887340\n",
      "Iteration 22, loss = 0.01029921\n",
      "Iteration 23, loss = 0.01472824\n",
      "Iteration 24, loss = 0.01028009\n",
      "Iteration 25, loss = 0.00238473\n",
      "Iteration 26, loss = 0.00113928\n",
      "Iteration 27, loss = 0.00073285\n",
      "Iteration 28, loss = 0.00067218\n",
      "Iteration 29, loss = 0.00064116\n",
      "Iteration 30, loss = 0.00062539\n",
      "Iteration 31, loss = 0.00061361\n",
      "Iteration 32, loss = 0.00060394\n",
      "Iteration 33, loss = 0.00059018\n",
      "Iteration 34, loss = 0.00058039\n",
      "Iteration 35, loss = 0.00057201\n",
      "Iteration 36, loss = 0.00056366\n",
      "Iteration 37, loss = 0.00055623\n",
      "Iteration 38, loss = 0.00054816\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.0001, batch_size=100, hidden_layer_sizes=(100, 100, 100), solver=adam; total time= 1.1min\n",
      "Iteration 1, loss = 0.40336516\n",
      "Iteration 2, loss = 0.16215639\n",
      "Iteration 3, loss = 0.11136080\n",
      "Iteration 4, loss = 0.08466519\n",
      "Iteration 5, loss = 0.06471102\n",
      "Iteration 6, loss = 0.05306944\n",
      "Iteration 7, loss = 0.04421388\n",
      "Iteration 8, loss = 0.03689123\n",
      "Iteration 9, loss = 0.03063532\n",
      "Iteration 10, loss = 0.02403827\n",
      "Iteration 11, loss = 0.02049442\n",
      "Iteration 12, loss = 0.01783682\n",
      "Iteration 13, loss = 0.02110054\n",
      "Iteration 14, loss = 0.01973304\n",
      "Iteration 15, loss = 0.01290902\n",
      "Iteration 16, loss = 0.01701253\n",
      "Iteration 17, loss = 0.01558202\n",
      "Iteration 18, loss = 0.01401378\n",
      "Iteration 19, loss = 0.00696248\n",
      "Iteration 20, loss = 0.01565257\n",
      "Iteration 21, loss = 0.00940307\n",
      "Iteration 22, loss = 0.01758601\n",
      "Iteration 23, loss = 0.00696162\n",
      "Iteration 24, loss = 0.01050463\n",
      "Iteration 25, loss = 0.01117932\n",
      "Iteration 26, loss = 0.00924377\n",
      "Iteration 27, loss = 0.00884643\n",
      "Iteration 28, loss = 0.00739091\n",
      "Iteration 29, loss = 0.01799803\n",
      "Iteration 30, loss = 0.00651017\n",
      "Iteration 31, loss = 0.00169420\n",
      "Iteration 32, loss = 0.00087040\n",
      "Iteration 33, loss = 0.00073648\n",
      "Iteration 34, loss = 0.00071285\n",
      "Iteration 35, loss = 0.00069861\n",
      "Iteration 36, loss = 0.00068637\n",
      "Iteration 37, loss = 0.00067603\n",
      "Iteration 38, loss = 0.00066657\n",
      "Iteration 39, loss = 0.00065744\n",
      "Iteration 40, loss = 0.00064885\n",
      "Iteration 41, loss = 0.00064004\n",
      "Iteration 42, loss = 0.00063122\n",
      "Iteration 43, loss = 0.00062226\n",
      "Iteration 44, loss = 0.00061288\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.0001, batch_size=100, hidden_layer_sizes=(100, 100, 100), solver=adam; total time= 1.1min\n",
      "Iteration 1, loss = 1.76392184\n",
      "Iteration 2, loss = 0.68781250\n",
      "Iteration 3, loss = 0.44817471\n",
      "Iteration 4, loss = 0.37415701\n",
      "Iteration 5, loss = 0.33474262\n",
      "Iteration 6, loss = 0.30760918\n",
      "Iteration 7, loss = 0.28614306\n",
      "Iteration 8, loss = 0.26945691\n",
      "Iteration 9, loss = 0.25477434\n",
      "Iteration 10, loss = 0.24277909\n",
      "Iteration 11, loss = 0.23073094\n",
      "Iteration 12, loss = 0.22103236\n",
      "Iteration 13, loss = 0.21155328\n",
      "Iteration 14, loss = 0.20347731\n",
      "Iteration 15, loss = 0.19560863\n",
      "Iteration 16, loss = 0.18863095\n",
      "Iteration 17, loss = 0.18205561\n",
      "Iteration 18, loss = 0.17559922\n",
      "Iteration 19, loss = 0.16997200\n",
      "Iteration 20, loss = 0.16481685\n",
      "Iteration 21, loss = 0.15952387\n",
      "Iteration 22, loss = 0.15413848\n",
      "Iteration 23, loss = 0.15011788\n",
      "Iteration 24, loss = 0.14551354\n",
      "Iteration 25, loss = 0.14128883\n",
      "Iteration 26, loss = 0.13735147\n",
      "Iteration 27, loss = 0.13341076\n",
      "Iteration 28, loss = 0.12954762\n",
      "Iteration 29, loss = 0.12607491\n",
      "Iteration 30, loss = 0.12286694\n",
      "Iteration 31, loss = 0.12006107\n",
      "Iteration 32, loss = 0.11663347\n",
      "Iteration 33, loss = 0.11331606\n",
      "Iteration 34, loss = 0.11040746\n",
      "Iteration 35, loss = 0.10793271\n",
      "Iteration 36, loss = 0.10529169\n",
      "Iteration 37, loss = 0.10211888\n",
      "Iteration 38, loss = 0.09965243\n",
      "Iteration 39, loss = 0.09740981\n",
      "Iteration 40, loss = 0.09483320\n",
      "Iteration 41, loss = 0.09268168\n",
      "Iteration 42, loss = 0.09061237\n",
      "Iteration 43, loss = 0.08846905\n",
      "Iteration 44, loss = 0.08610589\n",
      "Iteration 45, loss = 0.08428040\n",
      "Iteration 46, loss = 0.08249997\n",
      "Iteration 47, loss = 0.08036357\n",
      "Iteration 48, loss = 0.07883701\n",
      "Iteration 49, loss = 0.07678049\n",
      "Iteration 50, loss = 0.07505950\n",
      "Iteration 51, loss = 0.07312009\n",
      "Iteration 52, loss = 0.07163013\n",
      "Iteration 53, loss = 0.06985551\n",
      "Iteration 54, loss = 0.06838353\n",
      "Iteration 55, loss = 0.06724226\n",
      "Iteration 56, loss = 0.06536081\n",
      "Iteration 57, loss = 0.06418456\n",
      "Iteration 58, loss = 0.06257099\n",
      "Iteration 59, loss = 0.06119153\n",
      "Iteration 60, loss = 0.05975295\n",
      "Iteration 61, loss = 0.05842851\n",
      "Iteration 62, loss = 0.05691970\n",
      "Iteration 63, loss = 0.05601235\n",
      "Iteration 64, loss = 0.05471579\n",
      "Iteration 65, loss = 0.05377234\n",
      "Iteration 66, loss = 0.05209249\n",
      "Iteration 67, loss = 0.05109023\n",
      "Iteration 68, loss = 0.04982795\n",
      "Iteration 69, loss = 0.04882873\n",
      "Iteration 70, loss = 0.04802790\n",
      "Iteration 71, loss = 0.04679420\n",
      "Iteration 72, loss = 0.04579416\n",
      "Iteration 73, loss = 0.04482836\n",
      "Iteration 74, loss = 0.04403876\n",
      "Iteration 75, loss = 0.04287276\n",
      "Iteration 76, loss = 0.04221213\n",
      "Iteration 77, loss = 0.04108664\n",
      "Iteration 78, loss = 0.03990183\n",
      "Iteration 79, loss = 0.03941328\n",
      "Iteration 80, loss = 0.03835129\n",
      "Iteration 81, loss = 0.03747506\n",
      "Iteration 82, loss = 0.03671879\n",
      "Iteration 83, loss = 0.03569894\n",
      "Iteration 84, loss = 0.03501434\n",
      "Iteration 85, loss = 0.03429010\n",
      "Iteration 86, loss = 0.03363269\n",
      "Iteration 87, loss = 0.03290313\n",
      "Iteration 88, loss = 0.03209926\n",
      "Iteration 89, loss = 0.03153994\n",
      "Iteration 90, loss = 0.03085668\n",
      "Iteration 91, loss = 0.02988004\n",
      "Iteration 92, loss = 0.02913384\n",
      "Iteration 93, loss = 0.02874326\n",
      "Iteration 94, loss = 0.02804545\n",
      "Iteration 95, loss = 0.02724979\n",
      "Iteration 96, loss = 0.02683433\n",
      "Iteration 97, loss = 0.02610396\n",
      "Iteration 98, loss = 0.02547674\n",
      "Iteration 99, loss = 0.02486739\n",
      "Iteration 100, loss = 0.02443811\n",
      "Iteration 101, loss = 0.02404022\n",
      "Iteration 102, loss = 0.02343026\n",
      "Iteration 103, loss = 0.02289430\n",
      "Iteration 104, loss = 0.02251641\n",
      "Iteration 105, loss = 0.02198147\n",
      "Iteration 106, loss = 0.02129288\n",
      "Iteration 107, loss = 0.02084512\n",
      "Iteration 108, loss = 0.02056253\n",
      "Iteration 109, loss = 0.01984695\n",
      "Iteration 110, loss = 0.01950702\n",
      "Iteration 111, loss = 0.01921476\n",
      "Iteration 112, loss = 0.01874827\n",
      "Iteration 113, loss = 0.01835290\n",
      "Iteration 114, loss = 0.01772475\n",
      "Iteration 115, loss = 0.01749214\n",
      "Iteration 116, loss = 0.01718904\n",
      "Iteration 117, loss = 0.01665785\n",
      "Iteration 118, loss = 0.01640961\n",
      "Iteration 119, loss = 0.01595453\n",
      "Iteration 120, loss = 0.01575251\n",
      "Iteration 121, loss = 0.01528084\n",
      "Iteration 122, loss = 0.01496009\n",
      "Iteration 123, loss = 0.01475258\n",
      "Iteration 124, loss = 0.01441483\n",
      "Iteration 125, loss = 0.01409053\n",
      "Iteration 126, loss = 0.01377126\n",
      "Iteration 127, loss = 0.01344359\n",
      "Iteration 128, loss = 0.01306435\n",
      "Iteration 129, loss = 0.01286393\n",
      "Iteration 130, loss = 0.01263298\n",
      "Iteration 131, loss = 0.01239702\n",
      "Iteration 132, loss = 0.01191162\n",
      "Iteration 133, loss = 0.01183280\n",
      "Iteration 134, loss = 0.01160823\n",
      "Iteration 135, loss = 0.01128125\n",
      "Iteration 136, loss = 0.01106029\n",
      "Iteration 137, loss = 0.01089013\n",
      "Iteration 138, loss = 0.01063728\n",
      "Iteration 139, loss = 0.01043299\n",
      "Iteration 140, loss = 0.01015577\n",
      "Iteration 141, loss = 0.01002733\n",
      "Iteration 142, loss = 0.00982151\n",
      "Iteration 143, loss = 0.00961307\n",
      "Iteration 144, loss = 0.00940490\n",
      "Iteration 145, loss = 0.00914511\n",
      "Iteration 146, loss = 0.00910822\n",
      "Iteration 147, loss = 0.00890253\n",
      "Iteration 148, loss = 0.00872383\n",
      "Iteration 149, loss = 0.00849049\n",
      "Iteration 150, loss = 0.00831990\n",
      "Iteration 151, loss = 0.00825205\n",
      "Iteration 152, loss = 0.00801796\n",
      "Iteration 153, loss = 0.00785172\n",
      "Iteration 154, loss = 0.00775064\n",
      "Iteration 155, loss = 0.00763737\n",
      "Iteration 156, loss = 0.00743004\n",
      "Iteration 157, loss = 0.00731615\n",
      "Iteration 158, loss = 0.00718292\n",
      "Iteration 159, loss = 0.00709137\n",
      "Iteration 160, loss = 0.00692969\n",
      "Iteration 161, loss = 0.00678975\n",
      "Iteration 162, loss = 0.00668895\n",
      "Iteration 163, loss = 0.00651132\n",
      "Iteration 164, loss = 0.00643844\n",
      "Iteration 165, loss = 0.00630706\n",
      "Iteration 166, loss = 0.00619214\n",
      "Iteration 167, loss = 0.00614117\n",
      "Iteration 168, loss = 0.00603455\n",
      "Iteration 169, loss = 0.00588639\n",
      "Iteration 170, loss = 0.00582273\n",
      "Iteration 171, loss = 0.00574635\n",
      "Iteration 172, loss = 0.00563280\n",
      "Iteration 173, loss = 0.00552652\n",
      "Iteration 174, loss = 0.00544960\n",
      "Iteration 175, loss = 0.00537213\n",
      "Iteration 176, loss = 0.00532472\n",
      "Iteration 177, loss = 0.00521153\n",
      "Iteration 178, loss = 0.00514259\n",
      "Iteration 179, loss = 0.00503048\n",
      "Iteration 180, loss = 0.00500829\n",
      "Iteration 181, loss = 0.00489170\n",
      "Iteration 182, loss = 0.00481167\n",
      "Iteration 183, loss = 0.00474225\n",
      "Iteration 184, loss = 0.00467579\n",
      "Iteration 185, loss = 0.00466130\n",
      "Iteration 186, loss = 0.00453441\n",
      "Iteration 187, loss = 0.00450379\n",
      "Iteration 188, loss = 0.00441858\n",
      "Iteration 189, loss = 0.00436961\n",
      "Iteration 190, loss = 0.00431638\n",
      "Iteration 191, loss = 0.00424656\n",
      "Iteration 192, loss = 0.00419398\n",
      "Iteration 193, loss = 0.00413610\n",
      "Iteration 194, loss = 0.00407311\n",
      "Iteration 195, loss = 0.00403860\n",
      "Iteration 196, loss = 0.00397584\n",
      "Iteration 197, loss = 0.00394340\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.0001, batch_size=100, hidden_layer_sizes=(100, 100, 100), solver=sgd; total time= 3.6min\n",
      "Iteration 1, loss = 1.74683971\n",
      "Iteration 2, loss = 0.66833288\n",
      "Iteration 3, loss = 0.44459945\n",
      "Iteration 4, loss = 0.37283795\n",
      "Iteration 5, loss = 0.33598601\n",
      "Iteration 6, loss = 0.31161072\n",
      "Iteration 7, loss = 0.29333693\n",
      "Iteration 8, loss = 0.27789289\n",
      "Iteration 9, loss = 0.26398587\n",
      "Iteration 10, loss = 0.25293040\n",
      "Iteration 11, loss = 0.24150435\n",
      "Iteration 12, loss = 0.23160986\n",
      "Iteration 13, loss = 0.22225626\n",
      "Iteration 14, loss = 0.21363459\n",
      "Iteration 15, loss = 0.20537650\n",
      "Iteration 16, loss = 0.19775563\n",
      "Iteration 17, loss = 0.19081511\n",
      "Iteration 18, loss = 0.18402952\n",
      "Iteration 19, loss = 0.17726508\n",
      "Iteration 20, loss = 0.17159698\n",
      "Iteration 21, loss = 0.16617816\n",
      "Iteration 22, loss = 0.16093776\n",
      "Iteration 23, loss = 0.15580849\n",
      "Iteration 24, loss = 0.15079728\n",
      "Iteration 25, loss = 0.14654946\n",
      "Iteration 26, loss = 0.14225055\n",
      "Iteration 27, loss = 0.13852938\n",
      "Iteration 28, loss = 0.13435672\n",
      "Iteration 29, loss = 0.13116989\n",
      "Iteration 30, loss = 0.12750500\n",
      "Iteration 31, loss = 0.12428020\n",
      "Iteration 32, loss = 0.12097341\n",
      "Iteration 33, loss = 0.11722127\n",
      "Iteration 34, loss = 0.11481623\n",
      "Iteration 35, loss = 0.11161167\n",
      "Iteration 36, loss = 0.10926400\n",
      "Iteration 37, loss = 0.10596191\n",
      "Iteration 38, loss = 0.10315852\n",
      "Iteration 39, loss = 0.10092255\n",
      "Iteration 40, loss = 0.09815989\n",
      "Iteration 41, loss = 0.09598280\n",
      "Iteration 42, loss = 0.09408738\n",
      "Iteration 43, loss = 0.09116446\n",
      "Iteration 44, loss = 0.08983477\n",
      "Iteration 45, loss = 0.08731068\n",
      "Iteration 46, loss = 0.08547624\n",
      "Iteration 47, loss = 0.08312863\n",
      "Iteration 48, loss = 0.08143432\n",
      "Iteration 49, loss = 0.07967922\n",
      "Iteration 50, loss = 0.07740417\n",
      "Iteration 51, loss = 0.07584793\n",
      "Iteration 52, loss = 0.07375914\n",
      "Iteration 53, loss = 0.07239103\n",
      "Iteration 54, loss = 0.07055916\n",
      "Iteration 55, loss = 0.06909128\n",
      "Iteration 56, loss = 0.06723027\n",
      "Iteration 57, loss = 0.06595907\n",
      "Iteration 58, loss = 0.06435746\n",
      "Iteration 59, loss = 0.06278156\n",
      "Iteration 60, loss = 0.06130205\n",
      "Iteration 61, loss = 0.05991979\n",
      "Iteration 62, loss = 0.05874373\n",
      "Iteration 63, loss = 0.05752689\n",
      "Iteration 64, loss = 0.05598688\n",
      "Iteration 65, loss = 0.05480273\n",
      "Iteration 66, loss = 0.05325161\n",
      "Iteration 67, loss = 0.05230333\n",
      "Iteration 68, loss = 0.05099660\n",
      "Iteration 69, loss = 0.04963898\n",
      "Iteration 70, loss = 0.04856149\n",
      "Iteration 71, loss = 0.04760498\n",
      "Iteration 72, loss = 0.04637519\n",
      "Iteration 73, loss = 0.04534238\n",
      "Iteration 74, loss = 0.04454037\n",
      "Iteration 75, loss = 0.04349473\n",
      "Iteration 76, loss = 0.04251749\n",
      "Iteration 77, loss = 0.04131014\n",
      "Iteration 78, loss = 0.04008946\n",
      "Iteration 79, loss = 0.03946329\n",
      "Iteration 80, loss = 0.03871616\n",
      "Iteration 81, loss = 0.03738657\n",
      "Iteration 82, loss = 0.03674253\n",
      "Iteration 83, loss = 0.03585152\n",
      "Iteration 84, loss = 0.03522399\n",
      "Iteration 85, loss = 0.03444412\n",
      "Iteration 86, loss = 0.03331126\n",
      "Iteration 87, loss = 0.03296988\n",
      "Iteration 88, loss = 0.03202010\n",
      "Iteration 89, loss = 0.03094748\n",
      "Iteration 90, loss = 0.03038542\n",
      "Iteration 91, loss = 0.02992497\n",
      "Iteration 92, loss = 0.02915239\n",
      "Iteration 93, loss = 0.02854101\n",
      "Iteration 94, loss = 0.02787897\n",
      "Iteration 95, loss = 0.02699721\n",
      "Iteration 96, loss = 0.02671093\n",
      "Iteration 97, loss = 0.02604143\n",
      "Iteration 98, loss = 0.02542428\n",
      "Iteration 99, loss = 0.02457763\n",
      "Iteration 100, loss = 0.02423402\n",
      "Iteration 101, loss = 0.02363840\n",
      "Iteration 102, loss = 0.02305765\n",
      "Iteration 103, loss = 0.02272436\n",
      "Iteration 104, loss = 0.02206131\n",
      "Iteration 105, loss = 0.02179217\n",
      "Iteration 106, loss = 0.02101340\n",
      "Iteration 107, loss = 0.02054173\n",
      "Iteration 108, loss = 0.02023072\n",
      "Iteration 109, loss = 0.01964937\n",
      "Iteration 110, loss = 0.01926611\n",
      "Iteration 111, loss = 0.01874031\n",
      "Iteration 112, loss = 0.01830618\n",
      "Iteration 113, loss = 0.01805556\n",
      "Iteration 114, loss = 0.01756639\n",
      "Iteration 115, loss = 0.01737880\n",
      "Iteration 116, loss = 0.01670641\n",
      "Iteration 117, loss = 0.01642059\n",
      "Iteration 118, loss = 0.01613104\n",
      "Iteration 119, loss = 0.01572685\n",
      "Iteration 120, loss = 0.01544565\n",
      "Iteration 121, loss = 0.01510466\n",
      "Iteration 122, loss = 0.01468183\n",
      "Iteration 123, loss = 0.01430633\n",
      "Iteration 124, loss = 0.01401648\n",
      "Iteration 125, loss = 0.01388152\n",
      "Iteration 126, loss = 0.01347590\n",
      "Iteration 127, loss = 0.01336820\n",
      "Iteration 128, loss = 0.01301735\n",
      "Iteration 129, loss = 0.01267273\n",
      "Iteration 130, loss = 0.01242340\n",
      "Iteration 131, loss = 0.01229189\n",
      "Iteration 132, loss = 0.01185751\n",
      "Iteration 133, loss = 0.01165763\n",
      "Iteration 134, loss = 0.01149608\n",
      "Iteration 135, loss = 0.01119892\n",
      "Iteration 136, loss = 0.01106831\n",
      "Iteration 137, loss = 0.01072313\n",
      "Iteration 138, loss = 0.01056860\n",
      "Iteration 139, loss = 0.01032904\n",
      "Iteration 140, loss = 0.01018984\n",
      "Iteration 141, loss = 0.01007323\n",
      "Iteration 142, loss = 0.00973292\n",
      "Iteration 143, loss = 0.00957771\n",
      "Iteration 144, loss = 0.00941585\n",
      "Iteration 145, loss = 0.00928161\n",
      "Iteration 146, loss = 0.00904631\n",
      "Iteration 147, loss = 0.00886705\n",
      "Iteration 148, loss = 0.00881740\n",
      "Iteration 149, loss = 0.00855632\n",
      "Iteration 150, loss = 0.00833629\n",
      "Iteration 151, loss = 0.00822271\n",
      "Iteration 152, loss = 0.00809742\n",
      "Iteration 153, loss = 0.00792745\n",
      "Iteration 154, loss = 0.00779976\n",
      "Iteration 155, loss = 0.00764722\n",
      "Iteration 156, loss = 0.00758998\n",
      "Iteration 157, loss = 0.00735986\n",
      "Iteration 158, loss = 0.00725524\n",
      "Iteration 159, loss = 0.00718708\n",
      "Iteration 160, loss = 0.00703610\n",
      "Iteration 161, loss = 0.00689294\n",
      "Iteration 162, loss = 0.00684079\n",
      "Iteration 163, loss = 0.00669595\n",
      "Iteration 164, loss = 0.00656895\n",
      "Iteration 165, loss = 0.00645166\n",
      "Iteration 166, loss = 0.00635450\n",
      "Iteration 167, loss = 0.00625778\n",
      "Iteration 168, loss = 0.00614333\n",
      "Iteration 169, loss = 0.00601775\n",
      "Iteration 170, loss = 0.00596916\n",
      "Iteration 171, loss = 0.00578984\n",
      "Iteration 172, loss = 0.00574979\n",
      "Iteration 173, loss = 0.00563554\n",
      "Iteration 174, loss = 0.00560369\n",
      "Iteration 175, loss = 0.00552356\n",
      "Iteration 176, loss = 0.00539125\n",
      "Iteration 177, loss = 0.00533308\n",
      "Iteration 178, loss = 0.00527210\n",
      "Iteration 179, loss = 0.00520286\n",
      "Iteration 180, loss = 0.00510864\n",
      "Iteration 181, loss = 0.00505236\n",
      "Iteration 182, loss = 0.00495090\n",
      "Iteration 183, loss = 0.00488983\n",
      "Iteration 184, loss = 0.00478727\n",
      "Iteration 185, loss = 0.00475751\n",
      "Iteration 186, loss = 0.00471189\n",
      "Iteration 187, loss = 0.00455633\n",
      "Iteration 188, loss = 0.00451326\n",
      "Iteration 189, loss = 0.00445477\n",
      "Iteration 190, loss = 0.00439771\n",
      "Iteration 191, loss = 0.00436924\n",
      "Iteration 192, loss = 0.00430772\n",
      "Iteration 193, loss = 0.00422438\n",
      "Iteration 194, loss = 0.00418201\n",
      "Iteration 195, loss = 0.00412613\n",
      "Iteration 196, loss = 0.00409680\n",
      "Iteration 197, loss = 0.00402820\n",
      "Iteration 198, loss = 0.00396904\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.0001, batch_size=100, hidden_layer_sizes=(100, 100, 100), solver=sgd; total time= 4.1min\n",
      "Iteration 1, loss = 1.83920486\n",
      "Iteration 2, loss = 0.69580720\n",
      "Iteration 3, loss = 0.44753153\n",
      "Iteration 4, loss = 0.37266983\n",
      "Iteration 5, loss = 0.33124812\n",
      "Iteration 6, loss = 0.30300958\n",
      "Iteration 7, loss = 0.28140380\n",
      "Iteration 8, loss = 0.26285649\n",
      "Iteration 9, loss = 0.24790556\n",
      "Iteration 10, loss = 0.23424689\n",
      "Iteration 11, loss = 0.22288416\n",
      "Iteration 12, loss = 0.21201193\n",
      "Iteration 13, loss = 0.20289943\n",
      "Iteration 14, loss = 0.19450250\n",
      "Iteration 15, loss = 0.18649295\n",
      "Iteration 16, loss = 0.17929253\n",
      "Iteration 17, loss = 0.17271853\n",
      "Iteration 18, loss = 0.16597689\n",
      "Iteration 19, loss = 0.16031875\n",
      "Iteration 20, loss = 0.15516291\n",
      "Iteration 21, loss = 0.14998661\n",
      "Iteration 22, loss = 0.14486838\n",
      "Iteration 23, loss = 0.14047177\n",
      "Iteration 24, loss = 0.13611119\n",
      "Iteration 25, loss = 0.13184323\n",
      "Iteration 26, loss = 0.12804784\n",
      "Iteration 27, loss = 0.12493603\n",
      "Iteration 28, loss = 0.12085173\n",
      "Iteration 29, loss = 0.11762882\n",
      "Iteration 30, loss = 0.11441080\n",
      "Iteration 31, loss = 0.11150699\n",
      "Iteration 32, loss = 0.10827801\n",
      "Iteration 33, loss = 0.10575046\n",
      "Iteration 34, loss = 0.10285036\n",
      "Iteration 35, loss = 0.10030809\n",
      "Iteration 36, loss = 0.09783488\n",
      "Iteration 37, loss = 0.09490603\n",
      "Iteration 38, loss = 0.09289502\n",
      "Iteration 39, loss = 0.09085945\n",
      "Iteration 40, loss = 0.08872995\n",
      "Iteration 41, loss = 0.08621590\n",
      "Iteration 42, loss = 0.08409339\n",
      "Iteration 43, loss = 0.08230783\n",
      "Iteration 44, loss = 0.07986150\n",
      "Iteration 45, loss = 0.07830960\n",
      "Iteration 46, loss = 0.07651225\n",
      "Iteration 47, loss = 0.07494788\n",
      "Iteration 48, loss = 0.07337112\n",
      "Iteration 49, loss = 0.07166600\n",
      "Iteration 50, loss = 0.07003927\n",
      "Iteration 51, loss = 0.06817799\n",
      "Iteration 52, loss = 0.06635496\n",
      "Iteration 53, loss = 0.06514671\n",
      "Iteration 54, loss = 0.06359122\n",
      "Iteration 55, loss = 0.06206389\n",
      "Iteration 56, loss = 0.06074120\n",
      "Iteration 57, loss = 0.05918434\n",
      "Iteration 58, loss = 0.05803309\n",
      "Iteration 59, loss = 0.05658078\n",
      "Iteration 60, loss = 0.05531565\n",
      "Iteration 61, loss = 0.05448928\n",
      "Iteration 62, loss = 0.05306628\n",
      "Iteration 63, loss = 0.05191334\n",
      "Iteration 64, loss = 0.05038877\n",
      "Iteration 65, loss = 0.04973653\n",
      "Iteration 66, loss = 0.04865188\n",
      "Iteration 67, loss = 0.04748224\n",
      "Iteration 68, loss = 0.04644953\n",
      "Iteration 69, loss = 0.04523856\n",
      "Iteration 70, loss = 0.04438761\n",
      "Iteration 71, loss = 0.04348033\n",
      "Iteration 72, loss = 0.04264589\n",
      "Iteration 73, loss = 0.04171588\n",
      "Iteration 74, loss = 0.04057018\n",
      "Iteration 75, loss = 0.03946838\n",
      "Iteration 76, loss = 0.03883550\n",
      "Iteration 77, loss = 0.03791672\n",
      "Iteration 78, loss = 0.03712195\n",
      "Iteration 79, loss = 0.03637512\n",
      "Iteration 80, loss = 0.03553090\n",
      "Iteration 81, loss = 0.03459729\n",
      "Iteration 82, loss = 0.03411346\n",
      "Iteration 83, loss = 0.03319830\n",
      "Iteration 84, loss = 0.03254481\n",
      "Iteration 85, loss = 0.03148018\n",
      "Iteration 86, loss = 0.03124225\n",
      "Iteration 87, loss = 0.03047868\n",
      "Iteration 88, loss = 0.02970919\n",
      "Iteration 89, loss = 0.02884141\n",
      "Iteration 90, loss = 0.02855991\n",
      "Iteration 91, loss = 0.02776699\n",
      "Iteration 92, loss = 0.02726268\n",
      "Iteration 93, loss = 0.02635630\n",
      "Iteration 94, loss = 0.02601140\n",
      "Iteration 95, loss = 0.02548053\n",
      "Iteration 96, loss = 0.02489547\n",
      "Iteration 97, loss = 0.02425044\n",
      "Iteration 98, loss = 0.02373615\n",
      "Iteration 99, loss = 0.02329863\n",
      "Iteration 100, loss = 0.02264776\n",
      "Iteration 101, loss = 0.02199754\n",
      "Iteration 102, loss = 0.02171810\n",
      "Iteration 103, loss = 0.02124491\n",
      "Iteration 104, loss = 0.02071633\n",
      "Iteration 105, loss = 0.02030424\n",
      "Iteration 106, loss = 0.01992376\n",
      "Iteration 107, loss = 0.01936218\n",
      "Iteration 108, loss = 0.01900388\n",
      "Iteration 109, loss = 0.01854630\n",
      "Iteration 110, loss = 0.01818254\n",
      "Iteration 111, loss = 0.01763772\n",
      "Iteration 112, loss = 0.01732462\n",
      "Iteration 113, loss = 0.01689999\n",
      "Iteration 114, loss = 0.01669210\n",
      "Iteration 115, loss = 0.01625817\n",
      "Iteration 116, loss = 0.01599663\n",
      "Iteration 117, loss = 0.01555784\n",
      "Iteration 118, loss = 0.01519646\n",
      "Iteration 119, loss = 0.01471794\n",
      "Iteration 120, loss = 0.01456775\n",
      "Iteration 121, loss = 0.01420090\n",
      "Iteration 122, loss = 0.01400940\n",
      "Iteration 123, loss = 0.01361720\n",
      "Iteration 124, loss = 0.01323686\n",
      "Iteration 125, loss = 0.01313005\n",
      "Iteration 126, loss = 0.01275459\n",
      "Iteration 127, loss = 0.01252513\n",
      "Iteration 128, loss = 0.01216704\n",
      "Iteration 129, loss = 0.01194244\n",
      "Iteration 130, loss = 0.01155685\n",
      "Iteration 131, loss = 0.01149577\n",
      "Iteration 132, loss = 0.01121794\n",
      "Iteration 133, loss = 0.01097762\n",
      "Iteration 134, loss = 0.01076704\n",
      "Iteration 135, loss = 0.01056356\n",
      "Iteration 136, loss = 0.01028372\n",
      "Iteration 137, loss = 0.01014334\n",
      "Iteration 138, loss = 0.00994563\n",
      "Iteration 139, loss = 0.00967171\n",
      "Iteration 140, loss = 0.00961449\n",
      "Iteration 141, loss = 0.00934491\n",
      "Iteration 142, loss = 0.00913962\n",
      "Iteration 143, loss = 0.00904874\n",
      "Iteration 144, loss = 0.00872373\n",
      "Iteration 145, loss = 0.00867981\n",
      "Iteration 146, loss = 0.00842042\n",
      "Iteration 147, loss = 0.00829388\n",
      "Iteration 148, loss = 0.00814806\n",
      "Iteration 149, loss = 0.00798383\n",
      "Iteration 150, loss = 0.00787597\n",
      "Iteration 151, loss = 0.00771423\n",
      "Iteration 152, loss = 0.00756636\n",
      "Iteration 153, loss = 0.00736822\n",
      "Iteration 154, loss = 0.00729893\n",
      "Iteration 155, loss = 0.00714251\n",
      "Iteration 156, loss = 0.00704045\n",
      "Iteration 157, loss = 0.00687985\n",
      "Iteration 158, loss = 0.00672827\n",
      "Iteration 159, loss = 0.00664437\n",
      "Iteration 160, loss = 0.00648684\n",
      "Iteration 161, loss = 0.00639042\n",
      "Iteration 162, loss = 0.00636089\n",
      "Iteration 163, loss = 0.00615408\n",
      "Iteration 164, loss = 0.00603593\n",
      "Iteration 165, loss = 0.00599885\n",
      "Iteration 166, loss = 0.00588843\n",
      "Iteration 167, loss = 0.00578976\n",
      "Iteration 168, loss = 0.00567496\n",
      "Iteration 169, loss = 0.00551632\n",
      "Iteration 170, loss = 0.00548050\n",
      "Iteration 171, loss = 0.00536699\n",
      "Iteration 172, loss = 0.00529583\n",
      "Iteration 173, loss = 0.00528049\n",
      "Iteration 174, loss = 0.00511766\n",
      "Iteration 175, loss = 0.00506764\n",
      "Iteration 176, loss = 0.00500264\n",
      "Iteration 177, loss = 0.00488920\n",
      "Iteration 178, loss = 0.00483818\n",
      "Iteration 179, loss = 0.00475184\n",
      "Iteration 180, loss = 0.00471694\n",
      "Iteration 181, loss = 0.00465508\n",
      "Iteration 182, loss = 0.00455092\n",
      "Iteration 183, loss = 0.00449646\n",
      "Iteration 184, loss = 0.00443518\n",
      "Iteration 185, loss = 0.00433808\n",
      "Iteration 186, loss = 0.00433237\n",
      "Iteration 187, loss = 0.00424092\n",
      "Iteration 188, loss = 0.00418215\n",
      "Iteration 189, loss = 0.00409406\n",
      "Iteration 190, loss = 0.00409811\n",
      "Iteration 191, loss = 0.00405469\n",
      "Iteration 192, loss = 0.00396511\n",
      "Iteration 193, loss = 0.00393412\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.0001, batch_size=100, hidden_layer_sizes=(100, 100, 100), solver=sgd; total time= 3.9min\n",
      "Iteration 1, loss = 1.80298907\n",
      "Iteration 2, loss = 0.69980084\n",
      "Iteration 3, loss = 0.45532176\n",
      "Iteration 4, loss = 0.37769473\n",
      "Iteration 5, loss = 0.33648846\n",
      "Iteration 6, loss = 0.30938772\n",
      "Iteration 7, loss = 0.28887397\n",
      "Iteration 8, loss = 0.27152607\n",
      "Iteration 9, loss = 0.25670563\n",
      "Iteration 10, loss = 0.24414099\n",
      "Iteration 11, loss = 0.23203173\n",
      "Iteration 12, loss = 0.22117803\n",
      "Iteration 13, loss = 0.21191090\n",
      "Iteration 14, loss = 0.20310463\n",
      "Iteration 15, loss = 0.19435002\n",
      "Iteration 16, loss = 0.18768597\n",
      "Iteration 17, loss = 0.18025046\n",
      "Iteration 18, loss = 0.17359216\n",
      "Iteration 19, loss = 0.16760021\n",
      "Iteration 20, loss = 0.16163845\n",
      "Iteration 21, loss = 0.15646941\n",
      "Iteration 22, loss = 0.15185465\n",
      "Iteration 23, loss = 0.14662720\n",
      "Iteration 24, loss = 0.14244828\n",
      "Iteration 25, loss = 0.13813912\n",
      "Iteration 26, loss = 0.13464463\n",
      "Iteration 27, loss = 0.13043369\n",
      "Iteration 28, loss = 0.12723338\n",
      "Iteration 29, loss = 0.12330039\n",
      "Iteration 30, loss = 0.12059364\n",
      "Iteration 31, loss = 0.11710555\n",
      "Iteration 32, loss = 0.11420562\n",
      "Iteration 33, loss = 0.11142847\n",
      "Iteration 34, loss = 0.10861899\n",
      "Iteration 35, loss = 0.10578360\n",
      "Iteration 36, loss = 0.10308651\n",
      "Iteration 37, loss = 0.10067655\n",
      "Iteration 38, loss = 0.09800389\n",
      "Iteration 39, loss = 0.09626491\n",
      "Iteration 40, loss = 0.09387651\n",
      "Iteration 41, loss = 0.09175069\n",
      "Iteration 42, loss = 0.08944872\n",
      "Iteration 43, loss = 0.08720682\n",
      "Iteration 44, loss = 0.08513785\n",
      "Iteration 45, loss = 0.08321133\n",
      "Iteration 46, loss = 0.08126790\n",
      "Iteration 47, loss = 0.07926749\n",
      "Iteration 48, loss = 0.07751500\n",
      "Iteration 49, loss = 0.07568886\n",
      "Iteration 50, loss = 0.07372055\n",
      "Iteration 51, loss = 0.07240798\n",
      "Iteration 52, loss = 0.07051643\n",
      "Iteration 53, loss = 0.06895440\n",
      "Iteration 54, loss = 0.06721334\n",
      "Iteration 55, loss = 0.06589347\n",
      "Iteration 56, loss = 0.06466879\n",
      "Iteration 57, loss = 0.06277670\n",
      "Iteration 58, loss = 0.06143120\n",
      "Iteration 59, loss = 0.06017446\n",
      "Iteration 60, loss = 0.05874232\n",
      "Iteration 61, loss = 0.05726703\n",
      "Iteration 62, loss = 0.05645860\n",
      "Iteration 63, loss = 0.05475933\n",
      "Iteration 64, loss = 0.05357120\n",
      "Iteration 65, loss = 0.05228633\n",
      "Iteration 66, loss = 0.05111611\n",
      "Iteration 67, loss = 0.05029439\n",
      "Iteration 68, loss = 0.04899123\n",
      "Iteration 69, loss = 0.04765620\n",
      "Iteration 70, loss = 0.04673402\n",
      "Iteration 71, loss = 0.04551318\n",
      "Iteration 72, loss = 0.04431645\n",
      "Iteration 73, loss = 0.04364034\n",
      "Iteration 74, loss = 0.04262249\n",
      "Iteration 75, loss = 0.04139957\n",
      "Iteration 76, loss = 0.04074378\n",
      "Iteration 77, loss = 0.03978551\n",
      "Iteration 78, loss = 0.03881392\n",
      "Iteration 79, loss = 0.03789203\n",
      "Iteration 80, loss = 0.03694386\n",
      "Iteration 81, loss = 0.03633777\n",
      "Iteration 82, loss = 0.03561096\n",
      "Iteration 83, loss = 0.03473402\n",
      "Iteration 84, loss = 0.03386466\n",
      "Iteration 85, loss = 0.03310000\n",
      "Iteration 86, loss = 0.03256268\n",
      "Iteration 87, loss = 0.03171204\n",
      "Iteration 88, loss = 0.03092177\n",
      "Iteration 89, loss = 0.03039543\n",
      "Iteration 90, loss = 0.02971445\n",
      "Iteration 91, loss = 0.02912642\n",
      "Iteration 92, loss = 0.02839429\n",
      "Iteration 93, loss = 0.02790970\n",
      "Iteration 94, loss = 0.02714784\n",
      "Iteration 95, loss = 0.02650181\n",
      "Iteration 96, loss = 0.02611344\n",
      "Iteration 97, loss = 0.02540575\n",
      "Iteration 98, loss = 0.02489091\n",
      "Iteration 99, loss = 0.02421565\n",
      "Iteration 100, loss = 0.02377327\n",
      "Iteration 101, loss = 0.02337546\n",
      "Iteration 102, loss = 0.02276103\n",
      "Iteration 103, loss = 0.02230242\n",
      "Iteration 104, loss = 0.02187636\n",
      "Iteration 105, loss = 0.02129186\n",
      "Iteration 106, loss = 0.02064463\n",
      "Iteration 107, loss = 0.02027955\n",
      "Iteration 108, loss = 0.01997213\n",
      "Iteration 109, loss = 0.01940230\n",
      "Iteration 110, loss = 0.01889437\n",
      "Iteration 111, loss = 0.01840630\n",
      "Iteration 112, loss = 0.01829942\n",
      "Iteration 113, loss = 0.01783819\n",
      "Iteration 114, loss = 0.01745936\n",
      "Iteration 115, loss = 0.01717032\n",
      "Iteration 116, loss = 0.01668041\n",
      "Iteration 117, loss = 0.01632952\n",
      "Iteration 118, loss = 0.01601432\n",
      "Iteration 119, loss = 0.01566205\n",
      "Iteration 120, loss = 0.01537384\n",
      "Iteration 121, loss = 0.01502553\n",
      "Iteration 122, loss = 0.01465677\n",
      "Iteration 123, loss = 0.01439286\n",
      "Iteration 124, loss = 0.01397189\n",
      "Iteration 125, loss = 0.01386911\n",
      "Iteration 126, loss = 0.01361000\n",
      "Iteration 127, loss = 0.01316607\n",
      "Iteration 128, loss = 0.01292435\n",
      "Iteration 129, loss = 0.01270997\n",
      "Iteration 130, loss = 0.01235461\n",
      "Iteration 131, loss = 0.01218737\n",
      "Iteration 132, loss = 0.01195119\n",
      "Iteration 133, loss = 0.01170388\n",
      "Iteration 134, loss = 0.01140690\n",
      "Iteration 135, loss = 0.01110667\n",
      "Iteration 136, loss = 0.01100876\n",
      "Iteration 137, loss = 0.01074605\n",
      "Iteration 138, loss = 0.01062405\n",
      "Iteration 139, loss = 0.01031215\n",
      "Iteration 140, loss = 0.01017081\n",
      "Iteration 141, loss = 0.00992190\n",
      "Iteration 142, loss = 0.00962828\n",
      "Iteration 143, loss = 0.00944680\n",
      "Iteration 144, loss = 0.00937067\n",
      "Iteration 145, loss = 0.00925173\n",
      "Iteration 146, loss = 0.00889060\n",
      "Iteration 147, loss = 0.00882856\n",
      "Iteration 148, loss = 0.00865720\n",
      "Iteration 149, loss = 0.00846925\n",
      "Iteration 150, loss = 0.00836782\n",
      "Iteration 151, loss = 0.00818346\n",
      "Iteration 152, loss = 0.00795037\n",
      "Iteration 153, loss = 0.00783052\n",
      "Iteration 154, loss = 0.00764301\n",
      "Iteration 155, loss = 0.00758838\n",
      "Iteration 156, loss = 0.00746511\n",
      "Iteration 157, loss = 0.00733916\n",
      "Iteration 158, loss = 0.00710143\n",
      "Iteration 159, loss = 0.00697141\n",
      "Iteration 160, loss = 0.00686054\n",
      "Iteration 161, loss = 0.00683218\n",
      "Iteration 162, loss = 0.00665033\n",
      "Iteration 163, loss = 0.00652555\n",
      "Iteration 164, loss = 0.00644121\n",
      "Iteration 165, loss = 0.00632137\n",
      "Iteration 166, loss = 0.00624150\n",
      "Iteration 167, loss = 0.00613223\n",
      "Iteration 168, loss = 0.00598127\n",
      "Iteration 169, loss = 0.00583944\n",
      "Iteration 170, loss = 0.00582573\n",
      "Iteration 171, loss = 0.00577056\n",
      "Iteration 172, loss = 0.00560499\n",
      "Iteration 173, loss = 0.00554034\n",
      "Iteration 174, loss = 0.00542170\n",
      "Iteration 175, loss = 0.00532878\n",
      "Iteration 176, loss = 0.00529100\n",
      "Iteration 177, loss = 0.00518327\n",
      "Iteration 178, loss = 0.00510175\n",
      "Iteration 179, loss = 0.00504165\n",
      "Iteration 180, loss = 0.00494330\n",
      "Iteration 181, loss = 0.00490533\n",
      "Iteration 182, loss = 0.00481091\n",
      "Iteration 183, loss = 0.00475249\n",
      "Iteration 184, loss = 0.00468991\n",
      "Iteration 185, loss = 0.00459504\n",
      "Iteration 186, loss = 0.00454272\n",
      "Iteration 187, loss = 0.00448704\n",
      "Iteration 188, loss = 0.00440277\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.0001, batch_size=100, hidden_layer_sizes=(100, 100, 100), solver=sgd; total time= 3.7min\n",
      "Iteration 1, loss = 1.82670702\n",
      "Iteration 2, loss = 0.70705980\n",
      "Iteration 3, loss = 0.45184788\n",
      "Iteration 4, loss = 0.37884395\n",
      "Iteration 5, loss = 0.34013460\n",
      "Iteration 6, loss = 0.31361952\n",
      "Iteration 7, loss = 0.29337112\n",
      "Iteration 8, loss = 0.27673685\n",
      "Iteration 9, loss = 0.26329572\n",
      "Iteration 10, loss = 0.25067065\n",
      "Iteration 11, loss = 0.23992438\n",
      "Iteration 12, loss = 0.22963045\n",
      "Iteration 13, loss = 0.22014963\n",
      "Iteration 14, loss = 0.21120023\n",
      "Iteration 15, loss = 0.20386040\n",
      "Iteration 16, loss = 0.19634220\n",
      "Iteration 17, loss = 0.18873946\n",
      "Iteration 18, loss = 0.18211471\n",
      "Iteration 19, loss = 0.17564162\n",
      "Iteration 20, loss = 0.17026660\n",
      "Iteration 21, loss = 0.16461667\n",
      "Iteration 22, loss = 0.15922735\n",
      "Iteration 23, loss = 0.15394294\n",
      "Iteration 24, loss = 0.14890752\n",
      "Iteration 25, loss = 0.14489623\n",
      "Iteration 26, loss = 0.14043157\n",
      "Iteration 27, loss = 0.13649177\n",
      "Iteration 28, loss = 0.13258420\n",
      "Iteration 29, loss = 0.12859795\n",
      "Iteration 30, loss = 0.12543538\n",
      "Iteration 31, loss = 0.12210310\n",
      "Iteration 32, loss = 0.11872776\n",
      "Iteration 33, loss = 0.11553891\n",
      "Iteration 34, loss = 0.11252963\n",
      "Iteration 35, loss = 0.10961724\n",
      "Iteration 36, loss = 0.10651094\n",
      "Iteration 37, loss = 0.10406414\n",
      "Iteration 38, loss = 0.10152648\n",
      "Iteration 39, loss = 0.09911161\n",
      "Iteration 40, loss = 0.09635833\n",
      "Iteration 41, loss = 0.09419010\n",
      "Iteration 42, loss = 0.09186578\n",
      "Iteration 43, loss = 0.08964731\n",
      "Iteration 44, loss = 0.08735845\n",
      "Iteration 45, loss = 0.08542234\n",
      "Iteration 46, loss = 0.08358934\n",
      "Iteration 47, loss = 0.08205147\n",
      "Iteration 48, loss = 0.07978102\n",
      "Iteration 49, loss = 0.07762196\n",
      "Iteration 50, loss = 0.07655020\n",
      "Iteration 51, loss = 0.07403994\n",
      "Iteration 52, loss = 0.07257254\n",
      "Iteration 53, loss = 0.07113959\n",
      "Iteration 54, loss = 0.06931237\n",
      "Iteration 55, loss = 0.06778679\n",
      "Iteration 56, loss = 0.06629924\n",
      "Iteration 57, loss = 0.06494541\n",
      "Iteration 58, loss = 0.06310928\n",
      "Iteration 59, loss = 0.06189396\n",
      "Iteration 60, loss = 0.06021534\n",
      "Iteration 61, loss = 0.05907655\n",
      "Iteration 62, loss = 0.05746951\n",
      "Iteration 63, loss = 0.05636243\n",
      "Iteration 64, loss = 0.05505737\n",
      "Iteration 65, loss = 0.05392293\n",
      "Iteration 66, loss = 0.05269043\n",
      "Iteration 67, loss = 0.05168619\n",
      "Iteration 68, loss = 0.05088203\n",
      "Iteration 69, loss = 0.04905134\n",
      "Iteration 70, loss = 0.04788672\n",
      "Iteration 71, loss = 0.04700337\n",
      "Iteration 72, loss = 0.04587909\n",
      "Iteration 73, loss = 0.04480680\n",
      "Iteration 74, loss = 0.04368984\n",
      "Iteration 75, loss = 0.04288225\n",
      "Iteration 76, loss = 0.04173701\n",
      "Iteration 77, loss = 0.04093706\n",
      "Iteration 78, loss = 0.03985094\n",
      "Iteration 79, loss = 0.03907421\n",
      "Iteration 80, loss = 0.03833885\n",
      "Iteration 81, loss = 0.03735958\n",
      "Iteration 82, loss = 0.03642641\n",
      "Iteration 83, loss = 0.03573240\n",
      "Iteration 84, loss = 0.03475810\n",
      "Iteration 85, loss = 0.03409946\n",
      "Iteration 86, loss = 0.03331066\n",
      "Iteration 87, loss = 0.03266622\n",
      "Iteration 88, loss = 0.03191728\n",
      "Iteration 89, loss = 0.03119404\n",
      "Iteration 90, loss = 0.03037467\n",
      "Iteration 91, loss = 0.02965110\n",
      "Iteration 92, loss = 0.02898008\n",
      "Iteration 93, loss = 0.02838456\n",
      "Iteration 94, loss = 0.02746765\n",
      "Iteration 95, loss = 0.02711406\n",
      "Iteration 96, loss = 0.02649094\n",
      "Iteration 97, loss = 0.02576448\n",
      "Iteration 98, loss = 0.02538800\n",
      "Iteration 99, loss = 0.02475804\n",
      "Iteration 100, loss = 0.02426650\n",
      "Iteration 101, loss = 0.02378483\n",
      "Iteration 102, loss = 0.02306060\n",
      "Iteration 103, loss = 0.02268846\n",
      "Iteration 104, loss = 0.02198070\n",
      "Iteration 105, loss = 0.02168557\n",
      "Iteration 106, loss = 0.02111441\n",
      "Iteration 107, loss = 0.02062101\n",
      "Iteration 108, loss = 0.02023678\n",
      "Iteration 109, loss = 0.01986521\n",
      "Iteration 110, loss = 0.01921425\n",
      "Iteration 111, loss = 0.01905651\n",
      "Iteration 112, loss = 0.01853599\n",
      "Iteration 113, loss = 0.01823830\n",
      "Iteration 114, loss = 0.01773214\n",
      "Iteration 115, loss = 0.01733209\n",
      "Iteration 116, loss = 0.01697494\n",
      "Iteration 117, loss = 0.01651699\n",
      "Iteration 118, loss = 0.01613340\n",
      "Iteration 119, loss = 0.01574257\n",
      "Iteration 120, loss = 0.01540413\n",
      "Iteration 121, loss = 0.01520125\n",
      "Iteration 122, loss = 0.01472344\n",
      "Iteration 123, loss = 0.01441195\n",
      "Iteration 124, loss = 0.01405120\n",
      "Iteration 125, loss = 0.01390494\n",
      "Iteration 126, loss = 0.01351808\n",
      "Iteration 127, loss = 0.01325985\n",
      "Iteration 128, loss = 0.01290569\n",
      "Iteration 129, loss = 0.01266696\n",
      "Iteration 130, loss = 0.01234544\n",
      "Iteration 131, loss = 0.01208013\n",
      "Iteration 132, loss = 0.01184412\n",
      "Iteration 133, loss = 0.01165154\n",
      "Iteration 134, loss = 0.01137487\n",
      "Iteration 135, loss = 0.01110686\n",
      "Iteration 136, loss = 0.01092203\n",
      "Iteration 137, loss = 0.01068622\n",
      "Iteration 138, loss = 0.01043567\n",
      "Iteration 139, loss = 0.01022910\n",
      "Iteration 140, loss = 0.00999725\n",
      "Iteration 141, loss = 0.00988582\n",
      "Iteration 142, loss = 0.00964118\n",
      "Iteration 143, loss = 0.00949127\n",
      "Iteration 144, loss = 0.00920466\n",
      "Iteration 145, loss = 0.00904297\n",
      "Iteration 146, loss = 0.00882980\n",
      "Iteration 147, loss = 0.00874745\n",
      "Iteration 148, loss = 0.00848595\n",
      "Iteration 149, loss = 0.00841773\n",
      "Iteration 150, loss = 0.00823822\n",
      "Iteration 151, loss = 0.00810013\n",
      "Iteration 152, loss = 0.00801210\n",
      "Iteration 153, loss = 0.00777887\n",
      "Iteration 154, loss = 0.00767469\n",
      "Iteration 155, loss = 0.00752653\n",
      "Iteration 156, loss = 0.00742831\n",
      "Iteration 157, loss = 0.00727476\n",
      "Iteration 158, loss = 0.00711804\n",
      "Iteration 159, loss = 0.00699869\n",
      "Iteration 160, loss = 0.00681702\n",
      "Iteration 161, loss = 0.00676821\n",
      "Iteration 162, loss = 0.00663312\n",
      "Iteration 163, loss = 0.00655170\n",
      "Iteration 164, loss = 0.00640841\n",
      "Iteration 165, loss = 0.00630257\n",
      "Iteration 166, loss = 0.00616056\n",
      "Iteration 167, loss = 0.00604235\n",
      "Iteration 168, loss = 0.00596185\n",
      "Iteration 169, loss = 0.00587169\n",
      "Iteration 170, loss = 0.00582345\n",
      "Iteration 171, loss = 0.00569115\n",
      "Iteration 172, loss = 0.00562162\n",
      "Iteration 173, loss = 0.00553018\n",
      "Iteration 174, loss = 0.00540330\n",
      "Iteration 175, loss = 0.00539538\n",
      "Iteration 176, loss = 0.00525411\n",
      "Iteration 177, loss = 0.00515602\n",
      "Iteration 178, loss = 0.00512109\n",
      "Iteration 179, loss = 0.00500702\n",
      "Iteration 180, loss = 0.00496135\n",
      "Iteration 181, loss = 0.00489043\n",
      "Iteration 182, loss = 0.00483454\n",
      "Iteration 183, loss = 0.00471731\n",
      "Iteration 184, loss = 0.00466981\n",
      "Iteration 185, loss = 0.00464086\n",
      "Iteration 186, loss = 0.00455017\n",
      "Iteration 187, loss = 0.00448042\n",
      "Iteration 188, loss = 0.00443607\n",
      "Iteration 189, loss = 0.00435758\n",
      "Iteration 190, loss = 0.00431830\n",
      "Iteration 191, loss = 0.00425030\n",
      "Iteration 192, loss = 0.00420102\n",
      "Iteration 193, loss = 0.00412780\n",
      "Iteration 194, loss = 0.00408210\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.0001, batch_size=100, hidden_layer_sizes=(100, 100, 100), solver=sgd; total time= 3.9min\n",
      "Iteration 1, loss = 0.56121290\n",
      "Iteration 2, loss = 0.25776584\n",
      "Iteration 3, loss = 0.19994942\n",
      "Iteration 4, loss = 0.16359429\n",
      "Iteration 5, loss = 0.13886013\n",
      "Iteration 6, loss = 0.11976632\n",
      "Iteration 7, loss = 0.10544636\n",
      "Iteration 8, loss = 0.09166924\n",
      "Iteration 9, loss = 0.08203301\n",
      "Iteration 10, loss = 0.07294917\n",
      "Iteration 11, loss = 0.06504150\n",
      "Iteration 12, loss = 0.05785237\n",
      "Iteration 13, loss = 0.05165234\n",
      "Iteration 14, loss = 0.04597797\n",
      "Iteration 15, loss = 0.04024975\n",
      "Iteration 16, loss = 0.03636630\n",
      "Iteration 17, loss = 0.03230749\n",
      "Iteration 18, loss = 0.02908652\n",
      "Iteration 19, loss = 0.02643220\n",
      "Iteration 20, loss = 0.02359377\n",
      "Iteration 21, loss = 0.02108126\n",
      "Iteration 22, loss = 0.01859247\n",
      "Iteration 23, loss = 0.01683507\n",
      "Iteration 24, loss = 0.01523273\n",
      "Iteration 25, loss = 0.01338627\n",
      "Iteration 26, loss = 0.01201877\n",
      "Iteration 27, loss = 0.01074396\n",
      "Iteration 28, loss = 0.00996210\n",
      "Iteration 29, loss = 0.00927604\n",
      "Iteration 30, loss = 0.00794089\n",
      "Iteration 31, loss = 0.00679189\n",
      "Iteration 32, loss = 0.00629140\n",
      "Iteration 33, loss = 0.00557714\n",
      "Iteration 34, loss = 0.00507009\n",
      "Iteration 35, loss = 0.00513593\n",
      "Iteration 36, loss = 0.00446191\n",
      "Iteration 37, loss = 0.00391934\n",
      "Iteration 38, loss = 0.00340673\n",
      "Iteration 39, loss = 0.00326417\n",
      "Iteration 40, loss = 0.00296618\n",
      "Iteration 41, loss = 0.00290167\n",
      "Iteration 42, loss = 0.00260147\n",
      "Iteration 43, loss = 0.00237763\n",
      "Iteration 44, loss = 0.00208081\n",
      "Iteration 45, loss = 0.00201116\n",
      "Iteration 46, loss = 0.00188533\n",
      "Iteration 47, loss = 0.00172987\n",
      "Iteration 48, loss = 0.00166513\n",
      "Iteration 49, loss = 0.00154155\n",
      "Iteration 50, loss = 0.00139151\n",
      "Iteration 51, loss = 0.00128315\n",
      "Iteration 52, loss = 0.00121223\n",
      "Iteration 53, loss = 0.00120594\n",
      "Iteration 54, loss = 0.00118917\n",
      "Iteration 55, loss = 0.00168583\n",
      "Iteration 56, loss = 0.01394939\n",
      "Iteration 57, loss = 0.00506317\n",
      "Iteration 58, loss = 0.00166979\n",
      "Iteration 59, loss = 0.00110449\n",
      "Iteration 60, loss = 0.00100800\n",
      "Iteration 61, loss = 0.00094689\n",
      "Iteration 62, loss = 0.00091222\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.0001, batch_size=200, hidden_layer_sizes=(100,), solver=adam; total time=  39.7s\n",
      "Iteration 1, loss = 0.55533290\n",
      "Iteration 2, loss = 0.25814829\n",
      "Iteration 3, loss = 0.20606472\n",
      "Iteration 4, loss = 0.17186758\n",
      "Iteration 5, loss = 0.14539691\n",
      "Iteration 6, loss = 0.12656322\n",
      "Iteration 7, loss = 0.11253320\n",
      "Iteration 8, loss = 0.09749873\n",
      "Iteration 9, loss = 0.08696239\n",
      "Iteration 10, loss = 0.07644478\n",
      "Iteration 11, loss = 0.06868991\n",
      "Iteration 12, loss = 0.06228597\n",
      "Iteration 13, loss = 0.05453872\n",
      "Iteration 14, loss = 0.04982810\n",
      "Iteration 15, loss = 0.04424787\n",
      "Iteration 16, loss = 0.03978770\n",
      "Iteration 17, loss = 0.03581174\n",
      "Iteration 18, loss = 0.03128279\n",
      "Iteration 19, loss = 0.02928836\n",
      "Iteration 20, loss = 0.02568569\n",
      "Iteration 21, loss = 0.02399057\n",
      "Iteration 22, loss = 0.02096753\n",
      "Iteration 23, loss = 0.01892000\n",
      "Iteration 24, loss = 0.01715763\n",
      "Iteration 25, loss = 0.01517938\n",
      "Iteration 26, loss = 0.01364903\n",
      "Iteration 27, loss = 0.01219004\n",
      "Iteration 28, loss = 0.01134470\n",
      "Iteration 29, loss = 0.01033302\n",
      "Iteration 30, loss = 0.00880522\n",
      "Iteration 31, loss = 0.00808826\n",
      "Iteration 32, loss = 0.00782401\n",
      "Iteration 33, loss = 0.00697087\n",
      "Iteration 34, loss = 0.00638194\n",
      "Iteration 35, loss = 0.00590993\n",
      "Iteration 36, loss = 0.00527142\n",
      "Iteration 37, loss = 0.00431377\n",
      "Iteration 38, loss = 0.00396763\n",
      "Iteration 39, loss = 0.00348914\n",
      "Iteration 40, loss = 0.00343377\n",
      "Iteration 41, loss = 0.00297343\n",
      "Iteration 42, loss = 0.00272118\n",
      "Iteration 43, loss = 0.00294224\n",
      "Iteration 44, loss = 0.00237644\n",
      "Iteration 45, loss = 0.00209744\n",
      "Iteration 46, loss = 0.00203063\n",
      "Iteration 47, loss = 0.00198210\n",
      "Iteration 48, loss = 0.00179375\n",
      "Iteration 49, loss = 0.00161097\n",
      "Iteration 50, loss = 0.00150639\n",
      "Iteration 51, loss = 0.00141742\n",
      "Iteration 52, loss = 0.00130295\n",
      "Iteration 53, loss = 0.00126182\n",
      "Iteration 54, loss = 0.00119337\n",
      "Iteration 55, loss = 0.00113123\n",
      "Iteration 56, loss = 0.00108298\n",
      "Iteration 57, loss = 0.00101483\n",
      "Iteration 58, loss = 0.00108022\n",
      "Iteration 59, loss = 0.00550859\n",
      "Iteration 60, loss = 0.01856576\n",
      "Iteration 61, loss = 0.00332955\n",
      "Iteration 62, loss = 0.00135247\n",
      "Iteration 63, loss = 0.00106600\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.0001, batch_size=200, hidden_layer_sizes=(100,), solver=adam; total time=  39.2s\n",
      "Iteration 1, loss = 0.54477956\n",
      "Iteration 2, loss = 0.24492321\n",
      "Iteration 3, loss = 0.18933635\n",
      "Iteration 4, loss = 0.15478334\n",
      "Iteration 5, loss = 0.13057109\n",
      "Iteration 6, loss = 0.11136386\n",
      "Iteration 7, loss = 0.09633374\n",
      "Iteration 8, loss = 0.08355851\n",
      "Iteration 9, loss = 0.07388145\n",
      "Iteration 10, loss = 0.06478623\n",
      "Iteration 11, loss = 0.05783330\n",
      "Iteration 12, loss = 0.05105191\n",
      "Iteration 13, loss = 0.04543609\n",
      "Iteration 14, loss = 0.04028640\n",
      "Iteration 15, loss = 0.03625090\n",
      "Iteration 16, loss = 0.03221378\n",
      "Iteration 17, loss = 0.02887540\n",
      "Iteration 18, loss = 0.02583203\n",
      "Iteration 19, loss = 0.02272898\n",
      "Iteration 20, loss = 0.02035012\n",
      "Iteration 21, loss = 0.01803247\n",
      "Iteration 22, loss = 0.01552563\n",
      "Iteration 23, loss = 0.01456282\n",
      "Iteration 24, loss = 0.01301344\n",
      "Iteration 25, loss = 0.01129412\n",
      "Iteration 26, loss = 0.01013743\n",
      "Iteration 27, loss = 0.00920214\n",
      "Iteration 28, loss = 0.00810362\n",
      "Iteration 29, loss = 0.00714243\n",
      "Iteration 30, loss = 0.00687072\n",
      "Iteration 31, loss = 0.00631667\n",
      "Iteration 32, loss = 0.00550586\n",
      "Iteration 33, loss = 0.00493691\n",
      "Iteration 34, loss = 0.00448823\n",
      "Iteration 35, loss = 0.00407084\n",
      "Iteration 36, loss = 0.00359621\n",
      "Iteration 37, loss = 0.00328551\n",
      "Iteration 38, loss = 0.00293442\n",
      "Iteration 39, loss = 0.00280247\n",
      "Iteration 40, loss = 0.00260292\n",
      "Iteration 41, loss = 0.00226231\n",
      "Iteration 42, loss = 0.00214148\n",
      "Iteration 43, loss = 0.00194069\n",
      "Iteration 44, loss = 0.00186914\n",
      "Iteration 45, loss = 0.00178814\n",
      "Iteration 46, loss = 0.00155243\n",
      "Iteration 47, loss = 0.00144884\n",
      "Iteration 48, loss = 0.00137209\n",
      "Iteration 49, loss = 0.00134660\n",
      "Iteration 50, loss = 0.00124371\n",
      "Iteration 51, loss = 0.00118728\n",
      "Iteration 52, loss = 0.00122684\n",
      "Iteration 53, loss = 0.00110933\n",
      "Iteration 54, loss = 0.01720063\n",
      "Iteration 55, loss = 0.00499459\n",
      "Iteration 56, loss = 0.00151241\n",
      "Iteration 57, loss = 0.00114443\n",
      "Iteration 58, loss = 0.00100606\n",
      "Iteration 59, loss = 0.00095338\n",
      "Iteration 60, loss = 0.00091231\n",
      "Iteration 61, loss = 0.00087933\n",
      "Iteration 62, loss = 0.00085545\n",
      "Iteration 63, loss = 0.00082511\n",
      "Iteration 64, loss = 0.00081504\n",
      "Iteration 65, loss = 0.00078798\n",
      "Iteration 66, loss = 0.00076861\n",
      "Iteration 67, loss = 0.00075039\n",
      "Iteration 68, loss = 0.00073460\n",
      "Iteration 69, loss = 0.00072594\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.0001, batch_size=200, hidden_layer_sizes=(100,), solver=adam; total time=  46.5s\n",
      "Iteration 1, loss = 0.57071226\n",
      "Iteration 2, loss = 0.25236152\n",
      "Iteration 3, loss = 0.19328500\n",
      "Iteration 4, loss = 0.15686124\n",
      "Iteration 5, loss = 0.13276906\n",
      "Iteration 6, loss = 0.11326551\n",
      "Iteration 7, loss = 0.09904667\n",
      "Iteration 8, loss = 0.08644889\n",
      "Iteration 9, loss = 0.07602020\n",
      "Iteration 10, loss = 0.06757918\n",
      "Iteration 11, loss = 0.06053550\n",
      "Iteration 12, loss = 0.05458793\n",
      "Iteration 13, loss = 0.04827685\n",
      "Iteration 14, loss = 0.04348522\n",
      "Iteration 15, loss = 0.03932073\n",
      "Iteration 16, loss = 0.03524614\n",
      "Iteration 17, loss = 0.03158701\n",
      "Iteration 18, loss = 0.02807856\n",
      "Iteration 19, loss = 0.02593663\n",
      "Iteration 20, loss = 0.02320007\n",
      "Iteration 21, loss = 0.02031185\n",
      "Iteration 22, loss = 0.01898734\n",
      "Iteration 23, loss = 0.01637807\n",
      "Iteration 24, loss = 0.01527461\n",
      "Iteration 25, loss = 0.01354666\n",
      "Iteration 26, loss = 0.01223737\n",
      "Iteration 27, loss = 0.01082954\n",
      "Iteration 28, loss = 0.00979602\n",
      "Iteration 29, loss = 0.00865657\n",
      "Iteration 30, loss = 0.00775056\n",
      "Iteration 31, loss = 0.00748705\n",
      "Iteration 32, loss = 0.00629300\n",
      "Iteration 33, loss = 0.00584417\n",
      "Iteration 34, loss = 0.00515777\n",
      "Iteration 35, loss = 0.00492653\n",
      "Iteration 36, loss = 0.00450928\n",
      "Iteration 37, loss = 0.00399251\n",
      "Iteration 38, loss = 0.00349522\n",
      "Iteration 39, loss = 0.00326141\n",
      "Iteration 40, loss = 0.00305894\n",
      "Iteration 41, loss = 0.00275134\n",
      "Iteration 42, loss = 0.00249306\n",
      "Iteration 43, loss = 0.00253503\n",
      "Iteration 44, loss = 0.00237408\n",
      "Iteration 45, loss = 0.00309353\n",
      "Iteration 46, loss = 0.00210047\n",
      "Iteration 47, loss = 0.00243379\n",
      "Iteration 48, loss = 0.00166592\n",
      "Iteration 49, loss = 0.00150107\n",
      "Iteration 50, loss = 0.00142037\n",
      "Iteration 51, loss = 0.00135534\n",
      "Iteration 52, loss = 0.00125778\n",
      "Iteration 53, loss = 0.00116810\n",
      "Iteration 54, loss = 0.00114731\n",
      "Iteration 55, loss = 0.00112700\n",
      "Iteration 56, loss = 0.00101477\n",
      "Iteration 57, loss = 0.00098430\n",
      "Iteration 58, loss = 0.00093482\n",
      "Iteration 59, loss = 0.00092798\n",
      "Iteration 60, loss = 0.00087616\n",
      "Iteration 61, loss = 0.00084298\n",
      "Iteration 62, loss = 0.00082009\n",
      "Iteration 63, loss = 0.01626941\n",
      "Iteration 64, loss = 0.00766455\n",
      "Iteration 65, loss = 0.00200601\n",
      "Iteration 66, loss = 0.00103583\n",
      "Iteration 67, loss = 0.00089759\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.0001, batch_size=200, hidden_layer_sizes=(100,), solver=adam; total time=  48.0s\n",
      "Iteration 1, loss = 0.57304047\n",
      "Iteration 2, loss = 0.26127044\n",
      "Iteration 3, loss = 0.20230903\n",
      "Iteration 4, loss = 0.16542099\n",
      "Iteration 5, loss = 0.13810983\n",
      "Iteration 6, loss = 0.12034592\n",
      "Iteration 7, loss = 0.10400457\n",
      "Iteration 8, loss = 0.09109313\n",
      "Iteration 9, loss = 0.08074093\n",
      "Iteration 10, loss = 0.07169080\n",
      "Iteration 11, loss = 0.06408331\n",
      "Iteration 12, loss = 0.05779008\n",
      "Iteration 13, loss = 0.05084520\n",
      "Iteration 14, loss = 0.04563602\n",
      "Iteration 15, loss = 0.04119047\n",
      "Iteration 16, loss = 0.03631113\n",
      "Iteration 17, loss = 0.03283658\n",
      "Iteration 18, loss = 0.02978501\n",
      "Iteration 19, loss = 0.02750583\n",
      "Iteration 20, loss = 0.02365655\n",
      "Iteration 21, loss = 0.02087329\n",
      "Iteration 22, loss = 0.01969027\n",
      "Iteration 23, loss = 0.01664900\n",
      "Iteration 24, loss = 0.01576367\n",
      "Iteration 25, loss = 0.01390900\n",
      "Iteration 26, loss = 0.01208054\n",
      "Iteration 27, loss = 0.01089957\n",
      "Iteration 28, loss = 0.01020268\n",
      "Iteration 29, loss = 0.00910897\n",
      "Iteration 30, loss = 0.00811802\n",
      "Iteration 31, loss = 0.00778032\n",
      "Iteration 32, loss = 0.00709254\n",
      "Iteration 33, loss = 0.00581209\n",
      "Iteration 34, loss = 0.00541400\n",
      "Iteration 35, loss = 0.00519201\n",
      "Iteration 36, loss = 0.00429932\n",
      "Iteration 37, loss = 0.00402733\n",
      "Iteration 38, loss = 0.00362776\n",
      "Iteration 39, loss = 0.00354342\n",
      "Iteration 40, loss = 0.00322172\n",
      "Iteration 41, loss = 0.00302097\n",
      "Iteration 42, loss = 0.00257179\n",
      "Iteration 43, loss = 0.00244520\n",
      "Iteration 44, loss = 0.00225120\n",
      "Iteration 45, loss = 0.00203778\n",
      "Iteration 46, loss = 0.00196935\n",
      "Iteration 47, loss = 0.00192995\n",
      "Iteration 48, loss = 0.00169724\n",
      "Iteration 49, loss = 0.00155862\n",
      "Iteration 50, loss = 0.00146219\n",
      "Iteration 51, loss = 0.00142079\n",
      "Iteration 52, loss = 0.00129594\n",
      "Iteration 53, loss = 0.00129889\n",
      "Iteration 54, loss = 0.00130766\n",
      "Iteration 55, loss = 0.00114185\n",
      "Iteration 56, loss = 0.00111019\n",
      "Iteration 57, loss = 0.00133201\n",
      "Iteration 58, loss = 0.01768880\n",
      "Iteration 59, loss = 0.00484243\n",
      "Iteration 60, loss = 0.00171936\n",
      "Iteration 61, loss = 0.00109893\n",
      "Iteration 62, loss = 0.00098697\n",
      "Iteration 63, loss = 0.00093393\n",
      "Iteration 64, loss = 0.00090540\n",
      "Iteration 65, loss = 0.00087366\n",
      "Iteration 66, loss = 0.00084887\n",
      "Iteration 67, loss = 0.00082618\n",
      "Iteration 68, loss = 0.00080964\n",
      "Iteration 69, loss = 0.00079446\n",
      "Iteration 70, loss = 0.00077666\n",
      "Iteration 71, loss = 0.00076395\n",
      "Iteration 72, loss = 0.00074909\n",
      "Iteration 73, loss = 0.00073758\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.0001, batch_size=200, hidden_layer_sizes=(100,), solver=adam; total time=  54.8s\n",
      "Iteration 1, loss = 1.80050679\n",
      "Iteration 2, loss = 1.01919596\n",
      "Iteration 3, loss = 0.72665006\n",
      "Iteration 4, loss = 0.59786930\n",
      "Iteration 5, loss = 0.52650898\n",
      "Iteration 6, loss = 0.48086055\n",
      "Iteration 7, loss = 0.44878917\n",
      "Iteration 8, loss = 0.42492401\n",
      "Iteration 9, loss = 0.40641173\n",
      "Iteration 10, loss = 0.39132988\n",
      "Iteration 11, loss = 0.37884622\n",
      "Iteration 12, loss = 0.36818661\n",
      "Iteration 13, loss = 0.35895841\n",
      "Iteration 14, loss = 0.35089104\n",
      "Iteration 15, loss = 0.34364573\n",
      "Iteration 16, loss = 0.33708716\n",
      "Iteration 17, loss = 0.33117988\n",
      "Iteration 18, loss = 0.32582243\n",
      "Iteration 19, loss = 0.32064839\n",
      "Iteration 20, loss = 0.31590936\n",
      "Iteration 21, loss = 0.31155945\n",
      "Iteration 22, loss = 0.30741857\n",
      "Iteration 23, loss = 0.30340347\n",
      "Iteration 24, loss = 0.29964550\n",
      "Iteration 25, loss = 0.29618479\n",
      "Iteration 26, loss = 0.29273218\n",
      "Iteration 27, loss = 0.28935988\n",
      "Iteration 28, loss = 0.28604347\n",
      "Iteration 29, loss = 0.28294126\n",
      "Iteration 30, loss = 0.28002320\n",
      "Iteration 31, loss = 0.27707033\n",
      "Iteration 32, loss = 0.27417293\n",
      "Iteration 33, loss = 0.27138488\n",
      "Iteration 34, loss = 0.26875670\n",
      "Iteration 35, loss = 0.26603900\n",
      "Iteration 36, loss = 0.26346946\n",
      "Iteration 37, loss = 0.26100717\n",
      "Iteration 38, loss = 0.25846019\n",
      "Iteration 39, loss = 0.25603935\n",
      "Iteration 40, loss = 0.25372521\n",
      "Iteration 41, loss = 0.25129850\n",
      "Iteration 42, loss = 0.24906235\n",
      "Iteration 43, loss = 0.24674136\n",
      "Iteration 44, loss = 0.24462189\n",
      "Iteration 45, loss = 0.24239606\n",
      "Iteration 46, loss = 0.24028005\n",
      "Iteration 47, loss = 0.23824445\n",
      "Iteration 48, loss = 0.23617972\n",
      "Iteration 49, loss = 0.23412682\n",
      "Iteration 50, loss = 0.23210635\n",
      "Iteration 51, loss = 0.23014191\n",
      "Iteration 52, loss = 0.22815439\n",
      "Iteration 53, loss = 0.22632381\n",
      "Iteration 54, loss = 0.22434892\n",
      "Iteration 55, loss = 0.22251281\n",
      "Iteration 56, loss = 0.22064462\n",
      "Iteration 57, loss = 0.21887680\n",
      "Iteration 58, loss = 0.21698501\n",
      "Iteration 59, loss = 0.21524116\n",
      "Iteration 60, loss = 0.21361133\n",
      "Iteration 61, loss = 0.21171841\n",
      "Iteration 62, loss = 0.21017393\n",
      "Iteration 63, loss = 0.20843535\n",
      "Iteration 64, loss = 0.20682171\n",
      "Iteration 65, loss = 0.20522503\n",
      "Iteration 66, loss = 0.20361164\n",
      "Iteration 67, loss = 0.20199657\n",
      "Iteration 68, loss = 0.20049700\n",
      "Iteration 69, loss = 0.19890829\n",
      "Iteration 70, loss = 0.19748844\n",
      "Iteration 71, loss = 0.19596976\n",
      "Iteration 72, loss = 0.19451344\n",
      "Iteration 73, loss = 0.19310148\n",
      "Iteration 74, loss = 0.19166841\n",
      "Iteration 75, loss = 0.19032925\n",
      "Iteration 76, loss = 0.18891471\n",
      "Iteration 77, loss = 0.18752837\n",
      "Iteration 78, loss = 0.18623409\n",
      "Iteration 79, loss = 0.18487267\n",
      "Iteration 80, loss = 0.18358971\n",
      "Iteration 81, loss = 0.18229837\n",
      "Iteration 82, loss = 0.18100136\n",
      "Iteration 83, loss = 0.17976034\n",
      "Iteration 84, loss = 0.17851103\n",
      "Iteration 85, loss = 0.17730684\n",
      "Iteration 86, loss = 0.17600900\n",
      "Iteration 87, loss = 0.17482418\n",
      "Iteration 88, loss = 0.17370690\n",
      "Iteration 89, loss = 0.17246488\n",
      "Iteration 90, loss = 0.17130799\n",
      "Iteration 91, loss = 0.17018590\n",
      "Iteration 92, loss = 0.16908485\n",
      "Iteration 93, loss = 0.16786575\n",
      "Iteration 94, loss = 0.16681930\n",
      "Iteration 95, loss = 0.16571054\n",
      "Iteration 96, loss = 0.16465270\n",
      "Iteration 97, loss = 0.16355673\n",
      "Iteration 98, loss = 0.16251678\n",
      "Iteration 99, loss = 0.16145907\n",
      "Iteration 100, loss = 0.16051202\n",
      "Iteration 101, loss = 0.15951339\n",
      "Iteration 102, loss = 0.15845004\n",
      "Iteration 103, loss = 0.15742870\n",
      "Iteration 104, loss = 0.15646883\n",
      "Iteration 105, loss = 0.15545892\n",
      "Iteration 106, loss = 0.15457581\n",
      "Iteration 107, loss = 0.15366016\n",
      "Iteration 108, loss = 0.15264108\n",
      "Iteration 109, loss = 0.15171331\n",
      "Iteration 110, loss = 0.15083148\n",
      "Iteration 111, loss = 0.14990847\n",
      "Iteration 112, loss = 0.14903799\n",
      "Iteration 113, loss = 0.14819785\n",
      "Iteration 114, loss = 0.14733055\n",
      "Iteration 115, loss = 0.14646418\n",
      "Iteration 116, loss = 0.14553967\n",
      "Iteration 117, loss = 0.14473362\n",
      "Iteration 118, loss = 0.14396393\n",
      "Iteration 119, loss = 0.14306404\n",
      "Iteration 120, loss = 0.14217938\n",
      "Iteration 121, loss = 0.14149519\n",
      "Iteration 122, loss = 0.14067376\n",
      "Iteration 123, loss = 0.13982807\n",
      "Iteration 124, loss = 0.13905508\n",
      "Iteration 125, loss = 0.13829579\n",
      "Iteration 126, loss = 0.13749045\n",
      "Iteration 127, loss = 0.13674222\n",
      "Iteration 128, loss = 0.13599560\n",
      "Iteration 129, loss = 0.13528826\n",
      "Iteration 130, loss = 0.13455341\n",
      "Iteration 131, loss = 0.13383777\n",
      "Iteration 132, loss = 0.13305733\n",
      "Iteration 133, loss = 0.13234336\n",
      "Iteration 134, loss = 0.13163959\n",
      "Iteration 135, loss = 0.13090217\n",
      "Iteration 136, loss = 0.13028106\n",
      "Iteration 137, loss = 0.12949123\n",
      "Iteration 138, loss = 0.12889804\n",
      "Iteration 139, loss = 0.12817732\n",
      "Iteration 140, loss = 0.12748096\n",
      "Iteration 141, loss = 0.12689805\n",
      "Iteration 142, loss = 0.12620446\n",
      "Iteration 143, loss = 0.12552152\n",
      "Iteration 144, loss = 0.12490219\n",
      "Iteration 145, loss = 0.12428125\n",
      "Iteration 146, loss = 0.12365548\n",
      "Iteration 147, loss = 0.12303769\n",
      "Iteration 148, loss = 0.12238949\n",
      "Iteration 149, loss = 0.12176387\n",
      "Iteration 150, loss = 0.12113733\n",
      "Iteration 151, loss = 0.12061077\n",
      "Iteration 152, loss = 0.11994999\n",
      "Iteration 153, loss = 0.11933443\n",
      "Iteration 154, loss = 0.11883823\n",
      "Iteration 155, loss = 0.11821419\n",
      "Iteration 156, loss = 0.11759636\n",
      "Iteration 157, loss = 0.11708855\n",
      "Iteration 158, loss = 0.11653124\n",
      "Iteration 159, loss = 0.11591838\n",
      "Iteration 160, loss = 0.11538874\n",
      "Iteration 161, loss = 0.11479187\n",
      "Iteration 162, loss = 0.11433546\n",
      "Iteration 163, loss = 0.11371481\n",
      "Iteration 164, loss = 0.11323063\n",
      "Iteration 165, loss = 0.11265075\n",
      "Iteration 166, loss = 0.11219725\n",
      "Iteration 167, loss = 0.11166262\n",
      "Iteration 168, loss = 0.11108370\n",
      "Iteration 169, loss = 0.11048841\n",
      "Iteration 170, loss = 0.11006165\n",
      "Iteration 171, loss = 0.10959250\n",
      "Iteration 172, loss = 0.10904964\n",
      "Iteration 173, loss = 0.10864620\n",
      "Iteration 174, loss = 0.10809364\n",
      "Iteration 175, loss = 0.10758419\n",
      "Iteration 176, loss = 0.10709641\n",
      "Iteration 177, loss = 0.10660278\n",
      "Iteration 178, loss = 0.10616091\n",
      "Iteration 179, loss = 0.10566823\n",
      "Iteration 180, loss = 0.10523845\n",
      "Iteration 181, loss = 0.10468099\n",
      "Iteration 182, loss = 0.10429043\n",
      "Iteration 183, loss = 0.10383183\n",
      "Iteration 184, loss = 0.10336309\n",
      "Iteration 185, loss = 0.10289656\n",
      "Iteration 186, loss = 0.10253957\n",
      "Iteration 187, loss = 0.10198247\n",
      "Iteration 188, loss = 0.10157313\n",
      "Iteration 189, loss = 0.10111124\n",
      "Iteration 190, loss = 0.10069998\n",
      "Iteration 191, loss = 0.10024579\n",
      "Iteration 192, loss = 0.09986137\n",
      "Iteration 193, loss = 0.09940988\n",
      "Iteration 194, loss = 0.09897303\n",
      "Iteration 195, loss = 0.09854590\n",
      "Iteration 196, loss = 0.09809474\n",
      "Iteration 197, loss = 0.09769510\n",
      "Iteration 198, loss = 0.09733542\n",
      "Iteration 199, loss = 0.09691563\n",
      "Iteration 200, loss = 0.09648994\n",
      "[CV] END activation=relu, alpha=0.0001, batch_size=200, hidden_layer_sizes=(100,), solver=sgd; total time= 1.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.81541198\n",
      "Iteration 2, loss = 1.02964400\n",
      "Iteration 3, loss = 0.74037068\n",
      "Iteration 4, loss = 0.61217247\n",
      "Iteration 5, loss = 0.53895468\n",
      "Iteration 6, loss = 0.49133469\n",
      "Iteration 7, loss = 0.45756036\n",
      "Iteration 8, loss = 0.43230385\n",
      "Iteration 9, loss = 0.41238097\n",
      "Iteration 10, loss = 0.39633647\n",
      "Iteration 11, loss = 0.38286529\n",
      "Iteration 12, loss = 0.37154449\n",
      "Iteration 13, loss = 0.36176986\n",
      "Iteration 14, loss = 0.35322242\n",
      "Iteration 15, loss = 0.34550045\n",
      "Iteration 16, loss = 0.33855473\n",
      "Iteration 17, loss = 0.33223408\n",
      "Iteration 18, loss = 0.32641019\n",
      "Iteration 19, loss = 0.32120368\n",
      "Iteration 20, loss = 0.31617788\n",
      "Iteration 21, loss = 0.31159063\n",
      "Iteration 22, loss = 0.30719895\n",
      "Iteration 23, loss = 0.30305807\n",
      "Iteration 24, loss = 0.29916513\n",
      "Iteration 25, loss = 0.29549605\n",
      "Iteration 26, loss = 0.29199493\n",
      "Iteration 27, loss = 0.28859683\n",
      "Iteration 28, loss = 0.28525917\n",
      "Iteration 29, loss = 0.28209995\n",
      "Iteration 30, loss = 0.27913639\n",
      "Iteration 31, loss = 0.27608245\n",
      "Iteration 32, loss = 0.27328523\n",
      "Iteration 33, loss = 0.27055246\n",
      "Iteration 34, loss = 0.26784302\n",
      "Iteration 35, loss = 0.26543253\n",
      "Iteration 36, loss = 0.26285909\n",
      "Iteration 37, loss = 0.26034403\n",
      "Iteration 38, loss = 0.25793830\n",
      "Iteration 39, loss = 0.25571816\n",
      "Iteration 40, loss = 0.25346095\n",
      "Iteration 41, loss = 0.25126548\n",
      "Iteration 42, loss = 0.24908282\n",
      "Iteration 43, loss = 0.24697310\n",
      "Iteration 44, loss = 0.24481047\n",
      "Iteration 45, loss = 0.24273895\n",
      "Iteration 46, loss = 0.24078812\n",
      "Iteration 47, loss = 0.23886590\n",
      "Iteration 48, loss = 0.23690116\n",
      "Iteration 49, loss = 0.23498448\n",
      "Iteration 50, loss = 0.23318101\n",
      "Iteration 51, loss = 0.23136174\n",
      "Iteration 52, loss = 0.22951981\n",
      "Iteration 53, loss = 0.22779467\n",
      "Iteration 54, loss = 0.22604388\n",
      "Iteration 55, loss = 0.22433744\n",
      "Iteration 56, loss = 0.22265094\n",
      "Iteration 57, loss = 0.22099712\n",
      "Iteration 58, loss = 0.21937002\n",
      "Iteration 59, loss = 0.21786973\n",
      "Iteration 60, loss = 0.21618933\n",
      "Iteration 61, loss = 0.21459193\n",
      "Iteration 62, loss = 0.21305820\n",
      "Iteration 63, loss = 0.21152503\n",
      "Iteration 64, loss = 0.21007355\n",
      "Iteration 65, loss = 0.20864638\n",
      "Iteration 66, loss = 0.20726621\n",
      "Iteration 67, loss = 0.20590677\n",
      "Iteration 68, loss = 0.20444573\n",
      "Iteration 69, loss = 0.20303731\n",
      "Iteration 70, loss = 0.20177319\n",
      "Iteration 71, loss = 0.20038831\n",
      "Iteration 72, loss = 0.19911931\n",
      "Iteration 73, loss = 0.19780504\n",
      "Iteration 74, loss = 0.19647231\n",
      "Iteration 75, loss = 0.19520505\n",
      "Iteration 76, loss = 0.19399713\n",
      "Iteration 77, loss = 0.19277687\n",
      "Iteration 78, loss = 0.19159341\n",
      "Iteration 79, loss = 0.19031138\n",
      "Iteration 80, loss = 0.18924461\n",
      "Iteration 81, loss = 0.18800588\n",
      "Iteration 82, loss = 0.18681385\n",
      "Iteration 83, loss = 0.18565640\n",
      "Iteration 84, loss = 0.18459233\n",
      "Iteration 85, loss = 0.18351125\n",
      "Iteration 86, loss = 0.18243255\n",
      "Iteration 87, loss = 0.18127514\n",
      "Iteration 88, loss = 0.18012663\n",
      "Iteration 89, loss = 0.17915789\n",
      "Iteration 90, loss = 0.17810861\n",
      "Iteration 91, loss = 0.17706970\n",
      "Iteration 92, loss = 0.17604604\n",
      "Iteration 93, loss = 0.17506793\n",
      "Iteration 94, loss = 0.17399638\n",
      "Iteration 95, loss = 0.17303152\n",
      "Iteration 96, loss = 0.17205852\n",
      "Iteration 97, loss = 0.17109995\n",
      "Iteration 98, loss = 0.17013081\n",
      "Iteration 99, loss = 0.16927520\n",
      "Iteration 100, loss = 0.16826795\n",
      "Iteration 101, loss = 0.16737773\n",
      "Iteration 102, loss = 0.16647164\n",
      "Iteration 103, loss = 0.16552860\n",
      "Iteration 104, loss = 0.16469689\n",
      "Iteration 105, loss = 0.16373315\n",
      "Iteration 106, loss = 0.16283323\n",
      "Iteration 107, loss = 0.16203211\n",
      "Iteration 108, loss = 0.16111963\n",
      "Iteration 109, loss = 0.16031812\n",
      "Iteration 110, loss = 0.15946339\n",
      "Iteration 111, loss = 0.15855846\n",
      "Iteration 112, loss = 0.15776458\n",
      "Iteration 113, loss = 0.15698974\n",
      "Iteration 114, loss = 0.15618758\n",
      "Iteration 115, loss = 0.15540934\n",
      "Iteration 116, loss = 0.15454243\n",
      "Iteration 117, loss = 0.15373679\n",
      "Iteration 118, loss = 0.15296568\n",
      "Iteration 119, loss = 0.15218008\n",
      "Iteration 120, loss = 0.15139768\n",
      "Iteration 121, loss = 0.15062621\n",
      "Iteration 122, loss = 0.14990222\n",
      "Iteration 123, loss = 0.14910850\n",
      "Iteration 124, loss = 0.14841096\n",
      "Iteration 125, loss = 0.14764897\n",
      "Iteration 126, loss = 0.14695124\n",
      "Iteration 127, loss = 0.14621635\n",
      "Iteration 128, loss = 0.14553345\n",
      "Iteration 129, loss = 0.14478806\n",
      "Iteration 130, loss = 0.14404226\n",
      "Iteration 131, loss = 0.14345716\n",
      "Iteration 132, loss = 0.14264863\n",
      "Iteration 133, loss = 0.14195455\n",
      "Iteration 134, loss = 0.14138993\n",
      "Iteration 135, loss = 0.14062178\n",
      "Iteration 136, loss = 0.13997578\n",
      "Iteration 137, loss = 0.13928720\n",
      "Iteration 138, loss = 0.13874267\n",
      "Iteration 139, loss = 0.13805687\n",
      "Iteration 140, loss = 0.13733561\n",
      "Iteration 141, loss = 0.13677298\n",
      "Iteration 142, loss = 0.13605523\n",
      "Iteration 143, loss = 0.13549640\n",
      "Iteration 144, loss = 0.13488381\n",
      "Iteration 145, loss = 0.13427209\n",
      "Iteration 146, loss = 0.13361501\n",
      "Iteration 147, loss = 0.13302133\n",
      "Iteration 148, loss = 0.13239856\n",
      "Iteration 149, loss = 0.13185135\n",
      "Iteration 150, loss = 0.13120024\n",
      "Iteration 151, loss = 0.13065072\n",
      "Iteration 152, loss = 0.13004746\n",
      "Iteration 153, loss = 0.12944512\n",
      "Iteration 154, loss = 0.12886986\n",
      "Iteration 155, loss = 0.12830648\n",
      "Iteration 156, loss = 0.12778406\n",
      "Iteration 157, loss = 0.12712647\n",
      "Iteration 158, loss = 0.12662002\n",
      "Iteration 159, loss = 0.12612127\n",
      "Iteration 160, loss = 0.12548796\n",
      "Iteration 161, loss = 0.12503470\n",
      "Iteration 162, loss = 0.12438585\n",
      "Iteration 163, loss = 0.12397199\n",
      "Iteration 164, loss = 0.12338151\n",
      "Iteration 165, loss = 0.12285358\n",
      "Iteration 166, loss = 0.12225797\n",
      "Iteration 167, loss = 0.12183362\n",
      "Iteration 168, loss = 0.12131998\n",
      "Iteration 169, loss = 0.12083847\n",
      "Iteration 170, loss = 0.12026248\n",
      "Iteration 171, loss = 0.11978803\n",
      "Iteration 172, loss = 0.11930417\n",
      "Iteration 173, loss = 0.11880846\n",
      "Iteration 174, loss = 0.11831734\n",
      "Iteration 175, loss = 0.11779003\n",
      "Iteration 176, loss = 0.11735421\n",
      "Iteration 177, loss = 0.11680864\n",
      "Iteration 178, loss = 0.11636814\n",
      "Iteration 179, loss = 0.11584966\n",
      "Iteration 180, loss = 0.11539350\n",
      "Iteration 181, loss = 0.11493685\n",
      "Iteration 182, loss = 0.11447010\n",
      "Iteration 183, loss = 0.11398295\n",
      "Iteration 184, loss = 0.11357509\n",
      "Iteration 185, loss = 0.11306868\n",
      "Iteration 186, loss = 0.11265432\n",
      "Iteration 187, loss = 0.11217504\n",
      "Iteration 188, loss = 0.11173003\n",
      "Iteration 189, loss = 0.11125747\n",
      "Iteration 190, loss = 0.11083581\n",
      "Iteration 191, loss = 0.11043664\n",
      "Iteration 192, loss = 0.11000175\n",
      "Iteration 193, loss = 0.10953546\n",
      "Iteration 194, loss = 0.10911344\n",
      "Iteration 195, loss = 0.10868860\n",
      "Iteration 196, loss = 0.10830152\n",
      "Iteration 197, loss = 0.10784570\n",
      "Iteration 198, loss = 0.10747409\n",
      "Iteration 199, loss = 0.10700924\n",
      "Iteration 200, loss = 0.10663107\n",
      "[CV] END activation=relu, alpha=0.0001, batch_size=200, hidden_layer_sizes=(100,), solver=sgd; total time= 1.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.76276733\n",
      "Iteration 2, loss = 0.96880608\n",
      "Iteration 3, loss = 0.69752696\n",
      "Iteration 4, loss = 0.57952237\n",
      "Iteration 5, loss = 0.51336117\n",
      "Iteration 6, loss = 0.47023144\n",
      "Iteration 7, loss = 0.43976323\n",
      "Iteration 8, loss = 0.41693029\n",
      "Iteration 9, loss = 0.39883902\n",
      "Iteration 10, loss = 0.38409215\n",
      "Iteration 11, loss = 0.37192507\n",
      "Iteration 12, loss = 0.36145134\n",
      "Iteration 13, loss = 0.35225277\n",
      "Iteration 14, loss = 0.34419839\n",
      "Iteration 15, loss = 0.33696618\n",
      "Iteration 16, loss = 0.33064609\n",
      "Iteration 17, loss = 0.32453915\n",
      "Iteration 18, loss = 0.31903621\n",
      "Iteration 19, loss = 0.31409105\n",
      "Iteration 20, loss = 0.30943267\n",
      "Iteration 21, loss = 0.30487153\n",
      "Iteration 22, loss = 0.30087253\n",
      "Iteration 23, loss = 0.29682919\n",
      "Iteration 24, loss = 0.29307390\n",
      "Iteration 25, loss = 0.28940199\n",
      "Iteration 26, loss = 0.28601505\n",
      "Iteration 27, loss = 0.28276275\n",
      "Iteration 28, loss = 0.27956700\n",
      "Iteration 29, loss = 0.27648529\n",
      "Iteration 30, loss = 0.27347815\n",
      "Iteration 31, loss = 0.27065193\n",
      "Iteration 32, loss = 0.26790713\n",
      "Iteration 33, loss = 0.26513766\n",
      "Iteration 34, loss = 0.26250518\n",
      "Iteration 35, loss = 0.25989892\n",
      "Iteration 36, loss = 0.25750010\n",
      "Iteration 37, loss = 0.25508522\n",
      "Iteration 38, loss = 0.25252578\n",
      "Iteration 39, loss = 0.25024938\n",
      "Iteration 40, loss = 0.24789364\n",
      "Iteration 41, loss = 0.24568703\n",
      "Iteration 42, loss = 0.24357763\n",
      "Iteration 43, loss = 0.24148582\n",
      "Iteration 44, loss = 0.23927707\n",
      "Iteration 45, loss = 0.23705540\n",
      "Iteration 46, loss = 0.23511380\n",
      "Iteration 47, loss = 0.23304524\n",
      "Iteration 48, loss = 0.23117344\n",
      "Iteration 49, loss = 0.22924455\n",
      "Iteration 50, loss = 0.22733124\n",
      "Iteration 51, loss = 0.22549350\n",
      "Iteration 52, loss = 0.22376730\n",
      "Iteration 53, loss = 0.22187584\n",
      "Iteration 54, loss = 0.22012854\n",
      "Iteration 55, loss = 0.21830451\n",
      "Iteration 56, loss = 0.21668426\n",
      "Iteration 57, loss = 0.21504974\n",
      "Iteration 58, loss = 0.21328198\n",
      "Iteration 59, loss = 0.21171540\n",
      "Iteration 60, loss = 0.21016702\n",
      "Iteration 61, loss = 0.20854477\n",
      "Iteration 62, loss = 0.20704748\n",
      "Iteration 63, loss = 0.20543147\n",
      "Iteration 64, loss = 0.20398287\n",
      "Iteration 65, loss = 0.20252250\n",
      "Iteration 66, loss = 0.20112316\n",
      "Iteration 67, loss = 0.19967967\n",
      "Iteration 68, loss = 0.19827640\n",
      "Iteration 69, loss = 0.19683856\n",
      "Iteration 70, loss = 0.19549846\n",
      "Iteration 71, loss = 0.19410765\n",
      "Iteration 72, loss = 0.19280528\n",
      "Iteration 73, loss = 0.19148177\n",
      "Iteration 74, loss = 0.19006467\n",
      "Iteration 75, loss = 0.18882756\n",
      "Iteration 76, loss = 0.18749716\n",
      "Iteration 77, loss = 0.18640168\n",
      "Iteration 78, loss = 0.18512929\n",
      "Iteration 79, loss = 0.18392589\n",
      "Iteration 80, loss = 0.18273766\n",
      "Iteration 81, loss = 0.18166025\n",
      "Iteration 82, loss = 0.18036445\n",
      "Iteration 83, loss = 0.17925695\n",
      "Iteration 84, loss = 0.17812722\n",
      "Iteration 85, loss = 0.17694339\n",
      "Iteration 86, loss = 0.17584676\n",
      "Iteration 87, loss = 0.17481197\n",
      "Iteration 88, loss = 0.17367988\n",
      "Iteration 89, loss = 0.17262657\n",
      "Iteration 90, loss = 0.17161821\n",
      "Iteration 91, loss = 0.17054551\n",
      "Iteration 92, loss = 0.16946546\n",
      "Iteration 93, loss = 0.16848855\n",
      "Iteration 94, loss = 0.16754182\n",
      "Iteration 95, loss = 0.16650508\n",
      "Iteration 96, loss = 0.16551828\n",
      "Iteration 97, loss = 0.16455546\n",
      "Iteration 98, loss = 0.16356886\n",
      "Iteration 99, loss = 0.16263129\n",
      "Iteration 100, loss = 0.16175931\n",
      "Iteration 101, loss = 0.16069731\n",
      "Iteration 102, loss = 0.15990654\n",
      "Iteration 103, loss = 0.15894094\n",
      "Iteration 104, loss = 0.15811293\n",
      "Iteration 105, loss = 0.15724075\n",
      "Iteration 106, loss = 0.15628246\n",
      "Iteration 107, loss = 0.15548580\n",
      "Iteration 108, loss = 0.15456232\n",
      "Iteration 109, loss = 0.15377532\n",
      "Iteration 110, loss = 0.15291837\n",
      "Iteration 111, loss = 0.15213267\n",
      "Iteration 112, loss = 0.15120474\n",
      "Iteration 113, loss = 0.15045566\n",
      "Iteration 114, loss = 0.14964670\n",
      "Iteration 115, loss = 0.14886164\n",
      "Iteration 116, loss = 0.14811282\n",
      "Iteration 117, loss = 0.14734376\n",
      "Iteration 118, loss = 0.14657692\n",
      "Iteration 119, loss = 0.14582660\n",
      "Iteration 120, loss = 0.14505206\n",
      "Iteration 121, loss = 0.14429686\n",
      "Iteration 122, loss = 0.14360606\n",
      "Iteration 123, loss = 0.14287066\n",
      "Iteration 124, loss = 0.14212597\n",
      "Iteration 125, loss = 0.14137575\n",
      "Iteration 126, loss = 0.14067031\n",
      "Iteration 127, loss = 0.14001999\n",
      "Iteration 128, loss = 0.13926688\n",
      "Iteration 129, loss = 0.13862073\n",
      "Iteration 130, loss = 0.13792161\n",
      "Iteration 131, loss = 0.13721517\n",
      "Iteration 132, loss = 0.13651371\n",
      "Iteration 133, loss = 0.13590374\n",
      "Iteration 134, loss = 0.13522503\n",
      "Iteration 135, loss = 0.13464412\n",
      "Iteration 136, loss = 0.13396036\n",
      "Iteration 137, loss = 0.13324837\n",
      "Iteration 138, loss = 0.13268022\n",
      "Iteration 139, loss = 0.13201434\n",
      "Iteration 140, loss = 0.13137570\n",
      "Iteration 141, loss = 0.13085131\n",
      "Iteration 142, loss = 0.13017243\n",
      "Iteration 143, loss = 0.12949408\n",
      "Iteration 144, loss = 0.12903409\n",
      "Iteration 145, loss = 0.12837406\n",
      "Iteration 146, loss = 0.12775914\n",
      "Iteration 147, loss = 0.12713923\n",
      "Iteration 148, loss = 0.12660606\n",
      "Iteration 149, loss = 0.12602818\n",
      "Iteration 150, loss = 0.12541156\n",
      "Iteration 151, loss = 0.12493370\n",
      "Iteration 152, loss = 0.12428420\n",
      "Iteration 153, loss = 0.12374073\n",
      "Iteration 154, loss = 0.12320604\n",
      "Iteration 155, loss = 0.12261824\n",
      "Iteration 156, loss = 0.12218538\n",
      "Iteration 157, loss = 0.12149243\n",
      "Iteration 158, loss = 0.12099060\n",
      "Iteration 159, loss = 0.12043513\n",
      "Iteration 160, loss = 0.11993820\n",
      "Iteration 161, loss = 0.11943733\n",
      "Iteration 162, loss = 0.11897132\n",
      "Iteration 163, loss = 0.11828962\n",
      "Iteration 164, loss = 0.11786252\n",
      "Iteration 165, loss = 0.11737264\n",
      "Iteration 166, loss = 0.11682230\n",
      "Iteration 167, loss = 0.11639803\n",
      "Iteration 168, loss = 0.11586259\n",
      "Iteration 169, loss = 0.11533974\n",
      "Iteration 170, loss = 0.11483759\n",
      "Iteration 171, loss = 0.11436072\n",
      "Iteration 172, loss = 0.11382799\n",
      "Iteration 173, loss = 0.11340263\n",
      "Iteration 174, loss = 0.11288202\n",
      "Iteration 175, loss = 0.11243574\n",
      "Iteration 176, loss = 0.11196253\n",
      "Iteration 177, loss = 0.11148646\n",
      "Iteration 178, loss = 0.11107667\n",
      "Iteration 179, loss = 0.11055626\n",
      "Iteration 180, loss = 0.11015413\n",
      "Iteration 181, loss = 0.10963193\n",
      "Iteration 182, loss = 0.10915789\n",
      "Iteration 183, loss = 0.10879112\n",
      "Iteration 184, loss = 0.10832018\n",
      "Iteration 185, loss = 0.10783527\n",
      "Iteration 186, loss = 0.10742582\n",
      "Iteration 187, loss = 0.10705917\n",
      "Iteration 188, loss = 0.10652699\n",
      "Iteration 189, loss = 0.10612116\n",
      "Iteration 190, loss = 0.10570903\n",
      "Iteration 191, loss = 0.10522562\n",
      "Iteration 192, loss = 0.10488903\n",
      "Iteration 193, loss = 0.10442473\n",
      "Iteration 194, loss = 0.10400051\n",
      "Iteration 195, loss = 0.10357819\n",
      "Iteration 196, loss = 0.10326398\n",
      "Iteration 197, loss = 0.10278361\n",
      "Iteration 198, loss = 0.10241583\n",
      "Iteration 199, loss = 0.10196737\n",
      "Iteration 200, loss = 0.10165634\n",
      "[CV] END activation=relu, alpha=0.0001, batch_size=200, hidden_layer_sizes=(100,), solver=sgd; total time= 1.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.83042904\n",
      "Iteration 2, loss = 1.08014019\n",
      "Iteration 3, loss = 0.77322122\n",
      "Iteration 4, loss = 0.63042717\n",
      "Iteration 5, loss = 0.54955048\n",
      "Iteration 6, loss = 0.49779347\n",
      "Iteration 7, loss = 0.46166724\n",
      "Iteration 8, loss = 0.43510091\n",
      "Iteration 9, loss = 0.41453711\n",
      "Iteration 10, loss = 0.39806330\n",
      "Iteration 11, loss = 0.38443536\n",
      "Iteration 12, loss = 0.37300618\n",
      "Iteration 13, loss = 0.36302368\n",
      "Iteration 14, loss = 0.35446422\n",
      "Iteration 15, loss = 0.34662419\n",
      "Iteration 16, loss = 0.33978700\n",
      "Iteration 17, loss = 0.33351849\n",
      "Iteration 18, loss = 0.32768926\n",
      "Iteration 19, loss = 0.32235080\n",
      "Iteration 20, loss = 0.31735822\n",
      "Iteration 21, loss = 0.31272101\n",
      "Iteration 22, loss = 0.30834664\n",
      "Iteration 23, loss = 0.30418397\n",
      "Iteration 24, loss = 0.30017196\n",
      "Iteration 25, loss = 0.29645547\n",
      "Iteration 26, loss = 0.29278675\n",
      "Iteration 27, loss = 0.28950919\n",
      "Iteration 28, loss = 0.28610123\n",
      "Iteration 29, loss = 0.28284136\n",
      "Iteration 30, loss = 0.27973701\n",
      "Iteration 31, loss = 0.27672976\n",
      "Iteration 32, loss = 0.27385639\n",
      "Iteration 33, loss = 0.27096549\n",
      "Iteration 34, loss = 0.26819962\n",
      "Iteration 35, loss = 0.26557501\n",
      "Iteration 36, loss = 0.26300307\n",
      "Iteration 37, loss = 0.26042225\n",
      "Iteration 38, loss = 0.25787845\n",
      "Iteration 39, loss = 0.25550772\n",
      "Iteration 40, loss = 0.25311609\n",
      "Iteration 41, loss = 0.25075053\n",
      "Iteration 42, loss = 0.24849471\n",
      "Iteration 43, loss = 0.24633212\n",
      "Iteration 44, loss = 0.24412965\n",
      "Iteration 45, loss = 0.24203708\n",
      "Iteration 46, loss = 0.23990649\n",
      "Iteration 47, loss = 0.23790949\n",
      "Iteration 48, loss = 0.23590653\n",
      "Iteration 49, loss = 0.23396688\n",
      "Iteration 50, loss = 0.23193933\n",
      "Iteration 51, loss = 0.23017396\n",
      "Iteration 52, loss = 0.22827424\n",
      "Iteration 53, loss = 0.22656559\n",
      "Iteration 54, loss = 0.22461094\n",
      "Iteration 55, loss = 0.22296915\n",
      "Iteration 56, loss = 0.22115394\n",
      "Iteration 57, loss = 0.21943304\n",
      "Iteration 58, loss = 0.21791392\n",
      "Iteration 59, loss = 0.21628067\n",
      "Iteration 60, loss = 0.21473796\n",
      "Iteration 61, loss = 0.21314475\n",
      "Iteration 62, loss = 0.21158636\n",
      "Iteration 63, loss = 0.20999517\n",
      "Iteration 64, loss = 0.20854527\n",
      "Iteration 65, loss = 0.20696528\n",
      "Iteration 66, loss = 0.20560295\n",
      "Iteration 67, loss = 0.20413064\n",
      "Iteration 68, loss = 0.20279581\n",
      "Iteration 69, loss = 0.20134674\n",
      "Iteration 70, loss = 0.20005429\n",
      "Iteration 71, loss = 0.19859742\n",
      "Iteration 72, loss = 0.19725920\n",
      "Iteration 73, loss = 0.19600371\n",
      "Iteration 74, loss = 0.19469523\n",
      "Iteration 75, loss = 0.19332498\n",
      "Iteration 76, loss = 0.19216193\n",
      "Iteration 77, loss = 0.19086843\n",
      "Iteration 78, loss = 0.18961400\n",
      "Iteration 79, loss = 0.18843782\n",
      "Iteration 80, loss = 0.18724301\n",
      "Iteration 81, loss = 0.18604645\n",
      "Iteration 82, loss = 0.18483055\n",
      "Iteration 83, loss = 0.18370974\n",
      "Iteration 84, loss = 0.18256376\n",
      "Iteration 85, loss = 0.18138562\n",
      "Iteration 86, loss = 0.18026539\n",
      "Iteration 87, loss = 0.17914227\n",
      "Iteration 88, loss = 0.17816320\n",
      "Iteration 89, loss = 0.17706868\n",
      "Iteration 90, loss = 0.17595277\n",
      "Iteration 91, loss = 0.17483996\n",
      "Iteration 92, loss = 0.17394364\n",
      "Iteration 93, loss = 0.17283467\n",
      "Iteration 94, loss = 0.17187385\n",
      "Iteration 95, loss = 0.17083619\n",
      "Iteration 96, loss = 0.16978210\n",
      "Iteration 97, loss = 0.16885973\n",
      "Iteration 98, loss = 0.16785082\n",
      "Iteration 99, loss = 0.16694607\n",
      "Iteration 100, loss = 0.16591513\n",
      "Iteration 101, loss = 0.16508565\n",
      "Iteration 102, loss = 0.16405954\n",
      "Iteration 103, loss = 0.16317499\n",
      "Iteration 104, loss = 0.16236679\n",
      "Iteration 105, loss = 0.16144033\n",
      "Iteration 106, loss = 0.16052115\n",
      "Iteration 107, loss = 0.15964375\n",
      "Iteration 108, loss = 0.15870881\n",
      "Iteration 109, loss = 0.15791913\n",
      "Iteration 110, loss = 0.15703485\n",
      "Iteration 111, loss = 0.15620008\n",
      "Iteration 112, loss = 0.15537641\n",
      "Iteration 113, loss = 0.15446213\n",
      "Iteration 114, loss = 0.15369672\n",
      "Iteration 115, loss = 0.15290092\n",
      "Iteration 116, loss = 0.15214852\n",
      "Iteration 117, loss = 0.15128343\n",
      "Iteration 118, loss = 0.15052009\n",
      "Iteration 119, loss = 0.14972116\n",
      "Iteration 120, loss = 0.14902494\n",
      "Iteration 121, loss = 0.14820861\n",
      "Iteration 122, loss = 0.14752194\n",
      "Iteration 123, loss = 0.14673143\n",
      "Iteration 124, loss = 0.14598194\n",
      "Iteration 125, loss = 0.14529271\n",
      "Iteration 126, loss = 0.14451552\n",
      "Iteration 127, loss = 0.14382139\n",
      "Iteration 128, loss = 0.14305508\n",
      "Iteration 129, loss = 0.14241243\n",
      "Iteration 130, loss = 0.14166847\n",
      "Iteration 131, loss = 0.14109691\n",
      "Iteration 132, loss = 0.14031811\n",
      "Iteration 133, loss = 0.13964485\n",
      "Iteration 134, loss = 0.13903290\n",
      "Iteration 135, loss = 0.13830850\n",
      "Iteration 136, loss = 0.13770065\n",
      "Iteration 137, loss = 0.13703826\n",
      "Iteration 138, loss = 0.13639277\n",
      "Iteration 139, loss = 0.13576013\n",
      "Iteration 140, loss = 0.13508732\n",
      "Iteration 141, loss = 0.13439131\n",
      "Iteration 142, loss = 0.13384279\n",
      "Iteration 143, loss = 0.13314549\n",
      "Iteration 144, loss = 0.13264671\n",
      "Iteration 145, loss = 0.13201486\n",
      "Iteration 146, loss = 0.13139113\n",
      "Iteration 147, loss = 0.13073788\n",
      "Iteration 148, loss = 0.13022455\n",
      "Iteration 149, loss = 0.12970552\n",
      "Iteration 150, loss = 0.12908052\n",
      "Iteration 151, loss = 0.12848599\n",
      "Iteration 152, loss = 0.12790179\n",
      "Iteration 153, loss = 0.12732831\n",
      "Iteration 154, loss = 0.12673555\n",
      "Iteration 155, loss = 0.12618189\n",
      "Iteration 156, loss = 0.12562593\n",
      "Iteration 157, loss = 0.12502750\n",
      "Iteration 158, loss = 0.12451988\n",
      "Iteration 159, loss = 0.12391714\n",
      "Iteration 160, loss = 0.12346193\n",
      "Iteration 161, loss = 0.12289086\n",
      "Iteration 162, loss = 0.12230867\n",
      "Iteration 163, loss = 0.12182002\n",
      "Iteration 164, loss = 0.12128310\n",
      "Iteration 165, loss = 0.12078622\n",
      "Iteration 166, loss = 0.12029184\n",
      "Iteration 167, loss = 0.11974790\n",
      "Iteration 168, loss = 0.11927806\n",
      "Iteration 169, loss = 0.11875598\n",
      "Iteration 170, loss = 0.11830772\n",
      "Iteration 171, loss = 0.11769127\n",
      "Iteration 172, loss = 0.11720910\n",
      "Iteration 173, loss = 0.11676862\n",
      "Iteration 174, loss = 0.11625622\n",
      "Iteration 175, loss = 0.11573201\n",
      "Iteration 176, loss = 0.11529070\n",
      "Iteration 177, loss = 0.11480357\n",
      "Iteration 178, loss = 0.11439158\n",
      "Iteration 179, loss = 0.11383654\n",
      "Iteration 180, loss = 0.11339294\n",
      "Iteration 181, loss = 0.11297640\n",
      "Iteration 182, loss = 0.11246226\n",
      "Iteration 183, loss = 0.11206612\n",
      "Iteration 184, loss = 0.11152776\n",
      "Iteration 185, loss = 0.11108256\n",
      "Iteration 186, loss = 0.11064006\n",
      "Iteration 187, loss = 0.11012477\n",
      "Iteration 188, loss = 0.10976903\n",
      "Iteration 189, loss = 0.10928645\n",
      "Iteration 190, loss = 0.10885338\n",
      "Iteration 191, loss = 0.10841917\n",
      "Iteration 192, loss = 0.10801363\n",
      "Iteration 193, loss = 0.10753001\n",
      "Iteration 194, loss = 0.10716841\n",
      "Iteration 195, loss = 0.10667199\n",
      "Iteration 196, loss = 0.10631037\n",
      "Iteration 197, loss = 0.10593242\n",
      "Iteration 198, loss = 0.10550674\n",
      "Iteration 199, loss = 0.10504001\n",
      "Iteration 200, loss = 0.10465052\n",
      "[CV] END activation=relu, alpha=0.0001, batch_size=200, hidden_layer_sizes=(100,), solver=sgd; total time= 1.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.80772516\n",
      "Iteration 2, loss = 1.02273424\n",
      "Iteration 3, loss = 0.72182856\n",
      "Iteration 4, loss = 0.59503180\n",
      "Iteration 5, loss = 0.52599638\n",
      "Iteration 6, loss = 0.48132257\n",
      "Iteration 7, loss = 0.45000406\n",
      "Iteration 8, loss = 0.42630029\n",
      "Iteration 9, loss = 0.40747679\n",
      "Iteration 10, loss = 0.39230279\n",
      "Iteration 11, loss = 0.37964222\n",
      "Iteration 12, loss = 0.36867865\n",
      "Iteration 13, loss = 0.35909770\n",
      "Iteration 14, loss = 0.35069936\n",
      "Iteration 15, loss = 0.34322961\n",
      "Iteration 16, loss = 0.33663385\n",
      "Iteration 17, loss = 0.33024123\n",
      "Iteration 18, loss = 0.32462270\n",
      "Iteration 19, loss = 0.31930400\n",
      "Iteration 20, loss = 0.31429104\n",
      "Iteration 21, loss = 0.30982682\n",
      "Iteration 22, loss = 0.30534504\n",
      "Iteration 23, loss = 0.30126124\n",
      "Iteration 24, loss = 0.29750542\n",
      "Iteration 25, loss = 0.29372255\n",
      "Iteration 26, loss = 0.29019598\n",
      "Iteration 27, loss = 0.28688091\n",
      "Iteration 28, loss = 0.28357721\n",
      "Iteration 29, loss = 0.28047634\n",
      "Iteration 30, loss = 0.27732453\n",
      "Iteration 31, loss = 0.27444365\n",
      "Iteration 32, loss = 0.27153238\n",
      "Iteration 33, loss = 0.26862139\n",
      "Iteration 34, loss = 0.26615459\n",
      "Iteration 35, loss = 0.26341960\n",
      "Iteration 36, loss = 0.26090616\n",
      "Iteration 37, loss = 0.25848729\n",
      "Iteration 38, loss = 0.25600790\n",
      "Iteration 39, loss = 0.25359049\n",
      "Iteration 40, loss = 0.25126440\n",
      "Iteration 41, loss = 0.24904977\n",
      "Iteration 42, loss = 0.24681048\n",
      "Iteration 43, loss = 0.24466076\n",
      "Iteration 44, loss = 0.24253332\n",
      "Iteration 45, loss = 0.24043490\n",
      "Iteration 46, loss = 0.23836601\n",
      "Iteration 47, loss = 0.23637657\n",
      "Iteration 48, loss = 0.23432161\n",
      "Iteration 49, loss = 0.23240133\n",
      "Iteration 50, loss = 0.23052368\n",
      "Iteration 51, loss = 0.22857673\n",
      "Iteration 52, loss = 0.22671108\n",
      "Iteration 53, loss = 0.22487292\n",
      "Iteration 54, loss = 0.22302864\n",
      "Iteration 55, loss = 0.22127662\n",
      "Iteration 56, loss = 0.21956034\n",
      "Iteration 57, loss = 0.21784233\n",
      "Iteration 58, loss = 0.21618402\n",
      "Iteration 59, loss = 0.21442311\n",
      "Iteration 60, loss = 0.21291021\n",
      "Iteration 61, loss = 0.21122039\n",
      "Iteration 62, loss = 0.20973061\n",
      "Iteration 63, loss = 0.20815149\n",
      "Iteration 64, loss = 0.20659659\n",
      "Iteration 65, loss = 0.20511624\n",
      "Iteration 66, loss = 0.20355600\n",
      "Iteration 67, loss = 0.20202505\n",
      "Iteration 68, loss = 0.20067222\n",
      "Iteration 69, loss = 0.19929190\n",
      "Iteration 70, loss = 0.19778302\n",
      "Iteration 71, loss = 0.19646054\n",
      "Iteration 72, loss = 0.19504216\n",
      "Iteration 73, loss = 0.19363752\n",
      "Iteration 74, loss = 0.19222807\n",
      "Iteration 75, loss = 0.19099517\n",
      "Iteration 76, loss = 0.18974260\n",
      "Iteration 77, loss = 0.18837754\n",
      "Iteration 78, loss = 0.18715694\n",
      "Iteration 79, loss = 0.18591352\n",
      "Iteration 80, loss = 0.18463669\n",
      "Iteration 81, loss = 0.18347349\n",
      "Iteration 82, loss = 0.18222063\n",
      "Iteration 83, loss = 0.18100677\n",
      "Iteration 84, loss = 0.17984520\n",
      "Iteration 85, loss = 0.17871852\n",
      "Iteration 86, loss = 0.17754183\n",
      "Iteration 87, loss = 0.17645741\n",
      "Iteration 88, loss = 0.17528398\n",
      "Iteration 89, loss = 0.17418009\n",
      "Iteration 90, loss = 0.17305893\n",
      "Iteration 91, loss = 0.17206702\n",
      "Iteration 92, loss = 0.17099607\n",
      "Iteration 93, loss = 0.16996227\n",
      "Iteration 94, loss = 0.16889211\n",
      "Iteration 95, loss = 0.16782380\n",
      "Iteration 96, loss = 0.16689801\n",
      "Iteration 97, loss = 0.16586275\n",
      "Iteration 98, loss = 0.16485897\n",
      "Iteration 99, loss = 0.16383545\n",
      "Iteration 100, loss = 0.16288670\n",
      "Iteration 101, loss = 0.16190877\n",
      "Iteration 102, loss = 0.16101423\n",
      "Iteration 103, loss = 0.16007410\n",
      "Iteration 104, loss = 0.15916817\n",
      "Iteration 105, loss = 0.15824488\n",
      "Iteration 106, loss = 0.15739175\n",
      "Iteration 107, loss = 0.15648289\n",
      "Iteration 108, loss = 0.15556261\n",
      "Iteration 109, loss = 0.15471703\n",
      "Iteration 110, loss = 0.15390430\n",
      "Iteration 111, loss = 0.15303587\n",
      "Iteration 112, loss = 0.15219194\n",
      "Iteration 113, loss = 0.15134893\n",
      "Iteration 114, loss = 0.15049868\n",
      "Iteration 115, loss = 0.14971411\n",
      "Iteration 116, loss = 0.14890564\n",
      "Iteration 117, loss = 0.14814316\n",
      "Iteration 118, loss = 0.14728694\n",
      "Iteration 119, loss = 0.14648081\n",
      "Iteration 120, loss = 0.14573919\n",
      "Iteration 121, loss = 0.14499054\n",
      "Iteration 122, loss = 0.14421055\n",
      "Iteration 123, loss = 0.14344014\n",
      "Iteration 124, loss = 0.14268290\n",
      "Iteration 125, loss = 0.14200203\n",
      "Iteration 126, loss = 0.14122792\n",
      "Iteration 127, loss = 0.14050360\n",
      "Iteration 128, loss = 0.13978833\n",
      "Iteration 129, loss = 0.13898803\n",
      "Iteration 130, loss = 0.13836307\n",
      "Iteration 131, loss = 0.13761295\n",
      "Iteration 132, loss = 0.13694612\n",
      "Iteration 133, loss = 0.13624448\n",
      "Iteration 134, loss = 0.13557184\n",
      "Iteration 135, loss = 0.13495927\n",
      "Iteration 136, loss = 0.13424401\n",
      "Iteration 137, loss = 0.13353114\n",
      "Iteration 138, loss = 0.13297072\n",
      "Iteration 139, loss = 0.13226715\n",
      "Iteration 140, loss = 0.13160992\n",
      "Iteration 141, loss = 0.13100048\n",
      "Iteration 142, loss = 0.13037164\n",
      "Iteration 143, loss = 0.12969473\n",
      "Iteration 144, loss = 0.12906621\n",
      "Iteration 145, loss = 0.12849356\n",
      "Iteration 146, loss = 0.12782407\n",
      "Iteration 147, loss = 0.12726770\n",
      "Iteration 148, loss = 0.12661146\n",
      "Iteration 149, loss = 0.12601772\n",
      "Iteration 150, loss = 0.12537498\n",
      "Iteration 151, loss = 0.12487512\n",
      "Iteration 152, loss = 0.12432686\n",
      "Iteration 153, loss = 0.12368759\n",
      "Iteration 154, loss = 0.12301297\n",
      "Iteration 155, loss = 0.12258209\n",
      "Iteration 156, loss = 0.12196219\n",
      "Iteration 157, loss = 0.12141736\n",
      "Iteration 158, loss = 0.12084919\n",
      "Iteration 159, loss = 0.12025643\n",
      "Iteration 160, loss = 0.11978736\n",
      "Iteration 161, loss = 0.11920723\n",
      "Iteration 162, loss = 0.11865609\n",
      "Iteration 163, loss = 0.11813002\n",
      "Iteration 164, loss = 0.11755406\n",
      "Iteration 165, loss = 0.11706675\n",
      "Iteration 166, loss = 0.11658985\n",
      "Iteration 167, loss = 0.11601653\n",
      "Iteration 168, loss = 0.11552651\n",
      "Iteration 169, loss = 0.11494983\n",
      "Iteration 170, loss = 0.11450674\n",
      "Iteration 171, loss = 0.11393472\n",
      "Iteration 172, loss = 0.11353453\n",
      "Iteration 173, loss = 0.11299650\n",
      "Iteration 174, loss = 0.11251295\n",
      "Iteration 175, loss = 0.11202012\n",
      "Iteration 176, loss = 0.11152301\n",
      "Iteration 177, loss = 0.11103394\n",
      "Iteration 178, loss = 0.11052308\n",
      "Iteration 179, loss = 0.11004597\n",
      "Iteration 180, loss = 0.10962472\n",
      "Iteration 181, loss = 0.10912777\n",
      "Iteration 182, loss = 0.10866514\n",
      "Iteration 183, loss = 0.10817931\n",
      "Iteration 184, loss = 0.10774210\n",
      "Iteration 185, loss = 0.10728276\n",
      "Iteration 186, loss = 0.10679500\n",
      "Iteration 187, loss = 0.10635426\n",
      "Iteration 188, loss = 0.10592996\n",
      "Iteration 189, loss = 0.10546362\n",
      "Iteration 190, loss = 0.10503232\n",
      "Iteration 191, loss = 0.10456911\n",
      "Iteration 192, loss = 0.10408417\n",
      "Iteration 193, loss = 0.10370078\n",
      "Iteration 194, loss = 0.10327874\n",
      "Iteration 195, loss = 0.10282350\n",
      "Iteration 196, loss = 0.10245435\n",
      "Iteration 197, loss = 0.10194441\n",
      "Iteration 198, loss = 0.10156636\n",
      "Iteration 199, loss = 0.10110163\n",
      "Iteration 200, loss = 0.10070882\n",
      "[CV] END activation=relu, alpha=0.0001, batch_size=200, hidden_layer_sizes=(100,), solver=sgd; total time= 1.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.37380352\n",
      "Iteration 2, loss = 0.14050682\n",
      "Iteration 3, loss = 0.09300611\n",
      "Iteration 4, loss = 0.06555105\n",
      "Iteration 5, loss = 0.04773870\n",
      "Iteration 6, loss = 0.03469287\n",
      "Iteration 7, loss = 0.02529401\n",
      "Iteration 8, loss = 0.01818876\n",
      "Iteration 9, loss = 0.01178519\n",
      "Iteration 10, loss = 0.00905522\n",
      "Iteration 11, loss = 0.00654622\n",
      "Iteration 12, loss = 0.00440593\n",
      "Iteration 13, loss = 0.00756263\n",
      "Iteration 14, loss = 0.00413817\n",
      "Iteration 15, loss = 0.00677429\n",
      "Iteration 16, loss = 0.01673072\n",
      "Iteration 17, loss = 0.01389652\n",
      "Iteration 18, loss = 0.00410381\n",
      "Iteration 19, loss = 0.00199120\n",
      "Iteration 20, loss = 0.00092283\n",
      "Iteration 21, loss = 0.00066592\n",
      "Iteration 22, loss = 0.00060837\n",
      "Iteration 23, loss = 0.00057233\n",
      "Iteration 24, loss = 0.00054858\n",
      "Iteration 25, loss = 0.00052901\n",
      "Iteration 26, loss = 0.00051445\n",
      "Iteration 27, loss = 0.00049983\n",
      "Iteration 28, loss = 0.00048651\n",
      "Iteration 29, loss = 0.00047775\n",
      "Iteration 30, loss = 0.00046521\n",
      "Iteration 31, loss = 0.00045651\n",
      "Iteration 32, loss = 0.00044798\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.0001, batch_size=200, hidden_layer_sizes=(400, 100), solver=adam; total time= 1.1min\n",
      "Iteration 1, loss = 0.37862689\n",
      "Iteration 2, loss = 0.14431197\n",
      "Iteration 3, loss = 0.09393631\n",
      "Iteration 4, loss = 0.06802097\n",
      "Iteration 5, loss = 0.04515306\n",
      "Iteration 6, loss = 0.03387320\n",
      "Iteration 7, loss = 0.02527370\n",
      "Iteration 8, loss = 0.01959472\n",
      "Iteration 9, loss = 0.01260930\n",
      "Iteration 10, loss = 0.01019205\n",
      "Iteration 11, loss = 0.00788177\n",
      "Iteration 12, loss = 0.00689765\n",
      "Iteration 13, loss = 0.01383061\n",
      "Iteration 14, loss = 0.00808965\n",
      "Iteration 15, loss = 0.00623965\n",
      "Iteration 16, loss = 0.00807609\n",
      "Iteration 17, loss = 0.00302075\n",
      "Iteration 18, loss = 0.00189200\n",
      "Iteration 19, loss = 0.00097544\n",
      "Iteration 20, loss = 0.00069231\n",
      "Iteration 21, loss = 0.00062708\n",
      "Iteration 22, loss = 0.00058578\n",
      "Iteration 23, loss = 0.00056008\n",
      "Iteration 24, loss = 0.00053780\n",
      "Iteration 25, loss = 0.00052149\n",
      "Iteration 26, loss = 0.00050587\n",
      "Iteration 27, loss = 0.00049223\n",
      "Iteration 28, loss = 0.00047914\n",
      "Iteration 29, loss = 0.00046746\n",
      "Iteration 30, loss = 0.00045546\n",
      "Iteration 31, loss = 0.00044815\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.0001, batch_size=200, hidden_layer_sizes=(400, 100), solver=adam; total time= 1.1min\n",
      "Iteration 1, loss = 0.36099286\n",
      "Iteration 2, loss = 0.13677133\n",
      "Iteration 3, loss = 0.09000746\n",
      "Iteration 4, loss = 0.06278728\n",
      "Iteration 5, loss = 0.04613019\n",
      "Iteration 6, loss = 0.03175636\n",
      "Iteration 7, loss = 0.02465972\n",
      "Iteration 8, loss = 0.01489261\n",
      "Iteration 9, loss = 0.01351501\n",
      "Iteration 10, loss = 0.01209618\n",
      "Iteration 11, loss = 0.00753707\n",
      "Iteration 12, loss = 0.00821375\n",
      "Iteration 13, loss = 0.00418169\n",
      "Iteration 14, loss = 0.00184178\n",
      "Iteration 15, loss = 0.00175699\n",
      "Iteration 16, loss = 0.00114837\n",
      "Iteration 17, loss = 0.00089685\n",
      "Iteration 18, loss = 0.00079682\n",
      "Iteration 19, loss = 0.00072225\n",
      "Iteration 20, loss = 0.00067517\n",
      "Iteration 21, loss = 0.00063718\n",
      "Iteration 22, loss = 0.00059299\n",
      "Iteration 23, loss = 0.00056833\n",
      "Iteration 24, loss = 0.00053769\n",
      "Iteration 25, loss = 0.00051647\n",
      "Iteration 26, loss = 0.00049269\n",
      "Iteration 27, loss = 0.00047342\n",
      "Iteration 28, loss = 0.00045877\n",
      "Iteration 29, loss = 0.00044979\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.0001, batch_size=200, hidden_layer_sizes=(400, 100), solver=adam; total time=  59.4s\n",
      "Iteration 1, loss = 0.38072658\n",
      "Iteration 2, loss = 0.14331617\n",
      "Iteration 3, loss = 0.09445870\n",
      "Iteration 4, loss = 0.06608751\n",
      "Iteration 5, loss = 0.04940757\n",
      "Iteration 6, loss = 0.03503243\n",
      "Iteration 7, loss = 0.02487719\n",
      "Iteration 8, loss = 0.01695375\n",
      "Iteration 9, loss = 0.01378053\n",
      "Iteration 10, loss = 0.01081980\n",
      "Iteration 11, loss = 0.00554261\n",
      "Iteration 12, loss = 0.00384055\n",
      "Iteration 13, loss = 0.00252510\n",
      "Iteration 14, loss = 0.00215370\n",
      "Iteration 15, loss = 0.00145896\n",
      "Iteration 16, loss = 0.00118826\n",
      "Iteration 17, loss = 0.00092817\n",
      "Iteration 18, loss = 0.00083983\n",
      "Iteration 19, loss = 0.00076382\n",
      "Iteration 20, loss = 0.00070691\n",
      "Iteration 21, loss = 0.00065787\n",
      "Iteration 22, loss = 0.00061545\n",
      "Iteration 23, loss = 0.00058405\n",
      "Iteration 24, loss = 0.00054676\n",
      "Iteration 25, loss = 0.00052261\n",
      "Iteration 26, loss = 0.00050084\n",
      "Iteration 27, loss = 0.00048169\n",
      "Iteration 28, loss = 0.00046173\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.0001, batch_size=200, hidden_layer_sizes=(400, 100), solver=adam; total time=  55.2s\n",
      "Iteration 1, loss = 0.38266526\n",
      "Iteration 2, loss = 0.14592136\n",
      "Iteration 3, loss = 0.09260471\n",
      "Iteration 4, loss = 0.06327546\n",
      "Iteration 5, loss = 0.04566479\n",
      "Iteration 6, loss = 0.03242760\n",
      "Iteration 7, loss = 0.02406454\n",
      "Iteration 8, loss = 0.01710183\n",
      "Iteration 9, loss = 0.01463198\n",
      "Iteration 10, loss = 0.01040164\n",
      "Iteration 11, loss = 0.00613569\n",
      "Iteration 12, loss = 0.00429618\n",
      "Iteration 13, loss = 0.00292911\n",
      "Iteration 14, loss = 0.00243639\n",
      "Iteration 15, loss = 0.00211412\n",
      "Iteration 16, loss = 0.00127114\n",
      "Iteration 17, loss = 0.00091512\n",
      "Iteration 18, loss = 0.00077904\n",
      "Iteration 19, loss = 0.00071181\n",
      "Iteration 20, loss = 0.00065948\n",
      "Iteration 21, loss = 0.00062387\n",
      "Iteration 22, loss = 0.00058900\n",
      "Iteration 23, loss = 0.00056024\n",
      "Iteration 24, loss = 0.00052854\n",
      "Iteration 25, loss = 0.00050740\n",
      "Iteration 26, loss = 0.00048388\n",
      "Iteration 27, loss = 0.00046568\n",
      "Iteration 28, loss = 0.00045284\n",
      "Iteration 29, loss = 0.00043676\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.0001, batch_size=200, hidden_layer_sizes=(400, 100), solver=adam; total time=  56.8s\n",
      "Iteration 1, loss = 1.78903792\n",
      "Iteration 2, loss = 0.91519690\n",
      "Iteration 3, loss = 0.61085230\n",
      "Iteration 4, loss = 0.49355929\n",
      "Iteration 5, loss = 0.43118886\n",
      "Iteration 6, loss = 0.39213692\n",
      "Iteration 7, loss = 0.36487510\n",
      "Iteration 8, loss = 0.34459579\n",
      "Iteration 9, loss = 0.32832858\n",
      "Iteration 10, loss = 0.31461791\n",
      "Iteration 11, loss = 0.30329436\n",
      "Iteration 12, loss = 0.29287899\n",
      "Iteration 13, loss = 0.28390972\n",
      "Iteration 14, loss = 0.27590665\n",
      "Iteration 15, loss = 0.26843675\n",
      "Iteration 16, loss = 0.26151426\n",
      "Iteration 17, loss = 0.25507868\n",
      "Iteration 18, loss = 0.24876540\n",
      "Iteration 19, loss = 0.24280618\n",
      "Iteration 20, loss = 0.23747346\n",
      "Iteration 21, loss = 0.23241444\n",
      "Iteration 22, loss = 0.22731371\n",
      "Iteration 23, loss = 0.22263867\n",
      "Iteration 24, loss = 0.21804491\n",
      "Iteration 25, loss = 0.21374988\n",
      "Iteration 26, loss = 0.20947245\n",
      "Iteration 27, loss = 0.20537579\n",
      "Iteration 28, loss = 0.20125795\n",
      "Iteration 29, loss = 0.19751973\n",
      "Iteration 30, loss = 0.19379662\n",
      "Iteration 31, loss = 0.19041625\n",
      "Iteration 32, loss = 0.18705891\n",
      "Iteration 33, loss = 0.18365475\n",
      "Iteration 34, loss = 0.18051023\n",
      "Iteration 35, loss = 0.17734957\n",
      "Iteration 36, loss = 0.17448166\n",
      "Iteration 37, loss = 0.17152102\n",
      "Iteration 38, loss = 0.16873965\n",
      "Iteration 39, loss = 0.16577549\n",
      "Iteration 40, loss = 0.16322744\n",
      "Iteration 41, loss = 0.16064219\n",
      "Iteration 42, loss = 0.15808663\n",
      "Iteration 43, loss = 0.15562499\n",
      "Iteration 44, loss = 0.15319726\n",
      "Iteration 45, loss = 0.15105712\n",
      "Iteration 46, loss = 0.14866919\n",
      "Iteration 47, loss = 0.14662187\n",
      "Iteration 48, loss = 0.14432360\n",
      "Iteration 49, loss = 0.14234813\n",
      "Iteration 50, loss = 0.14017728\n",
      "Iteration 51, loss = 0.13824418\n",
      "Iteration 52, loss = 0.13641698\n",
      "Iteration 53, loss = 0.13453220\n",
      "Iteration 54, loss = 0.13269527\n",
      "Iteration 55, loss = 0.13087785\n",
      "Iteration 56, loss = 0.12901384\n",
      "Iteration 57, loss = 0.12755050\n",
      "Iteration 58, loss = 0.12570460\n",
      "Iteration 59, loss = 0.12412509\n",
      "Iteration 60, loss = 0.12248739\n",
      "Iteration 61, loss = 0.12081465\n",
      "Iteration 62, loss = 0.11929495\n",
      "Iteration 63, loss = 0.11779793\n",
      "Iteration 64, loss = 0.11632665\n",
      "Iteration 65, loss = 0.11480567\n",
      "Iteration 66, loss = 0.11338253\n",
      "Iteration 67, loss = 0.11215866\n",
      "Iteration 68, loss = 0.11070972\n",
      "Iteration 69, loss = 0.10936311\n",
      "Iteration 70, loss = 0.10809264\n",
      "Iteration 71, loss = 0.10663341\n",
      "Iteration 72, loss = 0.10543364\n",
      "Iteration 73, loss = 0.10424913\n",
      "Iteration 74, loss = 0.10295611\n",
      "Iteration 75, loss = 0.10176324\n",
      "Iteration 76, loss = 0.10066873\n",
      "Iteration 77, loss = 0.09945112\n",
      "Iteration 78, loss = 0.09823442\n",
      "Iteration 79, loss = 0.09730258\n",
      "Iteration 80, loss = 0.09611420\n",
      "Iteration 81, loss = 0.09516266\n",
      "Iteration 82, loss = 0.09397057\n",
      "Iteration 83, loss = 0.09291699\n",
      "Iteration 84, loss = 0.09193347\n",
      "Iteration 85, loss = 0.09084301\n",
      "Iteration 86, loss = 0.08984897\n",
      "Iteration 87, loss = 0.08884878\n",
      "Iteration 88, loss = 0.08789321\n",
      "Iteration 89, loss = 0.08679863\n",
      "Iteration 90, loss = 0.08613050\n",
      "Iteration 91, loss = 0.08507793\n",
      "Iteration 92, loss = 0.08419611\n",
      "Iteration 93, loss = 0.08314623\n",
      "Iteration 94, loss = 0.08223970\n",
      "Iteration 95, loss = 0.08149879\n",
      "Iteration 96, loss = 0.08058793\n",
      "Iteration 97, loss = 0.07966214\n",
      "Iteration 98, loss = 0.07883638\n",
      "Iteration 99, loss = 0.07794631\n",
      "Iteration 100, loss = 0.07734383\n",
      "Iteration 101, loss = 0.07627376\n",
      "Iteration 102, loss = 0.07565204\n",
      "Iteration 103, loss = 0.07484142\n",
      "Iteration 104, loss = 0.07407252\n",
      "Iteration 105, loss = 0.07327278\n",
      "Iteration 106, loss = 0.07249036\n",
      "Iteration 107, loss = 0.07173588\n",
      "Iteration 108, loss = 0.07102558\n",
      "Iteration 109, loss = 0.07027046\n",
      "Iteration 110, loss = 0.06956034\n",
      "Iteration 111, loss = 0.06886785\n",
      "Iteration 112, loss = 0.06819322\n",
      "Iteration 113, loss = 0.06758282\n",
      "Iteration 114, loss = 0.06683436\n",
      "Iteration 115, loss = 0.06607759\n",
      "Iteration 116, loss = 0.06540436\n",
      "Iteration 117, loss = 0.06484623\n",
      "Iteration 118, loss = 0.06421983\n",
      "Iteration 119, loss = 0.06357445\n",
      "Iteration 120, loss = 0.06287127\n",
      "Iteration 121, loss = 0.06223805\n",
      "Iteration 122, loss = 0.06159040\n",
      "Iteration 123, loss = 0.06095602\n",
      "Iteration 124, loss = 0.06039440\n",
      "Iteration 125, loss = 0.05987350\n",
      "Iteration 126, loss = 0.05912795\n",
      "Iteration 127, loss = 0.05861957\n",
      "Iteration 128, loss = 0.05805608\n",
      "Iteration 129, loss = 0.05749097\n",
      "Iteration 130, loss = 0.05698652\n",
      "Iteration 131, loss = 0.05631896\n",
      "Iteration 132, loss = 0.05582039\n",
      "Iteration 133, loss = 0.05520204\n",
      "Iteration 134, loss = 0.05469242\n",
      "Iteration 135, loss = 0.05426500\n",
      "Iteration 136, loss = 0.05374399\n",
      "Iteration 137, loss = 0.05317467\n",
      "Iteration 138, loss = 0.05263514\n",
      "Iteration 139, loss = 0.05222121\n",
      "Iteration 140, loss = 0.05163280\n",
      "Iteration 141, loss = 0.05118664\n",
      "Iteration 142, loss = 0.05056307\n",
      "Iteration 143, loss = 0.05020908\n",
      "Iteration 144, loss = 0.04967095\n",
      "Iteration 145, loss = 0.04929633\n",
      "Iteration 146, loss = 0.04875892\n",
      "Iteration 147, loss = 0.04826807\n",
      "Iteration 148, loss = 0.04779707\n",
      "Iteration 149, loss = 0.04743207\n",
      "Iteration 150, loss = 0.04694943\n",
      "Iteration 151, loss = 0.04646708\n",
      "Iteration 152, loss = 0.04599851\n",
      "Iteration 153, loss = 0.04568249\n",
      "Iteration 154, loss = 0.04526625\n",
      "Iteration 155, loss = 0.04475144\n",
      "Iteration 156, loss = 0.04439627\n",
      "Iteration 157, loss = 0.04393595\n",
      "Iteration 158, loss = 0.04353179\n",
      "Iteration 159, loss = 0.04314323\n",
      "Iteration 160, loss = 0.04278967\n",
      "Iteration 161, loss = 0.04230998\n",
      "Iteration 162, loss = 0.04191518\n",
      "Iteration 163, loss = 0.04157072\n",
      "Iteration 164, loss = 0.04117247\n",
      "Iteration 165, loss = 0.04080383\n",
      "Iteration 166, loss = 0.04035939\n",
      "Iteration 167, loss = 0.04011192\n",
      "Iteration 168, loss = 0.03973147\n",
      "Iteration 169, loss = 0.03935011\n",
      "Iteration 170, loss = 0.03892676\n",
      "Iteration 171, loss = 0.03864150\n",
      "Iteration 172, loss = 0.03826503\n",
      "Iteration 173, loss = 0.03796447\n",
      "Iteration 174, loss = 0.03759173\n",
      "Iteration 175, loss = 0.03724945\n",
      "Iteration 176, loss = 0.03697034\n",
      "Iteration 177, loss = 0.03652719\n",
      "Iteration 178, loss = 0.03626147\n",
      "Iteration 179, loss = 0.03589532\n",
      "Iteration 180, loss = 0.03560219\n",
      "Iteration 181, loss = 0.03522837\n",
      "Iteration 182, loss = 0.03496000\n",
      "Iteration 183, loss = 0.03467943\n",
      "Iteration 184, loss = 0.03430705\n",
      "Iteration 185, loss = 0.03408717\n",
      "Iteration 186, loss = 0.03373145\n",
      "Iteration 187, loss = 0.03341482\n",
      "Iteration 188, loss = 0.03318034\n",
      "Iteration 189, loss = 0.03279300\n",
      "Iteration 190, loss = 0.03259859\n",
      "Iteration 191, loss = 0.03227749\n",
      "Iteration 192, loss = 0.03199165\n",
      "Iteration 193, loss = 0.03163570\n",
      "Iteration 194, loss = 0.03146833\n",
      "Iteration 195, loss = 0.03114706\n",
      "Iteration 196, loss = 0.03083995\n",
      "Iteration 197, loss = 0.03054796\n",
      "Iteration 198, loss = 0.03038916\n",
      "Iteration 199, loss = 0.03003839\n",
      "Iteration 200, loss = 0.02982942\n",
      "[CV] END activation=relu, alpha=0.0001, batch_size=200, hidden_layer_sizes=(400, 100), solver=sgd; total time= 4.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.70110483\n",
      "Iteration 2, loss = 0.85103761\n",
      "Iteration 3, loss = 0.57745095\n",
      "Iteration 4, loss = 0.47226424\n",
      "Iteration 5, loss = 0.41739552\n",
      "Iteration 6, loss = 0.38267219\n",
      "Iteration 7, loss = 0.35800601\n",
      "Iteration 8, loss = 0.33939700\n",
      "Iteration 9, loss = 0.32444899\n",
      "Iteration 10, loss = 0.31199983\n",
      "Iteration 11, loss = 0.30117448\n",
      "Iteration 12, loss = 0.29167142\n",
      "Iteration 13, loss = 0.28335457\n",
      "Iteration 14, loss = 0.27587169\n",
      "Iteration 15, loss = 0.26880120\n",
      "Iteration 16, loss = 0.26247091\n",
      "Iteration 17, loss = 0.25606479\n",
      "Iteration 18, loss = 0.25075233\n",
      "Iteration 19, loss = 0.24533012\n",
      "Iteration 20, loss = 0.24004867\n",
      "Iteration 21, loss = 0.23546033\n",
      "Iteration 22, loss = 0.23067787\n",
      "Iteration 23, loss = 0.22624562\n",
      "Iteration 24, loss = 0.22184394\n",
      "Iteration 25, loss = 0.21788315\n",
      "Iteration 26, loss = 0.21374304\n",
      "Iteration 27, loss = 0.20979642\n",
      "Iteration 28, loss = 0.20636213\n",
      "Iteration 29, loss = 0.20273206\n",
      "Iteration 30, loss = 0.19924564\n",
      "Iteration 31, loss = 0.19586134\n",
      "Iteration 32, loss = 0.19269137\n",
      "Iteration 33, loss = 0.18956368\n",
      "Iteration 34, loss = 0.18633704\n",
      "Iteration 35, loss = 0.18316845\n",
      "Iteration 36, loss = 0.18047981\n",
      "Iteration 37, loss = 0.17765910\n",
      "Iteration 38, loss = 0.17483687\n",
      "Iteration 39, loss = 0.17192498\n",
      "Iteration 40, loss = 0.16935420\n",
      "Iteration 41, loss = 0.16670149\n",
      "Iteration 42, loss = 0.16418939\n",
      "Iteration 43, loss = 0.16165114\n",
      "Iteration 44, loss = 0.15935165\n",
      "Iteration 45, loss = 0.15695619\n",
      "Iteration 46, loss = 0.15481691\n",
      "Iteration 47, loss = 0.15226233\n",
      "Iteration 48, loss = 0.15019956\n",
      "Iteration 49, loss = 0.14805380\n",
      "Iteration 50, loss = 0.14602324\n",
      "Iteration 51, loss = 0.14398092\n",
      "Iteration 52, loss = 0.14180187\n",
      "Iteration 53, loss = 0.13993233\n",
      "Iteration 54, loss = 0.13805628\n",
      "Iteration 55, loss = 0.13616791\n",
      "Iteration 56, loss = 0.13411984\n",
      "Iteration 57, loss = 0.13243064\n",
      "Iteration 58, loss = 0.13055265\n",
      "Iteration 59, loss = 0.12891894\n",
      "Iteration 60, loss = 0.12708023\n",
      "Iteration 61, loss = 0.12550882\n",
      "Iteration 62, loss = 0.12370625\n",
      "Iteration 63, loss = 0.12224309\n",
      "Iteration 64, loss = 0.12053964\n",
      "Iteration 65, loss = 0.11908952\n",
      "Iteration 66, loss = 0.11744380\n",
      "Iteration 67, loss = 0.11574132\n",
      "Iteration 68, loss = 0.11460477\n",
      "Iteration 69, loss = 0.11304352\n",
      "Iteration 70, loss = 0.11153679\n",
      "Iteration 71, loss = 0.11031396\n",
      "Iteration 72, loss = 0.10874712\n",
      "Iteration 73, loss = 0.10738533\n",
      "Iteration 74, loss = 0.10608926\n",
      "Iteration 75, loss = 0.10474516\n",
      "Iteration 76, loss = 0.10353734\n",
      "Iteration 77, loss = 0.10206544\n",
      "Iteration 78, loss = 0.10102101\n",
      "Iteration 79, loss = 0.09956228\n",
      "Iteration 80, loss = 0.09856312\n",
      "Iteration 81, loss = 0.09723450\n",
      "Iteration 82, loss = 0.09603245\n",
      "Iteration 83, loss = 0.09500102\n",
      "Iteration 84, loss = 0.09385876\n",
      "Iteration 85, loss = 0.09280754\n",
      "Iteration 86, loss = 0.09143412\n",
      "Iteration 87, loss = 0.09041994\n",
      "Iteration 88, loss = 0.08928683\n",
      "Iteration 89, loss = 0.08840379\n",
      "Iteration 90, loss = 0.08713863\n",
      "Iteration 91, loss = 0.08629991\n",
      "Iteration 92, loss = 0.08529381\n",
      "Iteration 93, loss = 0.08430831\n",
      "Iteration 94, loss = 0.08332428\n",
      "Iteration 95, loss = 0.08235324\n",
      "Iteration 96, loss = 0.08122462\n",
      "Iteration 97, loss = 0.08047770\n",
      "Iteration 98, loss = 0.07955084\n",
      "Iteration 99, loss = 0.07859682\n",
      "Iteration 100, loss = 0.07775832\n",
      "Iteration 101, loss = 0.07674998\n",
      "Iteration 102, loss = 0.07596344\n",
      "Iteration 103, loss = 0.07514618\n",
      "Iteration 104, loss = 0.07434137\n",
      "Iteration 105, loss = 0.07341507\n",
      "Iteration 106, loss = 0.07260900\n",
      "Iteration 107, loss = 0.07178962\n",
      "Iteration 108, loss = 0.07098819\n",
      "Iteration 109, loss = 0.07017426\n",
      "Iteration 110, loss = 0.06942317\n",
      "Iteration 111, loss = 0.06871952\n",
      "Iteration 112, loss = 0.06799171\n",
      "Iteration 113, loss = 0.06716614\n",
      "Iteration 114, loss = 0.06638688\n",
      "Iteration 115, loss = 0.06580471\n",
      "Iteration 116, loss = 0.06510001\n",
      "Iteration 117, loss = 0.06427287\n",
      "Iteration 118, loss = 0.06354492\n",
      "Iteration 119, loss = 0.06295267\n",
      "Iteration 120, loss = 0.06227480\n",
      "Iteration 121, loss = 0.06162478\n",
      "Iteration 122, loss = 0.06094059\n",
      "Iteration 123, loss = 0.06027091\n",
      "Iteration 124, loss = 0.05970697\n",
      "Iteration 125, loss = 0.05905207\n",
      "Iteration 126, loss = 0.05833173\n",
      "Iteration 127, loss = 0.05777368\n",
      "Iteration 128, loss = 0.05724422\n",
      "Iteration 129, loss = 0.05658536\n",
      "Iteration 130, loss = 0.05606512\n",
      "Iteration 131, loss = 0.05543835\n",
      "Iteration 132, loss = 0.05482611\n",
      "Iteration 133, loss = 0.05430219\n",
      "Iteration 134, loss = 0.05373377\n",
      "Iteration 135, loss = 0.05319623\n",
      "Iteration 136, loss = 0.05258796\n",
      "Iteration 137, loss = 0.05210620\n",
      "Iteration 138, loss = 0.05152439\n",
      "Iteration 139, loss = 0.05110138\n",
      "Iteration 140, loss = 0.05053224\n",
      "Iteration 141, loss = 0.05000225\n",
      "Iteration 142, loss = 0.04960286\n",
      "Iteration 143, loss = 0.04897371\n",
      "Iteration 144, loss = 0.04841075\n",
      "Iteration 145, loss = 0.04808132\n",
      "Iteration 146, loss = 0.04757109\n",
      "Iteration 147, loss = 0.04712560\n",
      "Iteration 148, loss = 0.04667364\n",
      "Iteration 149, loss = 0.04619221\n",
      "Iteration 150, loss = 0.04572101\n",
      "Iteration 151, loss = 0.04530379\n",
      "Iteration 152, loss = 0.04473368\n",
      "Iteration 153, loss = 0.04441540\n",
      "Iteration 154, loss = 0.04388109\n",
      "Iteration 155, loss = 0.04349662\n",
      "Iteration 156, loss = 0.04305712\n",
      "Iteration 157, loss = 0.04270177\n",
      "Iteration 158, loss = 0.04223806\n",
      "Iteration 159, loss = 0.04190573\n",
      "Iteration 160, loss = 0.04135479\n",
      "Iteration 161, loss = 0.04098562\n",
      "Iteration 162, loss = 0.04066342\n",
      "Iteration 163, loss = 0.04025967\n",
      "Iteration 164, loss = 0.03982038\n",
      "Iteration 165, loss = 0.03946435\n",
      "Iteration 166, loss = 0.03915421\n",
      "Iteration 167, loss = 0.03871491\n",
      "Iteration 168, loss = 0.03830922\n",
      "Iteration 169, loss = 0.03799292\n",
      "Iteration 170, loss = 0.03763646\n",
      "Iteration 171, loss = 0.03729063\n",
      "Iteration 172, loss = 0.03689889\n",
      "Iteration 173, loss = 0.03654742\n",
      "Iteration 174, loss = 0.03620389\n",
      "Iteration 175, loss = 0.03584970\n",
      "Iteration 176, loss = 0.03556682\n",
      "Iteration 177, loss = 0.03518045\n",
      "Iteration 178, loss = 0.03484011\n",
      "Iteration 179, loss = 0.03450209\n",
      "Iteration 180, loss = 0.03417785\n",
      "Iteration 181, loss = 0.03403258\n",
      "Iteration 182, loss = 0.03356028\n",
      "Iteration 183, loss = 0.03334632\n",
      "Iteration 184, loss = 0.03297758\n",
      "Iteration 185, loss = 0.03270944\n",
      "Iteration 186, loss = 0.03234061\n",
      "Iteration 187, loss = 0.03202274\n",
      "Iteration 188, loss = 0.03180333\n",
      "Iteration 189, loss = 0.03150520\n",
      "Iteration 190, loss = 0.03121048\n",
      "Iteration 191, loss = 0.03094138\n",
      "Iteration 192, loss = 0.03066466\n",
      "Iteration 193, loss = 0.03036650\n",
      "Iteration 194, loss = 0.03009892\n",
      "Iteration 195, loss = 0.02982523\n",
      "Iteration 196, loss = 0.02950099\n",
      "Iteration 197, loss = 0.02926164\n",
      "Iteration 198, loss = 0.02902867\n",
      "Iteration 199, loss = 0.02878337\n",
      "Iteration 200, loss = 0.02846197\n",
      "[CV] END activation=relu, alpha=0.0001, batch_size=200, hidden_layer_sizes=(400, 100), solver=sgd; total time= 4.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.75358075\n",
      "Iteration 2, loss = 0.88753099\n",
      "Iteration 3, loss = 0.59101191\n",
      "Iteration 4, loss = 0.47764674\n",
      "Iteration 5, loss = 0.41900784\n",
      "Iteration 6, loss = 0.38233588\n",
      "Iteration 7, loss = 0.35689630\n",
      "Iteration 8, loss = 0.33793494\n",
      "Iteration 9, loss = 0.32261522\n",
      "Iteration 10, loss = 0.30995636\n",
      "Iteration 11, loss = 0.29941444\n",
      "Iteration 12, loss = 0.28989108\n",
      "Iteration 13, loss = 0.28119448\n",
      "Iteration 14, loss = 0.27372705\n",
      "Iteration 15, loss = 0.26642463\n",
      "Iteration 16, loss = 0.26008359\n",
      "Iteration 17, loss = 0.25393719\n",
      "Iteration 18, loss = 0.24823126\n",
      "Iteration 19, loss = 0.24279523\n",
      "Iteration 20, loss = 0.23752975\n",
      "Iteration 21, loss = 0.23295354\n",
      "Iteration 22, loss = 0.22787654\n",
      "Iteration 23, loss = 0.22351528\n",
      "Iteration 24, loss = 0.21911448\n",
      "Iteration 25, loss = 0.21497518\n",
      "Iteration 26, loss = 0.21102793\n",
      "Iteration 27, loss = 0.20723316\n",
      "Iteration 28, loss = 0.20330434\n",
      "Iteration 29, loss = 0.19973017\n",
      "Iteration 30, loss = 0.19612147\n",
      "Iteration 31, loss = 0.19275539\n",
      "Iteration 32, loss = 0.18976772\n",
      "Iteration 33, loss = 0.18654156\n",
      "Iteration 34, loss = 0.18325625\n",
      "Iteration 35, loss = 0.18029691\n",
      "Iteration 36, loss = 0.17729337\n",
      "Iteration 37, loss = 0.17436102\n",
      "Iteration 38, loss = 0.17173508\n",
      "Iteration 39, loss = 0.16908573\n",
      "Iteration 40, loss = 0.16661022\n",
      "Iteration 41, loss = 0.16388326\n",
      "Iteration 42, loss = 0.16136166\n",
      "Iteration 43, loss = 0.15899269\n",
      "Iteration 44, loss = 0.15634756\n",
      "Iteration 45, loss = 0.15430443\n",
      "Iteration 46, loss = 0.15217989\n",
      "Iteration 47, loss = 0.14999159\n",
      "Iteration 48, loss = 0.14784567\n",
      "Iteration 49, loss = 0.14585767\n",
      "Iteration 50, loss = 0.14377704\n",
      "Iteration 51, loss = 0.14186361\n",
      "Iteration 52, loss = 0.13998428\n",
      "Iteration 53, loss = 0.13818178\n",
      "Iteration 54, loss = 0.13636939\n",
      "Iteration 55, loss = 0.13431912\n",
      "Iteration 56, loss = 0.13262045\n",
      "Iteration 57, loss = 0.13076859\n",
      "Iteration 58, loss = 0.12919810\n",
      "Iteration 59, loss = 0.12743142\n",
      "Iteration 60, loss = 0.12581071\n",
      "Iteration 61, loss = 0.12431335\n",
      "Iteration 62, loss = 0.12256537\n",
      "Iteration 63, loss = 0.12102643\n",
      "Iteration 64, loss = 0.11953552\n",
      "Iteration 65, loss = 0.11807206\n",
      "Iteration 66, loss = 0.11666124\n",
      "Iteration 67, loss = 0.11509211\n",
      "Iteration 68, loss = 0.11366204\n",
      "Iteration 69, loss = 0.11242633\n",
      "Iteration 70, loss = 0.11098698\n",
      "Iteration 71, loss = 0.10959883\n",
      "Iteration 72, loss = 0.10833927\n",
      "Iteration 73, loss = 0.10695013\n",
      "Iteration 74, loss = 0.10573185\n",
      "Iteration 75, loss = 0.10439083\n",
      "Iteration 76, loss = 0.10322494\n",
      "Iteration 77, loss = 0.10196510\n",
      "Iteration 78, loss = 0.10074619\n",
      "Iteration 79, loss = 0.09973826\n",
      "Iteration 80, loss = 0.09859796\n",
      "Iteration 81, loss = 0.09737275\n",
      "Iteration 82, loss = 0.09617836\n",
      "Iteration 83, loss = 0.09501389\n",
      "Iteration 84, loss = 0.09415247\n",
      "Iteration 85, loss = 0.09298022\n",
      "Iteration 86, loss = 0.09165933\n",
      "Iteration 87, loss = 0.09083378\n",
      "Iteration 88, loss = 0.08966896\n",
      "Iteration 89, loss = 0.08875458\n",
      "Iteration 90, loss = 0.08774188\n",
      "Iteration 91, loss = 0.08688936\n",
      "Iteration 92, loss = 0.08572122\n",
      "Iteration 93, loss = 0.08483698\n",
      "Iteration 94, loss = 0.08384044\n",
      "Iteration 95, loss = 0.08294649\n",
      "Iteration 96, loss = 0.08207574\n",
      "Iteration 97, loss = 0.08116163\n",
      "Iteration 98, loss = 0.08031599\n",
      "Iteration 99, loss = 0.07944284\n",
      "Iteration 100, loss = 0.07849899\n",
      "Iteration 101, loss = 0.07766410\n",
      "Iteration 102, loss = 0.07673660\n",
      "Iteration 103, loss = 0.07619002\n",
      "Iteration 104, loss = 0.07513457\n",
      "Iteration 105, loss = 0.07439553\n",
      "Iteration 106, loss = 0.07357723\n",
      "Iteration 107, loss = 0.07288445\n",
      "Iteration 108, loss = 0.07206675\n",
      "Iteration 109, loss = 0.07130259\n",
      "Iteration 110, loss = 0.07045983\n",
      "Iteration 111, loss = 0.06979939\n",
      "Iteration 112, loss = 0.06905913\n",
      "Iteration 113, loss = 0.06828028\n",
      "Iteration 114, loss = 0.06753432\n",
      "Iteration 115, loss = 0.06681167\n",
      "Iteration 116, loss = 0.06613351\n",
      "Iteration 117, loss = 0.06550493\n",
      "Iteration 118, loss = 0.06481758\n",
      "Iteration 119, loss = 0.06413401\n",
      "Iteration 120, loss = 0.06348580\n",
      "Iteration 121, loss = 0.06270513\n",
      "Iteration 122, loss = 0.06226292\n",
      "Iteration 123, loss = 0.06154459\n",
      "Iteration 124, loss = 0.06089720\n",
      "Iteration 125, loss = 0.06022369\n",
      "Iteration 126, loss = 0.05968774\n",
      "Iteration 127, loss = 0.05898845\n",
      "Iteration 128, loss = 0.05849273\n",
      "Iteration 129, loss = 0.05791633\n",
      "Iteration 130, loss = 0.05729532\n",
      "Iteration 131, loss = 0.05669299\n",
      "Iteration 132, loss = 0.05611148\n",
      "Iteration 133, loss = 0.05561508\n",
      "Iteration 134, loss = 0.05507767\n",
      "Iteration 135, loss = 0.05460749\n",
      "Iteration 136, loss = 0.05388790\n",
      "Iteration 137, loss = 0.05351009\n",
      "Iteration 138, loss = 0.05286301\n",
      "Iteration 139, loss = 0.05241447\n",
      "Iteration 140, loss = 0.05179344\n",
      "Iteration 141, loss = 0.05149527\n",
      "Iteration 142, loss = 0.05091020\n",
      "Iteration 143, loss = 0.05034079\n",
      "Iteration 144, loss = 0.04991272\n",
      "Iteration 145, loss = 0.04935847\n",
      "Iteration 146, loss = 0.04887078\n",
      "Iteration 147, loss = 0.04845903\n",
      "Iteration 148, loss = 0.04792359\n",
      "Iteration 149, loss = 0.04746625\n",
      "Iteration 150, loss = 0.04701947\n",
      "Iteration 151, loss = 0.04652250\n",
      "Iteration 152, loss = 0.04608527\n",
      "Iteration 153, loss = 0.04569737\n",
      "Iteration 154, loss = 0.04520000\n",
      "Iteration 155, loss = 0.04487854\n",
      "Iteration 156, loss = 0.04440872\n",
      "Iteration 157, loss = 0.04394843\n",
      "Iteration 158, loss = 0.04352606\n",
      "Iteration 159, loss = 0.04314562\n",
      "Iteration 160, loss = 0.04278311\n",
      "Iteration 161, loss = 0.04222586\n",
      "Iteration 162, loss = 0.04190469\n",
      "Iteration 163, loss = 0.04157223\n",
      "Iteration 164, loss = 0.04118247\n",
      "Iteration 165, loss = 0.04075182\n",
      "Iteration 166, loss = 0.04027047\n",
      "Iteration 167, loss = 0.04002429\n",
      "Iteration 168, loss = 0.03959049\n",
      "Iteration 169, loss = 0.03929716\n",
      "Iteration 170, loss = 0.03886744\n",
      "Iteration 171, loss = 0.03848022\n",
      "Iteration 172, loss = 0.03812107\n",
      "Iteration 173, loss = 0.03781262\n",
      "Iteration 174, loss = 0.03749006\n",
      "Iteration 175, loss = 0.03711739\n",
      "Iteration 176, loss = 0.03671540\n",
      "Iteration 177, loss = 0.03638055\n",
      "Iteration 178, loss = 0.03614677\n",
      "Iteration 179, loss = 0.03578556\n",
      "Iteration 180, loss = 0.03544517\n",
      "Iteration 181, loss = 0.03506336\n",
      "Iteration 182, loss = 0.03477608\n",
      "Iteration 183, loss = 0.03443762\n",
      "Iteration 184, loss = 0.03414753\n",
      "Iteration 185, loss = 0.03381571\n",
      "Iteration 186, loss = 0.03351411\n",
      "Iteration 187, loss = 0.03325861\n",
      "Iteration 188, loss = 0.03292271\n",
      "Iteration 189, loss = 0.03256223\n",
      "Iteration 190, loss = 0.03233192\n",
      "Iteration 191, loss = 0.03204085\n",
      "Iteration 192, loss = 0.03181267\n",
      "Iteration 193, loss = 0.03152673\n",
      "Iteration 194, loss = 0.03120141\n",
      "Iteration 195, loss = 0.03099626\n",
      "Iteration 196, loss = 0.03067499\n",
      "Iteration 197, loss = 0.03037964\n",
      "Iteration 198, loss = 0.03009201\n",
      "Iteration 199, loss = 0.02981373\n",
      "Iteration 200, loss = 0.02947875\n",
      "[CV] END activation=relu, alpha=0.0001, batch_size=200, hidden_layer_sizes=(400, 100), solver=sgd; total time= 4.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.80278354\n",
      "Iteration 2, loss = 0.90222506\n",
      "Iteration 3, loss = 0.59344334\n",
      "Iteration 4, loss = 0.48072381\n",
      "Iteration 5, loss = 0.42233014\n",
      "Iteration 6, loss = 0.38585938\n",
      "Iteration 7, loss = 0.36024958\n",
      "Iteration 8, loss = 0.34081282\n",
      "Iteration 9, loss = 0.32534530\n",
      "Iteration 10, loss = 0.31240409\n",
      "Iteration 11, loss = 0.30127181\n",
      "Iteration 12, loss = 0.29161747\n",
      "Iteration 13, loss = 0.28303790\n",
      "Iteration 14, loss = 0.27503557\n",
      "Iteration 15, loss = 0.26797424\n",
      "Iteration 16, loss = 0.26100681\n",
      "Iteration 17, loss = 0.25510556\n",
      "Iteration 18, loss = 0.24910270\n",
      "Iteration 19, loss = 0.24343709\n",
      "Iteration 20, loss = 0.23840278\n",
      "Iteration 21, loss = 0.23322173\n",
      "Iteration 22, loss = 0.22868612\n",
      "Iteration 23, loss = 0.22397352\n",
      "Iteration 24, loss = 0.21966861\n",
      "Iteration 25, loss = 0.21545085\n",
      "Iteration 26, loss = 0.21133327\n",
      "Iteration 27, loss = 0.20732075\n",
      "Iteration 28, loss = 0.20372280\n",
      "Iteration 29, loss = 0.19995050\n",
      "Iteration 30, loss = 0.19630655\n",
      "Iteration 31, loss = 0.19287277\n",
      "Iteration 32, loss = 0.18955098\n",
      "Iteration 33, loss = 0.18630304\n",
      "Iteration 34, loss = 0.18315462\n",
      "Iteration 35, loss = 0.18008599\n",
      "Iteration 36, loss = 0.17741015\n",
      "Iteration 37, loss = 0.17445826\n",
      "Iteration 38, loss = 0.17159710\n",
      "Iteration 39, loss = 0.16887728\n",
      "Iteration 40, loss = 0.16621045\n",
      "Iteration 41, loss = 0.16344817\n",
      "Iteration 42, loss = 0.16110647\n",
      "Iteration 43, loss = 0.15874505\n",
      "Iteration 44, loss = 0.15631502\n",
      "Iteration 45, loss = 0.15415993\n",
      "Iteration 46, loss = 0.15173703\n",
      "Iteration 47, loss = 0.14953364\n",
      "Iteration 48, loss = 0.14727070\n",
      "Iteration 49, loss = 0.14533578\n",
      "Iteration 50, loss = 0.14330781\n",
      "Iteration 51, loss = 0.14106780\n",
      "Iteration 52, loss = 0.13926111\n",
      "Iteration 53, loss = 0.13732871\n",
      "Iteration 54, loss = 0.13563957\n",
      "Iteration 55, loss = 0.13368346\n",
      "Iteration 56, loss = 0.13191531\n",
      "Iteration 57, loss = 0.13026448\n",
      "Iteration 58, loss = 0.12845659\n",
      "Iteration 59, loss = 0.12686096\n",
      "Iteration 60, loss = 0.12525378\n",
      "Iteration 61, loss = 0.12346613\n",
      "Iteration 62, loss = 0.12201469\n",
      "Iteration 63, loss = 0.12035614\n",
      "Iteration 64, loss = 0.11897906\n",
      "Iteration 65, loss = 0.11746467\n",
      "Iteration 66, loss = 0.11605026\n",
      "Iteration 67, loss = 0.11458907\n",
      "Iteration 68, loss = 0.11328128\n",
      "Iteration 69, loss = 0.11184373\n",
      "Iteration 70, loss = 0.11050837\n",
      "Iteration 71, loss = 0.10916102\n",
      "Iteration 72, loss = 0.10792260\n",
      "Iteration 73, loss = 0.10663747\n",
      "Iteration 74, loss = 0.10540401\n",
      "Iteration 75, loss = 0.10409603\n",
      "Iteration 76, loss = 0.10288921\n",
      "Iteration 77, loss = 0.10174387\n",
      "Iteration 78, loss = 0.10047281\n",
      "Iteration 79, loss = 0.09938416\n",
      "Iteration 80, loss = 0.09830989\n",
      "Iteration 81, loss = 0.09706651\n",
      "Iteration 82, loss = 0.09605131\n",
      "Iteration 83, loss = 0.09508123\n",
      "Iteration 84, loss = 0.09392862\n",
      "Iteration 85, loss = 0.09281661\n",
      "Iteration 86, loss = 0.09177447\n",
      "Iteration 87, loss = 0.09087860\n",
      "Iteration 88, loss = 0.08985390\n",
      "Iteration 89, loss = 0.08876101\n",
      "Iteration 90, loss = 0.08778003\n",
      "Iteration 91, loss = 0.08679199\n",
      "Iteration 92, loss = 0.08587062\n",
      "Iteration 93, loss = 0.08493053\n",
      "Iteration 94, loss = 0.08405898\n",
      "Iteration 95, loss = 0.08315687\n",
      "Iteration 96, loss = 0.08211055\n",
      "Iteration 97, loss = 0.08132655\n",
      "Iteration 98, loss = 0.08042803\n",
      "Iteration 99, loss = 0.07955757\n",
      "Iteration 100, loss = 0.07883296\n",
      "Iteration 101, loss = 0.07798048\n",
      "Iteration 102, loss = 0.07703782\n",
      "Iteration 103, loss = 0.07634335\n",
      "Iteration 104, loss = 0.07548410\n",
      "Iteration 105, loss = 0.07468468\n",
      "Iteration 106, loss = 0.07395721\n",
      "Iteration 107, loss = 0.07322559\n",
      "Iteration 108, loss = 0.07233808\n",
      "Iteration 109, loss = 0.07147889\n",
      "Iteration 110, loss = 0.07090440\n",
      "Iteration 111, loss = 0.07011989\n",
      "Iteration 112, loss = 0.06945064\n",
      "Iteration 113, loss = 0.06869641\n",
      "Iteration 114, loss = 0.06804563\n",
      "Iteration 115, loss = 0.06735298\n",
      "Iteration 116, loss = 0.06656119\n",
      "Iteration 117, loss = 0.06596713\n",
      "Iteration 118, loss = 0.06508036\n",
      "Iteration 119, loss = 0.06469446\n",
      "Iteration 120, loss = 0.06395963\n",
      "Iteration 121, loss = 0.06327219\n",
      "Iteration 122, loss = 0.06275858\n",
      "Iteration 123, loss = 0.06206765\n",
      "Iteration 124, loss = 0.06134170\n",
      "Iteration 125, loss = 0.06087505\n",
      "Iteration 126, loss = 0.06011762\n",
      "Iteration 127, loss = 0.05950227\n",
      "Iteration 128, loss = 0.05894293\n",
      "Iteration 129, loss = 0.05838499\n",
      "Iteration 130, loss = 0.05784062\n",
      "Iteration 131, loss = 0.05720622\n",
      "Iteration 132, loss = 0.05669452\n",
      "Iteration 133, loss = 0.05612469\n",
      "Iteration 134, loss = 0.05559379\n",
      "Iteration 135, loss = 0.05508056\n",
      "Iteration 136, loss = 0.05457046\n",
      "Iteration 137, loss = 0.05398942\n",
      "Iteration 138, loss = 0.05343888\n",
      "Iteration 139, loss = 0.05304476\n",
      "Iteration 140, loss = 0.05242391\n",
      "Iteration 141, loss = 0.05190214\n",
      "Iteration 142, loss = 0.05144056\n",
      "Iteration 143, loss = 0.05094694\n",
      "Iteration 144, loss = 0.05044110\n",
      "Iteration 145, loss = 0.04999291\n",
      "Iteration 146, loss = 0.04944484\n",
      "Iteration 147, loss = 0.04896943\n",
      "Iteration 148, loss = 0.04860020\n",
      "Iteration 149, loss = 0.04804471\n",
      "Iteration 150, loss = 0.04758465\n",
      "Iteration 151, loss = 0.04724261\n",
      "Iteration 152, loss = 0.04680715\n",
      "Iteration 153, loss = 0.04618864\n",
      "Iteration 154, loss = 0.04592849\n",
      "Iteration 155, loss = 0.04535733\n",
      "Iteration 156, loss = 0.04501836\n",
      "Iteration 157, loss = 0.04458523\n",
      "Iteration 158, loss = 0.04415583\n",
      "Iteration 159, loss = 0.04380797\n",
      "Iteration 160, loss = 0.04337514\n",
      "Iteration 161, loss = 0.04297979\n",
      "Iteration 162, loss = 0.04257746\n",
      "Iteration 163, loss = 0.04206596\n",
      "Iteration 164, loss = 0.04183385\n",
      "Iteration 165, loss = 0.04134659\n",
      "Iteration 166, loss = 0.04104323\n",
      "Iteration 167, loss = 0.04061096\n",
      "Iteration 168, loss = 0.04029849\n",
      "Iteration 169, loss = 0.03985397\n",
      "Iteration 170, loss = 0.03944127\n",
      "Iteration 171, loss = 0.03915173\n",
      "Iteration 172, loss = 0.03876763\n",
      "Iteration 173, loss = 0.03841238\n",
      "Iteration 174, loss = 0.03808918\n",
      "Iteration 175, loss = 0.03773537\n",
      "Iteration 176, loss = 0.03729217\n",
      "Iteration 177, loss = 0.03712204\n",
      "Iteration 178, loss = 0.03669297\n",
      "Iteration 179, loss = 0.03641932\n",
      "Iteration 180, loss = 0.03601260\n",
      "Iteration 181, loss = 0.03570261\n",
      "Iteration 182, loss = 0.03537744\n",
      "Iteration 183, loss = 0.03509541\n",
      "Iteration 184, loss = 0.03469164\n",
      "Iteration 185, loss = 0.03447191\n",
      "Iteration 186, loss = 0.03410471\n",
      "Iteration 187, loss = 0.03378838\n",
      "Iteration 188, loss = 0.03357483\n",
      "Iteration 189, loss = 0.03320948\n",
      "Iteration 190, loss = 0.03293897\n",
      "Iteration 191, loss = 0.03262256\n",
      "Iteration 192, loss = 0.03227209\n",
      "Iteration 193, loss = 0.03206142\n",
      "Iteration 194, loss = 0.03174664\n",
      "Iteration 195, loss = 0.03146132\n",
      "Iteration 196, loss = 0.03123584\n",
      "Iteration 197, loss = 0.03091347\n",
      "Iteration 198, loss = 0.03069947\n",
      "Iteration 199, loss = 0.03042807\n",
      "Iteration 200, loss = 0.03014493\n",
      "[CV] END activation=relu, alpha=0.0001, batch_size=200, hidden_layer_sizes=(400, 100), solver=sgd; total time= 4.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.79669964\n",
      "Iteration 2, loss = 0.91354443\n",
      "Iteration 3, loss = 0.60909324\n",
      "Iteration 4, loss = 0.49267654\n",
      "Iteration 5, loss = 0.43173728\n",
      "Iteration 6, loss = 0.39379362\n",
      "Iteration 7, loss = 0.36695881\n",
      "Iteration 8, loss = 0.34695200\n",
      "Iteration 9, loss = 0.33140767\n",
      "Iteration 10, loss = 0.31803132\n",
      "Iteration 11, loss = 0.30670852\n",
      "Iteration 12, loss = 0.29665766\n",
      "Iteration 13, loss = 0.28819312\n",
      "Iteration 14, loss = 0.28023489\n",
      "Iteration 15, loss = 0.27281708\n",
      "Iteration 16, loss = 0.26616169\n",
      "Iteration 17, loss = 0.25999042\n",
      "Iteration 18, loss = 0.25422531\n",
      "Iteration 19, loss = 0.24865166\n",
      "Iteration 20, loss = 0.24319003\n",
      "Iteration 21, loss = 0.23839723\n",
      "Iteration 22, loss = 0.23365211\n",
      "Iteration 23, loss = 0.22911174\n",
      "Iteration 24, loss = 0.22460848\n",
      "Iteration 25, loss = 0.22036821\n",
      "Iteration 26, loss = 0.21630829\n",
      "Iteration 27, loss = 0.21239451\n",
      "Iteration 28, loss = 0.20866263\n",
      "Iteration 29, loss = 0.20492999\n",
      "Iteration 30, loss = 0.20143757\n",
      "Iteration 31, loss = 0.19779778\n",
      "Iteration 32, loss = 0.19478974\n",
      "Iteration 33, loss = 0.19133631\n",
      "Iteration 34, loss = 0.18820658\n",
      "Iteration 35, loss = 0.18524418\n",
      "Iteration 36, loss = 0.18217352\n",
      "Iteration 37, loss = 0.17908858\n",
      "Iteration 38, loss = 0.17611755\n",
      "Iteration 39, loss = 0.17364568\n",
      "Iteration 40, loss = 0.17085174\n",
      "Iteration 41, loss = 0.16842773\n",
      "Iteration 42, loss = 0.16564468\n",
      "Iteration 43, loss = 0.16311869\n",
      "Iteration 44, loss = 0.16072507\n",
      "Iteration 45, loss = 0.15809353\n",
      "Iteration 46, loss = 0.15607922\n",
      "Iteration 47, loss = 0.15370014\n",
      "Iteration 48, loss = 0.15152442\n",
      "Iteration 49, loss = 0.14919435\n",
      "Iteration 50, loss = 0.14713195\n",
      "Iteration 51, loss = 0.14505658\n",
      "Iteration 52, loss = 0.14293011\n",
      "Iteration 53, loss = 0.14108469\n",
      "Iteration 54, loss = 0.13911100\n",
      "Iteration 55, loss = 0.13716040\n",
      "Iteration 56, loss = 0.13526944\n",
      "Iteration 57, loss = 0.13333069\n",
      "Iteration 58, loss = 0.13160708\n",
      "Iteration 59, loss = 0.12983478\n",
      "Iteration 60, loss = 0.12838058\n",
      "Iteration 61, loss = 0.12653668\n",
      "Iteration 62, loss = 0.12478135\n",
      "Iteration 63, loss = 0.12325172\n",
      "Iteration 64, loss = 0.12160435\n",
      "Iteration 65, loss = 0.12001848\n",
      "Iteration 66, loss = 0.11843549\n",
      "Iteration 67, loss = 0.11711792\n",
      "Iteration 68, loss = 0.11572840\n",
      "Iteration 69, loss = 0.11419010\n",
      "Iteration 70, loss = 0.11281051\n",
      "Iteration 71, loss = 0.11138845\n",
      "Iteration 72, loss = 0.11006771\n",
      "Iteration 73, loss = 0.10885836\n",
      "Iteration 74, loss = 0.10747787\n",
      "Iteration 75, loss = 0.10620831\n",
      "Iteration 76, loss = 0.10487755\n",
      "Iteration 77, loss = 0.10369598\n",
      "Iteration 78, loss = 0.10250699\n",
      "Iteration 79, loss = 0.10127729\n",
      "Iteration 80, loss = 0.10023304\n",
      "Iteration 81, loss = 0.09901401\n",
      "Iteration 82, loss = 0.09782569\n",
      "Iteration 83, loss = 0.09667044\n",
      "Iteration 84, loss = 0.09560745\n",
      "Iteration 85, loss = 0.09453546\n",
      "Iteration 86, loss = 0.09335575\n",
      "Iteration 87, loss = 0.09229362\n",
      "Iteration 88, loss = 0.09122641\n",
      "Iteration 89, loss = 0.09032234\n",
      "Iteration 90, loss = 0.08928749\n",
      "Iteration 91, loss = 0.08831111\n",
      "Iteration 92, loss = 0.08746610\n",
      "Iteration 93, loss = 0.08633664\n",
      "Iteration 94, loss = 0.08541654\n",
      "Iteration 95, loss = 0.08452835\n",
      "Iteration 96, loss = 0.08364604\n",
      "Iteration 97, loss = 0.08275431\n",
      "Iteration 98, loss = 0.08192242\n",
      "Iteration 99, loss = 0.08100339\n",
      "Iteration 100, loss = 0.08005253\n",
      "Iteration 101, loss = 0.07933015\n",
      "Iteration 102, loss = 0.07843758\n",
      "Iteration 103, loss = 0.07755062\n",
      "Iteration 104, loss = 0.07676271\n",
      "Iteration 105, loss = 0.07593284\n",
      "Iteration 106, loss = 0.07510564\n",
      "Iteration 107, loss = 0.07433048\n",
      "Iteration 108, loss = 0.07354249\n",
      "Iteration 109, loss = 0.07282846\n",
      "Iteration 110, loss = 0.07198422\n",
      "Iteration 111, loss = 0.07130861\n",
      "Iteration 112, loss = 0.07061175\n",
      "Iteration 113, loss = 0.06988685\n",
      "Iteration 114, loss = 0.06917646\n",
      "Iteration 115, loss = 0.06842874\n",
      "Iteration 116, loss = 0.06770458\n",
      "Iteration 117, loss = 0.06700850\n",
      "Iteration 118, loss = 0.06638119\n",
      "Iteration 119, loss = 0.06572683\n",
      "Iteration 120, loss = 0.06510575\n",
      "Iteration 121, loss = 0.06441519\n",
      "Iteration 122, loss = 0.06379105\n",
      "Iteration 123, loss = 0.06311577\n",
      "Iteration 124, loss = 0.06249550\n",
      "Iteration 125, loss = 0.06190175\n",
      "Iteration 126, loss = 0.06124363\n",
      "Iteration 127, loss = 0.06062564\n",
      "Iteration 128, loss = 0.06011991\n",
      "Iteration 129, loss = 0.05952507\n",
      "Iteration 130, loss = 0.05887476\n",
      "Iteration 131, loss = 0.05827824\n",
      "Iteration 132, loss = 0.05784261\n",
      "Iteration 133, loss = 0.05714733\n",
      "Iteration 134, loss = 0.05663787\n",
      "Iteration 135, loss = 0.05603612\n",
      "Iteration 136, loss = 0.05552991\n",
      "Iteration 137, loss = 0.05506651\n",
      "Iteration 138, loss = 0.05454119\n",
      "Iteration 139, loss = 0.05401771\n",
      "Iteration 140, loss = 0.05347926\n",
      "Iteration 141, loss = 0.05290169\n",
      "Iteration 142, loss = 0.05251053\n",
      "Iteration 143, loss = 0.05184042\n",
      "Iteration 144, loss = 0.05141387\n",
      "Iteration 145, loss = 0.05098672\n",
      "Iteration 146, loss = 0.05046118\n",
      "Iteration 147, loss = 0.04998738\n",
      "Iteration 148, loss = 0.04953925\n",
      "Iteration 149, loss = 0.04904399\n",
      "Iteration 150, loss = 0.04861569\n",
      "Iteration 151, loss = 0.04810364\n",
      "Iteration 152, loss = 0.04766954\n",
      "Iteration 153, loss = 0.04721866\n",
      "Iteration 154, loss = 0.04674980\n",
      "Iteration 155, loss = 0.04632169\n",
      "Iteration 156, loss = 0.04592682\n",
      "Iteration 157, loss = 0.04548883\n",
      "Iteration 158, loss = 0.04510733\n",
      "Iteration 159, loss = 0.04467522\n",
      "Iteration 160, loss = 0.04423125\n",
      "Iteration 161, loss = 0.04389216\n",
      "Iteration 162, loss = 0.04346181\n",
      "Iteration 163, loss = 0.04299180\n",
      "Iteration 164, loss = 0.04266536\n",
      "Iteration 165, loss = 0.04228860\n",
      "Iteration 166, loss = 0.04188907\n",
      "Iteration 167, loss = 0.04145787\n",
      "Iteration 168, loss = 0.04116405\n",
      "Iteration 169, loss = 0.04069398\n",
      "Iteration 170, loss = 0.04034936\n",
      "Iteration 171, loss = 0.04005289\n",
      "Iteration 172, loss = 0.03958102\n",
      "Iteration 173, loss = 0.03924739\n",
      "Iteration 174, loss = 0.03899367\n",
      "Iteration 175, loss = 0.03859795\n",
      "Iteration 176, loss = 0.03820058\n",
      "Iteration 177, loss = 0.03784560\n",
      "Iteration 178, loss = 0.03749424\n",
      "Iteration 179, loss = 0.03722194\n",
      "Iteration 180, loss = 0.03690038\n",
      "Iteration 181, loss = 0.03658507\n",
      "Iteration 182, loss = 0.03623406\n",
      "Iteration 183, loss = 0.03590945\n",
      "Iteration 184, loss = 0.03559579\n",
      "Iteration 185, loss = 0.03522196\n",
      "Iteration 186, loss = 0.03496155\n",
      "Iteration 187, loss = 0.03473280\n",
      "Iteration 188, loss = 0.03437587\n",
      "Iteration 189, loss = 0.03406551\n",
      "Iteration 190, loss = 0.03375420\n",
      "Iteration 191, loss = 0.03351420\n",
      "Iteration 192, loss = 0.03315457\n",
      "Iteration 193, loss = 0.03295533\n",
      "Iteration 194, loss = 0.03262601\n",
      "Iteration 195, loss = 0.03240026\n",
      "Iteration 196, loss = 0.03199597\n",
      "Iteration 197, loss = 0.03175741\n",
      "Iteration 198, loss = 0.03144814\n",
      "Iteration 199, loss = 0.03117252\n",
      "Iteration 200, loss = 0.03094614\n",
      "[CV] END activation=relu, alpha=0.0001, batch_size=200, hidden_layer_sizes=(400, 100), solver=sgd; total time= 4.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.50504919\n",
      "Iteration 2, loss = 0.18952281\n",
      "Iteration 3, loss = 0.13720692\n",
      "Iteration 4, loss = 0.10573503\n",
      "Iteration 5, loss = 0.08649344\n",
      "Iteration 6, loss = 0.06785865\n",
      "Iteration 7, loss = 0.05724649\n",
      "Iteration 8, loss = 0.04714510\n",
      "Iteration 9, loss = 0.04023509\n",
      "Iteration 10, loss = 0.03036566\n",
      "Iteration 11, loss = 0.02267211\n",
      "Iteration 12, loss = 0.02186219\n",
      "Iteration 13, loss = 0.01988382\n",
      "Iteration 14, loss = 0.01673345\n",
      "Iteration 15, loss = 0.01404442\n",
      "Iteration 16, loss = 0.01085220\n",
      "Iteration 17, loss = 0.01120789\n",
      "Iteration 18, loss = 0.01263243\n",
      "Iteration 19, loss = 0.01572018\n",
      "Iteration 20, loss = 0.01234653\n",
      "Iteration 21, loss = 0.00941612\n",
      "Iteration 22, loss = 0.00420075\n",
      "Iteration 23, loss = 0.00306518\n",
      "Iteration 24, loss = 0.00418936\n",
      "Iteration 25, loss = 0.00907889\n",
      "Iteration 26, loss = 0.02272261\n",
      "Iteration 27, loss = 0.00782742\n",
      "Iteration 28, loss = 0.00507666\n",
      "Iteration 29, loss = 0.00892565\n",
      "Iteration 30, loss = 0.00730885\n",
      "Iteration 31, loss = 0.00395178\n",
      "Iteration 32, loss = 0.00251475\n",
      "Iteration 33, loss = 0.00544067\n",
      "Iteration 34, loss = 0.01260740\n",
      "Iteration 35, loss = 0.00631857\n",
      "Iteration 36, loss = 0.00739889\n",
      "Iteration 37, loss = 0.00202275\n",
      "Iteration 38, loss = 0.00058794\n",
      "Iteration 39, loss = 0.00039422\n",
      "Iteration 40, loss = 0.00036434\n",
      "Iteration 41, loss = 0.00034954\n",
      "Iteration 42, loss = 0.00033915\n",
      "Iteration 43, loss = 0.00033084\n",
      "Iteration 44, loss = 0.00032424\n",
      "Iteration 45, loss = 0.00031859\n",
      "Iteration 46, loss = 0.00031356\n",
      "Iteration 47, loss = 0.00030927\n",
      "Iteration 48, loss = 0.00030534\n",
      "Iteration 49, loss = 0.00030166\n",
      "Iteration 50, loss = 0.00029858\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.0001, batch_size=200, hidden_layer_sizes=(100, 100, 100), solver=adam; total time=  43.5s\n",
      "Iteration 1, loss = 0.52226235\n",
      "Iteration 2, loss = 0.18953322\n",
      "Iteration 3, loss = 0.13268919\n",
      "Iteration 4, loss = 0.10372325\n",
      "Iteration 5, loss = 0.08207077\n",
      "Iteration 6, loss = 0.06336474\n",
      "Iteration 7, loss = 0.05241102\n",
      "Iteration 8, loss = 0.04300649\n",
      "Iteration 9, loss = 0.03629792\n",
      "Iteration 10, loss = 0.03014824\n",
      "Iteration 11, loss = 0.02470014\n",
      "Iteration 12, loss = 0.02033166\n",
      "Iteration 13, loss = 0.02215702\n",
      "Iteration 14, loss = 0.01466084\n",
      "Iteration 15, loss = 0.01301571\n",
      "Iteration 16, loss = 0.00896794\n",
      "Iteration 17, loss = 0.00572623\n",
      "Iteration 18, loss = 0.00934444\n",
      "Iteration 19, loss = 0.02224077\n",
      "Iteration 20, loss = 0.01554450\n",
      "Iteration 21, loss = 0.00950482\n",
      "Iteration 22, loss = 0.00533433\n",
      "Iteration 23, loss = 0.00215670\n",
      "Iteration 24, loss = 0.00406534\n",
      "Iteration 25, loss = 0.00654155\n",
      "Iteration 26, loss = 0.01729789\n",
      "Iteration 27, loss = 0.01338410\n",
      "Iteration 28, loss = 0.00830586\n",
      "Iteration 29, loss = 0.00168119\n",
      "Iteration 30, loss = 0.00078167\n",
      "Iteration 31, loss = 0.00047605\n",
      "Iteration 32, loss = 0.00041638\n",
      "Iteration 33, loss = 0.00039023\n",
      "Iteration 34, loss = 0.00037196\n",
      "Iteration 35, loss = 0.00035739\n",
      "Iteration 36, loss = 0.00034483\n",
      "Iteration 37, loss = 0.00033543\n",
      "Iteration 38, loss = 0.00032650\n",
      "Iteration 39, loss = 0.00031779\n",
      "Iteration 40, loss = 0.00031120\n",
      "Iteration 41, loss = 0.00030515\n",
      "Iteration 42, loss = 0.00029950\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.0001, batch_size=200, hidden_layer_sizes=(100, 100, 100), solver=adam; total time=  39.0s\n",
      "Iteration 1, loss = 0.50346640\n",
      "Iteration 2, loss = 0.18340555\n",
      "Iteration 3, loss = 0.13034233\n",
      "Iteration 4, loss = 0.09709118\n",
      "Iteration 5, loss = 0.07807573\n",
      "Iteration 6, loss = 0.05987394\n",
      "Iteration 7, loss = 0.05114801\n",
      "Iteration 8, loss = 0.03932497\n",
      "Iteration 9, loss = 0.03219441\n",
      "Iteration 10, loss = 0.02726087\n",
      "Iteration 11, loss = 0.02034249\n",
      "Iteration 12, loss = 0.01891047\n",
      "Iteration 13, loss = 0.01601382\n",
      "Iteration 14, loss = 0.01306015\n",
      "Iteration 15, loss = 0.01156021\n",
      "Iteration 16, loss = 0.01232923\n",
      "Iteration 17, loss = 0.01241621\n",
      "Iteration 18, loss = 0.01640842\n",
      "Iteration 19, loss = 0.00691528\n",
      "Iteration 20, loss = 0.00478459\n",
      "Iteration 21, loss = 0.00163727\n",
      "Iteration 22, loss = 0.00082571\n",
      "Iteration 23, loss = 0.00060731\n",
      "Iteration 24, loss = 0.00053534\n",
      "Iteration 25, loss = 0.00049613\n",
      "Iteration 26, loss = 0.00047597\n",
      "Iteration 27, loss = 0.00045592\n",
      "Iteration 28, loss = 0.00041221\n",
      "Iteration 29, loss = 0.00038505\n",
      "Iteration 30, loss = 0.00036830\n",
      "Iteration 31, loss = 0.00036142\n",
      "Iteration 32, loss = 0.00034305\n",
      "Iteration 33, loss = 0.00033115\n",
      "Iteration 34, loss = 0.00031578\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.0001, batch_size=200, hidden_layer_sizes=(100, 100, 100), solver=adam; total time=  29.7s\n",
      "Iteration 1, loss = 0.50160086\n",
      "Iteration 2, loss = 0.18941276\n",
      "Iteration 3, loss = 0.13325354\n",
      "Iteration 4, loss = 0.10662284\n",
      "Iteration 5, loss = 0.08200107\n",
      "Iteration 6, loss = 0.06711808\n",
      "Iteration 7, loss = 0.05266145\n",
      "Iteration 8, loss = 0.04204662\n",
      "Iteration 9, loss = 0.03704333\n",
      "Iteration 10, loss = 0.02914178\n",
      "Iteration 11, loss = 0.02484730\n",
      "Iteration 12, loss = 0.02400076\n",
      "Iteration 13, loss = 0.01632200\n",
      "Iteration 14, loss = 0.01462252\n",
      "Iteration 15, loss = 0.01003553\n",
      "Iteration 16, loss = 0.01164916\n",
      "Iteration 17, loss = 0.01237154\n",
      "Iteration 18, loss = 0.01257714\n",
      "Iteration 19, loss = 0.00730542\n",
      "Iteration 20, loss = 0.00520215\n",
      "Iteration 21, loss = 0.00377107\n",
      "Iteration 22, loss = 0.00288214\n",
      "Iteration 23, loss = 0.02590855\n",
      "Iteration 24, loss = 0.00933937\n",
      "Iteration 25, loss = 0.01083611\n",
      "Iteration 26, loss = 0.01425328\n",
      "Iteration 27, loss = 0.00486063\n",
      "Iteration 28, loss = 0.00173594\n",
      "Iteration 29, loss = 0.00085186\n",
      "Iteration 30, loss = 0.00230220\n",
      "Iteration 31, loss = 0.01770722\n",
      "Iteration 32, loss = 0.01455378\n",
      "Iteration 33, loss = 0.00573776\n",
      "Iteration 34, loss = 0.00429612\n",
      "Iteration 35, loss = 0.00138135\n",
      "Iteration 36, loss = 0.00059834\n",
      "Iteration 37, loss = 0.00041803\n",
      "Iteration 38, loss = 0.00036113\n",
      "Iteration 39, loss = 0.00034568\n",
      "Iteration 40, loss = 0.00033395\n",
      "Iteration 41, loss = 0.00032522\n",
      "Iteration 42, loss = 0.00031778\n",
      "Iteration 43, loss = 0.00031148\n",
      "Iteration 44, loss = 0.00030633\n",
      "Iteration 45, loss = 0.00030135\n",
      "Iteration 46, loss = 0.00029731\n",
      "Iteration 47, loss = 0.00029320\n",
      "Iteration 48, loss = 0.00028949\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.0001, batch_size=200, hidden_layer_sizes=(100, 100, 100), solver=adam; total time=  43.5s\n",
      "Iteration 1, loss = 0.52338975\n",
      "Iteration 2, loss = 0.18560717\n",
      "Iteration 3, loss = 0.13118797\n",
      "Iteration 4, loss = 0.09465264\n",
      "Iteration 5, loss = 0.07503611\n",
      "Iteration 6, loss = 0.05979715\n",
      "Iteration 7, loss = 0.04837650\n",
      "Iteration 8, loss = 0.03991096\n",
      "Iteration 9, loss = 0.03288424\n",
      "Iteration 10, loss = 0.02744845\n",
      "Iteration 11, loss = 0.02108767\n",
      "Iteration 12, loss = 0.01986311\n",
      "Iteration 13, loss = 0.01737777\n",
      "Iteration 14, loss = 0.01367393\n",
      "Iteration 15, loss = 0.01205061\n",
      "Iteration 16, loss = 0.01048865\n",
      "Iteration 17, loss = 0.00845245\n",
      "Iteration 18, loss = 0.01184569\n",
      "Iteration 19, loss = 0.00651581\n",
      "Iteration 20, loss = 0.00474006\n",
      "Iteration 21, loss = 0.00530758\n",
      "Iteration 22, loss = 0.00925328\n",
      "Iteration 23, loss = 0.01396399\n",
      "Iteration 24, loss = 0.01499075\n",
      "Iteration 25, loss = 0.01053895\n",
      "Iteration 26, loss = 0.00957618\n",
      "Iteration 27, loss = 0.00394837\n",
      "Iteration 28, loss = 0.00124631\n",
      "Iteration 29, loss = 0.00130589\n",
      "Iteration 30, loss = 0.00061049\n",
      "Iteration 31, loss = 0.00041185\n",
      "Iteration 32, loss = 0.00037549\n",
      "Iteration 33, loss = 0.00035581\n",
      "Iteration 34, loss = 0.00034039\n",
      "Iteration 35, loss = 0.00033032\n",
      "Iteration 36, loss = 0.00031971\n",
      "Iteration 37, loss = 0.00031136\n",
      "Iteration 38, loss = 0.00030454\n",
      "Iteration 39, loss = 0.00029800\n",
      "Iteration 40, loss = 0.00029232\n",
      "Iteration 41, loss = 0.00028723\n",
      "Iteration 42, loss = 0.00028281\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.0001, batch_size=200, hidden_layer_sizes=(100, 100, 100), solver=adam; total time=  36.1s\n",
      "Iteration 1, loss = 2.15115416\n",
      "Iteration 2, loss = 1.43573489\n",
      "Iteration 3, loss = 0.78402943\n",
      "Iteration 4, loss = 0.56641924\n",
      "Iteration 5, loss = 0.47130417\n",
      "Iteration 6, loss = 0.41774127\n",
      "Iteration 7, loss = 0.38250516\n",
      "Iteration 8, loss = 0.35755947\n",
      "Iteration 9, loss = 0.33817112\n",
      "Iteration 10, loss = 0.32265726\n",
      "Iteration 11, loss = 0.30998702\n",
      "Iteration 12, loss = 0.29879265\n",
      "Iteration 13, loss = 0.28857344\n",
      "Iteration 14, loss = 0.27990477\n",
      "Iteration 15, loss = 0.27157832\n",
      "Iteration 16, loss = 0.26439285\n",
      "Iteration 17, loss = 0.25718381\n",
      "Iteration 18, loss = 0.25077080\n",
      "Iteration 19, loss = 0.24475754\n",
      "Iteration 20, loss = 0.23922091\n",
      "Iteration 21, loss = 0.23345067\n",
      "Iteration 22, loss = 0.22807790\n",
      "Iteration 23, loss = 0.22329859\n",
      "Iteration 24, loss = 0.21865477\n",
      "Iteration 25, loss = 0.21411979\n",
      "Iteration 26, loss = 0.20930184\n",
      "Iteration 27, loss = 0.20524167\n",
      "Iteration 28, loss = 0.20113352\n",
      "Iteration 29, loss = 0.19706981\n",
      "Iteration 30, loss = 0.19335341\n",
      "Iteration 31, loss = 0.19009846\n",
      "Iteration 32, loss = 0.18593081\n",
      "Iteration 33, loss = 0.18269464\n",
      "Iteration 34, loss = 0.17941024\n",
      "Iteration 35, loss = 0.17628833\n",
      "Iteration 36, loss = 0.17299757\n",
      "Iteration 37, loss = 0.17009619\n",
      "Iteration 38, loss = 0.16721759\n",
      "Iteration 39, loss = 0.16440960\n",
      "Iteration 40, loss = 0.16172894\n",
      "Iteration 41, loss = 0.15888380\n",
      "Iteration 42, loss = 0.15627370\n",
      "Iteration 43, loss = 0.15333906\n",
      "Iteration 44, loss = 0.15136569\n",
      "Iteration 45, loss = 0.14876520\n",
      "Iteration 46, loss = 0.14660094\n",
      "Iteration 47, loss = 0.14402161\n",
      "Iteration 48, loss = 0.14183775\n",
      "Iteration 49, loss = 0.13947765\n",
      "Iteration 50, loss = 0.13745219\n",
      "Iteration 51, loss = 0.13517650\n",
      "Iteration 52, loss = 0.13315471\n",
      "Iteration 53, loss = 0.13095595\n",
      "Iteration 54, loss = 0.12881095\n",
      "Iteration 55, loss = 0.12704812\n",
      "Iteration 56, loss = 0.12551335\n",
      "Iteration 57, loss = 0.12340772\n",
      "Iteration 58, loss = 0.12134502\n",
      "Iteration 59, loss = 0.11977617\n",
      "Iteration 60, loss = 0.11812426\n",
      "Iteration 61, loss = 0.11630468\n",
      "Iteration 62, loss = 0.11422492\n",
      "Iteration 63, loss = 0.11282069\n",
      "Iteration 64, loss = 0.11085131\n",
      "Iteration 65, loss = 0.10952751\n",
      "Iteration 66, loss = 0.10796111\n",
      "Iteration 67, loss = 0.10614156\n",
      "Iteration 68, loss = 0.10459780\n",
      "Iteration 69, loss = 0.10314511\n",
      "Iteration 70, loss = 0.10168821\n",
      "Iteration 71, loss = 0.10028428\n",
      "Iteration 72, loss = 0.09864716\n",
      "Iteration 73, loss = 0.09748467\n",
      "Iteration 74, loss = 0.09614160\n",
      "Iteration 75, loss = 0.09486911\n",
      "Iteration 76, loss = 0.09351296\n",
      "Iteration 77, loss = 0.09201470\n",
      "Iteration 78, loss = 0.09088432\n",
      "Iteration 79, loss = 0.08942582\n",
      "Iteration 80, loss = 0.08839844\n",
      "Iteration 81, loss = 0.08725082\n",
      "Iteration 82, loss = 0.08576582\n",
      "Iteration 83, loss = 0.08466239\n",
      "Iteration 84, loss = 0.08342430\n",
      "Iteration 85, loss = 0.08240286\n",
      "Iteration 86, loss = 0.08117622\n",
      "Iteration 87, loss = 0.08006126\n",
      "Iteration 88, loss = 0.07886561\n",
      "Iteration 89, loss = 0.07772228\n",
      "Iteration 90, loss = 0.07679440\n",
      "Iteration 91, loss = 0.07598106\n",
      "Iteration 92, loss = 0.07512090\n",
      "Iteration 93, loss = 0.07383235\n",
      "Iteration 94, loss = 0.07268982\n",
      "Iteration 95, loss = 0.07203765\n",
      "Iteration 96, loss = 0.07083060\n",
      "Iteration 97, loss = 0.07008709\n",
      "Iteration 98, loss = 0.06905633\n",
      "Iteration 99, loss = 0.06821849\n",
      "Iteration 100, loss = 0.06748124\n",
      "Iteration 101, loss = 0.06642755\n",
      "Iteration 102, loss = 0.06554840\n",
      "Iteration 103, loss = 0.06458312\n",
      "Iteration 104, loss = 0.06396524\n",
      "Iteration 105, loss = 0.06312921\n",
      "Iteration 106, loss = 0.06215420\n",
      "Iteration 107, loss = 0.06135495\n",
      "Iteration 108, loss = 0.06038666\n",
      "Iteration 109, loss = 0.05992514\n",
      "Iteration 110, loss = 0.05907984\n",
      "Iteration 111, loss = 0.05816717\n",
      "Iteration 112, loss = 0.05740222\n",
      "Iteration 113, loss = 0.05663944\n",
      "Iteration 114, loss = 0.05588889\n",
      "Iteration 115, loss = 0.05536551\n",
      "Iteration 116, loss = 0.05446832\n",
      "Iteration 117, loss = 0.05374792\n",
      "Iteration 118, loss = 0.05308818\n",
      "Iteration 119, loss = 0.05251922\n",
      "Iteration 120, loss = 0.05182892\n",
      "Iteration 121, loss = 0.05119839\n",
      "Iteration 122, loss = 0.05051788\n",
      "Iteration 123, loss = 0.04990504\n",
      "Iteration 124, loss = 0.04924245\n",
      "Iteration 125, loss = 0.04876391\n",
      "Iteration 126, loss = 0.04813099\n",
      "Iteration 127, loss = 0.04752622\n",
      "Iteration 128, loss = 0.04685624\n",
      "Iteration 129, loss = 0.04626799\n",
      "Iteration 130, loss = 0.04564463\n",
      "Iteration 131, loss = 0.04500119\n",
      "Iteration 132, loss = 0.04459370\n",
      "Iteration 133, loss = 0.04394012\n",
      "Iteration 134, loss = 0.04367212\n",
      "Iteration 135, loss = 0.04281978\n",
      "Iteration 136, loss = 0.04235569\n",
      "Iteration 137, loss = 0.04208068\n",
      "Iteration 138, loss = 0.04122840\n",
      "Iteration 139, loss = 0.04094658\n",
      "Iteration 140, loss = 0.04036031\n",
      "Iteration 141, loss = 0.03986962\n",
      "Iteration 142, loss = 0.03939801\n",
      "Iteration 143, loss = 0.03892815\n",
      "Iteration 144, loss = 0.03833186\n",
      "Iteration 145, loss = 0.03793247\n",
      "Iteration 146, loss = 0.03734574\n",
      "Iteration 147, loss = 0.03703190\n",
      "Iteration 148, loss = 0.03627907\n",
      "Iteration 149, loss = 0.03604151\n",
      "Iteration 150, loss = 0.03557253\n",
      "Iteration 151, loss = 0.03504394\n",
      "Iteration 152, loss = 0.03470166\n",
      "Iteration 153, loss = 0.03421102\n",
      "Iteration 154, loss = 0.03377563\n",
      "Iteration 155, loss = 0.03345679\n",
      "Iteration 156, loss = 0.03300098\n",
      "Iteration 157, loss = 0.03273197\n",
      "Iteration 158, loss = 0.03218531\n",
      "Iteration 159, loss = 0.03188952\n",
      "Iteration 160, loss = 0.03143237\n",
      "Iteration 161, loss = 0.03106013\n",
      "Iteration 162, loss = 0.03066032\n",
      "Iteration 163, loss = 0.03026195\n",
      "Iteration 164, loss = 0.02992432\n",
      "Iteration 165, loss = 0.02951846\n",
      "Iteration 166, loss = 0.02923245\n",
      "Iteration 167, loss = 0.02896015\n",
      "Iteration 168, loss = 0.02847867\n",
      "Iteration 169, loss = 0.02819210\n",
      "Iteration 170, loss = 0.02775516\n",
      "Iteration 171, loss = 0.02735342\n",
      "Iteration 172, loss = 0.02725644\n",
      "Iteration 173, loss = 0.02689969\n",
      "Iteration 174, loss = 0.02636318\n",
      "Iteration 175, loss = 0.02616924\n",
      "Iteration 176, loss = 0.02581158\n",
      "Iteration 177, loss = 0.02554464\n",
      "Iteration 178, loss = 0.02515358\n",
      "Iteration 179, loss = 0.02495771\n",
      "Iteration 180, loss = 0.02464549\n",
      "Iteration 181, loss = 0.02437867\n",
      "Iteration 182, loss = 0.02414686\n",
      "Iteration 183, loss = 0.02369239\n",
      "Iteration 184, loss = 0.02348296\n",
      "Iteration 185, loss = 0.02306926\n",
      "Iteration 186, loss = 0.02296978\n",
      "Iteration 187, loss = 0.02259669\n",
      "Iteration 188, loss = 0.02239477\n",
      "Iteration 189, loss = 0.02196623\n",
      "Iteration 190, loss = 0.02170710\n",
      "Iteration 191, loss = 0.02150899\n",
      "Iteration 192, loss = 0.02137805\n",
      "Iteration 193, loss = 0.02110336\n",
      "Iteration 194, loss = 0.02080774\n",
      "Iteration 195, loss = 0.02057189\n",
      "Iteration 196, loss = 0.02028013\n",
      "Iteration 197, loss = 0.02002744\n",
      "Iteration 198, loss = 0.01981373\n",
      "Iteration 199, loss = 0.01950549\n",
      "Iteration 200, loss = 0.01932507\n",
      "[CV] END activation=relu, alpha=0.0001, batch_size=200, hidden_layer_sizes=(100, 100, 100), solver=sgd; total time= 2.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.11440105\n",
      "Iteration 2, loss = 1.40859168\n",
      "Iteration 3, loss = 0.79352042\n",
      "Iteration 4, loss = 0.57670910\n",
      "Iteration 5, loss = 0.48575580\n",
      "Iteration 6, loss = 0.43357382\n",
      "Iteration 7, loss = 0.39834130\n",
      "Iteration 8, loss = 0.37233452\n",
      "Iteration 9, loss = 0.35157529\n",
      "Iteration 10, loss = 0.33426800\n",
      "Iteration 11, loss = 0.32074785\n",
      "Iteration 12, loss = 0.30874589\n",
      "Iteration 13, loss = 0.29833639\n",
      "Iteration 14, loss = 0.28922896\n",
      "Iteration 15, loss = 0.28106947\n",
      "Iteration 16, loss = 0.27320085\n",
      "Iteration 17, loss = 0.26585316\n",
      "Iteration 18, loss = 0.25931850\n",
      "Iteration 19, loss = 0.25320697\n",
      "Iteration 20, loss = 0.24704361\n",
      "Iteration 21, loss = 0.24166198\n",
      "Iteration 22, loss = 0.23643969\n",
      "Iteration 23, loss = 0.23099880\n",
      "Iteration 24, loss = 0.22592810\n",
      "Iteration 25, loss = 0.22121112\n",
      "Iteration 26, loss = 0.21671869\n",
      "Iteration 27, loss = 0.21214016\n",
      "Iteration 28, loss = 0.20808469\n",
      "Iteration 29, loss = 0.20441901\n",
      "Iteration 30, loss = 0.20030997\n",
      "Iteration 31, loss = 0.19650390\n",
      "Iteration 32, loss = 0.19250110\n",
      "Iteration 33, loss = 0.18890184\n",
      "Iteration 34, loss = 0.18539235\n",
      "Iteration 35, loss = 0.18229101\n",
      "Iteration 36, loss = 0.17882671\n",
      "Iteration 37, loss = 0.17595267\n",
      "Iteration 38, loss = 0.17305584\n",
      "Iteration 39, loss = 0.16952044\n",
      "Iteration 40, loss = 0.16674611\n",
      "Iteration 41, loss = 0.16365236\n",
      "Iteration 42, loss = 0.16106309\n",
      "Iteration 43, loss = 0.15811295\n",
      "Iteration 44, loss = 0.15558064\n",
      "Iteration 45, loss = 0.15276073\n",
      "Iteration 46, loss = 0.15059475\n",
      "Iteration 47, loss = 0.14810456\n",
      "Iteration 48, loss = 0.14598626\n",
      "Iteration 49, loss = 0.14351255\n",
      "Iteration 50, loss = 0.14112145\n",
      "Iteration 51, loss = 0.13881174\n",
      "Iteration 52, loss = 0.13682460\n",
      "Iteration 53, loss = 0.13471575\n",
      "Iteration 54, loss = 0.13261616\n",
      "Iteration 55, loss = 0.13068568\n",
      "Iteration 56, loss = 0.12846456\n",
      "Iteration 57, loss = 0.12677250\n",
      "Iteration 58, loss = 0.12478299\n",
      "Iteration 59, loss = 0.12299427\n",
      "Iteration 60, loss = 0.12102345\n",
      "Iteration 61, loss = 0.11937001\n",
      "Iteration 62, loss = 0.11769216\n",
      "Iteration 63, loss = 0.11581613\n",
      "Iteration 64, loss = 0.11419772\n",
      "Iteration 65, loss = 0.11279463\n",
      "Iteration 66, loss = 0.11131332\n",
      "Iteration 67, loss = 0.10953420\n",
      "Iteration 68, loss = 0.10803782\n",
      "Iteration 69, loss = 0.10659824\n",
      "Iteration 70, loss = 0.10488951\n",
      "Iteration 71, loss = 0.10368477\n",
      "Iteration 72, loss = 0.10220595\n",
      "Iteration 73, loss = 0.10086285\n",
      "Iteration 74, loss = 0.09921644\n",
      "Iteration 75, loss = 0.09784349\n",
      "Iteration 76, loss = 0.09668092\n",
      "Iteration 77, loss = 0.09537101\n",
      "Iteration 78, loss = 0.09418905\n",
      "Iteration 79, loss = 0.09269589\n",
      "Iteration 80, loss = 0.09144741\n",
      "Iteration 81, loss = 0.09057650\n",
      "Iteration 82, loss = 0.08935943\n",
      "Iteration 83, loss = 0.08804612\n",
      "Iteration 84, loss = 0.08670182\n",
      "Iteration 85, loss = 0.08592387\n",
      "Iteration 86, loss = 0.08482438\n",
      "Iteration 87, loss = 0.08356119\n",
      "Iteration 88, loss = 0.08265830\n",
      "Iteration 89, loss = 0.08158340\n",
      "Iteration 90, loss = 0.08073104\n",
      "Iteration 91, loss = 0.07936591\n",
      "Iteration 92, loss = 0.07852547\n",
      "Iteration 93, loss = 0.07737630\n",
      "Iteration 94, loss = 0.07643006\n",
      "Iteration 95, loss = 0.07569362\n",
      "Iteration 96, loss = 0.07464501\n",
      "Iteration 97, loss = 0.07367635\n",
      "Iteration 98, loss = 0.07260030\n",
      "Iteration 99, loss = 0.07194819\n",
      "Iteration 100, loss = 0.07088864\n",
      "Iteration 101, loss = 0.06999183\n",
      "Iteration 102, loss = 0.06902698\n",
      "Iteration 103, loss = 0.06820089\n",
      "Iteration 104, loss = 0.06746343\n",
      "Iteration 105, loss = 0.06660490\n",
      "Iteration 106, loss = 0.06584004\n",
      "Iteration 107, loss = 0.06492728\n",
      "Iteration 108, loss = 0.06414595\n",
      "Iteration 109, loss = 0.06324845\n",
      "Iteration 110, loss = 0.06267910\n",
      "Iteration 111, loss = 0.06194082\n",
      "Iteration 112, loss = 0.06123445\n",
      "Iteration 113, loss = 0.06027942\n",
      "Iteration 114, loss = 0.05965712\n",
      "Iteration 115, loss = 0.05895669\n",
      "Iteration 116, loss = 0.05817153\n",
      "Iteration 117, loss = 0.05743979\n",
      "Iteration 118, loss = 0.05680360\n",
      "Iteration 119, loss = 0.05620168\n",
      "Iteration 120, loss = 0.05545446\n",
      "Iteration 121, loss = 0.05490665\n",
      "Iteration 122, loss = 0.05391721\n",
      "Iteration 123, loss = 0.05347472\n",
      "Iteration 124, loss = 0.05273964\n",
      "Iteration 125, loss = 0.05212476\n",
      "Iteration 126, loss = 0.05150603\n",
      "Iteration 127, loss = 0.05091601\n",
      "Iteration 128, loss = 0.05032447\n",
      "Iteration 129, loss = 0.04962449\n",
      "Iteration 130, loss = 0.04910501\n",
      "Iteration 131, loss = 0.04859328\n",
      "Iteration 132, loss = 0.04777616\n",
      "Iteration 133, loss = 0.04725247\n",
      "Iteration 134, loss = 0.04683055\n",
      "Iteration 135, loss = 0.04604813\n",
      "Iteration 136, loss = 0.04558268\n",
      "Iteration 137, loss = 0.04504937\n",
      "Iteration 138, loss = 0.04456302\n",
      "Iteration 139, loss = 0.04402608\n",
      "Iteration 140, loss = 0.04362697\n",
      "Iteration 141, loss = 0.04302646\n",
      "Iteration 142, loss = 0.04256140\n",
      "Iteration 143, loss = 0.04199797\n",
      "Iteration 144, loss = 0.04146795\n",
      "Iteration 145, loss = 0.04105942\n",
      "Iteration 146, loss = 0.04045103\n",
      "Iteration 147, loss = 0.04013409\n",
      "Iteration 148, loss = 0.03948665\n",
      "Iteration 149, loss = 0.03898725\n",
      "Iteration 150, loss = 0.03854567\n",
      "Iteration 151, loss = 0.03806351\n",
      "Iteration 152, loss = 0.03770330\n",
      "Iteration 153, loss = 0.03718358\n",
      "Iteration 154, loss = 0.03688608\n",
      "Iteration 155, loss = 0.03627227\n",
      "Iteration 156, loss = 0.03608562\n",
      "Iteration 157, loss = 0.03547664\n",
      "Iteration 158, loss = 0.03494953\n",
      "Iteration 159, loss = 0.03445027\n",
      "Iteration 160, loss = 0.03422740\n",
      "Iteration 161, loss = 0.03379484\n",
      "Iteration 162, loss = 0.03347365\n",
      "Iteration 163, loss = 0.03301525\n",
      "Iteration 164, loss = 0.03270614\n",
      "Iteration 165, loss = 0.03228105\n",
      "Iteration 166, loss = 0.03186410\n",
      "Iteration 167, loss = 0.03152770\n",
      "Iteration 168, loss = 0.03108621\n",
      "Iteration 169, loss = 0.03065268\n",
      "Iteration 170, loss = 0.03041385\n",
      "Iteration 171, loss = 0.03004026\n",
      "Iteration 172, loss = 0.02966062\n",
      "Iteration 173, loss = 0.02937673\n",
      "Iteration 174, loss = 0.02895348\n",
      "Iteration 175, loss = 0.02868963\n",
      "Iteration 176, loss = 0.02835498\n",
      "Iteration 177, loss = 0.02792026\n",
      "Iteration 178, loss = 0.02757003\n",
      "Iteration 179, loss = 0.02725898\n",
      "Iteration 180, loss = 0.02684008\n",
      "Iteration 181, loss = 0.02667582\n",
      "Iteration 182, loss = 0.02646584\n",
      "Iteration 183, loss = 0.02594666\n",
      "Iteration 184, loss = 0.02568852\n",
      "Iteration 185, loss = 0.02530019\n",
      "Iteration 186, loss = 0.02504436\n",
      "Iteration 187, loss = 0.02489247\n",
      "Iteration 188, loss = 0.02442058\n",
      "Iteration 189, loss = 0.02426669\n",
      "Iteration 190, loss = 0.02390265\n",
      "Iteration 191, loss = 0.02377365\n",
      "Iteration 192, loss = 0.02328042\n",
      "Iteration 193, loss = 0.02305718\n",
      "Iteration 194, loss = 0.02281929\n",
      "Iteration 195, loss = 0.02240017\n",
      "Iteration 196, loss = 0.02229305\n",
      "Iteration 197, loss = 0.02187797\n",
      "Iteration 198, loss = 0.02185392\n",
      "Iteration 199, loss = 0.02151645\n",
      "Iteration 200, loss = 0.02127075\n",
      "[CV] END activation=relu, alpha=0.0001, batch_size=200, hidden_layer_sizes=(100, 100, 100), solver=sgd; total time= 2.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.10309261\n",
      "Iteration 2, loss = 1.39516534\n",
      "Iteration 3, loss = 0.82934636\n",
      "Iteration 4, loss = 0.58788448\n",
      "Iteration 5, loss = 0.47549251\n",
      "Iteration 6, loss = 0.41561185\n",
      "Iteration 7, loss = 0.37821072\n",
      "Iteration 8, loss = 0.35222300\n",
      "Iteration 9, loss = 0.33290240\n",
      "Iteration 10, loss = 0.31671456\n",
      "Iteration 11, loss = 0.30426570\n",
      "Iteration 12, loss = 0.29305242\n",
      "Iteration 13, loss = 0.28323794\n",
      "Iteration 14, loss = 0.27418557\n",
      "Iteration 15, loss = 0.26658108\n",
      "Iteration 16, loss = 0.25922267\n",
      "Iteration 17, loss = 0.25263877\n",
      "Iteration 18, loss = 0.24595232\n",
      "Iteration 19, loss = 0.24022826\n",
      "Iteration 20, loss = 0.23447957\n",
      "Iteration 21, loss = 0.22920404\n",
      "Iteration 22, loss = 0.22407482\n",
      "Iteration 23, loss = 0.21888153\n",
      "Iteration 24, loss = 0.21436626\n",
      "Iteration 25, loss = 0.21030024\n",
      "Iteration 26, loss = 0.20591853\n",
      "Iteration 27, loss = 0.20187703\n",
      "Iteration 28, loss = 0.19825180\n",
      "Iteration 29, loss = 0.19409860\n",
      "Iteration 30, loss = 0.19076334\n",
      "Iteration 31, loss = 0.18723603\n",
      "Iteration 32, loss = 0.18372670\n",
      "Iteration 33, loss = 0.18059858\n",
      "Iteration 34, loss = 0.17746141\n",
      "Iteration 35, loss = 0.17452389\n",
      "Iteration 36, loss = 0.17134269\n",
      "Iteration 37, loss = 0.16868879\n",
      "Iteration 38, loss = 0.16592612\n",
      "Iteration 39, loss = 0.16344954\n",
      "Iteration 40, loss = 0.16060194\n",
      "Iteration 41, loss = 0.15817201\n",
      "Iteration 42, loss = 0.15573265\n",
      "Iteration 43, loss = 0.15340874\n",
      "Iteration 44, loss = 0.15121358\n",
      "Iteration 45, loss = 0.14840512\n",
      "Iteration 46, loss = 0.14663275\n",
      "Iteration 47, loss = 0.14419961\n",
      "Iteration 48, loss = 0.14247141\n",
      "Iteration 49, loss = 0.14039382\n",
      "Iteration 50, loss = 0.13816656\n",
      "Iteration 51, loss = 0.13636251\n",
      "Iteration 52, loss = 0.13444754\n",
      "Iteration 53, loss = 0.13261313\n",
      "Iteration 54, loss = 0.13066337\n",
      "Iteration 55, loss = 0.12874886\n",
      "Iteration 56, loss = 0.12714789\n",
      "Iteration 57, loss = 0.12532269\n",
      "Iteration 58, loss = 0.12354289\n",
      "Iteration 59, loss = 0.12221050\n",
      "Iteration 60, loss = 0.12060265\n",
      "Iteration 61, loss = 0.11892356\n",
      "Iteration 62, loss = 0.11726576\n",
      "Iteration 63, loss = 0.11576466\n",
      "Iteration 64, loss = 0.11444245\n",
      "Iteration 65, loss = 0.11265580\n",
      "Iteration 66, loss = 0.11111508\n",
      "Iteration 67, loss = 0.10981424\n",
      "Iteration 68, loss = 0.10864595\n",
      "Iteration 69, loss = 0.10701137\n",
      "Iteration 70, loss = 0.10611689\n",
      "Iteration 71, loss = 0.10452237\n",
      "Iteration 72, loss = 0.10311651\n",
      "Iteration 73, loss = 0.10208412\n",
      "Iteration 74, loss = 0.10076974\n",
      "Iteration 75, loss = 0.09960939\n",
      "Iteration 76, loss = 0.09837110\n",
      "Iteration 77, loss = 0.09705654\n",
      "Iteration 78, loss = 0.09597903\n",
      "Iteration 79, loss = 0.09469746\n",
      "Iteration 80, loss = 0.09374992\n",
      "Iteration 81, loss = 0.09268435\n",
      "Iteration 82, loss = 0.09145176\n",
      "Iteration 83, loss = 0.09029819\n",
      "Iteration 84, loss = 0.08945653\n",
      "Iteration 85, loss = 0.08834726\n",
      "Iteration 86, loss = 0.08713718\n",
      "Iteration 87, loss = 0.08622102\n",
      "Iteration 88, loss = 0.08514845\n",
      "Iteration 89, loss = 0.08401659\n",
      "Iteration 90, loss = 0.08318966\n",
      "Iteration 91, loss = 0.08243051\n",
      "Iteration 92, loss = 0.08152785\n",
      "Iteration 93, loss = 0.08026147\n",
      "Iteration 94, loss = 0.07956775\n",
      "Iteration 95, loss = 0.07846730\n",
      "Iteration 96, loss = 0.07751236\n",
      "Iteration 97, loss = 0.07678216\n",
      "Iteration 98, loss = 0.07586954\n",
      "Iteration 99, loss = 0.07495260\n",
      "Iteration 100, loss = 0.07429452\n",
      "Iteration 101, loss = 0.07297601\n",
      "Iteration 102, loss = 0.07221908\n",
      "Iteration 103, loss = 0.07159453\n",
      "Iteration 104, loss = 0.07096009\n",
      "Iteration 105, loss = 0.06980398\n",
      "Iteration 106, loss = 0.06908639\n",
      "Iteration 107, loss = 0.06832574\n",
      "Iteration 108, loss = 0.06777510\n",
      "Iteration 109, loss = 0.06677192\n",
      "Iteration 110, loss = 0.06589696\n",
      "Iteration 111, loss = 0.06514705\n",
      "Iteration 112, loss = 0.06447729\n",
      "Iteration 113, loss = 0.06364313\n",
      "Iteration 114, loss = 0.06288364\n",
      "Iteration 115, loss = 0.06215655\n",
      "Iteration 116, loss = 0.06141727\n",
      "Iteration 117, loss = 0.06062266\n",
      "Iteration 118, loss = 0.06010539\n",
      "Iteration 119, loss = 0.05940896\n",
      "Iteration 120, loss = 0.05870832\n",
      "Iteration 121, loss = 0.05804956\n",
      "Iteration 122, loss = 0.05746785\n",
      "Iteration 123, loss = 0.05675462\n",
      "Iteration 124, loss = 0.05607672\n",
      "Iteration 125, loss = 0.05553743\n",
      "Iteration 126, loss = 0.05473686\n",
      "Iteration 127, loss = 0.05418360\n",
      "Iteration 128, loss = 0.05345422\n",
      "Iteration 129, loss = 0.05291048\n",
      "Iteration 130, loss = 0.05221447\n",
      "Iteration 131, loss = 0.05170209\n",
      "Iteration 132, loss = 0.05115042\n",
      "Iteration 133, loss = 0.05052409\n",
      "Iteration 134, loss = 0.04998296\n",
      "Iteration 135, loss = 0.04950471\n",
      "Iteration 136, loss = 0.04876379\n",
      "Iteration 137, loss = 0.04817914\n",
      "Iteration 138, loss = 0.04767171\n",
      "Iteration 139, loss = 0.04715968\n",
      "Iteration 140, loss = 0.04676310\n",
      "Iteration 141, loss = 0.04621807\n",
      "Iteration 142, loss = 0.04539563\n",
      "Iteration 143, loss = 0.04485368\n",
      "Iteration 144, loss = 0.04442164\n",
      "Iteration 145, loss = 0.04390653\n",
      "Iteration 146, loss = 0.04331860\n",
      "Iteration 147, loss = 0.04278071\n",
      "Iteration 148, loss = 0.04248466\n",
      "Iteration 149, loss = 0.04199386\n",
      "Iteration 150, loss = 0.04123385\n",
      "Iteration 151, loss = 0.04087631\n",
      "Iteration 152, loss = 0.04052416\n",
      "Iteration 153, loss = 0.03994129\n",
      "Iteration 154, loss = 0.03967598\n",
      "Iteration 155, loss = 0.03906803\n",
      "Iteration 156, loss = 0.03854012\n",
      "Iteration 157, loss = 0.03817226\n",
      "Iteration 158, loss = 0.03758170\n",
      "Iteration 159, loss = 0.03720304\n",
      "Iteration 160, loss = 0.03681609\n",
      "Iteration 161, loss = 0.03638667\n",
      "Iteration 162, loss = 0.03594660\n",
      "Iteration 163, loss = 0.03554613\n",
      "Iteration 164, loss = 0.03500169\n",
      "Iteration 165, loss = 0.03471911\n",
      "Iteration 166, loss = 0.03440563\n",
      "Iteration 167, loss = 0.03406408\n",
      "Iteration 168, loss = 0.03341244\n",
      "Iteration 169, loss = 0.03293373\n",
      "Iteration 170, loss = 0.03269204\n",
      "Iteration 171, loss = 0.03235492\n",
      "Iteration 172, loss = 0.03193260\n",
      "Iteration 173, loss = 0.03154889\n",
      "Iteration 174, loss = 0.03116585\n",
      "Iteration 175, loss = 0.03076640\n",
      "Iteration 176, loss = 0.03050758\n",
      "Iteration 177, loss = 0.03011549\n",
      "Iteration 178, loss = 0.02976920\n",
      "Iteration 179, loss = 0.02950462\n",
      "Iteration 180, loss = 0.02921371\n",
      "Iteration 181, loss = 0.02882755\n",
      "Iteration 182, loss = 0.02836235\n",
      "Iteration 183, loss = 0.02801256\n",
      "Iteration 184, loss = 0.02764797\n",
      "Iteration 185, loss = 0.02735965\n",
      "Iteration 186, loss = 0.02716895\n",
      "Iteration 187, loss = 0.02680752\n",
      "Iteration 188, loss = 0.02653119\n",
      "Iteration 189, loss = 0.02613980\n",
      "Iteration 190, loss = 0.02588837\n",
      "Iteration 191, loss = 0.02543939\n",
      "Iteration 192, loss = 0.02522954\n",
      "Iteration 193, loss = 0.02484846\n",
      "Iteration 194, loss = 0.02462320\n",
      "Iteration 195, loss = 0.02435274\n",
      "Iteration 196, loss = 0.02406723\n",
      "Iteration 197, loss = 0.02386252\n",
      "Iteration 198, loss = 0.02361317\n",
      "Iteration 199, loss = 0.02318277\n",
      "Iteration 200, loss = 0.02310961\n",
      "[CV] END activation=relu, alpha=0.0001, batch_size=200, hidden_layer_sizes=(100, 100, 100), solver=sgd; total time= 2.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.17190144\n",
      "Iteration 2, loss = 1.42372839\n",
      "Iteration 3, loss = 0.76697381\n",
      "Iteration 4, loss = 0.54010511\n",
      "Iteration 5, loss = 0.44883194\n",
      "Iteration 6, loss = 0.40079326\n",
      "Iteration 7, loss = 0.37012283\n",
      "Iteration 8, loss = 0.34857436\n",
      "Iteration 9, loss = 0.33153546\n",
      "Iteration 10, loss = 0.31738685\n",
      "Iteration 11, loss = 0.30568164\n",
      "Iteration 12, loss = 0.29554130\n",
      "Iteration 13, loss = 0.28655316\n",
      "Iteration 14, loss = 0.27781987\n",
      "Iteration 15, loss = 0.26992673\n",
      "Iteration 16, loss = 0.26241850\n",
      "Iteration 17, loss = 0.25608980\n",
      "Iteration 18, loss = 0.25006560\n",
      "Iteration 19, loss = 0.24411359\n",
      "Iteration 20, loss = 0.23811406\n",
      "Iteration 21, loss = 0.23271572\n",
      "Iteration 22, loss = 0.22793306\n",
      "Iteration 23, loss = 0.22289403\n",
      "Iteration 24, loss = 0.21792734\n",
      "Iteration 25, loss = 0.21379827\n",
      "Iteration 26, loss = 0.20849312\n",
      "Iteration 27, loss = 0.20502049\n",
      "Iteration 28, loss = 0.20071634\n",
      "Iteration 29, loss = 0.19650269\n",
      "Iteration 30, loss = 0.19286838\n",
      "Iteration 31, loss = 0.18920699\n",
      "Iteration 32, loss = 0.18561181\n",
      "Iteration 33, loss = 0.18232491\n",
      "Iteration 34, loss = 0.17903196\n",
      "Iteration 35, loss = 0.17604037\n",
      "Iteration 36, loss = 0.17297738\n",
      "Iteration 37, loss = 0.16986209\n",
      "Iteration 38, loss = 0.16718511\n",
      "Iteration 39, loss = 0.16415742\n",
      "Iteration 40, loss = 0.16163755\n",
      "Iteration 41, loss = 0.15883897\n",
      "Iteration 42, loss = 0.15638759\n",
      "Iteration 43, loss = 0.15420171\n",
      "Iteration 44, loss = 0.15217477\n",
      "Iteration 45, loss = 0.14970988\n",
      "Iteration 46, loss = 0.14709140\n",
      "Iteration 47, loss = 0.14524284\n",
      "Iteration 48, loss = 0.14321637\n",
      "Iteration 49, loss = 0.14084335\n",
      "Iteration 50, loss = 0.13887744\n",
      "Iteration 51, loss = 0.13675673\n",
      "Iteration 52, loss = 0.13480108\n",
      "Iteration 53, loss = 0.13322097\n",
      "Iteration 54, loss = 0.13129085\n",
      "Iteration 55, loss = 0.12953662\n",
      "Iteration 56, loss = 0.12775000\n",
      "Iteration 57, loss = 0.12610459\n",
      "Iteration 58, loss = 0.12436499\n",
      "Iteration 59, loss = 0.12252605\n",
      "Iteration 60, loss = 0.12113377\n",
      "Iteration 61, loss = 0.11946500\n",
      "Iteration 62, loss = 0.11817165\n",
      "Iteration 63, loss = 0.11636903\n",
      "Iteration 64, loss = 0.11470685\n",
      "Iteration 65, loss = 0.11367137\n",
      "Iteration 66, loss = 0.11211109\n",
      "Iteration 67, loss = 0.11043148\n",
      "Iteration 68, loss = 0.10928418\n",
      "Iteration 69, loss = 0.10792132\n",
      "Iteration 70, loss = 0.10645011\n",
      "Iteration 71, loss = 0.10488945\n",
      "Iteration 72, loss = 0.10358179\n",
      "Iteration 73, loss = 0.10231812\n",
      "Iteration 74, loss = 0.10095312\n",
      "Iteration 75, loss = 0.09986037\n",
      "Iteration 76, loss = 0.09816458\n",
      "Iteration 77, loss = 0.09694978\n",
      "Iteration 78, loss = 0.09601499\n",
      "Iteration 79, loss = 0.09487167\n",
      "Iteration 80, loss = 0.09348003\n",
      "Iteration 81, loss = 0.09247855\n",
      "Iteration 82, loss = 0.09152673\n",
      "Iteration 83, loss = 0.09018085\n",
      "Iteration 84, loss = 0.08901915\n",
      "Iteration 85, loss = 0.08784626\n",
      "Iteration 86, loss = 0.08691613\n",
      "Iteration 87, loss = 0.08574776\n",
      "Iteration 88, loss = 0.08460686\n",
      "Iteration 89, loss = 0.08349564\n",
      "Iteration 90, loss = 0.08254956\n",
      "Iteration 91, loss = 0.08162624\n",
      "Iteration 92, loss = 0.08069730\n",
      "Iteration 93, loss = 0.07954381\n",
      "Iteration 94, loss = 0.07856829\n",
      "Iteration 95, loss = 0.07755473\n",
      "Iteration 96, loss = 0.07695024\n",
      "Iteration 97, loss = 0.07588156\n",
      "Iteration 98, loss = 0.07487452\n",
      "Iteration 99, loss = 0.07402161\n",
      "Iteration 100, loss = 0.07331632\n",
      "Iteration 101, loss = 0.07222900\n",
      "Iteration 102, loss = 0.07152431\n",
      "Iteration 103, loss = 0.07046026\n",
      "Iteration 104, loss = 0.06979390\n",
      "Iteration 105, loss = 0.06885288\n",
      "Iteration 106, loss = 0.06813511\n",
      "Iteration 107, loss = 0.06734828\n",
      "Iteration 108, loss = 0.06632582\n",
      "Iteration 109, loss = 0.06566425\n",
      "Iteration 110, loss = 0.06484176\n",
      "Iteration 111, loss = 0.06404062\n",
      "Iteration 112, loss = 0.06335958\n",
      "Iteration 113, loss = 0.06262500\n",
      "Iteration 114, loss = 0.06190721\n",
      "Iteration 115, loss = 0.06116863\n",
      "Iteration 116, loss = 0.06048845\n",
      "Iteration 117, loss = 0.05979577\n",
      "Iteration 118, loss = 0.05910200\n",
      "Iteration 119, loss = 0.05858516\n",
      "Iteration 120, loss = 0.05771100\n",
      "Iteration 121, loss = 0.05699008\n",
      "Iteration 122, loss = 0.05642544\n",
      "Iteration 123, loss = 0.05587267\n",
      "Iteration 124, loss = 0.05524979\n",
      "Iteration 125, loss = 0.05458952\n",
      "Iteration 126, loss = 0.05393236\n",
      "Iteration 127, loss = 0.05329931\n",
      "Iteration 128, loss = 0.05268814\n",
      "Iteration 129, loss = 0.05208206\n",
      "Iteration 130, loss = 0.05162578\n",
      "Iteration 131, loss = 0.05093764\n",
      "Iteration 132, loss = 0.05047498\n",
      "Iteration 133, loss = 0.04981109\n",
      "Iteration 134, loss = 0.04915530\n",
      "Iteration 135, loss = 0.04860117\n",
      "Iteration 136, loss = 0.04789969\n",
      "Iteration 137, loss = 0.04757959\n",
      "Iteration 138, loss = 0.04692508\n",
      "Iteration 139, loss = 0.04649770\n",
      "Iteration 140, loss = 0.04599216\n",
      "Iteration 141, loss = 0.04547426\n",
      "Iteration 142, loss = 0.04493854\n",
      "Iteration 143, loss = 0.04449663\n",
      "Iteration 144, loss = 0.04383394\n",
      "Iteration 145, loss = 0.04334849\n",
      "Iteration 146, loss = 0.04287182\n",
      "Iteration 147, loss = 0.04257058\n",
      "Iteration 148, loss = 0.04199934\n",
      "Iteration 149, loss = 0.04144903\n",
      "Iteration 150, loss = 0.04091542\n",
      "Iteration 151, loss = 0.04052981\n",
      "Iteration 152, loss = 0.04011840\n",
      "Iteration 153, loss = 0.03976771\n",
      "Iteration 154, loss = 0.03945382\n",
      "Iteration 155, loss = 0.03871278\n",
      "Iteration 156, loss = 0.03847540\n",
      "Iteration 157, loss = 0.03788193\n",
      "Iteration 158, loss = 0.03734099\n",
      "Iteration 159, loss = 0.03696903\n",
      "Iteration 160, loss = 0.03648781\n",
      "Iteration 161, loss = 0.03615084\n",
      "Iteration 162, loss = 0.03589809\n",
      "Iteration 163, loss = 0.03532560\n",
      "Iteration 164, loss = 0.03498529\n",
      "Iteration 165, loss = 0.03456055\n",
      "Iteration 166, loss = 0.03413219\n",
      "Iteration 167, loss = 0.03382102\n",
      "Iteration 168, loss = 0.03345206\n",
      "Iteration 169, loss = 0.03304731\n",
      "Iteration 170, loss = 0.03267472\n",
      "Iteration 171, loss = 0.03218867\n",
      "Iteration 172, loss = 0.03186702\n",
      "Iteration 173, loss = 0.03154916\n",
      "Iteration 174, loss = 0.03103875\n",
      "Iteration 175, loss = 0.03079603\n",
      "Iteration 176, loss = 0.03039177\n",
      "Iteration 177, loss = 0.03013966\n",
      "Iteration 178, loss = 0.02971824\n",
      "Iteration 179, loss = 0.02951574\n",
      "Iteration 180, loss = 0.02910262\n",
      "Iteration 181, loss = 0.02865670\n",
      "Iteration 182, loss = 0.02842723\n",
      "Iteration 183, loss = 0.02804548\n",
      "Iteration 184, loss = 0.02770572\n",
      "Iteration 185, loss = 0.02747114\n",
      "Iteration 186, loss = 0.02702838\n",
      "Iteration 187, loss = 0.02683618\n",
      "Iteration 188, loss = 0.02653474\n",
      "Iteration 189, loss = 0.02632630\n",
      "Iteration 190, loss = 0.02598342\n",
      "Iteration 191, loss = 0.02564271\n",
      "Iteration 192, loss = 0.02546337\n",
      "Iteration 193, loss = 0.02498035\n",
      "Iteration 194, loss = 0.02476731\n",
      "Iteration 195, loss = 0.02449412\n",
      "Iteration 196, loss = 0.02421913\n",
      "Iteration 197, loss = 0.02391768\n",
      "Iteration 198, loss = 0.02363469\n",
      "Iteration 199, loss = 0.02345194\n",
      "Iteration 200, loss = 0.02324753\n",
      "[CV] END activation=relu, alpha=0.0001, batch_size=200, hidden_layer_sizes=(100, 100, 100), solver=sgd; total time= 2.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.10879206\n",
      "Iteration 2, loss = 1.27611697\n",
      "Iteration 3, loss = 0.69436502\n",
      "Iteration 4, loss = 0.51680343\n",
      "Iteration 5, loss = 0.43925801\n",
      "Iteration 6, loss = 0.39506037\n",
      "Iteration 7, loss = 0.36595424\n",
      "Iteration 8, loss = 0.34466709\n",
      "Iteration 9, loss = 0.32786057\n",
      "Iteration 10, loss = 0.31410858\n",
      "Iteration 11, loss = 0.30227433\n",
      "Iteration 12, loss = 0.29202761\n",
      "Iteration 13, loss = 0.28267506\n",
      "Iteration 14, loss = 0.27465971\n",
      "Iteration 15, loss = 0.26685260\n",
      "Iteration 16, loss = 0.26049259\n",
      "Iteration 17, loss = 0.25339068\n",
      "Iteration 18, loss = 0.24723804\n",
      "Iteration 19, loss = 0.24139147\n",
      "Iteration 20, loss = 0.23585547\n",
      "Iteration 21, loss = 0.23061042\n",
      "Iteration 22, loss = 0.22553208\n",
      "Iteration 23, loss = 0.22052406\n",
      "Iteration 24, loss = 0.21586115\n",
      "Iteration 25, loss = 0.21129772\n",
      "Iteration 26, loss = 0.20705363\n",
      "Iteration 27, loss = 0.20275723\n",
      "Iteration 28, loss = 0.19868845\n",
      "Iteration 29, loss = 0.19501950\n",
      "Iteration 30, loss = 0.19079747\n",
      "Iteration 31, loss = 0.18700932\n",
      "Iteration 32, loss = 0.18338945\n",
      "Iteration 33, loss = 0.17999420\n",
      "Iteration 34, loss = 0.17703129\n",
      "Iteration 35, loss = 0.17336281\n",
      "Iteration 36, loss = 0.17002511\n",
      "Iteration 37, loss = 0.16663095\n",
      "Iteration 38, loss = 0.16393930\n",
      "Iteration 39, loss = 0.16097424\n",
      "Iteration 40, loss = 0.15804571\n",
      "Iteration 41, loss = 0.15527010\n",
      "Iteration 42, loss = 0.15251243\n",
      "Iteration 43, loss = 0.14994680\n",
      "Iteration 44, loss = 0.14744980\n",
      "Iteration 45, loss = 0.14532363\n",
      "Iteration 46, loss = 0.14270058\n",
      "Iteration 47, loss = 0.14031533\n",
      "Iteration 48, loss = 0.13805768\n",
      "Iteration 49, loss = 0.13588924\n",
      "Iteration 50, loss = 0.13391668\n",
      "Iteration 51, loss = 0.13161344\n",
      "Iteration 52, loss = 0.12943236\n",
      "Iteration 53, loss = 0.12755177\n",
      "Iteration 54, loss = 0.12544147\n",
      "Iteration 55, loss = 0.12367769\n",
      "Iteration 56, loss = 0.12167482\n",
      "Iteration 57, loss = 0.11982677\n",
      "Iteration 58, loss = 0.11823922\n",
      "Iteration 59, loss = 0.11640490\n",
      "Iteration 60, loss = 0.11445787\n",
      "Iteration 61, loss = 0.11318291\n",
      "Iteration 62, loss = 0.11145806\n",
      "Iteration 63, loss = 0.10960188\n",
      "Iteration 64, loss = 0.10808223\n",
      "Iteration 65, loss = 0.10644115\n",
      "Iteration 66, loss = 0.10508431\n",
      "Iteration 67, loss = 0.10338624\n",
      "Iteration 68, loss = 0.10220315\n",
      "Iteration 69, loss = 0.10033967\n",
      "Iteration 70, loss = 0.09950098\n",
      "Iteration 71, loss = 0.09796608\n",
      "Iteration 72, loss = 0.09686457\n",
      "Iteration 73, loss = 0.09526462\n",
      "Iteration 74, loss = 0.09381958\n",
      "Iteration 75, loss = 0.09257233\n",
      "Iteration 76, loss = 0.09172021\n",
      "Iteration 77, loss = 0.09028455\n",
      "Iteration 78, loss = 0.08879040\n",
      "Iteration 79, loss = 0.08801453\n",
      "Iteration 80, loss = 0.08662759\n",
      "Iteration 81, loss = 0.08559188\n",
      "Iteration 82, loss = 0.08445508\n",
      "Iteration 83, loss = 0.08319598\n",
      "Iteration 84, loss = 0.08178291\n",
      "Iteration 85, loss = 0.08093090\n",
      "Iteration 86, loss = 0.07978664\n",
      "Iteration 87, loss = 0.07906774\n",
      "Iteration 88, loss = 0.07790906\n",
      "Iteration 89, loss = 0.07699783\n",
      "Iteration 90, loss = 0.07588729\n",
      "Iteration 91, loss = 0.07489124\n",
      "Iteration 92, loss = 0.07410223\n",
      "Iteration 93, loss = 0.07281031\n",
      "Iteration 94, loss = 0.07205672\n",
      "Iteration 95, loss = 0.07110664\n",
      "Iteration 96, loss = 0.06996996\n",
      "Iteration 97, loss = 0.06948904\n",
      "Iteration 98, loss = 0.06839949\n",
      "Iteration 99, loss = 0.06773088\n",
      "Iteration 100, loss = 0.06668252\n",
      "Iteration 101, loss = 0.06581553\n",
      "Iteration 102, loss = 0.06528599\n",
      "Iteration 103, loss = 0.06456375\n",
      "Iteration 104, loss = 0.06353142\n",
      "Iteration 105, loss = 0.06277411\n",
      "Iteration 106, loss = 0.06181711\n",
      "Iteration 107, loss = 0.06130997\n",
      "Iteration 108, loss = 0.06075376\n",
      "Iteration 109, loss = 0.05974292\n",
      "Iteration 110, loss = 0.05912340\n",
      "Iteration 111, loss = 0.05836845\n",
      "Iteration 112, loss = 0.05778303\n",
      "Iteration 113, loss = 0.05682559\n",
      "Iteration 114, loss = 0.05616791\n",
      "Iteration 115, loss = 0.05548379\n",
      "Iteration 116, loss = 0.05492044\n",
      "Iteration 117, loss = 0.05413008\n",
      "Iteration 118, loss = 0.05349165\n",
      "Iteration 119, loss = 0.05268833\n",
      "Iteration 120, loss = 0.05217265\n",
      "Iteration 121, loss = 0.05168134\n",
      "Iteration 122, loss = 0.05097465\n",
      "Iteration 123, loss = 0.05033694\n",
      "Iteration 124, loss = 0.04963868\n",
      "Iteration 125, loss = 0.04904372\n",
      "Iteration 126, loss = 0.04839898\n",
      "Iteration 127, loss = 0.04795132\n",
      "Iteration 128, loss = 0.04745544\n",
      "Iteration 129, loss = 0.04663688\n",
      "Iteration 130, loss = 0.04625198\n",
      "Iteration 131, loss = 0.04581483\n",
      "Iteration 132, loss = 0.04505489\n",
      "Iteration 133, loss = 0.04468212\n",
      "Iteration 134, loss = 0.04400467\n",
      "Iteration 135, loss = 0.04361659\n",
      "Iteration 136, loss = 0.04312344\n",
      "Iteration 137, loss = 0.04250151\n",
      "Iteration 138, loss = 0.04198647\n",
      "Iteration 139, loss = 0.04166206\n",
      "Iteration 140, loss = 0.04107600\n",
      "Iteration 141, loss = 0.04047201\n",
      "Iteration 142, loss = 0.04005771\n",
      "Iteration 143, loss = 0.03944271\n",
      "Iteration 144, loss = 0.03906417\n",
      "Iteration 145, loss = 0.03857976\n",
      "Iteration 146, loss = 0.03811532\n",
      "Iteration 147, loss = 0.03770481\n",
      "Iteration 148, loss = 0.03711116\n",
      "Iteration 149, loss = 0.03675280\n",
      "Iteration 150, loss = 0.03634375\n",
      "Iteration 151, loss = 0.03580967\n",
      "Iteration 152, loss = 0.03527200\n",
      "Iteration 153, loss = 0.03497320\n",
      "Iteration 154, loss = 0.03446050\n",
      "Iteration 155, loss = 0.03422658\n",
      "Iteration 156, loss = 0.03369765\n",
      "Iteration 157, loss = 0.03330414\n",
      "Iteration 158, loss = 0.03284352\n",
      "Iteration 159, loss = 0.03261202\n",
      "Iteration 160, loss = 0.03221576\n",
      "Iteration 161, loss = 0.03180562\n",
      "Iteration 162, loss = 0.03138279\n",
      "Iteration 163, loss = 0.03094797\n",
      "Iteration 164, loss = 0.03060230\n",
      "Iteration 165, loss = 0.03021853\n",
      "Iteration 166, loss = 0.02985601\n",
      "Iteration 167, loss = 0.02963719\n",
      "Iteration 168, loss = 0.02935773\n",
      "Iteration 169, loss = 0.02886240\n",
      "Iteration 170, loss = 0.02857283\n",
      "Iteration 171, loss = 0.02827749\n",
      "Iteration 172, loss = 0.02778969\n",
      "Iteration 173, loss = 0.02756245\n",
      "Iteration 174, loss = 0.02703594\n",
      "Iteration 175, loss = 0.02692585\n",
      "Iteration 176, loss = 0.02653643\n",
      "Iteration 177, loss = 0.02618406\n",
      "Iteration 178, loss = 0.02582841\n",
      "Iteration 179, loss = 0.02550508\n",
      "Iteration 180, loss = 0.02522093\n",
      "Iteration 181, loss = 0.02487384\n",
      "Iteration 182, loss = 0.02469442\n",
      "Iteration 183, loss = 0.02439809\n",
      "Iteration 184, loss = 0.02402911\n",
      "Iteration 185, loss = 0.02383217\n",
      "Iteration 186, loss = 0.02348352\n",
      "Iteration 187, loss = 0.02322937\n",
      "Iteration 188, loss = 0.02293098\n",
      "Iteration 189, loss = 0.02269627\n",
      "Iteration 190, loss = 0.02237864\n",
      "Iteration 191, loss = 0.02217173\n",
      "Iteration 192, loss = 0.02179848\n",
      "Iteration 193, loss = 0.02158463\n",
      "Iteration 194, loss = 0.02146983\n",
      "Iteration 195, loss = 0.02109365\n",
      "Iteration 196, loss = 0.02084945\n",
      "Iteration 197, loss = 0.02061382\n",
      "Iteration 198, loss = 0.02043818\n",
      "Iteration 199, loss = 0.02014732\n",
      "Iteration 200, loss = 0.01998744\n",
      "[CV] END activation=relu, alpha=0.0001, batch_size=200, hidden_layer_sizes=(100, 100, 100), solver=sgd; total time= 2.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.37598201\n",
      "Iteration 2, loss = 0.18153681\n",
      "Iteration 3, loss = 0.13329279\n",
      "Iteration 4, loss = 0.10423380\n",
      "Iteration 5, loss = 0.08424384\n",
      "Iteration 6, loss = 0.06947356\n",
      "Iteration 7, loss = 0.05767166\n",
      "Iteration 8, loss = 0.04924385\n",
      "Iteration 9, loss = 0.04296224\n",
      "Iteration 10, loss = 0.03692777\n",
      "Iteration 11, loss = 0.03353043\n",
      "Iteration 12, loss = 0.03039665\n",
      "Iteration 13, loss = 0.02713226\n",
      "Iteration 14, loss = 0.02391269\n",
      "Iteration 15, loss = 0.02270406\n",
      "Iteration 16, loss = 0.02139918\n",
      "Iteration 17, loss = 0.02007125\n",
      "Iteration 18, loss = 0.01963322\n",
      "Iteration 19, loss = 0.01963597\n",
      "Iteration 20, loss = 0.01609570\n",
      "Iteration 21, loss = 0.01812855\n",
      "Iteration 22, loss = 0.01612978\n",
      "Iteration 23, loss = 0.02059601\n",
      "Iteration 24, loss = 0.01681089\n",
      "Iteration 25, loss = 0.01377327\n",
      "Iteration 26, loss = 0.02034981\n",
      "Iteration 27, loss = 0.01689350\n",
      "Iteration 28, loss = 0.01321221\n",
      "Iteration 29, loss = 0.01545191\n",
      "Iteration 30, loss = 0.01863837\n",
      "Iteration 31, loss = 0.01623155\n",
      "Iteration 32, loss = 0.01432587\n",
      "Iteration 33, loss = 0.02038214\n",
      "Iteration 34, loss = 0.01391905\n",
      "Iteration 35, loss = 0.01202602\n",
      "Iteration 36, loss = 0.01175239\n",
      "Iteration 37, loss = 0.02711237\n",
      "Iteration 38, loss = 0.01440841\n",
      "Iteration 39, loss = 0.01213221\n",
      "Iteration 40, loss = 0.01453633\n",
      "Iteration 41, loss = 0.02259066\n",
      "Iteration 42, loss = 0.01612620\n",
      "Iteration 43, loss = 0.01262368\n",
      "Iteration 44, loss = 0.01160465\n",
      "Iteration 45, loss = 0.01210056\n",
      "Iteration 46, loss = 0.02736539\n",
      "Iteration 47, loss = 0.01397487\n",
      "Iteration 48, loss = 0.01187709\n",
      "Iteration 49, loss = 0.01127582\n",
      "Iteration 50, loss = 0.01157829\n",
      "Iteration 51, loss = 0.02637152\n",
      "Iteration 52, loss = 0.01521489\n",
      "Iteration 53, loss = 0.01187507\n",
      "Iteration 54, loss = 0.01126663\n",
      "Iteration 55, loss = 0.01100378\n",
      "Iteration 56, loss = 0.02741235\n",
      "Iteration 57, loss = 0.01453546\n",
      "Iteration 58, loss = 0.01328462\n",
      "Iteration 59, loss = 0.01196826\n",
      "Iteration 60, loss = 0.01094452\n",
      "Iteration 61, loss = 0.01065416\n",
      "Iteration 62, loss = 0.02497972\n",
      "Iteration 63, loss = 0.01591702\n",
      "Iteration 64, loss = 0.01169965\n",
      "Iteration 65, loss = 0.01091416\n",
      "Iteration 66, loss = 0.01052984\n",
      "Iteration 67, loss = 0.02061621\n",
      "Iteration 68, loss = 0.01813633\n",
      "Iteration 69, loss = 0.01175962\n",
      "Iteration 70, loss = 0.01080396\n",
      "Iteration 71, loss = 0.01046660\n",
      "Iteration 72, loss = 0.01060121\n",
      "Iteration 73, loss = 0.02804152\n",
      "Iteration 74, loss = 0.01277988\n",
      "Iteration 75, loss = 0.01105690\n",
      "Iteration 76, loss = 0.01048811\n",
      "Iteration 77, loss = 0.01038917\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.001, batch_size=50, hidden_layer_sizes=(100,), solver=adam; total time= 3.3min\n",
      "Iteration 1, loss = 0.39020182\n",
      "Iteration 2, loss = 0.18440937\n",
      "Iteration 3, loss = 0.13266632\n",
      "Iteration 4, loss = 0.10246664\n",
      "Iteration 5, loss = 0.08309578\n",
      "Iteration 6, loss = 0.06976511\n",
      "Iteration 7, loss = 0.05832254\n",
      "Iteration 8, loss = 0.04970694\n",
      "Iteration 9, loss = 0.04420892\n",
      "Iteration 10, loss = 0.03735049\n",
      "Iteration 11, loss = 0.03353780\n",
      "Iteration 12, loss = 0.02907334\n",
      "Iteration 13, loss = 0.02867664\n",
      "Iteration 14, loss = 0.02448948\n",
      "Iteration 15, loss = 0.02377650\n",
      "Iteration 16, loss = 0.02122903\n",
      "Iteration 17, loss = 0.02087285\n",
      "Iteration 18, loss = 0.02072623\n",
      "Iteration 19, loss = 0.02026189\n",
      "Iteration 20, loss = 0.01757568\n",
      "Iteration 21, loss = 0.01951647\n",
      "Iteration 22, loss = 0.01713766\n",
      "Iteration 23, loss = 0.01634157\n",
      "Iteration 24, loss = 0.01640251\n",
      "Iteration 25, loss = 0.02005994\n",
      "Iteration 26, loss = 0.01368407\n",
      "Iteration 27, loss = 0.02103291\n",
      "Iteration 28, loss = 0.01656404\n",
      "Iteration 29, loss = 0.01391946\n",
      "Iteration 30, loss = 0.01452289\n",
      "Iteration 31, loss = 0.01746065\n",
      "Iteration 32, loss = 0.01537759\n",
      "Iteration 33, loss = 0.01800738\n",
      "Iteration 34, loss = 0.01487402\n",
      "Iteration 35, loss = 0.01253477\n",
      "Iteration 36, loss = 0.02165303\n",
      "Iteration 37, loss = 0.01747092\n",
      "Iteration 38, loss = 0.01356510\n",
      "Iteration 39, loss = 0.01251492\n",
      "Iteration 40, loss = 0.01154407\n",
      "Iteration 41, loss = 0.01689775\n",
      "Iteration 42, loss = 0.02418609\n",
      "Iteration 43, loss = 0.01421601\n",
      "Iteration 44, loss = 0.01196492\n",
      "Iteration 45, loss = 0.01128246\n",
      "Iteration 46, loss = 0.01352124\n",
      "Iteration 47, loss = 0.02644481\n",
      "Iteration 48, loss = 0.01344666\n",
      "Iteration 49, loss = 0.01192338\n",
      "Iteration 50, loss = 0.01116831\n",
      "Iteration 51, loss = 0.02092436\n",
      "Iteration 52, loss = 0.01704022\n",
      "Iteration 53, loss = 0.01392957\n",
      "Iteration 54, loss = 0.01151777\n",
      "Iteration 55, loss = 0.01095367\n",
      "Iteration 56, loss = 0.01811051\n",
      "Iteration 57, loss = 0.01876805\n",
      "Iteration 58, loss = 0.01282310\n",
      "Iteration 59, loss = 0.01135572\n",
      "Iteration 60, loss = 0.01080253\n",
      "Iteration 61, loss = 0.01088966\n",
      "Iteration 62, loss = 0.02976711\n",
      "Iteration 63, loss = 0.01373756\n",
      "Iteration 64, loss = 0.01156858\n",
      "Iteration 65, loss = 0.01093741\n",
      "Iteration 66, loss = 0.02148880\n",
      "Iteration 67, loss = 0.01785062\n",
      "Iteration 68, loss = 0.01288074\n",
      "Iteration 69, loss = 0.01141904\n",
      "Iteration 70, loss = 0.01103636\n",
      "Iteration 71, loss = 0.01048128\n",
      "Iteration 72, loss = 0.02667102\n",
      "Iteration 73, loss = 0.01485780\n",
      "Iteration 74, loss = 0.01159557\n",
      "Iteration 75, loss = 0.01075313\n",
      "Iteration 76, loss = 0.01046414\n",
      "Iteration 77, loss = 0.02097221\n",
      "Iteration 78, loss = 0.01645308\n",
      "Iteration 79, loss = 0.01208685\n",
      "Iteration 80, loss = 0.01077731\n",
      "Iteration 81, loss = 0.01035117\n",
      "Iteration 82, loss = 0.01892057\n",
      "Iteration 83, loss = 0.02029111\n",
      "Iteration 84, loss = 0.01308953\n",
      "Iteration 85, loss = 0.01164521\n",
      "Iteration 86, loss = 0.01052290\n",
      "Iteration 87, loss = 0.01023498\n",
      "Iteration 88, loss = 0.02308618\n",
      "Iteration 89, loss = 0.01481010\n",
      "Iteration 90, loss = 0.01179577\n",
      "Iteration 91, loss = 0.01154295\n",
      "Iteration 92, loss = 0.01559263\n",
      "Iteration 93, loss = 0.01909846\n",
      "Iteration 94, loss = 0.01152205\n",
      "Iteration 95, loss = 0.01063243\n",
      "Iteration 96, loss = 0.01024610\n",
      "Iteration 97, loss = 0.02020125\n",
      "Iteration 98, loss = 0.01727962\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.001, batch_size=50, hidden_layer_sizes=(100,), solver=adam; total time=287.3min\n",
      "Iteration 1, loss = 0.36893270\n",
      "Iteration 2, loss = 0.17332090\n",
      "Iteration 3, loss = 0.12679884\n",
      "Iteration 4, loss = 0.09926552\n",
      "Iteration 5, loss = 0.08102492\n",
      "Iteration 6, loss = 0.06742870\n",
      "Iteration 7, loss = 0.05771515\n",
      "Iteration 8, loss = 0.04974160\n",
      "Iteration 9, loss = 0.04250862\n",
      "Iteration 10, loss = 0.03808350\n",
      "Iteration 11, loss = 0.03215532\n",
      "Iteration 12, loss = 0.03021363\n",
      "Iteration 13, loss = 0.02711589\n",
      "Iteration 14, loss = 0.02442193\n",
      "Iteration 15, loss = 0.02271461\n",
      "Iteration 16, loss = 0.02155080\n",
      "Iteration 17, loss = 0.01967945\n",
      "Iteration 18, loss = 0.01969280\n",
      "Iteration 19, loss = 0.01977094\n",
      "Iteration 20, loss = 0.01818496\n",
      "Iteration 21, loss = 0.01766370\n",
      "Iteration 22, loss = 0.01916067\n",
      "Iteration 23, loss = 0.01531764\n",
      "Iteration 24, loss = 0.01979567\n",
      "Iteration 25, loss = 0.01585465\n",
      "Iteration 26, loss = 0.01439944\n",
      "Iteration 27, loss = 0.01296861\n",
      "Iteration 28, loss = 0.02234231\n",
      "Iteration 29, loss = 0.01757537\n",
      "Iteration 30, loss = 0.01373321\n",
      "Iteration 31, loss = 0.01590165\n",
      "Iteration 32, loss = 0.01907157\n",
      "Iteration 33, loss = 0.01614455\n",
      "Iteration 34, loss = 0.01324793\n",
      "Iteration 35, loss = 0.01221251\n",
      "Iteration 36, loss = 0.02307173\n",
      "Iteration 37, loss = 0.01568033\n",
      "Iteration 38, loss = 0.01280370\n",
      "Iteration 39, loss = 0.01158618\n",
      "Iteration 40, loss = 0.01110748\n",
      "Iteration 41, loss = 0.02236645\n",
      "Iteration 42, loss = 0.02064054\n",
      "Iteration 43, loss = 0.01332801\n",
      "Iteration 44, loss = 0.01151251\n",
      "Iteration 45, loss = 0.01100584\n",
      "Iteration 46, loss = 0.02140090\n",
      "Iteration 47, loss = 0.01884825\n",
      "Iteration 48, loss = 0.01312311\n",
      "Iteration 49, loss = 0.01132734\n",
      "Iteration 50, loss = 0.01229273\n",
      "Iteration 51, loss = 0.02521192\n",
      "Iteration 52, loss = 0.01381409\n",
      "Iteration 53, loss = 0.01169826\n",
      "Iteration 54, loss = 0.01101012\n",
      "Iteration 55, loss = 0.01060332\n",
      "Iteration 56, loss = 0.02338712\n",
      "Iteration 57, loss = 0.01831985\n",
      "Iteration 58, loss = 0.01264200\n",
      "Iteration 59, loss = 0.01132285\n",
      "Iteration 60, loss = 0.01063241\n",
      "Iteration 61, loss = 0.02006654\n",
      "Iteration 62, loss = 0.01961815\n",
      "Iteration 63, loss = 0.01334337\n",
      "Iteration 64, loss = 0.01125115\n",
      "Iteration 65, loss = 0.01065294\n",
      "Iteration 66, loss = 0.01029692\n",
      "Iteration 67, loss = 0.02643001\n",
      "Iteration 68, loss = 0.01540786\n",
      "Iteration 69, loss = 0.01145862\n",
      "Iteration 70, loss = 0.01078004\n",
      "Iteration 71, loss = 0.01075395\n",
      "Iteration 72, loss = 0.01621517\n",
      "Iteration 73, loss = 0.02098137\n",
      "Iteration 74, loss = 0.01256717\n",
      "Iteration 75, loss = 0.01088275\n",
      "Iteration 76, loss = 0.01034697\n",
      "Iteration 77, loss = 0.01631347\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.001, batch_size=50, hidden_layer_sizes=(100,), solver=adam; total time= 4.6min\n",
      "Iteration 1, loss = 0.38294244\n",
      "Iteration 2, loss = 0.18232186\n",
      "Iteration 3, loss = 0.13366924\n",
      "Iteration 4, loss = 0.10374299\n",
      "Iteration 5, loss = 0.08519770\n",
      "Iteration 6, loss = 0.07021682\n",
      "Iteration 7, loss = 0.06099181\n",
      "Iteration 8, loss = 0.05067906\n",
      "Iteration 9, loss = 0.04435241\n",
      "Iteration 10, loss = 0.03796965\n",
      "Iteration 11, loss = 0.03385580\n",
      "Iteration 12, loss = 0.03100610\n",
      "Iteration 13, loss = 0.02700072\n",
      "Iteration 14, loss = 0.02557395\n",
      "Iteration 15, loss = 0.02401809\n",
      "Iteration 16, loss = 0.02220649\n",
      "Iteration 17, loss = 0.02096226\n",
      "Iteration 18, loss = 0.02090657\n",
      "Iteration 19, loss = 0.01930152\n",
      "Iteration 20, loss = 0.01901393\n",
      "Iteration 21, loss = 0.01715745\n",
      "Iteration 22, loss = 0.01875150\n",
      "Iteration 23, loss = 0.01761130\n",
      "Iteration 24, loss = 0.01778174\n",
      "Iteration 25, loss = 0.01817708\n",
      "Iteration 26, loss = 0.01672707\n",
      "Iteration 27, loss = 0.01414227\n",
      "Iteration 28, loss = 0.02003145\n",
      "Iteration 29, loss = 0.01817731\n",
      "Iteration 30, loss = 0.01498045\n",
      "Iteration 31, loss = 0.01496975\n",
      "Iteration 32, loss = 0.01739803\n",
      "Iteration 33, loss = 0.01821691\n",
      "Iteration 34, loss = 0.01345251\n",
      "Iteration 35, loss = 0.01643207\n",
      "Iteration 36, loss = 0.01653897\n",
      "Iteration 37, loss = 0.01950007\n",
      "Iteration 38, loss = 0.01485747\n",
      "Iteration 39, loss = 0.01301735\n",
      "Iteration 40, loss = 0.01560176\n",
      "Iteration 41, loss = 0.01977812\n",
      "Iteration 42, loss = 0.01653097\n",
      "Iteration 43, loss = 0.01377483\n",
      "Iteration 44, loss = 0.01186558\n",
      "Iteration 45, loss = 0.01131638\n",
      "Iteration 46, loss = 0.02402779\n",
      "Iteration 47, loss = 0.01508495\n",
      "Iteration 48, loss = 0.01401316\n",
      "Iteration 49, loss = 0.01199324\n",
      "Iteration 50, loss = 0.01240606\n",
      "Iteration 51, loss = 0.02397742\n",
      "Iteration 52, loss = 0.01477068\n",
      "Iteration 53, loss = 0.01218589\n",
      "Iteration 54, loss = 0.01141878\n",
      "Iteration 55, loss = 0.01513504\n",
      "Iteration 56, loss = 0.02181721\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.001, batch_size=50, hidden_layer_sizes=(100,), solver=adam; total time= 3.2min\n",
      "Iteration 1, loss = 0.37508771\n",
      "Iteration 2, loss = 0.18178827\n",
      "Iteration 3, loss = 0.13219210\n",
      "Iteration 4, loss = 0.10319486\n",
      "Iteration 5, loss = 0.08304530\n",
      "Iteration 6, loss = 0.06962766\n",
      "Iteration 7, loss = 0.05823846\n",
      "Iteration 8, loss = 0.04874197\n",
      "Iteration 9, loss = 0.04188251\n",
      "Iteration 10, loss = 0.03745933\n",
      "Iteration 11, loss = 0.03168914\n",
      "Iteration 12, loss = 0.03009954\n",
      "Iteration 13, loss = 0.02640536\n",
      "Iteration 14, loss = 0.02397402\n",
      "Iteration 15, loss = 0.02179362\n",
      "Iteration 16, loss = 0.02144639\n",
      "Iteration 17, loss = 0.02053727\n",
      "Iteration 18, loss = 0.01877236\n",
      "Iteration 19, loss = 0.01921014\n",
      "Iteration 20, loss = 0.01685220\n",
      "Iteration 21, loss = 0.01971606\n",
      "Iteration 22, loss = 0.01797102\n",
      "Iteration 23, loss = 0.01582449\n",
      "Iteration 24, loss = 0.01939405\n",
      "Iteration 25, loss = 0.01574468\n",
      "Iteration 26, loss = 0.01484554\n",
      "Iteration 27, loss = 0.01384770\n",
      "Iteration 28, loss = 0.02122262\n",
      "Iteration 29, loss = 0.01477459\n",
      "Iteration 30, loss = 0.01271488\n",
      "Iteration 31, loss = 0.01359499\n",
      "Iteration 32, loss = 0.02458309\n",
      "Iteration 33, loss = 0.01511829\n",
      "Iteration 34, loss = 0.01252874\n",
      "Iteration 35, loss = 0.01189844\n",
      "Iteration 36, loss = 0.01341250\n",
      "Iteration 37, loss = 0.02692650\n",
      "Iteration 38, loss = 0.01434005\n",
      "Iteration 39, loss = 0.01231727\n",
      "Iteration 40, loss = 0.01144722\n",
      "Iteration 41, loss = 0.01147159\n",
      "Iteration 42, loss = 0.02915336\n",
      "Iteration 43, loss = 0.01547283\n",
      "Iteration 44, loss = 0.01409742\n",
      "Iteration 45, loss = 0.01414869\n",
      "Iteration 46, loss = 0.01173486\n",
      "Iteration 47, loss = 0.01194736\n",
      "Iteration 48, loss = 0.02649008\n",
      "Iteration 49, loss = 0.01527932\n",
      "Iteration 50, loss = 0.01251035\n",
      "Iteration 51, loss = 0.01161961\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.001, batch_size=50, hidden_layer_sizes=(100,), solver=adam; total time= 3.5min\n",
      "Iteration 1, loss = 1.02726948\n",
      "Iteration 2, loss = 0.47820258\n",
      "Iteration 3, loss = 0.39342477\n",
      "Iteration 4, loss = 0.35306023\n",
      "Iteration 5, loss = 0.32734599\n",
      "Iteration 6, loss = 0.30811697\n",
      "Iteration 7, loss = 0.29264837\n",
      "Iteration 8, loss = 0.27947784\n",
      "Iteration 9, loss = 0.26766234\n",
      "Iteration 10, loss = 0.25718771\n",
      "Iteration 11, loss = 0.24756995\n",
      "Iteration 12, loss = 0.23896967\n",
      "Iteration 13, loss = 0.23080906\n",
      "Iteration 14, loss = 0.22353186\n",
      "Iteration 15, loss = 0.21668662\n",
      "Iteration 16, loss = 0.21003705\n",
      "Iteration 17, loss = 0.20405420\n",
      "Iteration 18, loss = 0.19820516\n",
      "Iteration 19, loss = 0.19302183\n",
      "Iteration 20, loss = 0.18811284\n",
      "Iteration 21, loss = 0.18303434\n",
      "Iteration 22, loss = 0.17857499\n",
      "Iteration 23, loss = 0.17439022\n",
      "Iteration 24, loss = 0.17019989\n",
      "Iteration 25, loss = 0.16629820\n",
      "Iteration 26, loss = 0.16248988\n",
      "Iteration 27, loss = 0.15904687\n",
      "Iteration 28, loss = 0.15553948\n",
      "Iteration 29, loss = 0.15229465\n",
      "Iteration 30, loss = 0.14908162\n",
      "Iteration 31, loss = 0.14620151\n",
      "Iteration 32, loss = 0.14323381\n",
      "Iteration 33, loss = 0.14041066\n",
      "Iteration 34, loss = 0.13759674\n",
      "Iteration 35, loss = 0.13506749\n",
      "Iteration 36, loss = 0.13267277\n",
      "Iteration 37, loss = 0.13010170\n",
      "Iteration 38, loss = 0.12760563\n",
      "Iteration 39, loss = 0.12543842\n",
      "Iteration 40, loss = 0.12319117\n",
      "Iteration 41, loss = 0.12101533\n",
      "Iteration 42, loss = 0.11903654\n",
      "Iteration 43, loss = 0.11697442\n",
      "Iteration 44, loss = 0.11510813\n",
      "Iteration 45, loss = 0.11324431\n",
      "Iteration 46, loss = 0.11137025\n",
      "Iteration 47, loss = 0.10965204\n",
      "Iteration 48, loss = 0.10770054\n",
      "Iteration 49, loss = 0.10622747\n",
      "Iteration 50, loss = 0.10451212\n",
      "Iteration 51, loss = 0.10291966\n",
      "Iteration 52, loss = 0.10136022\n",
      "Iteration 53, loss = 0.09977705\n",
      "Iteration 54, loss = 0.09826872\n",
      "Iteration 55, loss = 0.09686562\n",
      "Iteration 56, loss = 0.09539122\n",
      "Iteration 57, loss = 0.09412489\n",
      "Iteration 58, loss = 0.09291165\n",
      "Iteration 59, loss = 0.09135335\n",
      "Iteration 60, loss = 0.09024357\n",
      "Iteration 61, loss = 0.08905357\n",
      "Iteration 62, loss = 0.08775324\n",
      "Iteration 63, loss = 0.08659357\n",
      "Iteration 64, loss = 0.08552271\n",
      "Iteration 65, loss = 0.08425834\n",
      "Iteration 66, loss = 0.08298942\n",
      "Iteration 67, loss = 0.08193996\n",
      "Iteration 68, loss = 0.08095978\n",
      "Iteration 69, loss = 0.07989488\n",
      "Iteration 70, loss = 0.07889880\n",
      "Iteration 71, loss = 0.07792618\n",
      "Iteration 72, loss = 0.07690374\n",
      "Iteration 73, loss = 0.07590126\n",
      "Iteration 74, loss = 0.07495996\n",
      "Iteration 75, loss = 0.07403416\n",
      "Iteration 76, loss = 0.07302084\n",
      "Iteration 77, loss = 0.07218163\n",
      "Iteration 78, loss = 0.07140092\n",
      "Iteration 79, loss = 0.07045543\n",
      "Iteration 80, loss = 0.06963952\n",
      "Iteration 81, loss = 0.06883327\n",
      "Iteration 82, loss = 0.06793436\n",
      "Iteration 83, loss = 0.06720198\n",
      "Iteration 84, loss = 0.06636900\n",
      "Iteration 85, loss = 0.06561950\n",
      "Iteration 86, loss = 0.06492453\n",
      "Iteration 87, loss = 0.06406140\n",
      "Iteration 88, loss = 0.06343605\n",
      "Iteration 89, loss = 0.06279793\n",
      "Iteration 90, loss = 0.06198246\n",
      "Iteration 91, loss = 0.06130363\n",
      "Iteration 92, loss = 0.06073245\n",
      "Iteration 93, loss = 0.06006125\n",
      "Iteration 94, loss = 0.05925523\n",
      "Iteration 95, loss = 0.05867613\n",
      "Iteration 96, loss = 0.05805898\n",
      "Iteration 97, loss = 0.05745633\n",
      "Iteration 98, loss = 0.05688364\n",
      "Iteration 99, loss = 0.05621844\n",
      "Iteration 100, loss = 0.05568840\n",
      "Iteration 101, loss = 0.05506200\n",
      "Iteration 102, loss = 0.05458848\n",
      "Iteration 103, loss = 0.05393865\n",
      "Iteration 104, loss = 0.05332599\n",
      "Iteration 105, loss = 0.05282543\n",
      "Iteration 106, loss = 0.05226120\n",
      "Iteration 107, loss = 0.05179005\n",
      "Iteration 108, loss = 0.05126674\n",
      "Iteration 109, loss = 0.05070017\n",
      "Iteration 110, loss = 0.05017717\n",
      "Iteration 111, loss = 0.04973540\n",
      "Iteration 112, loss = 0.04918138\n",
      "Iteration 113, loss = 0.04882070\n",
      "Iteration 114, loss = 0.04840277\n",
      "Iteration 115, loss = 0.04783363\n",
      "Iteration 116, loss = 0.04738578\n",
      "Iteration 117, loss = 0.04693072\n",
      "Iteration 118, loss = 0.04645379\n",
      "Iteration 119, loss = 0.04600735\n",
      "Iteration 120, loss = 0.04559822\n",
      "Iteration 121, loss = 0.04521415\n",
      "Iteration 122, loss = 0.04473908\n",
      "Iteration 123, loss = 0.04434055\n",
      "Iteration 124, loss = 0.04395220\n",
      "Iteration 125, loss = 0.04352104\n",
      "Iteration 126, loss = 0.04310807\n",
      "Iteration 127, loss = 0.04280860\n",
      "Iteration 128, loss = 0.04233571\n",
      "Iteration 129, loss = 0.04198298\n",
      "Iteration 130, loss = 0.04157755\n",
      "Iteration 131, loss = 0.04132345\n",
      "Iteration 132, loss = 0.04092415\n",
      "Iteration 133, loss = 0.04052135\n",
      "Iteration 134, loss = 0.04018390\n",
      "Iteration 135, loss = 0.03990710\n",
      "Iteration 136, loss = 0.03951652\n",
      "Iteration 137, loss = 0.03914566\n",
      "Iteration 138, loss = 0.03883401\n",
      "Iteration 139, loss = 0.03852432\n",
      "Iteration 140, loss = 0.03823907\n",
      "Iteration 141, loss = 0.03791237\n",
      "Iteration 142, loss = 0.03758549\n",
      "Iteration 143, loss = 0.03726175\n",
      "Iteration 144, loss = 0.03700773\n",
      "Iteration 145, loss = 0.03665796\n",
      "Iteration 146, loss = 0.03635112\n",
      "Iteration 147, loss = 0.03602865\n",
      "Iteration 148, loss = 0.03584849\n",
      "Iteration 149, loss = 0.03544951\n",
      "Iteration 150, loss = 0.03520372\n",
      "Iteration 151, loss = 0.03495658\n",
      "Iteration 152, loss = 0.03471441\n",
      "Iteration 153, loss = 0.03441916\n",
      "Iteration 154, loss = 0.03421396\n",
      "Iteration 155, loss = 0.03392303\n",
      "Iteration 156, loss = 0.03359141\n",
      "Iteration 157, loss = 0.03344487\n",
      "Iteration 158, loss = 0.03314119\n",
      "Iteration 159, loss = 0.03283218\n",
      "Iteration 160, loss = 0.03262178\n",
      "Iteration 161, loss = 0.03241342\n",
      "Iteration 162, loss = 0.03209677\n",
      "Iteration 163, loss = 0.03190942\n",
      "Iteration 164, loss = 0.03172138\n",
      "Iteration 165, loss = 0.03142369\n",
      "Iteration 166, loss = 0.03126567\n",
      "Iteration 167, loss = 0.03100753\n",
      "Iteration 168, loss = 0.03084041\n",
      "Iteration 169, loss = 0.03053485\n",
      "Iteration 170, loss = 0.03033354\n",
      "Iteration 171, loss = 0.03016177\n",
      "Iteration 172, loss = 0.02992585\n",
      "Iteration 173, loss = 0.02978131\n",
      "Iteration 174, loss = 0.02952586\n",
      "Iteration 175, loss = 0.02931917\n",
      "Iteration 176, loss = 0.02910696\n",
      "Iteration 177, loss = 0.02893793\n",
      "Iteration 178, loss = 0.02874802\n",
      "Iteration 179, loss = 0.02852347\n",
      "Iteration 180, loss = 0.02835451\n",
      "Iteration 181, loss = 0.02818438\n",
      "Iteration 182, loss = 0.02800821\n",
      "Iteration 183, loss = 0.02779234\n",
      "Iteration 184, loss = 0.02760607\n",
      "Iteration 185, loss = 0.02742969\n",
      "Iteration 186, loss = 0.02725349\n",
      "Iteration 187, loss = 0.02705703\n",
      "Iteration 188, loss = 0.02693910\n",
      "Iteration 189, loss = 0.02673715\n",
      "Iteration 190, loss = 0.02664238\n",
      "Iteration 191, loss = 0.02641819\n",
      "Iteration 192, loss = 0.02628633\n",
      "Iteration 193, loss = 0.02610554\n",
      "Iteration 194, loss = 0.02593225\n",
      "Iteration 195, loss = 0.02577572\n",
      "Iteration 196, loss = 0.02556676\n",
      "Iteration 197, loss = 0.02546442\n",
      "Iteration 198, loss = 0.02535987\n",
      "Iteration 199, loss = 0.02518444\n",
      "Iteration 200, loss = 0.02498045\n",
      "[CV] END activation=relu, alpha=0.001, batch_size=50, hidden_layer_sizes=(100,), solver=sgd; total time= 6.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.04188712\n",
      "Iteration 2, loss = 0.47733878\n",
      "Iteration 3, loss = 0.39153785\n",
      "Iteration 4, loss = 0.35205955\n",
      "Iteration 5, loss = 0.32659932\n",
      "Iteration 6, loss = 0.30760910\n",
      "Iteration 7, loss = 0.29244830\n",
      "Iteration 8, loss = 0.27974686\n",
      "Iteration 9, loss = 0.26845572\n",
      "Iteration 10, loss = 0.25855343\n",
      "Iteration 11, loss = 0.24930717\n",
      "Iteration 12, loss = 0.24096914\n",
      "Iteration 13, loss = 0.23347818\n",
      "Iteration 14, loss = 0.22638281\n",
      "Iteration 15, loss = 0.21959104\n",
      "Iteration 16, loss = 0.21338556\n",
      "Iteration 17, loss = 0.20759735\n",
      "Iteration 18, loss = 0.20203000\n",
      "Iteration 19, loss = 0.19699545\n",
      "Iteration 20, loss = 0.19199163\n",
      "Iteration 21, loss = 0.18721241\n",
      "Iteration 22, loss = 0.18280330\n",
      "Iteration 23, loss = 0.17826325\n",
      "Iteration 24, loss = 0.17427047\n",
      "Iteration 25, loss = 0.17040668\n",
      "Iteration 26, loss = 0.16647016\n",
      "Iteration 27, loss = 0.16288994\n",
      "Iteration 28, loss = 0.15936655\n",
      "Iteration 29, loss = 0.15613528\n",
      "Iteration 30, loss = 0.15273436\n",
      "Iteration 31, loss = 0.14974278\n",
      "Iteration 32, loss = 0.14669347\n",
      "Iteration 33, loss = 0.14402685\n",
      "Iteration 34, loss = 0.14097181\n",
      "Iteration 35, loss = 0.13846478\n",
      "Iteration 36, loss = 0.13594222\n",
      "Iteration 37, loss = 0.13347786\n",
      "Iteration 38, loss = 0.13103504\n",
      "Iteration 39, loss = 0.12861692\n",
      "Iteration 40, loss = 0.12649102\n",
      "Iteration 41, loss = 0.12425079\n",
      "Iteration 42, loss = 0.12192053\n",
      "Iteration 43, loss = 0.12010294\n",
      "Iteration 44, loss = 0.11813000\n",
      "Iteration 45, loss = 0.11608880\n",
      "Iteration 46, loss = 0.11428179\n",
      "Iteration 47, loss = 0.11229794\n",
      "Iteration 48, loss = 0.11038155\n",
      "Iteration 49, loss = 0.10885387\n",
      "Iteration 50, loss = 0.10725328\n",
      "Iteration 51, loss = 0.10541991\n",
      "Iteration 52, loss = 0.10412652\n",
      "Iteration 53, loss = 0.10229603\n",
      "Iteration 54, loss = 0.10077820\n",
      "Iteration 55, loss = 0.09928286\n",
      "Iteration 56, loss = 0.09772598\n",
      "Iteration 57, loss = 0.09641839\n",
      "Iteration 58, loss = 0.09516531\n",
      "Iteration 59, loss = 0.09361951\n",
      "Iteration 60, loss = 0.09248622\n",
      "Iteration 61, loss = 0.09106836\n",
      "Iteration 62, loss = 0.08976547\n",
      "Iteration 63, loss = 0.08876144\n",
      "Iteration 64, loss = 0.08746681\n",
      "Iteration 65, loss = 0.08628060\n",
      "Iteration 66, loss = 0.08512621\n",
      "Iteration 67, loss = 0.08399863\n",
      "Iteration 68, loss = 0.08288720\n",
      "Iteration 69, loss = 0.08186899\n",
      "Iteration 70, loss = 0.08081912\n",
      "Iteration 71, loss = 0.07969630\n",
      "Iteration 72, loss = 0.07892253\n",
      "Iteration 73, loss = 0.07798566\n",
      "Iteration 74, loss = 0.07690652\n",
      "Iteration 75, loss = 0.07595129\n",
      "Iteration 76, loss = 0.07503623\n",
      "Iteration 77, loss = 0.07393548\n",
      "Iteration 78, loss = 0.07323388\n",
      "Iteration 79, loss = 0.07239151\n",
      "Iteration 80, loss = 0.07145879\n",
      "Iteration 81, loss = 0.07064095\n",
      "Iteration 82, loss = 0.06984531\n",
      "Iteration 83, loss = 0.06906841\n",
      "Iteration 84, loss = 0.06826988\n",
      "Iteration 85, loss = 0.06751006\n",
      "Iteration 86, loss = 0.06670274\n",
      "Iteration 87, loss = 0.06592547\n",
      "Iteration 88, loss = 0.06527020\n",
      "Iteration 89, loss = 0.06453033\n",
      "Iteration 90, loss = 0.06375374\n",
      "Iteration 91, loss = 0.06308293\n",
      "Iteration 92, loss = 0.06247743\n",
      "Iteration 93, loss = 0.06174139\n",
      "Iteration 94, loss = 0.06114201\n",
      "Iteration 95, loss = 0.06039654\n",
      "Iteration 96, loss = 0.05983061\n",
      "Iteration 97, loss = 0.05908726\n",
      "Iteration 98, loss = 0.05867952\n",
      "Iteration 99, loss = 0.05796143\n",
      "Iteration 100, loss = 0.05730768\n",
      "Iteration 101, loss = 0.05688280\n",
      "Iteration 102, loss = 0.05623285\n",
      "Iteration 103, loss = 0.05573769\n",
      "Iteration 104, loss = 0.05508111\n",
      "Iteration 105, loss = 0.05459443\n",
      "Iteration 106, loss = 0.05399554\n",
      "Iteration 107, loss = 0.05348792\n",
      "Iteration 108, loss = 0.05291618\n",
      "Iteration 109, loss = 0.05237885\n",
      "Iteration 110, loss = 0.05191977\n",
      "Iteration 111, loss = 0.05149111\n",
      "Iteration 112, loss = 0.05098933\n",
      "Iteration 113, loss = 0.05041711\n",
      "Iteration 114, loss = 0.05002796\n",
      "Iteration 115, loss = 0.04957433\n",
      "Iteration 116, loss = 0.04906572\n",
      "Iteration 117, loss = 0.04861889\n",
      "Iteration 118, loss = 0.04826384\n",
      "Iteration 119, loss = 0.04777176\n",
      "Iteration 120, loss = 0.04724937\n",
      "Iteration 121, loss = 0.04681159\n",
      "Iteration 122, loss = 0.04644893\n",
      "Iteration 123, loss = 0.04601758\n",
      "Iteration 124, loss = 0.04557165\n",
      "Iteration 125, loss = 0.04520258\n",
      "Iteration 126, loss = 0.04487314\n",
      "Iteration 127, loss = 0.04441662\n",
      "Iteration 128, loss = 0.04403611\n",
      "Iteration 129, loss = 0.04356654\n",
      "Iteration 130, loss = 0.04323921\n",
      "Iteration 131, loss = 0.04292196\n",
      "Iteration 132, loss = 0.04254016\n",
      "Iteration 133, loss = 0.04210228\n",
      "Iteration 134, loss = 0.04178529\n",
      "Iteration 135, loss = 0.04140794\n",
      "Iteration 136, loss = 0.04107570\n",
      "Iteration 137, loss = 0.04073422\n",
      "Iteration 138, loss = 0.04041989\n",
      "Iteration 139, loss = 0.04006418\n",
      "Iteration 140, loss = 0.03971445\n",
      "Iteration 141, loss = 0.03950540\n",
      "Iteration 142, loss = 0.03906370\n",
      "Iteration 143, loss = 0.03875717\n",
      "Iteration 144, loss = 0.03857809\n",
      "Iteration 145, loss = 0.03812657\n",
      "Iteration 146, loss = 0.03782855\n",
      "Iteration 147, loss = 0.03761015\n",
      "Iteration 148, loss = 0.03734048\n",
      "Iteration 149, loss = 0.03696985\n",
      "Iteration 150, loss = 0.03663671\n",
      "Iteration 151, loss = 0.03635037\n",
      "Iteration 152, loss = 0.03611284\n",
      "Iteration 153, loss = 0.03592410\n",
      "Iteration 154, loss = 0.03560435\n",
      "Iteration 155, loss = 0.03521204\n",
      "Iteration 156, loss = 0.03502924\n",
      "Iteration 157, loss = 0.03473820\n",
      "Iteration 158, loss = 0.03451230\n",
      "Iteration 159, loss = 0.03422518\n",
      "Iteration 160, loss = 0.03397814\n",
      "Iteration 161, loss = 0.03377575\n",
      "Iteration 162, loss = 0.03342352\n",
      "Iteration 163, loss = 0.03327882\n",
      "Iteration 164, loss = 0.03299585\n",
      "Iteration 165, loss = 0.03278155\n",
      "Iteration 166, loss = 0.03250123\n",
      "Iteration 167, loss = 0.03230115\n",
      "Iteration 168, loss = 0.03205295\n",
      "Iteration 169, loss = 0.03184830\n",
      "Iteration 170, loss = 0.03166817\n",
      "Iteration 171, loss = 0.03142250\n",
      "Iteration 172, loss = 0.03120810\n",
      "Iteration 173, loss = 0.03089711\n",
      "Iteration 174, loss = 0.03076569\n",
      "Iteration 175, loss = 0.03056630\n",
      "Iteration 176, loss = 0.03033038\n",
      "Iteration 177, loss = 0.03009348\n",
      "Iteration 178, loss = 0.02992932\n",
      "Iteration 179, loss = 0.02972464\n",
      "Iteration 180, loss = 0.02952285\n",
      "Iteration 181, loss = 0.02932650\n",
      "Iteration 182, loss = 0.02915818\n",
      "Iteration 183, loss = 0.02896079\n",
      "Iteration 184, loss = 0.02880865\n",
      "Iteration 185, loss = 0.02860426\n",
      "Iteration 186, loss = 0.02839726\n",
      "Iteration 187, loss = 0.02827647\n",
      "Iteration 188, loss = 0.02803884\n",
      "Iteration 189, loss = 0.02784466\n",
      "Iteration 190, loss = 0.02768664\n",
      "Iteration 191, loss = 0.02751168\n",
      "Iteration 192, loss = 0.02731996\n",
      "Iteration 193, loss = 0.02715198\n",
      "Iteration 194, loss = 0.02702074\n",
      "Iteration 195, loss = 0.02687275\n",
      "Iteration 196, loss = 0.02669712\n",
      "Iteration 197, loss = 0.02650960\n",
      "Iteration 198, loss = 0.02635802\n",
      "Iteration 199, loss = 0.02618039\n",
      "Iteration 200, loss = 0.02605913\n",
      "[CV] END activation=relu, alpha=0.001, batch_size=50, hidden_layer_sizes=(100,), solver=sgd; total time= 6.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.02118326\n",
      "Iteration 2, loss = 0.46880016\n",
      "Iteration 3, loss = 0.38386148\n",
      "Iteration 4, loss = 0.34468682\n",
      "Iteration 5, loss = 0.31983939\n",
      "Iteration 6, loss = 0.30195216\n",
      "Iteration 7, loss = 0.28765663\n",
      "Iteration 8, loss = 0.27545439\n",
      "Iteration 9, loss = 0.26492380\n",
      "Iteration 10, loss = 0.25544432\n",
      "Iteration 11, loss = 0.24701693\n",
      "Iteration 12, loss = 0.23948002\n",
      "Iteration 13, loss = 0.23227649\n",
      "Iteration 14, loss = 0.22539553\n",
      "Iteration 15, loss = 0.21906038\n",
      "Iteration 16, loss = 0.21306089\n",
      "Iteration 17, loss = 0.20757694\n",
      "Iteration 18, loss = 0.20206051\n",
      "Iteration 19, loss = 0.19713098\n",
      "Iteration 20, loss = 0.19241802\n",
      "Iteration 21, loss = 0.18786282\n",
      "Iteration 22, loss = 0.18361451\n",
      "Iteration 23, loss = 0.17933446\n",
      "Iteration 24, loss = 0.17521450\n",
      "Iteration 25, loss = 0.17178884\n",
      "Iteration 26, loss = 0.16775133\n",
      "Iteration 27, loss = 0.16437954\n",
      "Iteration 28, loss = 0.16084830\n",
      "Iteration 29, loss = 0.15777700\n",
      "Iteration 30, loss = 0.15468166\n",
      "Iteration 31, loss = 0.15143817\n",
      "Iteration 32, loss = 0.14853480\n",
      "Iteration 33, loss = 0.14581062\n",
      "Iteration 34, loss = 0.14308358\n",
      "Iteration 35, loss = 0.14046754\n",
      "Iteration 36, loss = 0.13798055\n",
      "Iteration 37, loss = 0.13550761\n",
      "Iteration 38, loss = 0.13306631\n",
      "Iteration 39, loss = 0.13060036\n",
      "Iteration 40, loss = 0.12822682\n",
      "Iteration 41, loss = 0.12624693\n",
      "Iteration 42, loss = 0.12412271\n",
      "Iteration 43, loss = 0.12196318\n",
      "Iteration 44, loss = 0.11996391\n",
      "Iteration 45, loss = 0.11790906\n",
      "Iteration 46, loss = 0.11610041\n",
      "Iteration 47, loss = 0.11422896\n",
      "Iteration 48, loss = 0.11245949\n",
      "Iteration 49, loss = 0.11079843\n",
      "Iteration 50, loss = 0.10889098\n",
      "Iteration 51, loss = 0.10735159\n",
      "Iteration 52, loss = 0.10569562\n",
      "Iteration 53, loss = 0.10417049\n",
      "Iteration 54, loss = 0.10270625\n",
      "Iteration 55, loss = 0.10111541\n",
      "Iteration 56, loss = 0.09959911\n",
      "Iteration 57, loss = 0.09825038\n",
      "Iteration 58, loss = 0.09670578\n",
      "Iteration 59, loss = 0.09544269\n",
      "Iteration 60, loss = 0.09410562\n",
      "Iteration 61, loss = 0.09281329\n",
      "Iteration 62, loss = 0.09158606\n",
      "Iteration 63, loss = 0.09038303\n",
      "Iteration 64, loss = 0.08920150\n",
      "Iteration 65, loss = 0.08796899\n",
      "Iteration 66, loss = 0.08677809\n",
      "Iteration 67, loss = 0.08564761\n",
      "Iteration 68, loss = 0.08455535\n",
      "Iteration 69, loss = 0.08365106\n",
      "Iteration 70, loss = 0.08259220\n",
      "Iteration 71, loss = 0.08145187\n",
      "Iteration 72, loss = 0.08040478\n",
      "Iteration 73, loss = 0.07954208\n",
      "Iteration 74, loss = 0.07844456\n",
      "Iteration 75, loss = 0.07745055\n",
      "Iteration 76, loss = 0.07670420\n",
      "Iteration 77, loss = 0.07573630\n",
      "Iteration 78, loss = 0.07492034\n",
      "Iteration 79, loss = 0.07388526\n",
      "Iteration 80, loss = 0.07316533\n",
      "Iteration 81, loss = 0.07243148\n",
      "Iteration 82, loss = 0.07151892\n",
      "Iteration 83, loss = 0.07067441\n",
      "Iteration 84, loss = 0.06997687\n",
      "Iteration 85, loss = 0.06909269\n",
      "Iteration 86, loss = 0.06826458\n",
      "Iteration 87, loss = 0.06759803\n",
      "Iteration 88, loss = 0.06689150\n",
      "Iteration 89, loss = 0.06623367\n",
      "Iteration 90, loss = 0.06542687\n",
      "Iteration 91, loss = 0.06463513\n",
      "Iteration 92, loss = 0.06408282\n",
      "Iteration 93, loss = 0.06338837\n",
      "Iteration 94, loss = 0.06260061\n",
      "Iteration 95, loss = 0.06198605\n",
      "Iteration 96, loss = 0.06136457\n",
      "Iteration 97, loss = 0.06071582\n",
      "Iteration 98, loss = 0.06010901\n",
      "Iteration 99, loss = 0.05946742\n",
      "Iteration 100, loss = 0.05889937\n",
      "Iteration 101, loss = 0.05825589\n",
      "Iteration 102, loss = 0.05769005\n",
      "Iteration 103, loss = 0.05703753\n",
      "Iteration 104, loss = 0.05658976\n",
      "Iteration 105, loss = 0.05600029\n",
      "Iteration 106, loss = 0.05540338\n",
      "Iteration 107, loss = 0.05499290\n",
      "Iteration 108, loss = 0.05421177\n",
      "Iteration 109, loss = 0.05383524\n",
      "Iteration 110, loss = 0.05329023\n",
      "Iteration 111, loss = 0.05285394\n",
      "Iteration 112, loss = 0.05221938\n",
      "Iteration 113, loss = 0.05188679\n",
      "Iteration 114, loss = 0.05136213\n",
      "Iteration 115, loss = 0.05082828\n",
      "Iteration 116, loss = 0.05042478\n",
      "Iteration 117, loss = 0.04987913\n",
      "Iteration 118, loss = 0.04947168\n",
      "Iteration 119, loss = 0.04893805\n",
      "Iteration 120, loss = 0.04844296\n",
      "Iteration 121, loss = 0.04806128\n",
      "Iteration 122, loss = 0.04761318\n",
      "Iteration 123, loss = 0.04724888\n",
      "Iteration 124, loss = 0.04684028\n",
      "Iteration 125, loss = 0.04641324\n",
      "Iteration 126, loss = 0.04598916\n",
      "Iteration 127, loss = 0.04551346\n",
      "Iteration 128, loss = 0.04514948\n",
      "Iteration 129, loss = 0.04477342\n",
      "Iteration 130, loss = 0.04440294\n",
      "Iteration 131, loss = 0.04406250\n",
      "Iteration 132, loss = 0.04357243\n",
      "Iteration 133, loss = 0.04318575\n",
      "Iteration 134, loss = 0.04293186\n",
      "Iteration 135, loss = 0.04251533\n",
      "Iteration 136, loss = 0.04208347\n",
      "Iteration 137, loss = 0.04181059\n",
      "Iteration 138, loss = 0.04136551\n",
      "Iteration 139, loss = 0.04106676\n",
      "Iteration 140, loss = 0.04085125\n",
      "Iteration 141, loss = 0.04041933\n",
      "Iteration 142, loss = 0.04013227\n",
      "Iteration 143, loss = 0.03974259\n",
      "Iteration 144, loss = 0.03948755\n",
      "Iteration 145, loss = 0.03910399\n",
      "Iteration 146, loss = 0.03881054\n",
      "Iteration 147, loss = 0.03853518\n",
      "Iteration 148, loss = 0.03822257\n",
      "Iteration 149, loss = 0.03784474\n",
      "Iteration 150, loss = 0.03763998\n",
      "Iteration 151, loss = 0.03731294\n",
      "Iteration 152, loss = 0.03699772\n",
      "Iteration 153, loss = 0.03669834\n",
      "Iteration 154, loss = 0.03645388\n",
      "Iteration 155, loss = 0.03609900\n",
      "Iteration 156, loss = 0.03589335\n",
      "Iteration 157, loss = 0.03563679\n",
      "Iteration 158, loss = 0.03537263\n",
      "Iteration 159, loss = 0.03503461\n",
      "Iteration 160, loss = 0.03475952\n",
      "Iteration 161, loss = 0.03453302\n",
      "Iteration 162, loss = 0.03428184\n",
      "Iteration 163, loss = 0.03409689\n",
      "Iteration 164, loss = 0.03378244\n",
      "Iteration 165, loss = 0.03357053\n",
      "Iteration 166, loss = 0.03327549\n",
      "Iteration 167, loss = 0.03305556\n",
      "Iteration 168, loss = 0.03280957\n",
      "Iteration 169, loss = 0.03262180\n",
      "Iteration 170, loss = 0.03231536\n",
      "Iteration 171, loss = 0.03216500\n",
      "Iteration 172, loss = 0.03191992\n",
      "Iteration 173, loss = 0.03167384\n",
      "Iteration 174, loss = 0.03142273\n",
      "Iteration 175, loss = 0.03125692\n",
      "Iteration 176, loss = 0.03109008\n",
      "Iteration 177, loss = 0.03077472\n",
      "Iteration 178, loss = 0.03063659\n",
      "Iteration 179, loss = 0.03034169\n",
      "Iteration 180, loss = 0.03021223\n",
      "Iteration 181, loss = 0.02996433\n",
      "Iteration 182, loss = 0.02982779\n",
      "Iteration 183, loss = 0.02958665\n",
      "Iteration 184, loss = 0.02942808\n",
      "Iteration 185, loss = 0.02920907\n",
      "Iteration 186, loss = 0.02900186\n",
      "Iteration 187, loss = 0.02884701\n",
      "Iteration 188, loss = 0.02865238\n",
      "Iteration 189, loss = 0.02843880\n",
      "Iteration 190, loss = 0.02827410\n",
      "Iteration 191, loss = 0.02812015\n",
      "Iteration 192, loss = 0.02792540\n",
      "Iteration 193, loss = 0.02772667\n",
      "Iteration 194, loss = 0.02749208\n",
      "Iteration 195, loss = 0.02741931\n",
      "Iteration 196, loss = 0.02724658\n",
      "Iteration 197, loss = 0.02707372\n",
      "Iteration 198, loss = 0.02685900\n",
      "Iteration 199, loss = 0.02675319\n",
      "Iteration 200, loss = 0.02656381\n",
      "[CV] END activation=relu, alpha=0.001, batch_size=50, hidden_layer_sizes=(100,), solver=sgd; total time= 6.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.05542375\n",
      "Iteration 2, loss = 0.47499126\n",
      "Iteration 3, loss = 0.38701435\n",
      "Iteration 4, loss = 0.34713678\n",
      "Iteration 5, loss = 0.32163914\n",
      "Iteration 6, loss = 0.30292890\n",
      "Iteration 7, loss = 0.28789096\n",
      "Iteration 8, loss = 0.27517439\n",
      "Iteration 9, loss = 0.26429406\n",
      "Iteration 10, loss = 0.25446043\n",
      "Iteration 11, loss = 0.24534126\n",
      "Iteration 12, loss = 0.23744287\n",
      "Iteration 13, loss = 0.23007245\n",
      "Iteration 14, loss = 0.22320496\n",
      "Iteration 15, loss = 0.21661380\n",
      "Iteration 16, loss = 0.21069285\n",
      "Iteration 17, loss = 0.20511485\n",
      "Iteration 18, loss = 0.19989694\n",
      "Iteration 19, loss = 0.19467655\n",
      "Iteration 20, loss = 0.18985955\n",
      "Iteration 21, loss = 0.18511893\n",
      "Iteration 22, loss = 0.18070741\n",
      "Iteration 23, loss = 0.17650524\n",
      "Iteration 24, loss = 0.17261737\n",
      "Iteration 25, loss = 0.16865098\n",
      "Iteration 26, loss = 0.16494197\n",
      "Iteration 27, loss = 0.16118788\n",
      "Iteration 28, loss = 0.15786861\n",
      "Iteration 29, loss = 0.15466049\n",
      "Iteration 30, loss = 0.15139476\n",
      "Iteration 31, loss = 0.14832107\n",
      "Iteration 32, loss = 0.14550415\n",
      "Iteration 33, loss = 0.14275021\n",
      "Iteration 34, loss = 0.13987288\n",
      "Iteration 35, loss = 0.13736842\n",
      "Iteration 36, loss = 0.13481579\n",
      "Iteration 37, loss = 0.13253997\n",
      "Iteration 38, loss = 0.12992970\n",
      "Iteration 39, loss = 0.12759452\n",
      "Iteration 40, loss = 0.12557634\n",
      "Iteration 41, loss = 0.12334241\n",
      "Iteration 42, loss = 0.12141380\n",
      "Iteration 43, loss = 0.11927049\n",
      "Iteration 44, loss = 0.11749647\n",
      "Iteration 45, loss = 0.11554052\n",
      "Iteration 46, loss = 0.11364683\n",
      "Iteration 47, loss = 0.11185978\n",
      "Iteration 48, loss = 0.11008111\n",
      "Iteration 49, loss = 0.10855951\n",
      "Iteration 50, loss = 0.10678150\n",
      "Iteration 51, loss = 0.10516163\n",
      "Iteration 52, loss = 0.10377719\n",
      "Iteration 53, loss = 0.10210043\n",
      "Iteration 54, loss = 0.10059960\n",
      "Iteration 55, loss = 0.09932012\n",
      "Iteration 56, loss = 0.09778794\n",
      "Iteration 57, loss = 0.09643391\n",
      "Iteration 58, loss = 0.09506279\n",
      "Iteration 59, loss = 0.09384352\n",
      "Iteration 60, loss = 0.09252673\n",
      "Iteration 61, loss = 0.09131023\n",
      "Iteration 62, loss = 0.09003441\n",
      "Iteration 63, loss = 0.08880065\n",
      "Iteration 64, loss = 0.08756872\n",
      "Iteration 65, loss = 0.08651174\n",
      "Iteration 66, loss = 0.08522406\n",
      "Iteration 67, loss = 0.08438904\n",
      "Iteration 68, loss = 0.08327534\n",
      "Iteration 69, loss = 0.08222769\n",
      "Iteration 70, loss = 0.08120883\n",
      "Iteration 71, loss = 0.08020563\n",
      "Iteration 72, loss = 0.07915663\n",
      "Iteration 73, loss = 0.07824699\n",
      "Iteration 74, loss = 0.07732941\n",
      "Iteration 75, loss = 0.07624897\n",
      "Iteration 76, loss = 0.07555688\n",
      "Iteration 77, loss = 0.07456795\n",
      "Iteration 78, loss = 0.07373335\n",
      "Iteration 79, loss = 0.07286506\n",
      "Iteration 80, loss = 0.07209400\n",
      "Iteration 81, loss = 0.07125969\n",
      "Iteration 82, loss = 0.07033096\n",
      "Iteration 83, loss = 0.06954530\n",
      "Iteration 84, loss = 0.06884910\n",
      "Iteration 85, loss = 0.06805746\n",
      "Iteration 86, loss = 0.06721751\n",
      "Iteration 87, loss = 0.06648192\n",
      "Iteration 88, loss = 0.06586331\n",
      "Iteration 89, loss = 0.06507634\n",
      "Iteration 90, loss = 0.06446006\n",
      "Iteration 91, loss = 0.06383116\n",
      "Iteration 92, loss = 0.06313155\n",
      "Iteration 93, loss = 0.06230183\n",
      "Iteration 94, loss = 0.06174771\n",
      "Iteration 95, loss = 0.06113993\n",
      "Iteration 96, loss = 0.06041911\n",
      "Iteration 97, loss = 0.05986266\n",
      "Iteration 98, loss = 0.05918383\n",
      "Iteration 99, loss = 0.05866677\n",
      "Iteration 100, loss = 0.05792340\n",
      "Iteration 101, loss = 0.05746930\n",
      "Iteration 102, loss = 0.05676148\n",
      "Iteration 103, loss = 0.05629886\n",
      "Iteration 104, loss = 0.05569938\n",
      "Iteration 105, loss = 0.05522050\n",
      "Iteration 106, loss = 0.05463123\n",
      "Iteration 107, loss = 0.05410975\n",
      "Iteration 108, loss = 0.05352094\n",
      "Iteration 109, loss = 0.05307614\n",
      "Iteration 110, loss = 0.05261600\n",
      "Iteration 111, loss = 0.05212682\n",
      "Iteration 112, loss = 0.05158449\n",
      "Iteration 113, loss = 0.05109605\n",
      "Iteration 114, loss = 0.05066354\n",
      "Iteration 115, loss = 0.05026960\n",
      "Iteration 116, loss = 0.04966423\n",
      "Iteration 117, loss = 0.04922879\n",
      "Iteration 118, loss = 0.04880615\n",
      "Iteration 119, loss = 0.04840153\n",
      "Iteration 120, loss = 0.04795085\n",
      "Iteration 121, loss = 0.04746487\n",
      "Iteration 122, loss = 0.04704360\n",
      "Iteration 123, loss = 0.04668845\n",
      "Iteration 124, loss = 0.04624236\n",
      "Iteration 125, loss = 0.04581839\n",
      "Iteration 126, loss = 0.04534515\n",
      "Iteration 127, loss = 0.04509425\n",
      "Iteration 128, loss = 0.04471742\n",
      "Iteration 129, loss = 0.04433312\n",
      "Iteration 130, loss = 0.04390301\n",
      "Iteration 131, loss = 0.04355169\n",
      "Iteration 132, loss = 0.04323480\n",
      "Iteration 133, loss = 0.04277389\n",
      "Iteration 134, loss = 0.04243483\n",
      "Iteration 135, loss = 0.04213491\n",
      "Iteration 136, loss = 0.04171033\n",
      "Iteration 137, loss = 0.04140962\n",
      "Iteration 138, loss = 0.04103653\n",
      "Iteration 139, loss = 0.04077564\n",
      "Iteration 140, loss = 0.04032244\n",
      "Iteration 141, loss = 0.04010278\n",
      "Iteration 142, loss = 0.03970138\n",
      "Iteration 143, loss = 0.03943589\n",
      "Iteration 144, loss = 0.03911924\n",
      "Iteration 145, loss = 0.03883258\n",
      "Iteration 146, loss = 0.03851257\n",
      "Iteration 147, loss = 0.03818191\n",
      "Iteration 148, loss = 0.03791798\n",
      "Iteration 149, loss = 0.03760611\n",
      "Iteration 150, loss = 0.03729587\n",
      "Iteration 151, loss = 0.03702578\n",
      "Iteration 152, loss = 0.03681053\n",
      "Iteration 153, loss = 0.03650826\n",
      "Iteration 154, loss = 0.03627556\n",
      "Iteration 155, loss = 0.03600766\n",
      "Iteration 156, loss = 0.03572829\n",
      "Iteration 157, loss = 0.03544327\n",
      "Iteration 158, loss = 0.03519409\n",
      "Iteration 159, loss = 0.03492039\n",
      "Iteration 160, loss = 0.03470216\n",
      "Iteration 161, loss = 0.03438617\n",
      "Iteration 162, loss = 0.03411229\n",
      "Iteration 163, loss = 0.03394267\n",
      "Iteration 164, loss = 0.03371760\n",
      "Iteration 165, loss = 0.03352271\n",
      "Iteration 166, loss = 0.03324758\n",
      "Iteration 167, loss = 0.03299783\n",
      "Iteration 168, loss = 0.03276462\n",
      "Iteration 169, loss = 0.03254679\n",
      "Iteration 170, loss = 0.03228738\n",
      "Iteration 171, loss = 0.03212012\n",
      "Iteration 172, loss = 0.03184250\n",
      "Iteration 173, loss = 0.03171838\n",
      "Iteration 174, loss = 0.03149097\n",
      "Iteration 175, loss = 0.03127173\n",
      "Iteration 176, loss = 0.03102944\n",
      "Iteration 177, loss = 0.03087647\n",
      "Iteration 178, loss = 0.03062355\n",
      "Iteration 179, loss = 0.03041006\n",
      "Iteration 180, loss = 0.03025175\n",
      "Iteration 181, loss = 0.03005921\n",
      "Iteration 182, loss = 0.02981876\n",
      "Iteration 183, loss = 0.02960161\n",
      "Iteration 184, loss = 0.02946892\n",
      "Iteration 185, loss = 0.02928134\n",
      "Iteration 186, loss = 0.02910608\n",
      "Iteration 187, loss = 0.02892249\n",
      "Iteration 188, loss = 0.02875166\n",
      "Iteration 189, loss = 0.02857575\n",
      "Iteration 190, loss = 0.02842866\n",
      "Iteration 191, loss = 0.02823573\n",
      "Iteration 192, loss = 0.02807565\n",
      "Iteration 193, loss = 0.02787422\n",
      "Iteration 194, loss = 0.02774055\n",
      "Iteration 195, loss = 0.02750950\n",
      "Iteration 196, loss = 0.02740875\n",
      "Iteration 197, loss = 0.02724095\n",
      "Iteration 198, loss = 0.02701047\n",
      "Iteration 199, loss = 0.02695069\n",
      "Iteration 200, loss = 0.02674685\n",
      "[CV] END activation=relu, alpha=0.001, batch_size=50, hidden_layer_sizes=(100,), solver=sgd; total time=16.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.06411355\n",
      "Iteration 2, loss = 0.47942593\n",
      "Iteration 3, loss = 0.39194098\n",
      "Iteration 4, loss = 0.35133974\n",
      "Iteration 5, loss = 0.32583437\n",
      "Iteration 6, loss = 0.30712339\n",
      "Iteration 7, loss = 0.29225327\n",
      "Iteration 8, loss = 0.27926828\n",
      "Iteration 9, loss = 0.26818146\n",
      "Iteration 10, loss = 0.25810533\n",
      "Iteration 11, loss = 0.24928852\n",
      "Iteration 12, loss = 0.24068201\n",
      "Iteration 13, loss = 0.23305832\n",
      "Iteration 14, loss = 0.22567346\n",
      "Iteration 15, loss = 0.21896345\n",
      "Iteration 16, loss = 0.21260116\n",
      "Iteration 17, loss = 0.20659269\n",
      "Iteration 18, loss = 0.20072163\n",
      "Iteration 19, loss = 0.19570035\n",
      "Iteration 20, loss = 0.19051478\n",
      "Iteration 21, loss = 0.18590403\n",
      "Iteration 22, loss = 0.18113772\n",
      "Iteration 23, loss = 0.17681389\n",
      "Iteration 24, loss = 0.17247936\n",
      "Iteration 25, loss = 0.16856719\n",
      "Iteration 26, loss = 0.16480270\n",
      "Iteration 27, loss = 0.16105210\n",
      "Iteration 28, loss = 0.15769317\n",
      "Iteration 29, loss = 0.15432093\n",
      "Iteration 30, loss = 0.15139486\n",
      "Iteration 31, loss = 0.14808906\n",
      "Iteration 32, loss = 0.14510997\n",
      "Iteration 33, loss = 0.14241764\n",
      "Iteration 34, loss = 0.13955898\n",
      "Iteration 35, loss = 0.13703809\n",
      "Iteration 36, loss = 0.13440888\n",
      "Iteration 37, loss = 0.13194911\n",
      "Iteration 38, loss = 0.12952306\n",
      "Iteration 39, loss = 0.12730130\n",
      "Iteration 40, loss = 0.12493258\n",
      "Iteration 41, loss = 0.12285715\n",
      "Iteration 42, loss = 0.12060400\n",
      "Iteration 43, loss = 0.11877936\n",
      "Iteration 44, loss = 0.11676671\n",
      "Iteration 45, loss = 0.11477208\n",
      "Iteration 46, loss = 0.11310557\n",
      "Iteration 47, loss = 0.11123340\n",
      "Iteration 48, loss = 0.10928795\n",
      "Iteration 49, loss = 0.10765876\n",
      "Iteration 50, loss = 0.10610156\n",
      "Iteration 51, loss = 0.10443216\n",
      "Iteration 52, loss = 0.10296371\n",
      "Iteration 53, loss = 0.10136302\n",
      "Iteration 54, loss = 0.09994209\n",
      "Iteration 55, loss = 0.09845028\n",
      "Iteration 56, loss = 0.09699338\n",
      "Iteration 57, loss = 0.09553218\n",
      "Iteration 58, loss = 0.09427304\n",
      "Iteration 59, loss = 0.09307087\n",
      "Iteration 60, loss = 0.09160956\n",
      "Iteration 61, loss = 0.09036015\n",
      "Iteration 62, loss = 0.08905950\n",
      "Iteration 63, loss = 0.08796564\n",
      "Iteration 64, loss = 0.08668311\n",
      "Iteration 65, loss = 0.08581531\n",
      "Iteration 66, loss = 0.08454129\n",
      "Iteration 67, loss = 0.08350883\n",
      "Iteration 68, loss = 0.08237069\n",
      "Iteration 69, loss = 0.08132275\n",
      "Iteration 70, loss = 0.08039222\n",
      "Iteration 71, loss = 0.07934979\n",
      "Iteration 72, loss = 0.07830149\n",
      "Iteration 73, loss = 0.07733797\n",
      "Iteration 74, loss = 0.07644661\n",
      "Iteration 75, loss = 0.07549933\n",
      "Iteration 76, loss = 0.07469207\n",
      "Iteration 77, loss = 0.07371838\n",
      "Iteration 78, loss = 0.07288037\n",
      "Iteration 79, loss = 0.07208921\n",
      "Iteration 80, loss = 0.07120452\n",
      "Iteration 81, loss = 0.07034345\n",
      "Iteration 82, loss = 0.06953774\n",
      "Iteration 83, loss = 0.06878993\n",
      "Iteration 84, loss = 0.06795938\n",
      "Iteration 85, loss = 0.06719686\n",
      "Iteration 86, loss = 0.06643591\n",
      "Iteration 87, loss = 0.06568973\n",
      "Iteration 88, loss = 0.06500141\n",
      "Iteration 89, loss = 0.06437928\n",
      "Iteration 90, loss = 0.06360139\n",
      "Iteration 91, loss = 0.06284038\n",
      "Iteration 92, loss = 0.06230701\n",
      "Iteration 93, loss = 0.06152118\n",
      "Iteration 94, loss = 0.06086302\n",
      "Iteration 95, loss = 0.06039939\n",
      "Iteration 96, loss = 0.05967477\n",
      "Iteration 97, loss = 0.05904394\n",
      "Iteration 98, loss = 0.05841998\n",
      "Iteration 99, loss = 0.05783702\n",
      "Iteration 100, loss = 0.05724177\n",
      "Iteration 101, loss = 0.05664007\n",
      "Iteration 102, loss = 0.05604215\n",
      "Iteration 103, loss = 0.05556054\n",
      "Iteration 104, loss = 0.05506586\n",
      "Iteration 105, loss = 0.05451520\n",
      "Iteration 106, loss = 0.05393196\n",
      "Iteration 107, loss = 0.05336604\n",
      "Iteration 108, loss = 0.05284236\n",
      "Iteration 109, loss = 0.05240834\n",
      "Iteration 110, loss = 0.05188678\n",
      "Iteration 111, loss = 0.05146715\n",
      "Iteration 112, loss = 0.05094047\n",
      "Iteration 113, loss = 0.05055942\n",
      "Iteration 114, loss = 0.05001127\n",
      "Iteration 115, loss = 0.04951637\n",
      "Iteration 116, loss = 0.04911948\n",
      "Iteration 117, loss = 0.04866703\n",
      "Iteration 118, loss = 0.04818903\n",
      "Iteration 119, loss = 0.04783456\n",
      "Iteration 120, loss = 0.04735295\n",
      "Iteration 121, loss = 0.04692885\n",
      "Iteration 122, loss = 0.04646213\n",
      "Iteration 123, loss = 0.04612220\n",
      "Iteration 124, loss = 0.04567222\n",
      "Iteration 125, loss = 0.04529433\n",
      "Iteration 126, loss = 0.04485382\n",
      "Iteration 127, loss = 0.04455852\n",
      "Iteration 128, loss = 0.04415545\n",
      "Iteration 129, loss = 0.04369479\n",
      "Iteration 130, loss = 0.04338649\n",
      "Iteration 131, loss = 0.04297431\n",
      "Iteration 132, loss = 0.04265961\n",
      "Iteration 133, loss = 0.04232925\n",
      "Iteration 134, loss = 0.04194524\n",
      "Iteration 135, loss = 0.04154479\n",
      "Iteration 136, loss = 0.04127642\n",
      "Iteration 137, loss = 0.04095835\n",
      "Iteration 138, loss = 0.04057568\n",
      "Iteration 139, loss = 0.04025389\n",
      "Iteration 140, loss = 0.03994744\n",
      "Iteration 141, loss = 0.03958392\n",
      "Iteration 142, loss = 0.03919625\n",
      "Iteration 143, loss = 0.03895666\n",
      "Iteration 144, loss = 0.03864579\n",
      "Iteration 145, loss = 0.03835239\n",
      "Iteration 146, loss = 0.03804520\n",
      "Iteration 147, loss = 0.03780399\n",
      "Iteration 148, loss = 0.03751578\n",
      "Iteration 149, loss = 0.03714226\n",
      "Iteration 150, loss = 0.03691861\n",
      "Iteration 151, loss = 0.03660399\n",
      "Iteration 152, loss = 0.03634634\n",
      "Iteration 153, loss = 0.03609780\n",
      "Iteration 154, loss = 0.03583088\n",
      "Iteration 155, loss = 0.03554168\n",
      "Iteration 156, loss = 0.03525584\n",
      "Iteration 157, loss = 0.03503038\n",
      "Iteration 158, loss = 0.03467803\n",
      "Iteration 159, loss = 0.03451118\n",
      "Iteration 160, loss = 0.03421177\n",
      "Iteration 161, loss = 0.03398205\n",
      "Iteration 162, loss = 0.03374629\n",
      "Iteration 163, loss = 0.03350581\n",
      "Iteration 164, loss = 0.03331652\n",
      "Iteration 165, loss = 0.03306080\n",
      "Iteration 166, loss = 0.03277912\n",
      "Iteration 167, loss = 0.03260207\n",
      "Iteration 168, loss = 0.03236150\n",
      "Iteration 169, loss = 0.03209581\n",
      "Iteration 170, loss = 0.03189326\n",
      "Iteration 171, loss = 0.03161839\n",
      "Iteration 172, loss = 0.03148225\n",
      "Iteration 173, loss = 0.03122521\n",
      "Iteration 174, loss = 0.03109891\n",
      "Iteration 175, loss = 0.03080538\n",
      "Iteration 176, loss = 0.03064032\n",
      "Iteration 177, loss = 0.03044680\n",
      "Iteration 178, loss = 0.03024176\n",
      "Iteration 179, loss = 0.02997353\n",
      "Iteration 180, loss = 0.02983023\n",
      "Iteration 181, loss = 0.02961273\n",
      "Iteration 182, loss = 0.02938587\n",
      "Iteration 183, loss = 0.02924582\n",
      "Iteration 184, loss = 0.02911431\n",
      "Iteration 185, loss = 0.02885128\n",
      "Iteration 186, loss = 0.02875141\n",
      "Iteration 187, loss = 0.02848770\n",
      "Iteration 188, loss = 0.02831974\n",
      "Iteration 189, loss = 0.02816870\n",
      "Iteration 190, loss = 0.02797948\n",
      "Iteration 191, loss = 0.02784893\n",
      "Iteration 192, loss = 0.02775274\n",
      "Iteration 193, loss = 0.02751860\n",
      "Iteration 194, loss = 0.02739940\n",
      "Iteration 195, loss = 0.02720529\n",
      "Iteration 196, loss = 0.02700686\n",
      "Iteration 197, loss = 0.02679328\n",
      "Iteration 198, loss = 0.02670828\n",
      "Iteration 199, loss = 0.02654424\n",
      "Iteration 200, loss = 0.02637827\n",
      "[CV] END activation=relu, alpha=0.001, batch_size=50, hidden_layer_sizes=(100,), solver=sgd; total time=16.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.27179083\n",
      "Iteration 2, loss = 0.11144361\n",
      "Iteration 3, loss = 0.07644696\n",
      "Iteration 4, loss = 0.05674223\n",
      "Iteration 5, loss = 0.04610887\n",
      "Iteration 6, loss = 0.04100843\n",
      "Iteration 7, loss = 0.03455073\n",
      "Iteration 8, loss = 0.03711340\n",
      "Iteration 9, loss = 0.03126576\n",
      "Iteration 10, loss = 0.03052724\n",
      "Iteration 11, loss = 0.03067104\n",
      "Iteration 12, loss = 0.02967100\n",
      "Iteration 13, loss = 0.02930814\n",
      "Iteration 14, loss = 0.02695224\n",
      "Iteration 15, loss = 0.03033732\n",
      "Iteration 16, loss = 0.02640274\n",
      "Iteration 17, loss = 0.03075528\n",
      "Iteration 18, loss = 0.02879926\n",
      "Iteration 19, loss = 0.02622107\n",
      "Iteration 20, loss = 0.02099293\n",
      "Iteration 21, loss = 0.03145149\n",
      "Iteration 22, loss = 0.02531901\n",
      "Iteration 23, loss = 0.02441353\n",
      "Iteration 24, loss = 0.03160239\n",
      "Iteration 25, loss = 0.02007225\n",
      "Iteration 26, loss = 0.02749648\n",
      "Iteration 27, loss = 0.02771845\n",
      "Iteration 28, loss = 0.02051536\n",
      "Iteration 29, loss = 0.02351764\n",
      "Iteration 30, loss = 0.02710355\n",
      "Iteration 31, loss = 0.02564355\n",
      "Iteration 32, loss = 0.02364599\n",
      "Iteration 33, loss = 0.02191701\n",
      "Iteration 34, loss = 0.02599982\n",
      "Iteration 35, loss = 0.02142843\n",
      "Iteration 36, loss = 0.02445867\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.001, batch_size=50, hidden_layer_sizes=(400, 100), solver=adam; total time=21.0min\n",
      "Iteration 1, loss = 0.27605977\n",
      "Iteration 2, loss = 0.11140859\n",
      "Iteration 3, loss = 0.07627200\n",
      "Iteration 4, loss = 0.05664939\n",
      "Iteration 5, loss = 0.04547932\n",
      "Iteration 6, loss = 0.04177011\n",
      "Iteration 7, loss = 0.03564202\n",
      "Iteration 8, loss = 0.03277515\n",
      "Iteration 9, loss = 0.03409074\n",
      "Iteration 10, loss = 0.03197358\n",
      "Iteration 11, loss = 0.03031336\n",
      "Iteration 12, loss = 0.02868657\n",
      "Iteration 13, loss = 0.03156701\n",
      "Iteration 14, loss = 0.02793371\n",
      "Iteration 15, loss = 0.03065643\n",
      "Iteration 16, loss = 0.02784124\n",
      "Iteration 17, loss = 0.02553592\n",
      "Iteration 18, loss = 0.02765512\n",
      "Iteration 19, loss = 0.03310307\n",
      "Iteration 20, loss = 0.02559156\n",
      "Iteration 21, loss = 0.02900037\n",
      "Iteration 22, loss = 0.02198030\n",
      "Iteration 23, loss = 0.03076695\n",
      "Iteration 24, loss = 0.02488129\n",
      "Iteration 25, loss = 0.02197761\n",
      "Iteration 26, loss = 0.02905877\n",
      "Iteration 27, loss = 0.02931110\n",
      "Iteration 28, loss = 0.02257108\n",
      "Iteration 29, loss = 0.02213373\n",
      "Iteration 30, loss = 0.02708754\n",
      "Iteration 31, loss = 0.02727226\n",
      "Iteration 32, loss = 0.02188953\n",
      "Iteration 33, loss = 0.02600295\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.001, batch_size=50, hidden_layer_sizes=(400, 100), solver=adam; total time=789.9min\n",
      "Iteration 1, loss = 0.27927723\n",
      "Iteration 2, loss = 0.11070988\n",
      "Iteration 3, loss = 0.07870266\n",
      "Iteration 4, loss = 0.05842638\n",
      "Iteration 5, loss = 0.04206081\n",
      "Iteration 6, loss = 0.03942872\n",
      "Iteration 7, loss = 0.03775111\n",
      "Iteration 8, loss = 0.03332259\n",
      "Iteration 9, loss = 0.02978854\n",
      "Iteration 10, loss = 0.03403830\n",
      "Iteration 11, loss = 0.02991678\n",
      "Iteration 12, loss = 0.03053883\n",
      "Iteration 13, loss = 0.02254690\n",
      "Iteration 14, loss = 0.03083455\n",
      "Iteration 15, loss = 0.03118772\n",
      "Iteration 16, loss = 0.02776406\n",
      "Iteration 17, loss = 0.02212038\n",
      "Iteration 18, loss = 0.02889159\n",
      "Iteration 19, loss = 0.02947501\n",
      "Iteration 20, loss = 0.02791014\n",
      "Iteration 21, loss = 0.02357345\n",
      "Iteration 22, loss = 0.02676903\n",
      "Iteration 23, loss = 0.02769264\n",
      "Iteration 24, loss = 0.02361012\n",
      "Iteration 25, loss = 0.02578197\n",
      "Iteration 26, loss = 0.01990232\n",
      "Iteration 27, loss = 0.03037638\n",
      "Iteration 28, loss = 0.02600407\n",
      "Iteration 29, loss = 0.02564400\n",
      "Iteration 30, loss = 0.02192970\n",
      "Iteration 31, loss = 0.02414939\n",
      "Iteration 32, loss = 0.01761757\n",
      "Iteration 33, loss = 0.02727448\n",
      "Iteration 34, loss = 0.02493525\n",
      "Iteration 35, loss = 0.02131813\n",
      "Iteration 36, loss = 0.02417680\n",
      "Iteration 37, loss = 0.02160398\n",
      "Iteration 38, loss = 0.02492275\n",
      "Iteration 39, loss = 0.02277700\n",
      "Iteration 40, loss = 0.02115306\n",
      "Iteration 41, loss = 0.02229985\n",
      "Iteration 42, loss = 0.02482924\n",
      "Iteration 43, loss = 0.01830749\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.001, batch_size=50, hidden_layer_sizes=(400, 100), solver=adam; total time= 7.1min\n",
      "Iteration 1, loss = 0.27868805\n",
      "Iteration 2, loss = 0.11182297\n",
      "Iteration 3, loss = 0.07800418\n",
      "Iteration 4, loss = 0.05593912\n",
      "Iteration 5, loss = 0.04848386\n",
      "Iteration 6, loss = 0.03811628\n",
      "Iteration 7, loss = 0.03600571\n",
      "Iteration 8, loss = 0.03337767\n",
      "Iteration 9, loss = 0.03538897\n",
      "Iteration 10, loss = 0.02980652\n",
      "Iteration 11, loss = 0.02997547\n",
      "Iteration 12, loss = 0.03068582\n",
      "Iteration 13, loss = 0.02530838\n",
      "Iteration 14, loss = 0.02926632\n",
      "Iteration 15, loss = 0.03363267\n",
      "Iteration 16, loss = 0.02674723\n",
      "Iteration 17, loss = 0.02695061\n",
      "Iteration 18, loss = 0.02330613\n",
      "Iteration 19, loss = 0.02965700\n",
      "Iteration 20, loss = 0.02771460\n",
      "Iteration 21, loss = 0.03047515\n",
      "Iteration 22, loss = 0.02610447\n",
      "Iteration 23, loss = 0.02337136\n",
      "Iteration 24, loss = 0.02930963\n",
      "Iteration 25, loss = 0.02535596\n",
      "Iteration 26, loss = 0.02567807\n",
      "Iteration 27, loss = 0.02822322\n",
      "Iteration 28, loss = 0.02307068\n",
      "Iteration 29, loss = 0.02205900\n",
      "Iteration 30, loss = 0.02958090\n",
      "Iteration 31, loss = 0.02245064\n",
      "Iteration 32, loss = 0.02080311\n",
      "Iteration 33, loss = 0.02660297\n",
      "Iteration 34, loss = 0.02659663\n",
      "Iteration 35, loss = 0.02292481\n",
      "Iteration 36, loss = 0.02166414\n",
      "Iteration 37, loss = 0.02522167\n",
      "Iteration 38, loss = 0.02401310\n",
      "Iteration 39, loss = 0.02229590\n",
      "Iteration 40, loss = 0.02183895\n",
      "Iteration 41, loss = 0.02548106\n",
      "Iteration 42, loss = 0.02185141\n",
      "Iteration 43, loss = 0.02611962\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.001, batch_size=50, hidden_layer_sizes=(400, 100), solver=adam; total time= 7.1min\n",
      "Iteration 1, loss = 0.27364273\n",
      "Iteration 2, loss = 0.11261718\n",
      "Iteration 3, loss = 0.07906642\n",
      "Iteration 4, loss = 0.05773930\n",
      "Iteration 5, loss = 0.04545658\n",
      "Iteration 6, loss = 0.04273505\n",
      "Iteration 7, loss = 0.03641369\n",
      "Iteration 8, loss = 0.03154679\n",
      "Iteration 9, loss = 0.03355722\n",
      "Iteration 10, loss = 0.03441955\n",
      "Iteration 11, loss = 0.03261817\n",
      "Iteration 12, loss = 0.02694620\n",
      "Iteration 13, loss = 0.03189339\n",
      "Iteration 14, loss = 0.02540076\n",
      "Iteration 15, loss = 0.02798671\n",
      "Iteration 16, loss = 0.02658733\n",
      "Iteration 17, loss = 0.03296171\n",
      "Iteration 18, loss = 0.02700206\n",
      "Iteration 19, loss = 0.02994891\n",
      "Iteration 20, loss = 0.02427843\n",
      "Iteration 21, loss = 0.02445063\n",
      "Iteration 22, loss = 0.02781050\n",
      "Iteration 23, loss = 0.02488024\n",
      "Iteration 24, loss = 0.02979635\n",
      "Iteration 25, loss = 0.02715408\n",
      "Iteration 26, loss = 0.02580148\n",
      "Iteration 27, loss = 0.02362486\n",
      "Iteration 28, loss = 0.02589296\n",
      "Iteration 29, loss = 0.02305806\n",
      "Iteration 30, loss = 0.02551509\n",
      "Iteration 31, loss = 0.02553170\n",
      "Iteration 32, loss = 0.02836287\n",
      "Iteration 33, loss = 0.02155318\n",
      "Iteration 34, loss = 0.02414946\n",
      "Iteration 35, loss = 0.02689804\n",
      "Iteration 36, loss = 0.02293156\n",
      "Iteration 37, loss = 0.02654098\n",
      "Iteration 38, loss = 0.02222008\n",
      "Iteration 39, loss = 0.02360963\n",
      "Iteration 40, loss = 0.02004069\n",
      "Iteration 41, loss = 0.02476069\n",
      "Iteration 42, loss = 0.02315045\n",
      "Iteration 43, loss = 0.02333526\n",
      "Iteration 44, loss = 0.02240604\n",
      "Iteration 45, loss = 0.02286086\n",
      "Iteration 46, loss = 0.02162674\n",
      "Iteration 47, loss = 0.02198677\n",
      "Iteration 48, loss = 0.02182674\n",
      "Iteration 49, loss = 0.02332902\n",
      "Iteration 50, loss = 0.02616996\n",
      "Iteration 51, loss = 0.02018473\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.001, batch_size=50, hidden_layer_sizes=(400, 100), solver=adam; total time=10.0min\n",
      "Iteration 1, loss = 0.93168845\n",
      "Iteration 2, loss = 0.38532479\n",
      "Iteration 3, loss = 0.31777376\n",
      "Iteration 4, loss = 0.28157366\n",
      "Iteration 5, loss = 0.25539446\n",
      "Iteration 6, loss = 0.23492127\n",
      "Iteration 7, loss = 0.21760532\n",
      "Iteration 8, loss = 0.20312558\n",
      "Iteration 9, loss = 0.18985346\n",
      "Iteration 10, loss = 0.17863095\n",
      "Iteration 11, loss = 0.16861165\n",
      "Iteration 12, loss = 0.15927869\n",
      "Iteration 13, loss = 0.15151690\n",
      "Iteration 14, loss = 0.14396237\n",
      "Iteration 15, loss = 0.13729139\n",
      "Iteration 16, loss = 0.13064530\n",
      "Iteration 17, loss = 0.12455076\n",
      "Iteration 18, loss = 0.11952508\n",
      "Iteration 19, loss = 0.11415160\n",
      "Iteration 20, loss = 0.10960545\n",
      "Iteration 21, loss = 0.10489499\n",
      "Iteration 22, loss = 0.10084893\n",
      "Iteration 23, loss = 0.09664261\n",
      "Iteration 24, loss = 0.09285410\n",
      "Iteration 25, loss = 0.08907922\n",
      "Iteration 26, loss = 0.08601995\n",
      "Iteration 27, loss = 0.08279466\n",
      "Iteration 28, loss = 0.07991627\n",
      "Iteration 29, loss = 0.07697777\n",
      "Iteration 30, loss = 0.07409858\n",
      "Iteration 31, loss = 0.07151518\n",
      "Iteration 32, loss = 0.06906548\n",
      "Iteration 33, loss = 0.06658624\n",
      "Iteration 34, loss = 0.06440301\n",
      "Iteration 35, loss = 0.06239838\n",
      "Iteration 36, loss = 0.06012929\n",
      "Iteration 37, loss = 0.05817058\n",
      "Iteration 38, loss = 0.05655633\n",
      "Iteration 39, loss = 0.05463077\n",
      "Iteration 40, loss = 0.05307287\n",
      "Iteration 41, loss = 0.05123366\n",
      "Iteration 42, loss = 0.04992860\n",
      "Iteration 43, loss = 0.04825224\n",
      "Iteration 44, loss = 0.04692474\n",
      "Iteration 45, loss = 0.04557388\n",
      "Iteration 46, loss = 0.04413446\n",
      "Iteration 47, loss = 0.04287814\n",
      "Iteration 48, loss = 0.04170198\n",
      "Iteration 49, loss = 0.04054770\n",
      "Iteration 50, loss = 0.03949304\n",
      "Iteration 51, loss = 0.03812925\n",
      "Iteration 52, loss = 0.03740991\n",
      "Iteration 53, loss = 0.03627865\n",
      "Iteration 54, loss = 0.03537136\n",
      "Iteration 55, loss = 0.03442975\n",
      "Iteration 56, loss = 0.03360231\n",
      "Iteration 57, loss = 0.03287998\n",
      "Iteration 58, loss = 0.03190598\n",
      "Iteration 59, loss = 0.03114052\n",
      "Iteration 60, loss = 0.03046094\n",
      "Iteration 61, loss = 0.02976245\n",
      "Iteration 62, loss = 0.02921701\n",
      "Iteration 63, loss = 0.02846417\n",
      "Iteration 64, loss = 0.02778667\n",
      "Iteration 65, loss = 0.02714323\n",
      "Iteration 66, loss = 0.02663111\n",
      "Iteration 67, loss = 0.02605909\n",
      "Iteration 68, loss = 0.02561941\n",
      "Iteration 69, loss = 0.02508388\n",
      "Iteration 70, loss = 0.02459360\n",
      "Iteration 71, loss = 0.02410059\n",
      "Iteration 72, loss = 0.02369584\n",
      "Iteration 73, loss = 0.02326464\n",
      "Iteration 74, loss = 0.02280089\n",
      "Iteration 75, loss = 0.02251134\n",
      "Iteration 76, loss = 0.02199450\n",
      "Iteration 77, loss = 0.02161809\n",
      "Iteration 78, loss = 0.02127571\n",
      "Iteration 79, loss = 0.02097465\n",
      "Iteration 80, loss = 0.02061792\n",
      "Iteration 81, loss = 0.02033594\n",
      "Iteration 82, loss = 0.01999821\n",
      "Iteration 83, loss = 0.01966881\n",
      "Iteration 84, loss = 0.01942479\n",
      "Iteration 85, loss = 0.01917169\n",
      "Iteration 86, loss = 0.01883675\n",
      "Iteration 87, loss = 0.01859511\n",
      "Iteration 88, loss = 0.01843799\n",
      "Iteration 89, loss = 0.01819508\n",
      "Iteration 90, loss = 0.01790681\n",
      "Iteration 91, loss = 0.01769648\n",
      "Iteration 92, loss = 0.01755112\n",
      "Iteration 93, loss = 0.01732983\n",
      "Iteration 94, loss = 0.01709938\n",
      "Iteration 95, loss = 0.01697216\n",
      "Iteration 96, loss = 0.01674490\n",
      "Iteration 97, loss = 0.01658029\n",
      "Iteration 98, loss = 0.01639892\n",
      "Iteration 99, loss = 0.01626463\n",
      "Iteration 100, loss = 0.01605814\n",
      "Iteration 101, loss = 0.01591049\n",
      "Iteration 102, loss = 0.01580222\n",
      "Iteration 103, loss = 0.01563987\n",
      "Iteration 104, loss = 0.01551879\n",
      "Iteration 105, loss = 0.01541968\n",
      "Iteration 106, loss = 0.01524409\n",
      "Iteration 107, loss = 0.01516858\n",
      "Iteration 108, loss = 0.01503695\n",
      "Iteration 109, loss = 0.01490806\n",
      "Iteration 110, loss = 0.01481796\n",
      "Iteration 111, loss = 0.01468904\n",
      "Iteration 112, loss = 0.01460215\n",
      "Iteration 113, loss = 0.01450169\n",
      "Iteration 114, loss = 0.01437999\n",
      "Iteration 115, loss = 0.01430545\n",
      "Iteration 116, loss = 0.01421211\n",
      "Iteration 117, loss = 0.01414187\n",
      "Iteration 118, loss = 0.01405334\n",
      "Iteration 119, loss = 0.01397231\n",
      "Iteration 120, loss = 0.01389510\n",
      "Iteration 121, loss = 0.01380597\n",
      "Iteration 122, loss = 0.01373067\n",
      "Iteration 123, loss = 0.01366409\n",
      "Iteration 124, loss = 0.01359031\n",
      "Iteration 125, loss = 0.01352387\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.001, batch_size=50, hidden_layer_sizes=(400, 100), solver=sgd; total time= 6.2min\n",
      "Iteration 1, loss = 0.97976586\n",
      "Iteration 2, loss = 0.39004312\n",
      "Iteration 3, loss = 0.31914629\n",
      "Iteration 4, loss = 0.28270256\n",
      "Iteration 5, loss = 0.25681665\n",
      "Iteration 6, loss = 0.23606497\n",
      "Iteration 7, loss = 0.21976594\n",
      "Iteration 8, loss = 0.20574857\n",
      "Iteration 9, loss = 0.19285893\n",
      "Iteration 10, loss = 0.18174180\n",
      "Iteration 11, loss = 0.17172194\n",
      "Iteration 12, loss = 0.16339956\n",
      "Iteration 13, loss = 0.15471650\n",
      "Iteration 14, loss = 0.14730721\n",
      "Iteration 15, loss = 0.14003255\n",
      "Iteration 16, loss = 0.13421013\n",
      "Iteration 17, loss = 0.12793134\n",
      "Iteration 18, loss = 0.12212797\n",
      "Iteration 19, loss = 0.11724199\n",
      "Iteration 20, loss = 0.11235276\n",
      "Iteration 21, loss = 0.10799434\n",
      "Iteration 22, loss = 0.10361313\n",
      "Iteration 23, loss = 0.09960983\n",
      "Iteration 24, loss = 0.09594471\n",
      "Iteration 25, loss = 0.09190149\n",
      "Iteration 26, loss = 0.08881135\n",
      "Iteration 27, loss = 0.08545603\n",
      "Iteration 28, loss = 0.08249996\n",
      "Iteration 29, loss = 0.07964714\n",
      "Iteration 30, loss = 0.07683310\n",
      "Iteration 31, loss = 0.07422033\n",
      "Iteration 32, loss = 0.07166868\n",
      "Iteration 33, loss = 0.06952070\n",
      "Iteration 34, loss = 0.06691864\n",
      "Iteration 35, loss = 0.06488019\n",
      "Iteration 36, loss = 0.06273768\n",
      "Iteration 37, loss = 0.06086767\n",
      "Iteration 38, loss = 0.05888203\n",
      "Iteration 39, loss = 0.05689762\n",
      "Iteration 40, loss = 0.05536729\n",
      "Iteration 41, loss = 0.05368388\n",
      "Iteration 42, loss = 0.05212780\n",
      "Iteration 43, loss = 0.05067220\n",
      "Iteration 44, loss = 0.04912502\n",
      "Iteration 45, loss = 0.04771161\n",
      "Iteration 46, loss = 0.04631479\n",
      "Iteration 47, loss = 0.04507546\n",
      "Iteration 48, loss = 0.04395225\n",
      "Iteration 49, loss = 0.04268383\n",
      "Iteration 50, loss = 0.04144541\n",
      "Iteration 51, loss = 0.04030132\n",
      "Iteration 52, loss = 0.03938164\n",
      "Iteration 53, loss = 0.03838693\n",
      "Iteration 54, loss = 0.03740697\n",
      "Iteration 55, loss = 0.03644520\n",
      "Iteration 56, loss = 0.03535569\n",
      "Iteration 57, loss = 0.03469019\n",
      "Iteration 58, loss = 0.03373198\n",
      "Iteration 59, loss = 0.03299068\n",
      "Iteration 60, loss = 0.03239057\n",
      "Iteration 61, loss = 0.03152464\n",
      "Iteration 62, loss = 0.03078430\n",
      "Iteration 63, loss = 0.03002725\n",
      "Iteration 64, loss = 0.02940698\n",
      "Iteration 65, loss = 0.02886761\n",
      "Iteration 66, loss = 0.02804321\n",
      "Iteration 67, loss = 0.02766883\n",
      "Iteration 68, loss = 0.02701795\n",
      "Iteration 69, loss = 0.02646077\n",
      "Iteration 70, loss = 0.02605557\n",
      "Iteration 71, loss = 0.02549709\n",
      "Iteration 72, loss = 0.02500178\n",
      "Iteration 73, loss = 0.02453876\n",
      "Iteration 74, loss = 0.02417622\n",
      "Iteration 75, loss = 0.02360457\n",
      "Iteration 76, loss = 0.02331114\n",
      "Iteration 77, loss = 0.02284879\n",
      "Iteration 78, loss = 0.02247894\n",
      "Iteration 79, loss = 0.02213226\n",
      "Iteration 80, loss = 0.02175835\n",
      "Iteration 81, loss = 0.02138666\n",
      "Iteration 82, loss = 0.02112917\n",
      "Iteration 83, loss = 0.02077401\n",
      "Iteration 84, loss = 0.02048582\n",
      "Iteration 85, loss = 0.02017000\n",
      "Iteration 86, loss = 0.01984593\n",
      "Iteration 87, loss = 0.01965086\n",
      "Iteration 88, loss = 0.01936806\n",
      "Iteration 89, loss = 0.01913378\n",
      "Iteration 90, loss = 0.01885210\n",
      "Iteration 91, loss = 0.01861901\n",
      "Iteration 92, loss = 0.01836936\n",
      "Iteration 93, loss = 0.01814825\n",
      "Iteration 94, loss = 0.01797560\n",
      "Iteration 95, loss = 0.01771411\n",
      "Iteration 96, loss = 0.01757692\n",
      "Iteration 97, loss = 0.01736236\n",
      "Iteration 98, loss = 0.01714303\n",
      "Iteration 99, loss = 0.01696479\n",
      "Iteration 100, loss = 0.01682412\n",
      "Iteration 101, loss = 0.01671518\n",
      "Iteration 102, loss = 0.01650369\n",
      "Iteration 103, loss = 0.01635390\n",
      "Iteration 104, loss = 0.01620523\n",
      "Iteration 105, loss = 0.01604275\n",
      "Iteration 106, loss = 0.01591791\n",
      "Iteration 107, loss = 0.01579555\n",
      "Iteration 108, loss = 0.01564986\n",
      "Iteration 109, loss = 0.01553127\n",
      "Iteration 110, loss = 0.01541824\n",
      "Iteration 111, loss = 0.01525349\n",
      "Iteration 112, loss = 0.01514450\n",
      "Iteration 113, loss = 0.01505808\n",
      "Iteration 114, loss = 0.01494621\n",
      "Iteration 115, loss = 0.01480621\n",
      "Iteration 116, loss = 0.01473456\n",
      "Iteration 117, loss = 0.01463323\n",
      "Iteration 118, loss = 0.01454349\n",
      "Iteration 119, loss = 0.01444317\n",
      "Iteration 120, loss = 0.01435111\n",
      "Iteration 121, loss = 0.01424407\n",
      "Iteration 122, loss = 0.01419549\n",
      "Iteration 123, loss = 0.01408129\n",
      "Iteration 124, loss = 0.01402498\n",
      "Iteration 125, loss = 0.01394676\n",
      "Iteration 126, loss = 0.01387798\n",
      "Iteration 127, loss = 0.01378313\n",
      "Iteration 128, loss = 0.01371806\n",
      "Iteration 129, loss = 0.01364590\n",
      "Iteration 130, loss = 0.01359617\n",
      "Iteration 131, loss = 0.01354011\n",
      "Iteration 132, loss = 0.01344501\n",
      "Iteration 133, loss = 0.01339273\n",
      "Iteration 134, loss = 0.01333358\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.001, batch_size=50, hidden_layer_sizes=(400, 100), solver=sgd; total time= 6.8min\n",
      "Iteration 1, loss = 0.93510612\n",
      "Iteration 2, loss = 0.38479063\n",
      "Iteration 3, loss = 0.31373075\n",
      "Iteration 4, loss = 0.27780051\n",
      "Iteration 5, loss = 0.25196527\n",
      "Iteration 6, loss = 0.23268300\n",
      "Iteration 7, loss = 0.21600744\n",
      "Iteration 8, loss = 0.20190264\n",
      "Iteration 9, loss = 0.18960885\n",
      "Iteration 10, loss = 0.17891953\n",
      "Iteration 11, loss = 0.16888695\n",
      "Iteration 12, loss = 0.15997910\n",
      "Iteration 13, loss = 0.15196781\n",
      "Iteration 14, loss = 0.14454115\n",
      "Iteration 15, loss = 0.13783002\n",
      "Iteration 16, loss = 0.13137994\n",
      "Iteration 17, loss = 0.12523546\n",
      "Iteration 18, loss = 0.12001310\n",
      "Iteration 19, loss = 0.11485603\n",
      "Iteration 20, loss = 0.11007276\n",
      "Iteration 21, loss = 0.10577542\n",
      "Iteration 22, loss = 0.10133365\n",
      "Iteration 23, loss = 0.09762278\n",
      "Iteration 24, loss = 0.09357839\n",
      "Iteration 25, loss = 0.09032167\n",
      "Iteration 26, loss = 0.08713998\n",
      "Iteration 27, loss = 0.08377196\n",
      "Iteration 28, loss = 0.08058715\n",
      "Iteration 29, loss = 0.07809396\n",
      "Iteration 30, loss = 0.07554218\n",
      "Iteration 31, loss = 0.07261116\n",
      "Iteration 32, loss = 0.07014259\n",
      "Iteration 33, loss = 0.06793942\n",
      "Iteration 34, loss = 0.06607425\n",
      "Iteration 35, loss = 0.06371079\n",
      "Iteration 36, loss = 0.06187432\n",
      "Iteration 37, loss = 0.05985447\n",
      "Iteration 38, loss = 0.05819709\n",
      "Iteration 39, loss = 0.05603954\n",
      "Iteration 40, loss = 0.05447087\n",
      "Iteration 41, loss = 0.05302378\n",
      "Iteration 42, loss = 0.05147153\n",
      "Iteration 43, loss = 0.04996747\n",
      "Iteration 44, loss = 0.04848109\n",
      "Iteration 45, loss = 0.04718417\n",
      "Iteration 46, loss = 0.04577329\n",
      "Iteration 47, loss = 0.04460551\n",
      "Iteration 48, loss = 0.04332764\n",
      "Iteration 49, loss = 0.04217678\n",
      "Iteration 50, loss = 0.04094725\n",
      "Iteration 51, loss = 0.03993473\n",
      "Iteration 52, loss = 0.03888012\n",
      "Iteration 53, loss = 0.03793281\n",
      "Iteration 54, loss = 0.03694130\n",
      "Iteration 55, loss = 0.03587332\n",
      "Iteration 56, loss = 0.03530237\n",
      "Iteration 57, loss = 0.03437121\n",
      "Iteration 58, loss = 0.03353563\n",
      "Iteration 59, loss = 0.03269187\n",
      "Iteration 60, loss = 0.03184724\n",
      "Iteration 61, loss = 0.03124225\n",
      "Iteration 62, loss = 0.03045933\n",
      "Iteration 63, loss = 0.02992248\n",
      "Iteration 64, loss = 0.02924015\n",
      "Iteration 65, loss = 0.02862967\n",
      "Iteration 66, loss = 0.02799058\n",
      "Iteration 67, loss = 0.02735325\n",
      "Iteration 68, loss = 0.02692026\n",
      "Iteration 69, loss = 0.02636709\n",
      "Iteration 70, loss = 0.02594157\n",
      "Iteration 71, loss = 0.02542783\n",
      "Iteration 72, loss = 0.02497334\n",
      "Iteration 73, loss = 0.02446154\n",
      "Iteration 74, loss = 0.02401048\n",
      "Iteration 75, loss = 0.02363635\n",
      "Iteration 76, loss = 0.02317912\n",
      "Iteration 77, loss = 0.02280652\n",
      "Iteration 78, loss = 0.02250522\n",
      "Iteration 79, loss = 0.02210846\n",
      "Iteration 80, loss = 0.02178246\n",
      "Iteration 81, loss = 0.02142351\n",
      "Iteration 82, loss = 0.02109245\n",
      "Iteration 83, loss = 0.02084549\n",
      "Iteration 84, loss = 0.02052190\n",
      "Iteration 85, loss = 0.02026844\n",
      "Iteration 86, loss = 0.01998582\n",
      "Iteration 87, loss = 0.01969013\n",
      "Iteration 88, loss = 0.01947303\n",
      "Iteration 89, loss = 0.01916624\n",
      "Iteration 90, loss = 0.01899791\n",
      "Iteration 91, loss = 0.01875412\n",
      "Iteration 92, loss = 0.01851091\n",
      "Iteration 93, loss = 0.01829313\n",
      "Iteration 94, loss = 0.01804789\n",
      "Iteration 95, loss = 0.01786332\n",
      "Iteration 96, loss = 0.01767283\n",
      "Iteration 97, loss = 0.01748009\n",
      "Iteration 98, loss = 0.01734717\n",
      "Iteration 99, loss = 0.01713493\n",
      "Iteration 100, loss = 0.01696488\n",
      "Iteration 101, loss = 0.01683295\n",
      "Iteration 102, loss = 0.01659464\n",
      "Iteration 103, loss = 0.01647486\n",
      "Iteration 104, loss = 0.01637424\n",
      "Iteration 105, loss = 0.01622515\n",
      "Iteration 106, loss = 0.01605939\n",
      "Iteration 107, loss = 0.01594024\n",
      "Iteration 108, loss = 0.01579663\n",
      "Iteration 109, loss = 0.01562492\n",
      "Iteration 110, loss = 0.01553509\n",
      "Iteration 111, loss = 0.01541567\n",
      "Iteration 112, loss = 0.01527436\n",
      "Iteration 113, loss = 0.01516244\n",
      "Iteration 114, loss = 0.01507160\n",
      "Iteration 115, loss = 0.01496767\n",
      "Iteration 116, loss = 0.01483160\n",
      "Iteration 117, loss = 0.01476272\n",
      "Iteration 118, loss = 0.01465980\n",
      "Iteration 119, loss = 0.01455907\n",
      "Iteration 120, loss = 0.01445464\n",
      "Iteration 121, loss = 0.01436951\n",
      "Iteration 122, loss = 0.01430654\n",
      "Iteration 123, loss = 0.01420558\n",
      "Iteration 124, loss = 0.01413474\n",
      "Iteration 125, loss = 0.01403509\n",
      "Iteration 126, loss = 0.01396962\n",
      "Iteration 127, loss = 0.01388337\n",
      "Iteration 128, loss = 0.01378941\n",
      "Iteration 129, loss = 0.01374249\n",
      "Iteration 130, loss = 0.01367198\n",
      "Iteration 131, loss = 0.01358052\n",
      "Iteration 132, loss = 0.01354145\n",
      "Iteration 133, loss = 0.01346865\n",
      "Iteration 134, loss = 0.01340177\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.001, batch_size=50, hidden_layer_sizes=(400, 100), solver=sgd; total time= 7.0min\n",
      "Iteration 1, loss = 0.91659980\n",
      "Iteration 2, loss = 0.38452081\n",
      "Iteration 3, loss = 0.31508557\n",
      "Iteration 4, loss = 0.27906677\n",
      "Iteration 5, loss = 0.25402752\n",
      "Iteration 6, loss = 0.23371209\n",
      "Iteration 7, loss = 0.21623733\n",
      "Iteration 8, loss = 0.20187880\n",
      "Iteration 9, loss = 0.18897485\n",
      "Iteration 10, loss = 0.17769610\n",
      "Iteration 11, loss = 0.16745695\n",
      "Iteration 12, loss = 0.15832603\n",
      "Iteration 13, loss = 0.15033928\n",
      "Iteration 14, loss = 0.14228902\n",
      "Iteration 15, loss = 0.13559593\n",
      "Iteration 16, loss = 0.12929530\n",
      "Iteration 17, loss = 0.12342126\n",
      "Iteration 18, loss = 0.11804987\n",
      "Iteration 19, loss = 0.11295807\n",
      "Iteration 20, loss = 0.10800324\n",
      "Iteration 21, loss = 0.10349891\n",
      "Iteration 22, loss = 0.09966679\n",
      "Iteration 23, loss = 0.09572384\n",
      "Iteration 24, loss = 0.09199888\n",
      "Iteration 25, loss = 0.08826083\n",
      "Iteration 26, loss = 0.08527854\n",
      "Iteration 27, loss = 0.08167741\n",
      "Iteration 28, loss = 0.07900519\n",
      "Iteration 29, loss = 0.07619117\n",
      "Iteration 30, loss = 0.07385421\n",
      "Iteration 31, loss = 0.07086153\n",
      "Iteration 32, loss = 0.06841023\n",
      "Iteration 33, loss = 0.06602840\n",
      "Iteration 34, loss = 0.06406448\n",
      "Iteration 35, loss = 0.06172786\n",
      "Iteration 36, loss = 0.06024350\n",
      "Iteration 37, loss = 0.05816025\n",
      "Iteration 38, loss = 0.05587060\n",
      "Iteration 39, loss = 0.05436423\n",
      "Iteration 40, loss = 0.05259369\n",
      "Iteration 41, loss = 0.05087858\n",
      "Iteration 42, loss = 0.04956951\n",
      "Iteration 43, loss = 0.04811183\n",
      "Iteration 44, loss = 0.04667040\n",
      "Iteration 45, loss = 0.04540198\n",
      "Iteration 46, loss = 0.04414153\n",
      "Iteration 47, loss = 0.04265062\n",
      "Iteration 48, loss = 0.04163236\n",
      "Iteration 49, loss = 0.04052403\n",
      "Iteration 50, loss = 0.03956173\n",
      "Iteration 51, loss = 0.03836983\n",
      "Iteration 52, loss = 0.03733196\n",
      "Iteration 53, loss = 0.03648437\n",
      "Iteration 54, loss = 0.03535689\n",
      "Iteration 55, loss = 0.03455615\n",
      "Iteration 56, loss = 0.03368903\n",
      "Iteration 57, loss = 0.03298366\n",
      "Iteration 58, loss = 0.03216358\n",
      "Iteration 59, loss = 0.03127930\n",
      "Iteration 60, loss = 0.03062779\n",
      "Iteration 61, loss = 0.02999151\n",
      "Iteration 62, loss = 0.02939515\n",
      "Iteration 63, loss = 0.02860105\n",
      "Iteration 64, loss = 0.02803139\n",
      "Iteration 65, loss = 0.02738895\n",
      "Iteration 66, loss = 0.02693013\n",
      "Iteration 67, loss = 0.02629459\n",
      "Iteration 68, loss = 0.02582879\n",
      "Iteration 69, loss = 0.02532327\n",
      "Iteration 70, loss = 0.02478460\n",
      "Iteration 71, loss = 0.02432633\n",
      "Iteration 72, loss = 0.02385921\n",
      "Iteration 73, loss = 0.02345504\n",
      "Iteration 74, loss = 0.02304029\n",
      "Iteration 75, loss = 0.02266773\n",
      "Iteration 76, loss = 0.02224771\n",
      "Iteration 77, loss = 0.02194347\n",
      "Iteration 78, loss = 0.02144506\n",
      "Iteration 79, loss = 0.02123632\n",
      "Iteration 80, loss = 0.02090026\n",
      "Iteration 81, loss = 0.02055816\n",
      "Iteration 82, loss = 0.02029649\n",
      "Iteration 83, loss = 0.02002310\n",
      "Iteration 84, loss = 0.01966087\n",
      "Iteration 85, loss = 0.01938795\n",
      "Iteration 86, loss = 0.01915846\n",
      "Iteration 87, loss = 0.01890880\n",
      "Iteration 88, loss = 0.01864801\n",
      "Iteration 89, loss = 0.01844785\n",
      "Iteration 90, loss = 0.01823195\n",
      "Iteration 91, loss = 0.01799405\n",
      "Iteration 92, loss = 0.01781331\n",
      "Iteration 93, loss = 0.01757257\n",
      "Iteration 94, loss = 0.01741282\n",
      "Iteration 95, loss = 0.01708272\n",
      "Iteration 96, loss = 0.01699098\n",
      "Iteration 97, loss = 0.01682214\n",
      "Iteration 98, loss = 0.01662793\n",
      "Iteration 99, loss = 0.01647709\n",
      "Iteration 100, loss = 0.01626612\n",
      "Iteration 101, loss = 0.01617827\n",
      "Iteration 102, loss = 0.01603582\n",
      "Iteration 103, loss = 0.01585639\n",
      "Iteration 104, loss = 0.01573242\n",
      "Iteration 105, loss = 0.01560197\n",
      "Iteration 106, loss = 0.01544464\n",
      "Iteration 107, loss = 0.01534865\n",
      "Iteration 108, loss = 0.01519559\n",
      "Iteration 109, loss = 0.01511196\n",
      "Iteration 110, loss = 0.01498302\n",
      "Iteration 111, loss = 0.01485589\n",
      "Iteration 112, loss = 0.01476986\n",
      "Iteration 113, loss = 0.01468151\n",
      "Iteration 114, loss = 0.01457115\n",
      "Iteration 115, loss = 0.01448243\n",
      "Iteration 116, loss = 0.01438687\n",
      "Iteration 117, loss = 0.01430089\n",
      "Iteration 118, loss = 0.01420320\n",
      "Iteration 119, loss = 0.01412632\n",
      "Iteration 120, loss = 0.01401517\n",
      "Iteration 121, loss = 0.01393947\n",
      "Iteration 122, loss = 0.01388949\n",
      "Iteration 123, loss = 0.01381704\n",
      "Iteration 124, loss = 0.01371027\n",
      "Iteration 125, loss = 0.01365799\n",
      "Iteration 126, loss = 0.01359162\n",
      "Iteration 127, loss = 0.01353295\n",
      "Iteration 128, loss = 0.01345836\n",
      "Iteration 129, loss = 0.01338000\n",
      "Iteration 130, loss = 0.01332185\n",
      "Iteration 131, loss = 0.01326625\n",
      "Iteration 132, loss = 0.01322435\n",
      "Iteration 133, loss = 0.01317013\n",
      "Iteration 134, loss = 0.01309697\n",
      "Iteration 135, loss = 0.01304278\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.001, batch_size=50, hidden_layer_sizes=(400, 100), solver=sgd; total time= 7.1min\n",
      "Iteration 1, loss = 0.99339753\n",
      "Iteration 2, loss = 0.40467687\n",
      "Iteration 3, loss = 0.33001319\n",
      "Iteration 4, loss = 0.29241503\n",
      "Iteration 5, loss = 0.26758839\n",
      "Iteration 6, loss = 0.24716919\n",
      "Iteration 7, loss = 0.23006662\n",
      "Iteration 8, loss = 0.21534408\n",
      "Iteration 9, loss = 0.20238919\n",
      "Iteration 10, loss = 0.19068144\n",
      "Iteration 11, loss = 0.17991611\n",
      "Iteration 12, loss = 0.17070190\n",
      "Iteration 13, loss = 0.16183611\n",
      "Iteration 14, loss = 0.15385936\n",
      "Iteration 15, loss = 0.14631785\n",
      "Iteration 16, loss = 0.13944266\n",
      "Iteration 17, loss = 0.13353961\n",
      "Iteration 18, loss = 0.12764428\n",
      "Iteration 19, loss = 0.12172632\n",
      "Iteration 20, loss = 0.11654482\n",
      "Iteration 21, loss = 0.11202398\n",
      "Iteration 22, loss = 0.10758718\n",
      "Iteration 23, loss = 0.10316651\n",
      "Iteration 24, loss = 0.09931140\n",
      "Iteration 25, loss = 0.09541327\n",
      "Iteration 26, loss = 0.09177088\n",
      "Iteration 27, loss = 0.08854773\n",
      "Iteration 28, loss = 0.08545764\n",
      "Iteration 29, loss = 0.08225666\n",
      "Iteration 30, loss = 0.07915286\n",
      "Iteration 31, loss = 0.07661391\n",
      "Iteration 32, loss = 0.07400162\n",
      "Iteration 33, loss = 0.07152469\n",
      "Iteration 34, loss = 0.06899451\n",
      "Iteration 35, loss = 0.06687612\n",
      "Iteration 36, loss = 0.06465073\n",
      "Iteration 37, loss = 0.06269901\n",
      "Iteration 38, loss = 0.06044010\n",
      "Iteration 39, loss = 0.05857205\n",
      "Iteration 40, loss = 0.05659625\n",
      "Iteration 41, loss = 0.05504160\n",
      "Iteration 42, loss = 0.05338557\n",
      "Iteration 43, loss = 0.05165012\n",
      "Iteration 44, loss = 0.05029381\n",
      "Iteration 45, loss = 0.04895495\n",
      "Iteration 46, loss = 0.04747671\n",
      "Iteration 47, loss = 0.04617889\n",
      "Iteration 48, loss = 0.04487347\n",
      "Iteration 49, loss = 0.04362041\n",
      "Iteration 50, loss = 0.04236896\n",
      "Iteration 51, loss = 0.04138708\n",
      "Iteration 52, loss = 0.04015428\n",
      "Iteration 53, loss = 0.03908704\n",
      "Iteration 54, loss = 0.03811836\n",
      "Iteration 55, loss = 0.03723720\n",
      "Iteration 56, loss = 0.03606549\n",
      "Iteration 57, loss = 0.03520191\n",
      "Iteration 58, loss = 0.03449489\n",
      "Iteration 59, loss = 0.03347235\n",
      "Iteration 60, loss = 0.03268686\n",
      "Iteration 61, loss = 0.03209344\n",
      "Iteration 62, loss = 0.03133708\n",
      "Iteration 63, loss = 0.03059136\n",
      "Iteration 64, loss = 0.02983518\n",
      "Iteration 65, loss = 0.02913524\n",
      "Iteration 66, loss = 0.02855362\n",
      "Iteration 67, loss = 0.02800700\n",
      "Iteration 68, loss = 0.02740482\n",
      "Iteration 69, loss = 0.02689245\n",
      "Iteration 70, loss = 0.02628768\n",
      "Iteration 71, loss = 0.02568189\n",
      "Iteration 72, loss = 0.02525667\n",
      "Iteration 73, loss = 0.02476054\n",
      "Iteration 74, loss = 0.02433931\n",
      "Iteration 75, loss = 0.02381857\n",
      "Iteration 76, loss = 0.02349537\n",
      "Iteration 77, loss = 0.02311857\n",
      "Iteration 78, loss = 0.02272749\n",
      "Iteration 79, loss = 0.02234024\n",
      "Iteration 80, loss = 0.02192913\n",
      "Iteration 81, loss = 0.02166404\n",
      "Iteration 82, loss = 0.02125528\n",
      "Iteration 83, loss = 0.02096895\n",
      "Iteration 84, loss = 0.02055216\n",
      "Iteration 85, loss = 0.02032377\n",
      "Iteration 86, loss = 0.02000448\n",
      "Iteration 87, loss = 0.01977151\n",
      "Iteration 88, loss = 0.01949795\n",
      "Iteration 89, loss = 0.01921221\n",
      "Iteration 90, loss = 0.01895032\n",
      "Iteration 91, loss = 0.01875345\n",
      "Iteration 92, loss = 0.01851379\n",
      "Iteration 93, loss = 0.01827921\n",
      "Iteration 94, loss = 0.01804266\n",
      "Iteration 95, loss = 0.01788952\n",
      "Iteration 96, loss = 0.01767800\n",
      "Iteration 97, loss = 0.01745189\n",
      "Iteration 98, loss = 0.01731080\n",
      "Iteration 99, loss = 0.01712080\n",
      "Iteration 100, loss = 0.01692380\n",
      "Iteration 101, loss = 0.01678817\n",
      "Iteration 102, loss = 0.01661310\n",
      "Iteration 103, loss = 0.01646386\n",
      "Iteration 104, loss = 0.01635174\n",
      "Iteration 105, loss = 0.01615328\n",
      "Iteration 106, loss = 0.01599539\n",
      "Iteration 107, loss = 0.01587835\n",
      "Iteration 108, loss = 0.01578523\n",
      "Iteration 109, loss = 0.01562386\n",
      "Iteration 110, loss = 0.01551895\n",
      "Iteration 111, loss = 0.01536439\n",
      "Iteration 112, loss = 0.01526267\n",
      "Iteration 113, loss = 0.01516320\n",
      "Iteration 114, loss = 0.01504135\n",
      "Iteration 115, loss = 0.01493286\n",
      "Iteration 116, loss = 0.01486445\n",
      "Iteration 117, loss = 0.01471279\n",
      "Iteration 118, loss = 0.01464237\n",
      "Iteration 119, loss = 0.01454603\n",
      "Iteration 120, loss = 0.01443907\n",
      "Iteration 121, loss = 0.01436095\n",
      "Iteration 122, loss = 0.01429660\n",
      "Iteration 123, loss = 0.01418033\n",
      "Iteration 124, loss = 0.01413124\n",
      "Iteration 125, loss = 0.01404151\n",
      "Iteration 126, loss = 0.01395344\n",
      "Iteration 127, loss = 0.01385671\n",
      "Iteration 128, loss = 0.01382206\n",
      "Iteration 129, loss = 0.01373182\n",
      "Iteration 130, loss = 0.01365599\n",
      "Iteration 131, loss = 0.01359069\n",
      "Iteration 132, loss = 0.01353061\n",
      "Iteration 133, loss = 0.01345223\n",
      "Iteration 134, loss = 0.01340333\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.001, batch_size=50, hidden_layer_sizes=(400, 100), solver=sgd; total time= 8.6min\n",
      "Iteration 1, loss = 0.34037252\n",
      "Iteration 2, loss = 0.13707978\n",
      "Iteration 3, loss = 0.09702897\n",
      "Iteration 4, loss = 0.07267417\n",
      "Iteration 5, loss = 0.06260959\n",
      "Iteration 6, loss = 0.04859058\n",
      "Iteration 7, loss = 0.04188953\n",
      "Iteration 8, loss = 0.04149235\n",
      "Iteration 9, loss = 0.03517092\n",
      "Iteration 10, loss = 0.03105386\n",
      "Iteration 11, loss = 0.03148922\n",
      "Iteration 12, loss = 0.03208517\n",
      "Iteration 13, loss = 0.02511256\n",
      "Iteration 14, loss = 0.02306650\n",
      "Iteration 15, loss = 0.02955980\n",
      "Iteration 16, loss = 0.02521402\n",
      "Iteration 17, loss = 0.02848143\n",
      "Iteration 18, loss = 0.02203521\n",
      "Iteration 19, loss = 0.02774010\n",
      "Iteration 20, loss = 0.02251591\n",
      "Iteration 21, loss = 0.02397807\n",
      "Iteration 22, loss = 0.02122544\n",
      "Iteration 23, loss = 0.02264837\n",
      "Iteration 24, loss = 0.02372064\n",
      "Iteration 25, loss = 0.02132208\n",
      "Iteration 26, loss = 0.02903338\n",
      "Iteration 27, loss = 0.01903181\n",
      "Iteration 28, loss = 0.02236481\n",
      "Iteration 29, loss = 0.02332295\n",
      "Iteration 30, loss = 0.02448766\n",
      "Iteration 31, loss = 0.01670730\n",
      "Iteration 32, loss = 0.02504176\n",
      "Iteration 33, loss = 0.02032300\n",
      "Iteration 34, loss = 0.01656987\n",
      "Iteration 35, loss = 0.02881426\n",
      "Iteration 36, loss = 0.02158788\n",
      "Iteration 37, loss = 0.02087289\n",
      "Iteration 38, loss = 0.02069359\n",
      "Iteration 39, loss = 0.02102364\n",
      "Iteration 40, loss = 0.02366079\n",
      "Iteration 41, loss = 0.02365063\n",
      "Iteration 42, loss = 0.02111757\n",
      "Iteration 43, loss = 0.01867101\n",
      "Iteration 44, loss = 0.02302268\n",
      "Iteration 45, loss = 0.01884017\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.001, batch_size=50, hidden_layer_sizes=(100, 100, 100), solver=adam; total time= 2.3min\n",
      "Iteration 1, loss = 0.35256634\n",
      "Iteration 2, loss = 0.14481552\n",
      "Iteration 3, loss = 0.10070527\n",
      "Iteration 4, loss = 0.07887334\n",
      "Iteration 5, loss = 0.06498200\n",
      "Iteration 6, loss = 0.05371212\n",
      "Iteration 7, loss = 0.04585148\n",
      "Iteration 8, loss = 0.04135698\n",
      "Iteration 9, loss = 0.03483473\n",
      "Iteration 10, loss = 0.03653167\n",
      "Iteration 11, loss = 0.03096519\n",
      "Iteration 12, loss = 0.03039945\n",
      "Iteration 13, loss = 0.02821852\n",
      "Iteration 14, loss = 0.02955886\n",
      "Iteration 15, loss = 0.02579449\n",
      "Iteration 16, loss = 0.02339794\n",
      "Iteration 17, loss = 0.02545568\n",
      "Iteration 18, loss = 0.02838506\n",
      "Iteration 19, loss = 0.02466498\n",
      "Iteration 20, loss = 0.02181928\n",
      "Iteration 21, loss = 0.02825280\n",
      "Iteration 22, loss = 0.02205650\n",
      "Iteration 23, loss = 0.02288297\n",
      "Iteration 24, loss = 0.02241901\n",
      "Iteration 25, loss = 0.02393739\n",
      "Iteration 26, loss = 0.02577175\n",
      "Iteration 27, loss = 0.02119961\n",
      "Iteration 28, loss = 0.01936620\n",
      "Iteration 29, loss = 0.02347482\n",
      "Iteration 30, loss = 0.03073775\n",
      "Iteration 31, loss = 0.01718371\n",
      "Iteration 32, loss = 0.02356828\n",
      "Iteration 33, loss = 0.02330041\n",
      "Iteration 34, loss = 0.02110190\n",
      "Iteration 35, loss = 0.02196018\n",
      "Iteration 36, loss = 0.02102158\n",
      "Iteration 37, loss = 0.02114611\n",
      "Iteration 38, loss = 0.02296286\n",
      "Iteration 39, loss = 0.01630108\n",
      "Iteration 40, loss = 0.02180084\n",
      "Iteration 41, loss = 0.02665012\n",
      "Iteration 42, loss = 0.01974736\n",
      "Iteration 43, loss = 0.01864054\n",
      "Iteration 44, loss = 0.02303496\n",
      "Iteration 45, loss = 0.02488382\n",
      "Iteration 46, loss = 0.01793939\n",
      "Iteration 47, loss = 0.02503947\n",
      "Iteration 48, loss = 0.02082950\n",
      "Iteration 49, loss = 0.02312953\n",
      "Iteration 50, loss = 0.01750741\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.001, batch_size=50, hidden_layer_sizes=(100, 100, 100), solver=adam; total time= 2.9min\n",
      "Iteration 1, loss = 0.34006512\n",
      "Iteration 2, loss = 0.13601588\n",
      "Iteration 3, loss = 0.09842535\n",
      "Iteration 4, loss = 0.07424323\n",
      "Iteration 5, loss = 0.06224793\n",
      "Iteration 6, loss = 0.05224394\n",
      "Iteration 7, loss = 0.04223984\n",
      "Iteration 8, loss = 0.03821314\n",
      "Iteration 9, loss = 0.03512785\n",
      "Iteration 10, loss = 0.03368712\n",
      "Iteration 11, loss = 0.02910171\n",
      "Iteration 12, loss = 0.03241223\n",
      "Iteration 13, loss = 0.02638379\n",
      "Iteration 14, loss = 0.02593138\n",
      "Iteration 15, loss = 0.02509109\n",
      "Iteration 16, loss = 0.02856009\n",
      "Iteration 17, loss = 0.02195162\n",
      "Iteration 18, loss = 0.02607017\n",
      "Iteration 19, loss = 0.02641946\n",
      "Iteration 20, loss = 0.02044146\n",
      "Iteration 21, loss = 0.02375748\n",
      "Iteration 22, loss = 0.02528159\n",
      "Iteration 23, loss = 0.02271881\n",
      "Iteration 24, loss = 0.02079425\n",
      "Iteration 25, loss = 0.02271428\n",
      "Iteration 26, loss = 0.02397813\n",
      "Iteration 27, loss = 0.02245709\n",
      "Iteration 28, loss = 0.02458530\n",
      "Iteration 29, loss = 0.02222403\n",
      "Iteration 30, loss = 0.01887293\n",
      "Iteration 31, loss = 0.02032922\n",
      "Iteration 32, loss = 0.01969331\n",
      "Iteration 33, loss = 0.02621933\n",
      "Iteration 34, loss = 0.02145926\n",
      "Iteration 35, loss = 0.02248476\n",
      "Iteration 36, loss = 0.02411342\n",
      "Iteration 37, loss = 0.02365989\n",
      "Iteration 38, loss = 0.01943563\n",
      "Iteration 39, loss = 0.01473720\n",
      "Iteration 40, loss = 0.02729024\n",
      "Iteration 41, loss = 0.01967058\n",
      "Iteration 42, loss = 0.01959573\n",
      "Iteration 43, loss = 0.02452765\n",
      "Iteration 44, loss = 0.01922226\n",
      "Iteration 45, loss = 0.02345432\n",
      "Iteration 46, loss = 0.02007982\n",
      "Iteration 47, loss = 0.02086276\n",
      "Iteration 48, loss = 0.02107428\n",
      "Iteration 49, loss = 0.02285691\n",
      "Iteration 50, loss = 0.02126870\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.001, batch_size=50, hidden_layer_sizes=(100, 100, 100), solver=adam; total time= 2.5min\n",
      "Iteration 1, loss = 0.34855013\n",
      "Iteration 2, loss = 0.14622737\n",
      "Iteration 3, loss = 0.10455659\n",
      "Iteration 4, loss = 0.07796035\n",
      "Iteration 5, loss = 0.06499507\n",
      "Iteration 6, loss = 0.05083971\n",
      "Iteration 7, loss = 0.04387171\n",
      "Iteration 8, loss = 0.04233846\n",
      "Iteration 9, loss = 0.03465358\n",
      "Iteration 10, loss = 0.03729750\n",
      "Iteration 11, loss = 0.03034107\n",
      "Iteration 12, loss = 0.02764285\n",
      "Iteration 13, loss = 0.02862878\n",
      "Iteration 14, loss = 0.02642126\n",
      "Iteration 15, loss = 0.02746085\n",
      "Iteration 16, loss = 0.02664088\n",
      "Iteration 17, loss = 0.02589047\n",
      "Iteration 18, loss = 0.02545225\n",
      "Iteration 19, loss = 0.02296757\n",
      "Iteration 20, loss = 0.02184452\n",
      "Iteration 21, loss = 0.03184311\n",
      "Iteration 22, loss = 0.01695642\n",
      "Iteration 23, loss = 0.02743771\n",
      "Iteration 24, loss = 0.02085749\n",
      "Iteration 25, loss = 0.02138311\n",
      "Iteration 26, loss = 0.02495713\n",
      "Iteration 27, loss = 0.01919454\n",
      "Iteration 28, loss = 0.01851544\n",
      "Iteration 29, loss = 0.03043341\n",
      "Iteration 30, loss = 0.01920684\n",
      "Iteration 31, loss = 0.01999568\n",
      "Iteration 32, loss = 0.02498861\n",
      "Iteration 33, loss = 0.02415199\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.001, batch_size=50, hidden_layer_sizes=(100, 100, 100), solver=adam; total time= 1.8min\n",
      "Iteration 1, loss = 0.34102964\n",
      "Iteration 2, loss = 0.13948687\n",
      "Iteration 3, loss = 0.09790677\n",
      "Iteration 4, loss = 0.07679015\n",
      "Iteration 5, loss = 0.06325441\n",
      "Iteration 6, loss = 0.05078801\n",
      "Iteration 7, loss = 0.04510052\n",
      "Iteration 8, loss = 0.03891952\n",
      "Iteration 9, loss = 0.03723114\n",
      "Iteration 10, loss = 0.03400349\n",
      "Iteration 11, loss = 0.02980169\n",
      "Iteration 12, loss = 0.03062370\n",
      "Iteration 13, loss = 0.02982509\n",
      "Iteration 14, loss = 0.02745701\n",
      "Iteration 15, loss = 0.02624583\n",
      "Iteration 16, loss = 0.02558656\n",
      "Iteration 17, loss = 0.02662212\n",
      "Iteration 18, loss = 0.02004056\n",
      "Iteration 19, loss = 0.02707001\n",
      "Iteration 20, loss = 0.02563142\n",
      "Iteration 21, loss = 0.02877486\n",
      "Iteration 22, loss = 0.01963793\n",
      "Iteration 23, loss = 0.02206192\n",
      "Iteration 24, loss = 0.02871754\n",
      "Iteration 25, loss = 0.02498175\n",
      "Iteration 26, loss = 0.02298440\n",
      "Iteration 27, loss = 0.02334237\n",
      "Iteration 28, loss = 0.02057696\n",
      "Iteration 29, loss = 0.02163401\n",
      "Iteration 30, loss = 0.02366388\n",
      "Iteration 31, loss = 0.02408784\n",
      "Iteration 32, loss = 0.02683138\n",
      "Iteration 33, loss = 0.01705164\n",
      "Iteration 34, loss = 0.02703224\n",
      "Iteration 35, loss = 0.02336144\n",
      "Iteration 36, loss = 0.02035806\n",
      "Iteration 37, loss = 0.02281293\n",
      "Iteration 38, loss = 0.02230643\n",
      "Iteration 39, loss = 0.01851702\n",
      "Iteration 40, loss = 0.02222292\n",
      "Iteration 41, loss = 0.02549308\n",
      "Iteration 42, loss = 0.02024411\n",
      "Iteration 43, loss = 0.02410329\n",
      "Iteration 44, loss = 0.02512899\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.001, batch_size=50, hidden_layer_sizes=(100, 100, 100), solver=adam; total time= 2.4min\n",
      "Iteration 1, loss = 1.23838200\n",
      "Iteration 2, loss = 0.42868740\n",
      "Iteration 3, loss = 0.32959723\n",
      "Iteration 4, loss = 0.28697837\n",
      "Iteration 5, loss = 0.25786648\n",
      "Iteration 6, loss = 0.23556233\n",
      "Iteration 7, loss = 0.21509244\n",
      "Iteration 8, loss = 0.19922313\n",
      "Iteration 9, loss = 0.18521523\n",
      "Iteration 10, loss = 0.17330783\n",
      "Iteration 11, loss = 0.16121192\n",
      "Iteration 12, loss = 0.15214048\n",
      "Iteration 13, loss = 0.14273714\n",
      "Iteration 14, loss = 0.13529815\n",
      "Iteration 15, loss = 0.12818678\n",
      "Iteration 16, loss = 0.12163906\n",
      "Iteration 17, loss = 0.11566291\n",
      "Iteration 18, loss = 0.11002232\n",
      "Iteration 19, loss = 0.10401017\n",
      "Iteration 20, loss = 0.09962073\n",
      "Iteration 21, loss = 0.09517040\n",
      "Iteration 22, loss = 0.09012670\n",
      "Iteration 23, loss = 0.08691645\n",
      "Iteration 24, loss = 0.08246624\n",
      "Iteration 25, loss = 0.07898287\n",
      "Iteration 26, loss = 0.07528032\n",
      "Iteration 27, loss = 0.07214935\n",
      "Iteration 28, loss = 0.06939545\n",
      "Iteration 29, loss = 0.06626749\n",
      "Iteration 30, loss = 0.06361381\n",
      "Iteration 31, loss = 0.06050548\n",
      "Iteration 32, loss = 0.05813269\n",
      "Iteration 33, loss = 0.05564885\n",
      "Iteration 34, loss = 0.05353951\n",
      "Iteration 35, loss = 0.05119698\n",
      "Iteration 36, loss = 0.04921329\n",
      "Iteration 37, loss = 0.04745900\n",
      "Iteration 38, loss = 0.04522861\n",
      "Iteration 39, loss = 0.04332764\n",
      "Iteration 40, loss = 0.04186057\n",
      "Iteration 41, loss = 0.04046716\n",
      "Iteration 42, loss = 0.03866374\n",
      "Iteration 43, loss = 0.03739672\n",
      "Iteration 44, loss = 0.03540126\n",
      "Iteration 45, loss = 0.03434792\n",
      "Iteration 46, loss = 0.03293182\n",
      "Iteration 47, loss = 0.03166785\n",
      "Iteration 48, loss = 0.03057483\n",
      "Iteration 49, loss = 0.02915609\n",
      "Iteration 50, loss = 0.02811929\n",
      "Iteration 51, loss = 0.02760686\n",
      "Iteration 52, loss = 0.02673447\n",
      "Iteration 53, loss = 0.02550169\n",
      "Iteration 54, loss = 0.02454704\n",
      "Iteration 55, loss = 0.02367544\n",
      "Iteration 56, loss = 0.02321641\n",
      "Iteration 57, loss = 0.02205591\n",
      "Iteration 58, loss = 0.02139356\n",
      "Iteration 59, loss = 0.02077932\n",
      "Iteration 60, loss = 0.02020654\n",
      "Iteration 61, loss = 0.01972341\n",
      "Iteration 62, loss = 0.01911152\n",
      "Iteration 63, loss = 0.01839188\n",
      "Iteration 64, loss = 0.01788347\n",
      "Iteration 65, loss = 0.01766396\n",
      "Iteration 66, loss = 0.01703479\n",
      "Iteration 67, loss = 0.01657841\n",
      "Iteration 68, loss = 0.01620440\n",
      "Iteration 69, loss = 0.01566514\n",
      "Iteration 70, loss = 0.01513332\n",
      "Iteration 71, loss = 0.01502099\n",
      "Iteration 72, loss = 0.01460883\n",
      "Iteration 73, loss = 0.01407457\n",
      "Iteration 74, loss = 0.01375940\n",
      "Iteration 75, loss = 0.01354037\n",
      "Iteration 76, loss = 0.01334032\n",
      "Iteration 77, loss = 0.01286432\n",
      "Iteration 78, loss = 0.01272394\n",
      "Iteration 79, loss = 0.01235604\n",
      "Iteration 80, loss = 0.01215690\n",
      "Iteration 81, loss = 0.01190412\n",
      "Iteration 82, loss = 0.01165168\n",
      "Iteration 83, loss = 0.01149746\n",
      "Iteration 84, loss = 0.01124022\n",
      "Iteration 85, loss = 0.01110574\n",
      "Iteration 86, loss = 0.01091982\n",
      "Iteration 87, loss = 0.01075978\n",
      "Iteration 88, loss = 0.01052448\n",
      "Iteration 89, loss = 0.01039305\n",
      "Iteration 90, loss = 0.01026883\n",
      "Iteration 91, loss = 0.01008105\n",
      "Iteration 92, loss = 0.01000268\n",
      "Iteration 93, loss = 0.00986163\n",
      "Iteration 94, loss = 0.00970676\n",
      "Iteration 95, loss = 0.00959351\n",
      "Iteration 96, loss = 0.00950293\n",
      "Iteration 97, loss = 0.00933408\n",
      "Iteration 98, loss = 0.00924623\n",
      "Iteration 99, loss = 0.00919126\n",
      "Iteration 100, loss = 0.00908386\n",
      "Iteration 101, loss = 0.00904442\n",
      "Iteration 102, loss = 0.00891217\n",
      "Iteration 103, loss = 0.00886036\n",
      "Iteration 104, loss = 0.00878409\n",
      "Iteration 105, loss = 0.00869900\n",
      "Iteration 106, loss = 0.00860214\n",
      "Iteration 107, loss = 0.00858010\n",
      "Iteration 108, loss = 0.00850145\n",
      "Iteration 109, loss = 0.00842843\n",
      "Iteration 110, loss = 0.00838502\n",
      "Iteration 111, loss = 0.00832578\n",
      "Iteration 112, loss = 0.00827703\n",
      "Iteration 113, loss = 0.00821626\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.001, batch_size=50, hidden_layer_sizes=(100, 100, 100), solver=sgd; total time= 3.0min\n",
      "Iteration 1, loss = 1.13399355\n",
      "Iteration 2, loss = 0.40962155\n",
      "Iteration 3, loss = 0.33627573\n",
      "Iteration 4, loss = 0.29705085\n",
      "Iteration 5, loss = 0.26738304\n",
      "Iteration 6, loss = 0.24491269\n",
      "Iteration 7, loss = 0.22541647\n",
      "Iteration 8, loss = 0.20846498\n",
      "Iteration 9, loss = 0.19421280\n",
      "Iteration 10, loss = 0.18085834\n",
      "Iteration 11, loss = 0.17028611\n",
      "Iteration 12, loss = 0.16062062\n",
      "Iteration 13, loss = 0.15165053\n",
      "Iteration 14, loss = 0.14414827\n",
      "Iteration 15, loss = 0.13682235\n",
      "Iteration 16, loss = 0.12935069\n",
      "Iteration 17, loss = 0.12324195\n",
      "Iteration 18, loss = 0.11730968\n",
      "Iteration 19, loss = 0.11183832\n",
      "Iteration 20, loss = 0.10663330\n",
      "Iteration 21, loss = 0.10188223\n",
      "Iteration 22, loss = 0.09753818\n",
      "Iteration 23, loss = 0.09315268\n",
      "Iteration 24, loss = 0.08961681\n",
      "Iteration 25, loss = 0.08535872\n",
      "Iteration 26, loss = 0.08205079\n",
      "Iteration 27, loss = 0.07841421\n",
      "Iteration 28, loss = 0.07517152\n",
      "Iteration 29, loss = 0.07262364\n",
      "Iteration 30, loss = 0.06980818\n",
      "Iteration 31, loss = 0.06722133\n",
      "Iteration 32, loss = 0.06453574\n",
      "Iteration 33, loss = 0.06186036\n",
      "Iteration 34, loss = 0.05951401\n",
      "Iteration 35, loss = 0.05717871\n",
      "Iteration 36, loss = 0.05557625\n",
      "Iteration 37, loss = 0.05288623\n",
      "Iteration 38, loss = 0.05107063\n",
      "Iteration 39, loss = 0.04874511\n",
      "Iteration 40, loss = 0.04735331\n",
      "Iteration 41, loss = 0.04552917\n",
      "Iteration 42, loss = 0.04367677\n",
      "Iteration 43, loss = 0.04219611\n",
      "Iteration 44, loss = 0.04077276\n",
      "Iteration 45, loss = 0.03913997\n",
      "Iteration 46, loss = 0.03790182\n",
      "Iteration 47, loss = 0.03665665\n",
      "Iteration 48, loss = 0.03501879\n",
      "Iteration 49, loss = 0.03411734\n",
      "Iteration 50, loss = 0.03280416\n",
      "Iteration 51, loss = 0.03144095\n",
      "Iteration 52, loss = 0.03046047\n",
      "Iteration 53, loss = 0.02931149\n",
      "Iteration 54, loss = 0.02858425\n",
      "Iteration 55, loss = 0.02729747\n",
      "Iteration 56, loss = 0.02642377\n",
      "Iteration 57, loss = 0.02560869\n",
      "Iteration 58, loss = 0.02477176\n",
      "Iteration 59, loss = 0.02407313\n",
      "Iteration 60, loss = 0.02328007\n",
      "Iteration 61, loss = 0.02247173\n",
      "Iteration 62, loss = 0.02170040\n",
      "Iteration 63, loss = 0.02079883\n",
      "Iteration 64, loss = 0.02052554\n",
      "Iteration 65, loss = 0.01968235\n",
      "Iteration 66, loss = 0.01915666\n",
      "Iteration 67, loss = 0.01854860\n",
      "Iteration 68, loss = 0.01792519\n",
      "Iteration 69, loss = 0.01738192\n",
      "Iteration 70, loss = 0.01700223\n",
      "Iteration 71, loss = 0.01665254\n",
      "Iteration 72, loss = 0.01601613\n",
      "Iteration 73, loss = 0.01565034\n",
      "Iteration 74, loss = 0.01516527\n",
      "Iteration 75, loss = 0.01486290\n",
      "Iteration 76, loss = 0.01445482\n",
      "Iteration 77, loss = 0.01413059\n",
      "Iteration 78, loss = 0.01385779\n",
      "Iteration 79, loss = 0.01337735\n",
      "Iteration 80, loss = 0.01310974\n",
      "Iteration 81, loss = 0.01297039\n",
      "Iteration 82, loss = 0.01261521\n",
      "Iteration 83, loss = 0.01239348\n",
      "Iteration 84, loss = 0.01214610\n",
      "Iteration 85, loss = 0.01201992\n",
      "Iteration 86, loss = 0.01169024\n",
      "Iteration 87, loss = 0.01152994\n",
      "Iteration 88, loss = 0.01114956\n",
      "Iteration 89, loss = 0.01109963\n",
      "Iteration 90, loss = 0.01094391\n",
      "Iteration 91, loss = 0.01072959\n",
      "Iteration 92, loss = 0.01052085\n",
      "Iteration 93, loss = 0.01031369\n",
      "Iteration 94, loss = 0.01023882\n",
      "Iteration 95, loss = 0.01006510\n",
      "Iteration 96, loss = 0.00995525\n",
      "Iteration 97, loss = 0.00983797\n",
      "Iteration 98, loss = 0.00965826\n",
      "Iteration 99, loss = 0.00960069\n",
      "Iteration 100, loss = 0.00952898\n",
      "Iteration 101, loss = 0.00939763\n",
      "Iteration 102, loss = 0.00927758\n",
      "Iteration 103, loss = 0.00921337\n",
      "Iteration 104, loss = 0.00914528\n",
      "Iteration 105, loss = 0.00898371\n",
      "Iteration 106, loss = 0.00894576\n",
      "Iteration 107, loss = 0.00893243\n",
      "Iteration 108, loss = 0.00879398\n",
      "Iteration 109, loss = 0.00869476\n",
      "Iteration 110, loss = 0.00869405\n",
      "Iteration 111, loss = 0.00860247\n",
      "Iteration 112, loss = 0.00854626\n",
      "Iteration 113, loss = 0.00848072\n",
      "Iteration 114, loss = 0.00842552\n",
      "Iteration 115, loss = 0.00838739\n",
      "Iteration 116, loss = 0.00830748\n",
      "Iteration 117, loss = 0.00823899\n",
      "Iteration 118, loss = 0.00821788\n",
      "Iteration 119, loss = 0.00818209\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.001, batch_size=50, hidden_layer_sizes=(100, 100, 100), solver=sgd; total time= 3.1min\n",
      "Iteration 1, loss = 1.27348538\n",
      "Iteration 2, loss = 0.42256002\n",
      "Iteration 3, loss = 0.33238355\n",
      "Iteration 4, loss = 0.29167483\n",
      "Iteration 5, loss = 0.26207848\n",
      "Iteration 6, loss = 0.23944597\n",
      "Iteration 7, loss = 0.21940797\n",
      "Iteration 8, loss = 0.20249999\n",
      "Iteration 9, loss = 0.18896217\n",
      "Iteration 10, loss = 0.17686591\n",
      "Iteration 11, loss = 0.16512835\n",
      "Iteration 12, loss = 0.15497927\n",
      "Iteration 13, loss = 0.14677217\n",
      "Iteration 14, loss = 0.13818794\n",
      "Iteration 15, loss = 0.13126545\n",
      "Iteration 16, loss = 0.12434932\n",
      "Iteration 17, loss = 0.11810655\n",
      "Iteration 18, loss = 0.11250354\n",
      "Iteration 19, loss = 0.10649813\n",
      "Iteration 20, loss = 0.10194089\n",
      "Iteration 21, loss = 0.09720980\n",
      "Iteration 22, loss = 0.09305372\n",
      "Iteration 23, loss = 0.08825370\n",
      "Iteration 24, loss = 0.08440056\n",
      "Iteration 25, loss = 0.08086503\n",
      "Iteration 26, loss = 0.07773806\n",
      "Iteration 27, loss = 0.07365556\n",
      "Iteration 28, loss = 0.07095933\n",
      "Iteration 29, loss = 0.06791878\n",
      "Iteration 30, loss = 0.06513075\n",
      "Iteration 31, loss = 0.06209979\n",
      "Iteration 32, loss = 0.05961878\n",
      "Iteration 33, loss = 0.05755811\n",
      "Iteration 34, loss = 0.05488208\n",
      "Iteration 35, loss = 0.05277101\n",
      "Iteration 36, loss = 0.05122870\n",
      "Iteration 37, loss = 0.04900975\n",
      "Iteration 38, loss = 0.04699394\n",
      "Iteration 39, loss = 0.04515923\n",
      "Iteration 40, loss = 0.04341485\n",
      "Iteration 41, loss = 0.04179511\n",
      "Iteration 42, loss = 0.03997415\n",
      "Iteration 43, loss = 0.03858024\n",
      "Iteration 44, loss = 0.03697343\n",
      "Iteration 45, loss = 0.03544678\n",
      "Iteration 46, loss = 0.03420805\n",
      "Iteration 47, loss = 0.03295993\n",
      "Iteration 48, loss = 0.03193207\n",
      "Iteration 49, loss = 0.03074742\n",
      "Iteration 50, loss = 0.02956309\n",
      "Iteration 51, loss = 0.02848971\n",
      "Iteration 52, loss = 0.02730401\n",
      "Iteration 53, loss = 0.02634242\n",
      "Iteration 54, loss = 0.02556824\n",
      "Iteration 55, loss = 0.02461103\n",
      "Iteration 56, loss = 0.02358868\n",
      "Iteration 57, loss = 0.02278482\n",
      "Iteration 58, loss = 0.02207378\n",
      "Iteration 59, loss = 0.02148029\n",
      "Iteration 60, loss = 0.02076205\n",
      "Iteration 61, loss = 0.02017829\n",
      "Iteration 62, loss = 0.01946469\n",
      "Iteration 63, loss = 0.01873394\n",
      "Iteration 64, loss = 0.01820643\n",
      "Iteration 65, loss = 0.01774970\n",
      "Iteration 66, loss = 0.01722829\n",
      "Iteration 67, loss = 0.01672432\n",
      "Iteration 68, loss = 0.01638504\n",
      "Iteration 69, loss = 0.01589650\n",
      "Iteration 70, loss = 0.01564107\n",
      "Iteration 71, loss = 0.01514792\n",
      "Iteration 72, loss = 0.01473341\n",
      "Iteration 73, loss = 0.01448767\n",
      "Iteration 74, loss = 0.01414349\n",
      "Iteration 75, loss = 0.01386941\n",
      "Iteration 76, loss = 0.01351338\n",
      "Iteration 77, loss = 0.01321267\n",
      "Iteration 78, loss = 0.01299275\n",
      "Iteration 79, loss = 0.01268499\n",
      "Iteration 80, loss = 0.01254030\n",
      "Iteration 81, loss = 0.01220326\n",
      "Iteration 82, loss = 0.01207999\n",
      "Iteration 83, loss = 0.01183732\n",
      "Iteration 84, loss = 0.01165919\n",
      "Iteration 85, loss = 0.01141261\n",
      "Iteration 86, loss = 0.01131828\n",
      "Iteration 87, loss = 0.01107561\n",
      "Iteration 88, loss = 0.01085600\n",
      "Iteration 89, loss = 0.01077647\n",
      "Iteration 90, loss = 0.01065181\n",
      "Iteration 91, loss = 0.01043998\n",
      "Iteration 92, loss = 0.01039220\n",
      "Iteration 93, loss = 0.01020662\n",
      "Iteration 94, loss = 0.01009027\n",
      "Iteration 95, loss = 0.00996332\n",
      "Iteration 96, loss = 0.00986258\n",
      "Iteration 97, loss = 0.00966492\n",
      "Iteration 98, loss = 0.00960168\n",
      "Iteration 99, loss = 0.00945471\n",
      "Iteration 100, loss = 0.00933932\n",
      "Iteration 101, loss = 0.00926622\n",
      "Iteration 102, loss = 0.00924209\n",
      "Iteration 103, loss = 0.00913079\n",
      "Iteration 104, loss = 0.00904360\n",
      "Iteration 105, loss = 0.00896674\n",
      "Iteration 106, loss = 0.00887630\n",
      "Iteration 107, loss = 0.00879194\n",
      "Iteration 108, loss = 0.00874226\n",
      "Iteration 109, loss = 0.00865348\n",
      "Iteration 110, loss = 0.00857571\n",
      "Iteration 111, loss = 0.00853169\n",
      "Iteration 112, loss = 0.00848973\n",
      "Iteration 113, loss = 0.00839374\n",
      "Iteration 114, loss = 0.00835447\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.001, batch_size=50, hidden_layer_sizes=(100, 100, 100), solver=sgd; total time= 2.9min\n",
      "Iteration 1, loss = 1.27197805\n",
      "Iteration 2, loss = 0.43253093\n",
      "Iteration 3, loss = 0.33583969\n",
      "Iteration 4, loss = 0.29252245\n",
      "Iteration 5, loss = 0.26212040\n",
      "Iteration 6, loss = 0.23811636\n",
      "Iteration 7, loss = 0.21860815\n",
      "Iteration 8, loss = 0.20192080\n",
      "Iteration 9, loss = 0.18778111\n",
      "Iteration 10, loss = 0.17556622\n",
      "Iteration 11, loss = 0.16446064\n",
      "Iteration 12, loss = 0.15447775\n",
      "Iteration 13, loss = 0.14551843\n",
      "Iteration 14, loss = 0.13798057\n",
      "Iteration 15, loss = 0.13086598\n",
      "Iteration 16, loss = 0.12453173\n",
      "Iteration 17, loss = 0.11782896\n",
      "Iteration 18, loss = 0.11283064\n",
      "Iteration 19, loss = 0.10767300\n",
      "Iteration 20, loss = 0.10275160\n",
      "Iteration 21, loss = 0.09816193\n",
      "Iteration 22, loss = 0.09399902\n",
      "Iteration 23, loss = 0.09006075\n",
      "Iteration 24, loss = 0.08656234\n",
      "Iteration 25, loss = 0.08295588\n",
      "Iteration 26, loss = 0.08010600\n",
      "Iteration 27, loss = 0.07626407\n",
      "Iteration 28, loss = 0.07290355\n",
      "Iteration 29, loss = 0.07046934\n",
      "Iteration 30, loss = 0.06826707\n",
      "Iteration 31, loss = 0.06503725\n",
      "Iteration 32, loss = 0.06287980\n",
      "Iteration 33, loss = 0.06045083\n",
      "Iteration 34, loss = 0.05800818\n",
      "Iteration 35, loss = 0.05562447\n",
      "Iteration 36, loss = 0.05369699\n",
      "Iteration 37, loss = 0.05137933\n",
      "Iteration 38, loss = 0.04995058\n",
      "Iteration 39, loss = 0.04827015\n",
      "Iteration 40, loss = 0.04680755\n",
      "Iteration 41, loss = 0.04439734\n",
      "Iteration 42, loss = 0.04298386\n",
      "Iteration 43, loss = 0.04107290\n",
      "Iteration 44, loss = 0.03965912\n",
      "Iteration 45, loss = 0.03862303\n",
      "Iteration 46, loss = 0.03695325\n",
      "Iteration 47, loss = 0.03612312\n",
      "Iteration 48, loss = 0.03479406\n",
      "Iteration 49, loss = 0.03310930\n",
      "Iteration 50, loss = 0.03209877\n",
      "Iteration 51, loss = 0.03109382\n",
      "Iteration 52, loss = 0.03012022\n",
      "Iteration 53, loss = 0.02923977\n",
      "Iteration 54, loss = 0.02799396\n",
      "Iteration 55, loss = 0.02769039\n",
      "Iteration 56, loss = 0.02633665\n",
      "Iteration 57, loss = 0.02557389\n",
      "Iteration 58, loss = 0.02475278\n",
      "Iteration 59, loss = 0.02375162\n",
      "Iteration 60, loss = 0.02323450\n",
      "Iteration 61, loss = 0.02261298\n",
      "Iteration 62, loss = 0.02181271\n",
      "Iteration 63, loss = 0.02113351\n",
      "Iteration 64, loss = 0.02045103\n",
      "Iteration 65, loss = 0.01996047\n",
      "Iteration 66, loss = 0.01939447\n",
      "Iteration 67, loss = 0.01884022\n",
      "Iteration 68, loss = 0.01829867\n",
      "Iteration 69, loss = 0.01792551\n",
      "Iteration 70, loss = 0.01738031\n",
      "Iteration 71, loss = 0.01692629\n",
      "Iteration 72, loss = 0.01642500\n",
      "Iteration 73, loss = 0.01614859\n",
      "Iteration 74, loss = 0.01551662\n",
      "Iteration 75, loss = 0.01529371\n",
      "Iteration 76, loss = 0.01496747\n",
      "Iteration 77, loss = 0.01466119\n",
      "Iteration 78, loss = 0.01429535\n",
      "Iteration 79, loss = 0.01397088\n",
      "Iteration 80, loss = 0.01361177\n",
      "Iteration 81, loss = 0.01332246\n",
      "Iteration 82, loss = 0.01304849\n",
      "Iteration 83, loss = 0.01281502\n",
      "Iteration 84, loss = 0.01251391\n",
      "Iteration 85, loss = 0.01242981\n",
      "Iteration 86, loss = 0.01214269\n",
      "Iteration 87, loss = 0.01190918\n",
      "Iteration 88, loss = 0.01160283\n",
      "Iteration 89, loss = 0.01150891\n",
      "Iteration 90, loss = 0.01132278\n",
      "Iteration 91, loss = 0.01111692\n",
      "Iteration 92, loss = 0.01093300\n",
      "Iteration 93, loss = 0.01076351\n",
      "Iteration 94, loss = 0.01069425\n",
      "Iteration 95, loss = 0.01046691\n",
      "Iteration 96, loss = 0.01036850\n",
      "Iteration 97, loss = 0.01015458\n",
      "Iteration 98, loss = 0.01006806\n",
      "Iteration 99, loss = 0.01000262\n",
      "Iteration 100, loss = 0.00989278\n",
      "Iteration 101, loss = 0.00976540\n",
      "Iteration 102, loss = 0.00965812\n",
      "Iteration 103, loss = 0.00954317\n",
      "Iteration 104, loss = 0.00950022\n",
      "Iteration 105, loss = 0.00934752\n",
      "Iteration 106, loss = 0.00925184\n",
      "Iteration 107, loss = 0.00918343\n",
      "Iteration 108, loss = 0.00906778\n",
      "Iteration 109, loss = 0.00903918\n",
      "Iteration 110, loss = 0.00895154\n",
      "Iteration 111, loss = 0.00893031\n",
      "Iteration 112, loss = 0.00886126\n",
      "Iteration 113, loss = 0.00876614\n",
      "Iteration 114, loss = 0.00868845\n",
      "Iteration 115, loss = 0.00866017\n",
      "Iteration 116, loss = 0.00860545\n",
      "Iteration 117, loss = 0.00854264\n",
      "Iteration 118, loss = 0.00847864\n",
      "Iteration 119, loss = 0.00843397\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.001, batch_size=50, hidden_layer_sizes=(100, 100, 100), solver=sgd; total time= 3.1min\n",
      "Iteration 1, loss = 1.14465287\n",
      "Iteration 2, loss = 0.40446468\n",
      "Iteration 3, loss = 0.32243946\n",
      "Iteration 4, loss = 0.28252372\n",
      "Iteration 5, loss = 0.25457439\n",
      "Iteration 6, loss = 0.23307544\n",
      "Iteration 7, loss = 0.21585181\n",
      "Iteration 8, loss = 0.20079842\n",
      "Iteration 9, loss = 0.18763999\n",
      "Iteration 10, loss = 0.17551626\n",
      "Iteration 11, loss = 0.16524302\n",
      "Iteration 12, loss = 0.15656108\n",
      "Iteration 13, loss = 0.14785880\n",
      "Iteration 14, loss = 0.14000813\n",
      "Iteration 15, loss = 0.13333005\n",
      "Iteration 16, loss = 0.12616725\n",
      "Iteration 17, loss = 0.12081804\n",
      "Iteration 18, loss = 0.11486218\n",
      "Iteration 19, loss = 0.10989148\n",
      "Iteration 20, loss = 0.10499643\n",
      "Iteration 21, loss = 0.10045424\n",
      "Iteration 22, loss = 0.09665558\n",
      "Iteration 23, loss = 0.09220961\n",
      "Iteration 24, loss = 0.08847322\n",
      "Iteration 25, loss = 0.08497938\n",
      "Iteration 26, loss = 0.08089686\n",
      "Iteration 27, loss = 0.07819254\n",
      "Iteration 28, loss = 0.07547171\n",
      "Iteration 29, loss = 0.07246658\n",
      "Iteration 30, loss = 0.06935214\n",
      "Iteration 31, loss = 0.06723799\n",
      "Iteration 32, loss = 0.06409333\n",
      "Iteration 33, loss = 0.06188518\n",
      "Iteration 34, loss = 0.05970967\n",
      "Iteration 35, loss = 0.05807884\n",
      "Iteration 36, loss = 0.05523214\n",
      "Iteration 37, loss = 0.05296102\n",
      "Iteration 38, loss = 0.05130188\n",
      "Iteration 39, loss = 0.04948890\n",
      "Iteration 40, loss = 0.04731513\n",
      "Iteration 41, loss = 0.04587381\n",
      "Iteration 42, loss = 0.04421516\n",
      "Iteration 43, loss = 0.04236680\n",
      "Iteration 44, loss = 0.04110407\n",
      "Iteration 45, loss = 0.03996109\n",
      "Iteration 46, loss = 0.03810452\n",
      "Iteration 47, loss = 0.03696555\n",
      "Iteration 48, loss = 0.03556185\n",
      "Iteration 49, loss = 0.03433223\n",
      "Iteration 50, loss = 0.03306447\n",
      "Iteration 51, loss = 0.03176730\n",
      "Iteration 52, loss = 0.03068104\n",
      "Iteration 53, loss = 0.02938538\n",
      "Iteration 54, loss = 0.02831896\n",
      "Iteration 55, loss = 0.02781248\n",
      "Iteration 56, loss = 0.02656908\n",
      "Iteration 57, loss = 0.02558325\n",
      "Iteration 58, loss = 0.02486280\n",
      "Iteration 59, loss = 0.02390995\n",
      "Iteration 60, loss = 0.02315131\n",
      "Iteration 61, loss = 0.02231350\n",
      "Iteration 62, loss = 0.02188206\n",
      "Iteration 63, loss = 0.02103464\n",
      "Iteration 64, loss = 0.02042246\n",
      "Iteration 65, loss = 0.02014429\n",
      "Iteration 66, loss = 0.01908412\n",
      "Iteration 67, loss = 0.01871891\n",
      "Iteration 68, loss = 0.01797944\n",
      "Iteration 69, loss = 0.01758752\n",
      "Iteration 70, loss = 0.01704849\n",
      "Iteration 71, loss = 0.01660294\n",
      "Iteration 72, loss = 0.01622748\n",
      "Iteration 73, loss = 0.01552570\n",
      "Iteration 74, loss = 0.01540608\n",
      "Iteration 75, loss = 0.01499582\n",
      "Iteration 76, loss = 0.01459635\n",
      "Iteration 77, loss = 0.01430716\n",
      "Iteration 78, loss = 0.01388435\n",
      "Iteration 79, loss = 0.01371291\n",
      "Iteration 80, loss = 0.01333666\n",
      "Iteration 81, loss = 0.01310301\n",
      "Iteration 82, loss = 0.01280944\n",
      "Iteration 83, loss = 0.01267204\n",
      "Iteration 84, loss = 0.01228947\n",
      "Iteration 85, loss = 0.01208762\n",
      "Iteration 86, loss = 0.01191294\n",
      "Iteration 87, loss = 0.01164409\n",
      "Iteration 88, loss = 0.01142278\n",
      "Iteration 89, loss = 0.01126456\n",
      "Iteration 90, loss = 0.01119051\n",
      "Iteration 91, loss = 0.01096197\n",
      "Iteration 92, loss = 0.01069183\n",
      "Iteration 93, loss = 0.01067047\n",
      "Iteration 94, loss = 0.01041213\n",
      "Iteration 95, loss = 0.01032177\n",
      "Iteration 96, loss = 0.01012324\n",
      "Iteration 97, loss = 0.01014926\n",
      "Iteration 98, loss = 0.00996068\n",
      "Iteration 99, loss = 0.00972832\n",
      "Iteration 100, loss = 0.00971254\n",
      "Iteration 101, loss = 0.00961500\n",
      "Iteration 102, loss = 0.00946912\n",
      "Iteration 103, loss = 0.00939916\n",
      "Iteration 104, loss = 0.00927326\n",
      "Iteration 105, loss = 0.00918629\n",
      "Iteration 106, loss = 0.00910599\n",
      "Iteration 107, loss = 0.00911044\n",
      "Iteration 108, loss = 0.00896347\n",
      "Iteration 109, loss = 0.00887355\n",
      "Iteration 110, loss = 0.00883714\n",
      "Iteration 111, loss = 0.00874553\n",
      "Iteration 112, loss = 0.00870311\n",
      "Iteration 113, loss = 0.00866319\n",
      "Iteration 114, loss = 0.00854113\n",
      "Iteration 115, loss = 0.00853071\n",
      "Iteration 116, loss = 0.00847361\n",
      "Iteration 117, loss = 0.00840962\n",
      "Iteration 118, loss = 0.00839065\n",
      "Iteration 119, loss = 0.00833689\n",
      "Iteration 120, loss = 0.00825530\n",
      "Iteration 121, loss = 0.00821239\n",
      "Iteration 122, loss = 0.00818019\n",
      "Iteration 123, loss = 0.00814824\n",
      "Iteration 124, loss = 0.00811949\n",
      "Iteration 125, loss = 0.00804150\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.001, batch_size=50, hidden_layer_sizes=(100, 100, 100), solver=sgd; total time= 3.3min\n",
      "Iteration 1, loss = 0.44808657\n",
      "Iteration 2, loss = 0.21360884\n",
      "Iteration 3, loss = 0.16217985\n",
      "Iteration 4, loss = 0.13095017\n",
      "Iteration 5, loss = 0.10882108\n",
      "Iteration 6, loss = 0.09094783\n",
      "Iteration 7, loss = 0.07781466\n",
      "Iteration 8, loss = 0.06608080\n",
      "Iteration 9, loss = 0.05735771\n",
      "Iteration 10, loss = 0.05049036\n",
      "Iteration 11, loss = 0.04490891\n",
      "Iteration 12, loss = 0.03871623\n",
      "Iteration 13, loss = 0.03370946\n",
      "Iteration 14, loss = 0.03001451\n",
      "Iteration 15, loss = 0.02723199\n",
      "Iteration 16, loss = 0.02488072\n",
      "Iteration 17, loss = 0.02156789\n",
      "Iteration 18, loss = 0.01880582\n",
      "Iteration 19, loss = 0.01798535\n",
      "Iteration 20, loss = 0.01559487\n",
      "Iteration 21, loss = 0.01492256\n",
      "Iteration 22, loss = 0.01379226\n",
      "Iteration 23, loss = 0.01240858\n",
      "Iteration 24, loss = 0.01181066\n",
      "Iteration 25, loss = 0.01355165\n",
      "Iteration 26, loss = 0.01053544\n",
      "Iteration 27, loss = 0.00930986\n",
      "Iteration 28, loss = 0.00961998\n",
      "Iteration 29, loss = 0.01394197\n",
      "Iteration 30, loss = 0.01205872\n",
      "Iteration 31, loss = 0.00836507\n",
      "Iteration 32, loss = 0.00756160\n",
      "Iteration 33, loss = 0.00723929\n",
      "Iteration 34, loss = 0.00725534\n",
      "Iteration 35, loss = 0.01477859\n",
      "Iteration 36, loss = 0.01770552\n",
      "Iteration 37, loss = 0.01068241\n",
      "Iteration 38, loss = 0.00746986\n",
      "Iteration 39, loss = 0.00700770\n",
      "Iteration 40, loss = 0.00681252\n",
      "Iteration 41, loss = 0.00664519\n",
      "Iteration 42, loss = 0.00651281\n",
      "Iteration 43, loss = 0.00645419\n",
      "Iteration 44, loss = 0.00655285\n",
      "Iteration 45, loss = 0.01057485\n",
      "Iteration 46, loss = 0.02211029\n",
      "Iteration 47, loss = 0.01004229\n",
      "Iteration 48, loss = 0.00722478\n",
      "Iteration 49, loss = 0.00665741\n",
      "Iteration 50, loss = 0.00651973\n",
      "Iteration 51, loss = 0.00638061\n",
      "Iteration 52, loss = 0.00627048\n",
      "Iteration 53, loss = 0.00618886\n",
      "Iteration 54, loss = 0.00634391\n",
      "Iteration 55, loss = 0.02203015\n",
      "Iteration 56, loss = 0.01211449\n",
      "Iteration 57, loss = 0.00719855\n",
      "Iteration 58, loss = 0.00660789\n",
      "Iteration 59, loss = 0.00642474\n",
      "Iteration 60, loss = 0.00630372\n",
      "Iteration 61, loss = 0.00619232\n",
      "Iteration 62, loss = 0.00609982\n",
      "Iteration 63, loss = 0.00601091\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.001, batch_size=100, hidden_layer_sizes=(100,), solver=adam; total time= 1.2min\n",
      "Iteration 1, loss = 0.43524537\n",
      "Iteration 2, loss = 0.21514577\n",
      "Iteration 3, loss = 0.16527755\n",
      "Iteration 4, loss = 0.13299489\n",
      "Iteration 5, loss = 0.11015192\n",
      "Iteration 6, loss = 0.09384407\n",
      "Iteration 7, loss = 0.08008251\n",
      "Iteration 8, loss = 0.06797606\n",
      "Iteration 9, loss = 0.05932381\n",
      "Iteration 10, loss = 0.05185530\n",
      "Iteration 11, loss = 0.04591414\n",
      "Iteration 12, loss = 0.04031739\n",
      "Iteration 13, loss = 0.03585022\n",
      "Iteration 14, loss = 0.03089948\n",
      "Iteration 15, loss = 0.02770353\n",
      "Iteration 16, loss = 0.02517197\n",
      "Iteration 17, loss = 0.02263540\n",
      "Iteration 18, loss = 0.02054157\n",
      "Iteration 19, loss = 0.01797040\n",
      "Iteration 20, loss = 0.01652602\n",
      "Iteration 21, loss = 0.01585341\n",
      "Iteration 22, loss = 0.01475775\n",
      "Iteration 23, loss = 0.01362425\n",
      "Iteration 24, loss = 0.01213113\n",
      "Iteration 25, loss = 0.01117916\n",
      "Iteration 26, loss = 0.01201206\n",
      "Iteration 27, loss = 0.01118733\n",
      "Iteration 28, loss = 0.01175468\n",
      "Iteration 29, loss = 0.01178887\n",
      "Iteration 30, loss = 0.00925556\n",
      "Iteration 31, loss = 0.00849311\n",
      "Iteration 32, loss = 0.00819798\n",
      "Iteration 33, loss = 0.00812405\n",
      "Iteration 34, loss = 0.01550313\n",
      "Iteration 35, loss = 0.01107219\n",
      "Iteration 36, loss = 0.01292223\n",
      "Iteration 37, loss = 0.00751905\n",
      "Iteration 38, loss = 0.00710349\n",
      "Iteration 39, loss = 0.00687094\n",
      "Iteration 40, loss = 0.00674472\n",
      "Iteration 41, loss = 0.00666078\n",
      "Iteration 42, loss = 0.01030699\n",
      "Iteration 43, loss = 0.02253857\n",
      "Iteration 44, loss = 0.00831649\n",
      "Iteration 45, loss = 0.00709967\n",
      "Iteration 46, loss = 0.00675533\n",
      "Iteration 47, loss = 0.00659215\n",
      "Iteration 48, loss = 0.00649508\n",
      "Iteration 49, loss = 0.00645218\n",
      "Iteration 50, loss = 0.01770818\n",
      "Iteration 51, loss = 0.01694482\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.001, batch_size=100, hidden_layer_sizes=(100,), solver=adam; total time=  47.7s\n",
      "Iteration 1, loss = 0.44609547\n",
      "Iteration 2, loss = 0.20816660\n",
      "Iteration 3, loss = 0.15614391\n",
      "Iteration 4, loss = 0.12483862\n",
      "Iteration 5, loss = 0.10323724\n",
      "Iteration 6, loss = 0.08744764\n",
      "Iteration 7, loss = 0.07437884\n",
      "Iteration 8, loss = 0.06411227\n",
      "Iteration 9, loss = 0.05636877\n",
      "Iteration 10, loss = 0.04932678\n",
      "Iteration 11, loss = 0.04301383\n",
      "Iteration 12, loss = 0.03785183\n",
      "Iteration 13, loss = 0.03247266\n",
      "Iteration 14, loss = 0.02948740\n",
      "Iteration 15, loss = 0.02683197\n",
      "Iteration 16, loss = 0.02458697\n",
      "Iteration 17, loss = 0.02082722\n",
      "Iteration 18, loss = 0.01882566\n",
      "Iteration 19, loss = 0.01776075\n",
      "Iteration 20, loss = 0.01591157\n",
      "Iteration 21, loss = 0.01430920\n",
      "Iteration 22, loss = 0.01358744\n",
      "Iteration 23, loss = 0.01243036\n",
      "Iteration 24, loss = 0.01135402\n",
      "Iteration 25, loss = 0.01377476\n",
      "Iteration 26, loss = 0.01066373\n",
      "Iteration 27, loss = 0.00950052\n",
      "Iteration 28, loss = 0.00871050\n",
      "Iteration 29, loss = 0.01010174\n",
      "Iteration 30, loss = 0.01483989\n",
      "Iteration 31, loss = 0.01021542\n",
      "Iteration 32, loss = 0.00895964\n",
      "Iteration 33, loss = 0.00737783\n",
      "Iteration 34, loss = 0.00755644\n",
      "Iteration 35, loss = 0.00713856\n",
      "Iteration 36, loss = 0.00692510\n",
      "Iteration 37, loss = 0.01857915\n",
      "Iteration 38, loss = 0.01702776\n",
      "Iteration 39, loss = 0.00900209\n",
      "Iteration 40, loss = 0.00707602\n",
      "Iteration 41, loss = 0.00677627\n",
      "Iteration 42, loss = 0.00662107\n",
      "Iteration 43, loss = 0.00650994\n",
      "Iteration 44, loss = 0.00641146\n",
      "Iteration 45, loss = 0.00639255\n",
      "Iteration 46, loss = 0.01676192\n",
      "Iteration 47, loss = 0.01322667\n",
      "Iteration 48, loss = 0.00760112\n",
      "Iteration 49, loss = 0.00686763\n",
      "Iteration 50, loss = 0.00648103\n",
      "Iteration 51, loss = 0.00634274\n",
      "Iteration 52, loss = 0.00623463\n",
      "Iteration 53, loss = 0.00613205\n",
      "Iteration 54, loss = 0.01073909\n",
      "Iteration 55, loss = 0.01829915\n",
      "Iteration 56, loss = 0.00889847\n",
      "Iteration 57, loss = 0.00682818\n",
      "Iteration 58, loss = 0.00646697\n",
      "Iteration 59, loss = 0.00632356\n",
      "Iteration 60, loss = 0.00619678\n",
      "Iteration 61, loss = 0.00608986\n",
      "Iteration 62, loss = 0.00599998\n",
      "Iteration 63, loss = 0.00593858\n",
      "Iteration 64, loss = 0.00585946\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.001, batch_size=100, hidden_layer_sizes=(100,), solver=adam; total time= 1.3min\n",
      "Iteration 1, loss = 0.44184784\n",
      "Iteration 2, loss = 0.21747785\n",
      "Iteration 3, loss = 0.16555218\n",
      "Iteration 4, loss = 0.13236524\n",
      "Iteration 5, loss = 0.10935351\n",
      "Iteration 6, loss = 0.09203049\n",
      "Iteration 7, loss = 0.07883368\n",
      "Iteration 8, loss = 0.06835515\n",
      "Iteration 9, loss = 0.05916511\n",
      "Iteration 10, loss = 0.05199963\n",
      "Iteration 11, loss = 0.04509783\n",
      "Iteration 12, loss = 0.04067765\n",
      "Iteration 13, loss = 0.03461661\n",
      "Iteration 14, loss = 0.03153181\n",
      "Iteration 15, loss = 0.02766105\n",
      "Iteration 16, loss = 0.02469996\n",
      "Iteration 17, loss = 0.02200996\n",
      "Iteration 18, loss = 0.02020719\n",
      "Iteration 19, loss = 0.01784731\n",
      "Iteration 20, loss = 0.01640450\n",
      "Iteration 21, loss = 0.01513481\n",
      "Iteration 22, loss = 0.01417667\n",
      "Iteration 23, loss = 0.01360608\n",
      "Iteration 24, loss = 0.01290227\n",
      "Iteration 25, loss = 0.01274969\n",
      "Iteration 26, loss = 0.01141191\n",
      "Iteration 27, loss = 0.00963930\n",
      "Iteration 28, loss = 0.01182844\n",
      "Iteration 29, loss = 0.01208862\n",
      "Iteration 30, loss = 0.00952550\n",
      "Iteration 31, loss = 0.00866865\n",
      "Iteration 32, loss = 0.00808599\n",
      "Iteration 33, loss = 0.00828383\n",
      "Iteration 34, loss = 0.01647157\n",
      "Iteration 35, loss = 0.01505370\n",
      "Iteration 36, loss = 0.00814916\n",
      "Iteration 37, loss = 0.00722806\n",
      "Iteration 38, loss = 0.00694008\n",
      "Iteration 39, loss = 0.00673390\n",
      "Iteration 40, loss = 0.00664454\n",
      "Iteration 41, loss = 0.00657735\n",
      "Iteration 42, loss = 0.01257858\n",
      "Iteration 43, loss = 0.02026251\n",
      "Iteration 44, loss = 0.00814040\n",
      "Iteration 45, loss = 0.00701427\n",
      "Iteration 46, loss = 0.00666036\n",
      "Iteration 47, loss = 0.00651525\n",
      "Iteration 48, loss = 0.00640743\n",
      "Iteration 49, loss = 0.00629412\n",
      "Iteration 50, loss = 0.00624664\n",
      "Iteration 51, loss = 0.00622087\n",
      "Iteration 52, loss = 0.00625329\n",
      "Iteration 53, loss = 0.02528813\n",
      "Iteration 54, loss = 0.01234559\n",
      "Iteration 55, loss = 0.00800797\n",
      "Iteration 56, loss = 0.00674742\n",
      "Iteration 57, loss = 0.00644693\n",
      "Iteration 58, loss = 0.00630655\n",
      "Iteration 59, loss = 0.00620305\n",
      "Iteration 60, loss = 0.00609955\n",
      "Iteration 61, loss = 0.00605518\n",
      "Iteration 62, loss = 0.00592751\n",
      "Iteration 63, loss = 0.00592498\n",
      "Iteration 64, loss = 0.00687737\n",
      "Iteration 65, loss = 0.02644172\n",
      "Iteration 66, loss = 0.00879911\n",
      "Iteration 67, loss = 0.00694104\n",
      "Iteration 68, loss = 0.00630798\n",
      "Iteration 69, loss = 0.00614791\n",
      "Iteration 70, loss = 0.00604741\n",
      "Iteration 71, loss = 0.00594888\n",
      "Iteration 72, loss = 0.00585019\n",
      "Iteration 73, loss = 0.00577313\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.001, batch_size=100, hidden_layer_sizes=(100,), solver=adam; total time= 1.1min\n",
      "Iteration 1, loss = 0.45183037\n",
      "Iteration 2, loss = 0.21065128\n",
      "Iteration 3, loss = 0.16087252\n",
      "Iteration 4, loss = 0.12959725\n",
      "Iteration 5, loss = 0.10806369\n",
      "Iteration 6, loss = 0.09096882\n",
      "Iteration 7, loss = 0.07861707\n",
      "Iteration 8, loss = 0.06863867\n",
      "Iteration 9, loss = 0.06002720\n",
      "Iteration 10, loss = 0.05188632\n",
      "Iteration 11, loss = 0.04595022\n",
      "Iteration 12, loss = 0.04004399\n",
      "Iteration 13, loss = 0.03528572\n",
      "Iteration 14, loss = 0.03091016\n",
      "Iteration 15, loss = 0.02843318\n",
      "Iteration 16, loss = 0.02460703\n",
      "Iteration 17, loss = 0.02225913\n",
      "Iteration 18, loss = 0.02040279\n",
      "Iteration 19, loss = 0.01892550\n",
      "Iteration 20, loss = 0.01611957\n",
      "Iteration 21, loss = 0.01477589\n",
      "Iteration 22, loss = 0.01377998\n",
      "Iteration 23, loss = 0.01285098\n",
      "Iteration 24, loss = 0.01282445\n",
      "Iteration 25, loss = 0.01166569\n",
      "Iteration 26, loss = 0.01218274\n",
      "Iteration 27, loss = 0.01057458\n",
      "Iteration 28, loss = 0.01109722\n",
      "Iteration 29, loss = 0.00985627\n",
      "Iteration 30, loss = 0.00901555\n",
      "Iteration 31, loss = 0.00815427\n",
      "Iteration 32, loss = 0.00753729\n",
      "Iteration 33, loss = 0.00780648\n",
      "Iteration 34, loss = 0.01507336\n",
      "Iteration 35, loss = 0.01871187\n",
      "Iteration 36, loss = 0.00892421\n",
      "Iteration 37, loss = 0.00733418\n",
      "Iteration 38, loss = 0.00701927\n",
      "Iteration 39, loss = 0.00685122\n",
      "Iteration 40, loss = 0.00674872\n",
      "Iteration 41, loss = 0.00674984\n",
      "Iteration 42, loss = 0.00690725\n",
      "Iteration 43, loss = 0.01696075\n",
      "Iteration 44, loss = 0.01384161\n",
      "Iteration 45, loss = 0.00868771\n",
      "Iteration 46, loss = 0.00704965\n",
      "Iteration 47, loss = 0.00666004\n",
      "Iteration 48, loss = 0.00652157\n",
      "Iteration 49, loss = 0.00641725\n",
      "Iteration 50, loss = 0.00639763\n",
      "Iteration 51, loss = 0.00631502\n",
      "Iteration 52, loss = 0.00628479\n",
      "Iteration 53, loss = 0.02054673\n",
      "Iteration 54, loss = 0.01761132\n",
      "Iteration 55, loss = 0.00752378\n",
      "Iteration 56, loss = 0.00674249\n",
      "Iteration 57, loss = 0.00654242\n",
      "Iteration 58, loss = 0.00640428\n",
      "Iteration 59, loss = 0.00628512\n",
      "Iteration 60, loss = 0.00617419\n",
      "Iteration 61, loss = 0.00610038\n",
      "Iteration 62, loss = 0.00814938\n",
      "Iteration 63, loss = 0.02545354\n",
      "Iteration 64, loss = 0.00956771\n",
      "Iteration 65, loss = 0.00694232\n",
      "Iteration 66, loss = 0.00651075\n",
      "Iteration 67, loss = 0.00636134\n",
      "Iteration 68, loss = 0.00624718\n",
      "Iteration 69, loss = 0.00614873\n",
      "Iteration 70, loss = 0.00605291\n",
      "Iteration 71, loss = 0.00596370\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.001, batch_size=100, hidden_layer_sizes=(100,), solver=adam; total time= 1.5min\n",
      "Iteration 1, loss = 1.50478760\n",
      "Iteration 2, loss = 0.70691280\n",
      "Iteration 3, loss = 0.52143946\n",
      "Iteration 4, loss = 0.44638405\n",
      "Iteration 5, loss = 0.40437273\n",
      "Iteration 6, loss = 0.37696182\n",
      "Iteration 7, loss = 0.35674912\n",
      "Iteration 8, loss = 0.34086493\n",
      "Iteration 9, loss = 0.32816282\n",
      "Iteration 10, loss = 0.31707518\n",
      "Iteration 11, loss = 0.30747816\n",
      "Iteration 12, loss = 0.29882242\n",
      "Iteration 13, loss = 0.29111081\n",
      "Iteration 14, loss = 0.28398224\n",
      "Iteration 15, loss = 0.27746903\n",
      "Iteration 16, loss = 0.27145794\n",
      "Iteration 17, loss = 0.26576499\n",
      "Iteration 18, loss = 0.26041809\n",
      "Iteration 19, loss = 0.25547267\n",
      "Iteration 20, loss = 0.25063957\n",
      "Iteration 21, loss = 0.24603492\n",
      "Iteration 22, loss = 0.24160588\n",
      "Iteration 23, loss = 0.23739580\n",
      "Iteration 24, loss = 0.23347842\n",
      "Iteration 25, loss = 0.22934292\n",
      "Iteration 26, loss = 0.22567799\n",
      "Iteration 27, loss = 0.22204022\n",
      "Iteration 28, loss = 0.21858070\n",
      "Iteration 29, loss = 0.21522960\n",
      "Iteration 30, loss = 0.21195044\n",
      "Iteration 31, loss = 0.20871152\n",
      "Iteration 32, loss = 0.20560570\n",
      "Iteration 33, loss = 0.20271354\n",
      "Iteration 34, loss = 0.19968601\n",
      "Iteration 35, loss = 0.19690747\n",
      "Iteration 36, loss = 0.19416514\n",
      "Iteration 37, loss = 0.19141188\n",
      "Iteration 38, loss = 0.18884959\n",
      "Iteration 39, loss = 0.18625147\n",
      "Iteration 40, loss = 0.18390956\n",
      "Iteration 41, loss = 0.18148340\n",
      "Iteration 42, loss = 0.17899512\n",
      "Iteration 43, loss = 0.17677362\n",
      "Iteration 44, loss = 0.17455609\n",
      "Iteration 45, loss = 0.17232215\n",
      "Iteration 46, loss = 0.17037002\n",
      "Iteration 47, loss = 0.16809016\n",
      "Iteration 48, loss = 0.16625145\n",
      "Iteration 49, loss = 0.16419688\n",
      "Iteration 50, loss = 0.16217067\n",
      "Iteration 51, loss = 0.16050201\n",
      "Iteration 52, loss = 0.15860492\n",
      "Iteration 53, loss = 0.15680068\n",
      "Iteration 54, loss = 0.15502210\n",
      "Iteration 55, loss = 0.15331145\n",
      "Iteration 56, loss = 0.15161181\n",
      "Iteration 57, loss = 0.14993875\n",
      "Iteration 58, loss = 0.14842175\n",
      "Iteration 59, loss = 0.14675246\n",
      "Iteration 60, loss = 0.14525148\n",
      "Iteration 61, loss = 0.14368610\n",
      "Iteration 62, loss = 0.14226353\n",
      "Iteration 63, loss = 0.14079075\n",
      "Iteration 64, loss = 0.13932243\n",
      "Iteration 65, loss = 0.13798772\n",
      "Iteration 66, loss = 0.13660126\n",
      "Iteration 67, loss = 0.13522663\n",
      "Iteration 68, loss = 0.13380493\n",
      "Iteration 69, loss = 0.13248389\n",
      "Iteration 70, loss = 0.13127685\n",
      "Iteration 71, loss = 0.13000719\n",
      "Iteration 72, loss = 0.12873614\n",
      "Iteration 73, loss = 0.12761496\n",
      "Iteration 74, loss = 0.12644318\n",
      "Iteration 75, loss = 0.12515076\n",
      "Iteration 76, loss = 0.12412989\n",
      "Iteration 77, loss = 0.12293160\n",
      "Iteration 78, loss = 0.12178575\n",
      "Iteration 79, loss = 0.12067282\n",
      "Iteration 80, loss = 0.11960177\n",
      "Iteration 81, loss = 0.11857374\n",
      "Iteration 82, loss = 0.11759860\n",
      "Iteration 83, loss = 0.11644345\n",
      "Iteration 84, loss = 0.11535394\n",
      "Iteration 85, loss = 0.11448873\n",
      "Iteration 86, loss = 0.11348089\n",
      "Iteration 87, loss = 0.11252177\n",
      "Iteration 88, loss = 0.11150481\n",
      "Iteration 89, loss = 0.11066578\n",
      "Iteration 90, loss = 0.10967584\n",
      "Iteration 91, loss = 0.10876282\n",
      "Iteration 92, loss = 0.10772337\n",
      "Iteration 93, loss = 0.10695391\n",
      "Iteration 94, loss = 0.10609969\n",
      "Iteration 95, loss = 0.10515952\n",
      "Iteration 96, loss = 0.10436836\n",
      "Iteration 97, loss = 0.10356131\n",
      "Iteration 98, loss = 0.10273434\n",
      "Iteration 99, loss = 0.10187200\n",
      "Iteration 100, loss = 0.10102207\n",
      "Iteration 101, loss = 0.10032896\n",
      "Iteration 102, loss = 0.09956220\n",
      "Iteration 103, loss = 0.09886486\n",
      "Iteration 104, loss = 0.09813904\n",
      "Iteration 105, loss = 0.09725489\n",
      "Iteration 106, loss = 0.09663900\n",
      "Iteration 107, loss = 0.09578013\n",
      "Iteration 108, loss = 0.09516855\n",
      "Iteration 109, loss = 0.09436464\n",
      "Iteration 110, loss = 0.09379793\n",
      "Iteration 111, loss = 0.09300015\n",
      "Iteration 112, loss = 0.09235571\n",
      "Iteration 113, loss = 0.09172042\n",
      "Iteration 114, loss = 0.09099614\n",
      "Iteration 115, loss = 0.09028957\n",
      "Iteration 116, loss = 0.08960654\n",
      "Iteration 117, loss = 0.08905860\n",
      "Iteration 118, loss = 0.08844475\n",
      "Iteration 119, loss = 0.08773556\n",
      "Iteration 120, loss = 0.08708324\n",
      "Iteration 121, loss = 0.08658875\n",
      "Iteration 122, loss = 0.08593828\n",
      "Iteration 123, loss = 0.08532930\n",
      "Iteration 124, loss = 0.08475428\n",
      "Iteration 125, loss = 0.08417792\n",
      "Iteration 126, loss = 0.08358928\n",
      "Iteration 127, loss = 0.08298162\n",
      "Iteration 128, loss = 0.08241459\n",
      "Iteration 129, loss = 0.08180391\n",
      "Iteration 130, loss = 0.08131789\n",
      "Iteration 131, loss = 0.08079208\n",
      "Iteration 132, loss = 0.08026167\n",
      "Iteration 133, loss = 0.07974105\n",
      "Iteration 134, loss = 0.07918792\n",
      "Iteration 135, loss = 0.07864962\n",
      "Iteration 136, loss = 0.07798357\n",
      "Iteration 137, loss = 0.07766030\n",
      "Iteration 138, loss = 0.07705978\n",
      "Iteration 139, loss = 0.07654348\n",
      "Iteration 140, loss = 0.07597909\n",
      "Iteration 141, loss = 0.07552961\n",
      "Iteration 142, loss = 0.07506273\n",
      "Iteration 143, loss = 0.07459836\n",
      "Iteration 144, loss = 0.07400132\n",
      "Iteration 145, loss = 0.07367042\n",
      "Iteration 146, loss = 0.07317548\n",
      "Iteration 147, loss = 0.07270876\n",
      "Iteration 148, loss = 0.07225500\n",
      "Iteration 149, loss = 0.07176482\n",
      "Iteration 150, loss = 0.07129933\n",
      "Iteration 151, loss = 0.07082463\n",
      "Iteration 152, loss = 0.07044312\n",
      "Iteration 153, loss = 0.06998477\n",
      "Iteration 154, loss = 0.06962709\n",
      "Iteration 155, loss = 0.06914208\n",
      "Iteration 156, loss = 0.06871491\n",
      "Iteration 157, loss = 0.06826700\n",
      "Iteration 158, loss = 0.06789300\n",
      "Iteration 159, loss = 0.06740386\n",
      "Iteration 160, loss = 0.06702029\n",
      "Iteration 161, loss = 0.06668528\n",
      "Iteration 162, loss = 0.06621696\n",
      "Iteration 163, loss = 0.06586190\n",
      "Iteration 164, loss = 0.06540720\n",
      "Iteration 165, loss = 0.06507148\n",
      "Iteration 166, loss = 0.06470896\n",
      "Iteration 167, loss = 0.06421297\n",
      "Iteration 168, loss = 0.06398007\n",
      "Iteration 169, loss = 0.06354810\n",
      "Iteration 170, loss = 0.06321708\n",
      "Iteration 171, loss = 0.06283823\n",
      "Iteration 172, loss = 0.06245232\n",
      "Iteration 173, loss = 0.06205362\n",
      "Iteration 174, loss = 0.06170642\n",
      "Iteration 175, loss = 0.06142716\n",
      "Iteration 176, loss = 0.06104124\n",
      "Iteration 177, loss = 0.06071217\n",
      "Iteration 178, loss = 0.06034074\n",
      "Iteration 179, loss = 0.06008086\n",
      "Iteration 180, loss = 0.05972748\n",
      "Iteration 181, loss = 0.05936516\n",
      "Iteration 182, loss = 0.05902589\n",
      "Iteration 183, loss = 0.05868002\n",
      "Iteration 184, loss = 0.05838454\n",
      "Iteration 185, loss = 0.05804863\n",
      "Iteration 186, loss = 0.05774572\n",
      "Iteration 187, loss = 0.05737770\n",
      "Iteration 188, loss = 0.05714152\n",
      "Iteration 189, loss = 0.05675898\n",
      "Iteration 190, loss = 0.05647847\n",
      "Iteration 191, loss = 0.05623341\n",
      "Iteration 192, loss = 0.05586494\n",
      "Iteration 193, loss = 0.05555453\n",
      "Iteration 194, loss = 0.05528135\n",
      "Iteration 195, loss = 0.05496156\n",
      "Iteration 196, loss = 0.05464067\n",
      "Iteration 197, loss = 0.05439835\n",
      "Iteration 198, loss = 0.05413322\n",
      "Iteration 199, loss = 0.05376763\n",
      "Iteration 200, loss = 0.05356532\n",
      "[CV] END activation=relu, alpha=0.001, batch_size=100, hidden_layer_sizes=(100,), solver=sgd; total time= 1.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.38755764\n",
      "Iteration 2, loss = 0.64648307\n",
      "Iteration 3, loss = 0.49583601\n",
      "Iteration 4, loss = 0.43219640\n",
      "Iteration 5, loss = 0.39558569\n",
      "Iteration 6, loss = 0.37087833\n",
      "Iteration 7, loss = 0.35242730\n",
      "Iteration 8, loss = 0.33781785\n",
      "Iteration 9, loss = 0.32547252\n",
      "Iteration 10, loss = 0.31524018\n",
      "Iteration 11, loss = 0.30624495\n",
      "Iteration 12, loss = 0.29788817\n",
      "Iteration 13, loss = 0.29086245\n",
      "Iteration 14, loss = 0.28419787\n",
      "Iteration 15, loss = 0.27794313\n",
      "Iteration 16, loss = 0.27219263\n",
      "Iteration 17, loss = 0.26693953\n",
      "Iteration 18, loss = 0.26179410\n",
      "Iteration 19, loss = 0.25695005\n",
      "Iteration 20, loss = 0.25250478\n",
      "Iteration 21, loss = 0.24813566\n",
      "Iteration 22, loss = 0.24392694\n",
      "Iteration 23, loss = 0.24013173\n",
      "Iteration 24, loss = 0.23604401\n",
      "Iteration 25, loss = 0.23236305\n",
      "Iteration 26, loss = 0.22878742\n",
      "Iteration 27, loss = 0.22553546\n",
      "Iteration 28, loss = 0.22204674\n",
      "Iteration 29, loss = 0.21895960\n",
      "Iteration 30, loss = 0.21573032\n",
      "Iteration 31, loss = 0.21271456\n",
      "Iteration 32, loss = 0.20979480\n",
      "Iteration 33, loss = 0.20677418\n",
      "Iteration 34, loss = 0.20419799\n",
      "Iteration 35, loss = 0.20126529\n",
      "Iteration 36, loss = 0.19871893\n",
      "Iteration 37, loss = 0.19622083\n",
      "Iteration 38, loss = 0.19375187\n",
      "Iteration 39, loss = 0.19132698\n",
      "Iteration 40, loss = 0.18896239\n",
      "Iteration 41, loss = 0.18650309\n",
      "Iteration 42, loss = 0.18450755\n",
      "Iteration 43, loss = 0.18213770\n",
      "Iteration 44, loss = 0.18009855\n",
      "Iteration 45, loss = 0.17799825\n",
      "Iteration 46, loss = 0.17593871\n",
      "Iteration 47, loss = 0.17387792\n",
      "Iteration 48, loss = 0.17178594\n",
      "Iteration 49, loss = 0.16998616\n",
      "Iteration 50, loss = 0.16812382\n",
      "Iteration 51, loss = 0.16631603\n",
      "Iteration 52, loss = 0.16444998\n",
      "Iteration 53, loss = 0.16285632\n",
      "Iteration 54, loss = 0.16096348\n",
      "Iteration 55, loss = 0.15933176\n",
      "Iteration 56, loss = 0.15756669\n",
      "Iteration 57, loss = 0.15608090\n",
      "Iteration 58, loss = 0.15437503\n",
      "Iteration 59, loss = 0.15269709\n",
      "Iteration 60, loss = 0.15122698\n",
      "Iteration 61, loss = 0.14961253\n",
      "Iteration 62, loss = 0.14816744\n",
      "Iteration 63, loss = 0.14667720\n",
      "Iteration 64, loss = 0.14525203\n",
      "Iteration 65, loss = 0.14372096\n",
      "Iteration 66, loss = 0.14237154\n",
      "Iteration 67, loss = 0.14105152\n",
      "Iteration 68, loss = 0.13965211\n",
      "Iteration 69, loss = 0.13825303\n",
      "Iteration 70, loss = 0.13700801\n",
      "Iteration 71, loss = 0.13580509\n",
      "Iteration 72, loss = 0.13453370\n",
      "Iteration 73, loss = 0.13325709\n",
      "Iteration 74, loss = 0.13205794\n",
      "Iteration 75, loss = 0.13084760\n",
      "Iteration 76, loss = 0.12956798\n",
      "Iteration 77, loss = 0.12855771\n",
      "Iteration 78, loss = 0.12733071\n",
      "Iteration 79, loss = 0.12627914\n",
      "Iteration 80, loss = 0.12525255\n",
      "Iteration 81, loss = 0.12395546\n",
      "Iteration 82, loss = 0.12288560\n",
      "Iteration 83, loss = 0.12186683\n",
      "Iteration 84, loss = 0.12087035\n",
      "Iteration 85, loss = 0.11987347\n",
      "Iteration 86, loss = 0.11884744\n",
      "Iteration 87, loss = 0.11784289\n",
      "Iteration 88, loss = 0.11686328\n",
      "Iteration 89, loss = 0.11587031\n",
      "Iteration 90, loss = 0.11500520\n",
      "Iteration 91, loss = 0.11394245\n",
      "Iteration 92, loss = 0.11318393\n",
      "Iteration 93, loss = 0.11223693\n",
      "Iteration 94, loss = 0.11128986\n",
      "Iteration 95, loss = 0.11044815\n",
      "Iteration 96, loss = 0.10949375\n",
      "Iteration 97, loss = 0.10869301\n",
      "Iteration 98, loss = 0.10775618\n",
      "Iteration 99, loss = 0.10705216\n",
      "Iteration 100, loss = 0.10619056\n",
      "Iteration 101, loss = 0.10536558\n",
      "Iteration 102, loss = 0.10460633\n",
      "Iteration 103, loss = 0.10375114\n",
      "Iteration 104, loss = 0.10302917\n",
      "Iteration 105, loss = 0.10217197\n",
      "Iteration 106, loss = 0.10140995\n",
      "Iteration 107, loss = 0.10074364\n",
      "Iteration 108, loss = 0.09988630\n",
      "Iteration 109, loss = 0.09918377\n",
      "Iteration 110, loss = 0.09852878\n",
      "Iteration 111, loss = 0.09784837\n",
      "Iteration 112, loss = 0.09708953\n",
      "Iteration 113, loss = 0.09635698\n",
      "Iteration 114, loss = 0.09577762\n",
      "Iteration 115, loss = 0.09494695\n",
      "Iteration 116, loss = 0.09438662\n",
      "Iteration 117, loss = 0.09374341\n",
      "Iteration 118, loss = 0.09305094\n",
      "Iteration 119, loss = 0.09245116\n",
      "Iteration 120, loss = 0.09177218\n",
      "Iteration 121, loss = 0.09119552\n",
      "Iteration 122, loss = 0.09049587\n",
      "Iteration 123, loss = 0.08991452\n",
      "Iteration 124, loss = 0.08927444\n",
      "Iteration 125, loss = 0.08871270\n",
      "Iteration 126, loss = 0.08809511\n",
      "Iteration 127, loss = 0.08752820\n",
      "Iteration 128, loss = 0.08694109\n",
      "Iteration 129, loss = 0.08638552\n",
      "Iteration 130, loss = 0.08580534\n",
      "Iteration 131, loss = 0.08521785\n",
      "Iteration 132, loss = 0.08473458\n",
      "Iteration 133, loss = 0.08415069\n",
      "Iteration 134, loss = 0.08358343\n",
      "Iteration 135, loss = 0.08309723\n",
      "Iteration 136, loss = 0.08241279\n",
      "Iteration 137, loss = 0.08203265\n",
      "Iteration 138, loss = 0.08143310\n",
      "Iteration 139, loss = 0.08096815\n",
      "Iteration 140, loss = 0.08038197\n",
      "Iteration 141, loss = 0.07994986\n",
      "Iteration 142, loss = 0.07942774\n",
      "Iteration 143, loss = 0.07889823\n",
      "Iteration 144, loss = 0.07844958\n",
      "Iteration 145, loss = 0.07793404\n",
      "Iteration 146, loss = 0.07748415\n",
      "Iteration 147, loss = 0.07696638\n",
      "Iteration 148, loss = 0.07649190\n",
      "Iteration 149, loss = 0.07608574\n",
      "Iteration 150, loss = 0.07559914\n",
      "Iteration 151, loss = 0.07512098\n",
      "Iteration 152, loss = 0.07468788\n",
      "Iteration 153, loss = 0.07424752\n",
      "Iteration 154, loss = 0.07372770\n",
      "Iteration 155, loss = 0.07336377\n",
      "Iteration 156, loss = 0.07297171\n",
      "Iteration 157, loss = 0.07250743\n",
      "Iteration 158, loss = 0.07207086\n",
      "Iteration 159, loss = 0.07171592\n",
      "Iteration 160, loss = 0.07124131\n",
      "Iteration 161, loss = 0.07079250\n",
      "Iteration 162, loss = 0.07046318\n",
      "Iteration 163, loss = 0.06996502\n",
      "Iteration 164, loss = 0.06954724\n",
      "Iteration 165, loss = 0.06931200\n",
      "Iteration 166, loss = 0.06880096\n",
      "Iteration 167, loss = 0.06845297\n",
      "Iteration 168, loss = 0.06799674\n",
      "Iteration 169, loss = 0.06759823\n",
      "Iteration 170, loss = 0.06725592\n",
      "Iteration 171, loss = 0.06686398\n",
      "Iteration 172, loss = 0.06651416\n",
      "Iteration 173, loss = 0.06614976\n",
      "Iteration 174, loss = 0.06576635\n",
      "Iteration 175, loss = 0.06539638\n",
      "Iteration 176, loss = 0.06504049\n",
      "Iteration 177, loss = 0.06464532\n",
      "Iteration 178, loss = 0.06424815\n",
      "Iteration 179, loss = 0.06404131\n",
      "Iteration 180, loss = 0.06359857\n",
      "Iteration 181, loss = 0.06327162\n",
      "Iteration 182, loss = 0.06286430\n",
      "Iteration 183, loss = 0.06256694\n",
      "Iteration 184, loss = 0.06214854\n",
      "Iteration 185, loss = 0.06191211\n",
      "Iteration 186, loss = 0.06159708\n",
      "Iteration 187, loss = 0.06120237\n",
      "Iteration 188, loss = 0.06089636\n",
      "Iteration 189, loss = 0.06053652\n",
      "Iteration 190, loss = 0.06024176\n",
      "Iteration 191, loss = 0.05993391\n",
      "Iteration 192, loss = 0.05955311\n",
      "Iteration 193, loss = 0.05926252\n",
      "Iteration 194, loss = 0.05893754\n",
      "Iteration 195, loss = 0.05866303\n",
      "Iteration 196, loss = 0.05833656\n",
      "Iteration 197, loss = 0.05802151\n",
      "Iteration 198, loss = 0.05777380\n",
      "Iteration 199, loss = 0.05744151\n",
      "Iteration 200, loss = 0.05708059\n",
      "[CV] END activation=relu, alpha=0.001, batch_size=100, hidden_layer_sizes=(100,), solver=sgd; total time= 1.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.41773288\n",
      "Iteration 2, loss = 0.66847715\n",
      "Iteration 3, loss = 0.50633986\n",
      "Iteration 4, loss = 0.43739322\n",
      "Iteration 5, loss = 0.39817694\n",
      "Iteration 6, loss = 0.37230445\n",
      "Iteration 7, loss = 0.35322117\n",
      "Iteration 8, loss = 0.33836418\n",
      "Iteration 9, loss = 0.32621667\n",
      "Iteration 10, loss = 0.31599459\n",
      "Iteration 11, loss = 0.30701743\n",
      "Iteration 12, loss = 0.29903734\n",
      "Iteration 13, loss = 0.29196242\n",
      "Iteration 14, loss = 0.28544721\n",
      "Iteration 15, loss = 0.27933560\n",
      "Iteration 16, loss = 0.27386339\n",
      "Iteration 17, loss = 0.26851429\n",
      "Iteration 18, loss = 0.26358992\n",
      "Iteration 19, loss = 0.25890623\n",
      "Iteration 20, loss = 0.25435037\n",
      "Iteration 21, loss = 0.25012414\n",
      "Iteration 22, loss = 0.24575972\n",
      "Iteration 23, loss = 0.24185284\n",
      "Iteration 24, loss = 0.23805416\n",
      "Iteration 25, loss = 0.23421051\n",
      "Iteration 26, loss = 0.23058027\n",
      "Iteration 27, loss = 0.22705772\n",
      "Iteration 28, loss = 0.22368369\n",
      "Iteration 29, loss = 0.22037157\n",
      "Iteration 30, loss = 0.21714701\n",
      "Iteration 31, loss = 0.21404923\n",
      "Iteration 32, loss = 0.21070865\n",
      "Iteration 33, loss = 0.20780555\n",
      "Iteration 34, loss = 0.20491905\n",
      "Iteration 35, loss = 0.20196548\n",
      "Iteration 36, loss = 0.19928703\n",
      "Iteration 37, loss = 0.19626862\n",
      "Iteration 38, loss = 0.19360929\n",
      "Iteration 39, loss = 0.19101488\n",
      "Iteration 40, loss = 0.18836102\n",
      "Iteration 41, loss = 0.18590530\n",
      "Iteration 42, loss = 0.18333770\n",
      "Iteration 43, loss = 0.18091357\n",
      "Iteration 44, loss = 0.17857426\n",
      "Iteration 45, loss = 0.17623647\n",
      "Iteration 46, loss = 0.17394878\n",
      "Iteration 47, loss = 0.17181798\n",
      "Iteration 48, loss = 0.16973340\n",
      "Iteration 49, loss = 0.16763025\n",
      "Iteration 50, loss = 0.16552455\n",
      "Iteration 51, loss = 0.16359615\n",
      "Iteration 52, loss = 0.16149795\n",
      "Iteration 53, loss = 0.15949028\n",
      "Iteration 54, loss = 0.15780749\n",
      "Iteration 55, loss = 0.15587241\n",
      "Iteration 56, loss = 0.15415392\n",
      "Iteration 57, loss = 0.15229015\n",
      "Iteration 58, loss = 0.15060420\n",
      "Iteration 59, loss = 0.14885118\n",
      "Iteration 60, loss = 0.14719007\n",
      "Iteration 61, loss = 0.14565734\n",
      "Iteration 62, loss = 0.14413063\n",
      "Iteration 63, loss = 0.14256763\n",
      "Iteration 64, loss = 0.14102582\n",
      "Iteration 65, loss = 0.13947255\n",
      "Iteration 66, loss = 0.13802435\n",
      "Iteration 67, loss = 0.13659845\n",
      "Iteration 68, loss = 0.13524713\n",
      "Iteration 69, loss = 0.13387676\n",
      "Iteration 70, loss = 0.13265179\n",
      "Iteration 71, loss = 0.13125350\n",
      "Iteration 72, loss = 0.12994739\n",
      "Iteration 73, loss = 0.12880914\n",
      "Iteration 74, loss = 0.12749446\n",
      "Iteration 75, loss = 0.12624511\n",
      "Iteration 76, loss = 0.12508180\n",
      "Iteration 77, loss = 0.12382446\n",
      "Iteration 78, loss = 0.12279747\n",
      "Iteration 79, loss = 0.12172372\n",
      "Iteration 80, loss = 0.12047698\n",
      "Iteration 81, loss = 0.11947954\n",
      "Iteration 82, loss = 0.11836035\n",
      "Iteration 83, loss = 0.11747134\n",
      "Iteration 84, loss = 0.11627788\n",
      "Iteration 85, loss = 0.11521150\n",
      "Iteration 86, loss = 0.11418403\n",
      "Iteration 87, loss = 0.11330373\n",
      "Iteration 88, loss = 0.11227315\n",
      "Iteration 89, loss = 0.11130138\n",
      "Iteration 90, loss = 0.11047069\n",
      "Iteration 91, loss = 0.10948369\n",
      "Iteration 92, loss = 0.10859142\n",
      "Iteration 93, loss = 0.10767437\n",
      "Iteration 94, loss = 0.10691052\n",
      "Iteration 95, loss = 0.10601938\n",
      "Iteration 96, loss = 0.10505629\n",
      "Iteration 97, loss = 0.10431073\n",
      "Iteration 98, loss = 0.10339135\n",
      "Iteration 99, loss = 0.10264638\n",
      "Iteration 100, loss = 0.10175712\n",
      "Iteration 101, loss = 0.10093447\n",
      "Iteration 102, loss = 0.10011701\n",
      "Iteration 103, loss = 0.09933602\n",
      "Iteration 104, loss = 0.09867334\n",
      "Iteration 105, loss = 0.09790915\n",
      "Iteration 106, loss = 0.09716257\n",
      "Iteration 107, loss = 0.09637742\n",
      "Iteration 108, loss = 0.09570381\n",
      "Iteration 109, loss = 0.09493531\n",
      "Iteration 110, loss = 0.09426693\n",
      "Iteration 111, loss = 0.09352502\n",
      "Iteration 112, loss = 0.09289050\n",
      "Iteration 113, loss = 0.09216145\n",
      "Iteration 114, loss = 0.09154268\n",
      "Iteration 115, loss = 0.09091298\n",
      "Iteration 116, loss = 0.09024312\n",
      "Iteration 117, loss = 0.08963704\n",
      "Iteration 118, loss = 0.08885617\n",
      "Iteration 119, loss = 0.08825414\n",
      "Iteration 120, loss = 0.08767874\n",
      "Iteration 121, loss = 0.08712427\n",
      "Iteration 122, loss = 0.08645337\n",
      "Iteration 123, loss = 0.08577549\n",
      "Iteration 124, loss = 0.08515760\n",
      "Iteration 125, loss = 0.08465457\n",
      "Iteration 126, loss = 0.08416410\n",
      "Iteration 127, loss = 0.08356597\n",
      "Iteration 128, loss = 0.08291371\n",
      "Iteration 129, loss = 0.08233011\n",
      "Iteration 130, loss = 0.08185244\n",
      "Iteration 131, loss = 0.08119543\n",
      "Iteration 132, loss = 0.08070589\n",
      "Iteration 133, loss = 0.08022395\n",
      "Iteration 134, loss = 0.07969812\n",
      "Iteration 135, loss = 0.07916383\n",
      "Iteration 136, loss = 0.07861603\n",
      "Iteration 137, loss = 0.07802418\n",
      "Iteration 138, loss = 0.07758786\n",
      "Iteration 139, loss = 0.07701996\n",
      "Iteration 140, loss = 0.07656097\n",
      "Iteration 141, loss = 0.07605283\n",
      "Iteration 142, loss = 0.07560447\n",
      "Iteration 143, loss = 0.07504854\n",
      "Iteration 144, loss = 0.07464021\n",
      "Iteration 145, loss = 0.07410895\n",
      "Iteration 146, loss = 0.07365651\n",
      "Iteration 147, loss = 0.07309575\n",
      "Iteration 148, loss = 0.07282465\n",
      "Iteration 149, loss = 0.07223246\n",
      "Iteration 150, loss = 0.07181072\n",
      "Iteration 151, loss = 0.07136549\n",
      "Iteration 152, loss = 0.07102846\n",
      "Iteration 153, loss = 0.07051326\n",
      "Iteration 154, loss = 0.07012306\n",
      "Iteration 155, loss = 0.06970525\n",
      "Iteration 156, loss = 0.06930703\n",
      "Iteration 157, loss = 0.06887931\n",
      "Iteration 158, loss = 0.06843019\n",
      "Iteration 159, loss = 0.06801206\n",
      "Iteration 160, loss = 0.06762250\n",
      "Iteration 161, loss = 0.06724391\n",
      "Iteration 162, loss = 0.06682629\n",
      "Iteration 163, loss = 0.06641527\n",
      "Iteration 164, loss = 0.06605958\n",
      "Iteration 165, loss = 0.06565917\n",
      "Iteration 166, loss = 0.06518542\n",
      "Iteration 167, loss = 0.06488835\n",
      "Iteration 168, loss = 0.06450921\n",
      "Iteration 169, loss = 0.06415979\n",
      "Iteration 170, loss = 0.06383083\n",
      "Iteration 171, loss = 0.06340005\n",
      "Iteration 172, loss = 0.06303498\n",
      "Iteration 173, loss = 0.06263880\n",
      "Iteration 174, loss = 0.06241380\n",
      "Iteration 175, loss = 0.06199178\n",
      "Iteration 176, loss = 0.06162172\n",
      "Iteration 177, loss = 0.06127518\n",
      "Iteration 178, loss = 0.06093012\n",
      "Iteration 179, loss = 0.06061182\n",
      "Iteration 180, loss = 0.06029575\n",
      "Iteration 181, loss = 0.05989894\n",
      "Iteration 182, loss = 0.05957199\n",
      "Iteration 183, loss = 0.05926609\n",
      "Iteration 184, loss = 0.05887302\n",
      "Iteration 185, loss = 0.05862965\n",
      "Iteration 186, loss = 0.05825335\n",
      "Iteration 187, loss = 0.05794826\n",
      "Iteration 188, loss = 0.05769627\n",
      "Iteration 189, loss = 0.05731997\n",
      "Iteration 190, loss = 0.05701088\n",
      "Iteration 191, loss = 0.05675960\n",
      "Iteration 192, loss = 0.05643718\n",
      "Iteration 193, loss = 0.05613419\n",
      "Iteration 194, loss = 0.05583765\n",
      "Iteration 195, loss = 0.05552563\n",
      "Iteration 196, loss = 0.05528035\n",
      "Iteration 197, loss = 0.05491401\n",
      "Iteration 198, loss = 0.05464774\n",
      "Iteration 199, loss = 0.05434245\n",
      "Iteration 200, loss = 0.05407678\n",
      "[CV] END activation=relu, alpha=0.001, batch_size=100, hidden_layer_sizes=(100,), solver=sgd; total time= 1.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.36163356\n",
      "Iteration 2, loss = 0.64404845\n",
      "Iteration 3, loss = 0.49489834\n",
      "Iteration 4, loss = 0.43086305\n",
      "Iteration 5, loss = 0.39349245\n",
      "Iteration 6, loss = 0.36868432\n",
      "Iteration 7, loss = 0.35014506\n",
      "Iteration 8, loss = 0.33561543\n",
      "Iteration 9, loss = 0.32368288\n",
      "Iteration 10, loss = 0.31328793\n",
      "Iteration 11, loss = 0.30413213\n",
      "Iteration 12, loss = 0.29616226\n",
      "Iteration 13, loss = 0.28869872\n",
      "Iteration 14, loss = 0.28201206\n",
      "Iteration 15, loss = 0.27582386\n",
      "Iteration 16, loss = 0.26990681\n",
      "Iteration 17, loss = 0.26439126\n",
      "Iteration 18, loss = 0.25886196\n",
      "Iteration 19, loss = 0.25419160\n",
      "Iteration 20, loss = 0.24930507\n",
      "Iteration 21, loss = 0.24482436\n",
      "Iteration 22, loss = 0.24039399\n",
      "Iteration 23, loss = 0.23641318\n",
      "Iteration 24, loss = 0.23230985\n",
      "Iteration 25, loss = 0.22844271\n",
      "Iteration 26, loss = 0.22477431\n",
      "Iteration 27, loss = 0.22118877\n",
      "Iteration 28, loss = 0.21765417\n",
      "Iteration 29, loss = 0.21466200\n",
      "Iteration 30, loss = 0.21111521\n",
      "Iteration 31, loss = 0.20797366\n",
      "Iteration 32, loss = 0.20496786\n",
      "Iteration 33, loss = 0.20187602\n",
      "Iteration 34, loss = 0.19908296\n",
      "Iteration 35, loss = 0.19630854\n",
      "Iteration 36, loss = 0.19366416\n",
      "Iteration 37, loss = 0.19085444\n",
      "Iteration 38, loss = 0.18825744\n",
      "Iteration 39, loss = 0.18572541\n",
      "Iteration 40, loss = 0.18347224\n",
      "Iteration 41, loss = 0.18094529\n",
      "Iteration 42, loss = 0.17850750\n",
      "Iteration 43, loss = 0.17634652\n",
      "Iteration 44, loss = 0.17408220\n",
      "Iteration 45, loss = 0.17201505\n",
      "Iteration 46, loss = 0.16978730\n",
      "Iteration 47, loss = 0.16791507\n",
      "Iteration 48, loss = 0.16578650\n",
      "Iteration 49, loss = 0.16391122\n",
      "Iteration 50, loss = 0.16193282\n",
      "Iteration 51, loss = 0.16013330\n",
      "Iteration 52, loss = 0.15827762\n",
      "Iteration 53, loss = 0.15652802\n",
      "Iteration 54, loss = 0.15470304\n",
      "Iteration 55, loss = 0.15298327\n",
      "Iteration 56, loss = 0.15133665\n",
      "Iteration 57, loss = 0.14969299\n",
      "Iteration 58, loss = 0.14803349\n",
      "Iteration 59, loss = 0.14661228\n",
      "Iteration 60, loss = 0.14497327\n",
      "Iteration 61, loss = 0.14342130\n",
      "Iteration 62, loss = 0.14200919\n",
      "Iteration 63, loss = 0.14053933\n",
      "Iteration 64, loss = 0.13913886\n",
      "Iteration 65, loss = 0.13768875\n",
      "Iteration 66, loss = 0.13645701\n",
      "Iteration 67, loss = 0.13498777\n",
      "Iteration 68, loss = 0.13367761\n",
      "Iteration 69, loss = 0.13234838\n",
      "Iteration 70, loss = 0.13102349\n",
      "Iteration 71, loss = 0.12973846\n",
      "Iteration 72, loss = 0.12849039\n",
      "Iteration 73, loss = 0.12727823\n",
      "Iteration 74, loss = 0.12611568\n",
      "Iteration 75, loss = 0.12494745\n",
      "Iteration 76, loss = 0.12376471\n",
      "Iteration 77, loss = 0.12251368\n",
      "Iteration 78, loss = 0.12146594\n",
      "Iteration 79, loss = 0.12040132\n",
      "Iteration 80, loss = 0.11923749\n",
      "Iteration 81, loss = 0.11812521\n",
      "Iteration 82, loss = 0.11718225\n",
      "Iteration 83, loss = 0.11615603\n",
      "Iteration 84, loss = 0.11516773\n",
      "Iteration 85, loss = 0.11409687\n",
      "Iteration 86, loss = 0.11314505\n",
      "Iteration 87, loss = 0.11223205\n",
      "Iteration 88, loss = 0.11112400\n",
      "Iteration 89, loss = 0.11028661\n",
      "Iteration 90, loss = 0.10938377\n",
      "Iteration 91, loss = 0.10845740\n",
      "Iteration 92, loss = 0.10755141\n",
      "Iteration 93, loss = 0.10665759\n",
      "Iteration 94, loss = 0.10578728\n",
      "Iteration 95, loss = 0.10503780\n",
      "Iteration 96, loss = 0.10404104\n",
      "Iteration 97, loss = 0.10332044\n",
      "Iteration 98, loss = 0.10246858\n",
      "Iteration 99, loss = 0.10168497\n",
      "Iteration 100, loss = 0.10088560\n",
      "Iteration 101, loss = 0.10010533\n",
      "Iteration 102, loss = 0.09936032\n",
      "Iteration 103, loss = 0.09847196\n",
      "Iteration 104, loss = 0.09770544\n",
      "Iteration 105, loss = 0.09699987\n",
      "Iteration 106, loss = 0.09627000\n",
      "Iteration 107, loss = 0.09555510\n",
      "Iteration 108, loss = 0.09484629\n",
      "Iteration 109, loss = 0.09415886\n",
      "Iteration 110, loss = 0.09341940\n",
      "Iteration 111, loss = 0.09275552\n",
      "Iteration 112, loss = 0.09199065\n",
      "Iteration 113, loss = 0.09144506\n",
      "Iteration 114, loss = 0.09074756\n",
      "Iteration 115, loss = 0.09002905\n",
      "Iteration 116, loss = 0.08953760\n",
      "Iteration 117, loss = 0.08879200\n",
      "Iteration 118, loss = 0.08817002\n",
      "Iteration 119, loss = 0.08753057\n",
      "Iteration 120, loss = 0.08692378\n",
      "Iteration 121, loss = 0.08631946\n",
      "Iteration 122, loss = 0.08568409\n",
      "Iteration 123, loss = 0.08513733\n",
      "Iteration 124, loss = 0.08455864\n",
      "Iteration 125, loss = 0.08396191\n",
      "Iteration 126, loss = 0.08348101\n",
      "Iteration 127, loss = 0.08284995\n",
      "Iteration 128, loss = 0.08223615\n",
      "Iteration 129, loss = 0.08176547\n",
      "Iteration 130, loss = 0.08116830\n",
      "Iteration 131, loss = 0.08060437\n",
      "Iteration 132, loss = 0.08006455\n",
      "Iteration 133, loss = 0.07959698\n",
      "Iteration 134, loss = 0.07902260\n",
      "Iteration 135, loss = 0.07849094\n",
      "Iteration 136, loss = 0.07801592\n",
      "Iteration 137, loss = 0.07742911\n",
      "Iteration 138, loss = 0.07695466\n",
      "Iteration 139, loss = 0.07648211\n",
      "Iteration 140, loss = 0.07593725\n",
      "Iteration 141, loss = 0.07552251\n",
      "Iteration 142, loss = 0.07494729\n",
      "Iteration 143, loss = 0.07451217\n",
      "Iteration 144, loss = 0.07398799\n",
      "Iteration 145, loss = 0.07368754\n",
      "Iteration 146, loss = 0.07306376\n",
      "Iteration 147, loss = 0.07273620\n",
      "Iteration 148, loss = 0.07223661\n",
      "Iteration 149, loss = 0.07171453\n",
      "Iteration 150, loss = 0.07127423\n",
      "Iteration 151, loss = 0.07090493\n",
      "Iteration 152, loss = 0.07044554\n",
      "Iteration 153, loss = 0.06999319\n",
      "Iteration 154, loss = 0.06954746\n",
      "Iteration 155, loss = 0.06918128\n",
      "Iteration 156, loss = 0.06872725\n",
      "Iteration 157, loss = 0.06831060\n",
      "Iteration 158, loss = 0.06792609\n",
      "Iteration 159, loss = 0.06749862\n",
      "Iteration 160, loss = 0.06706660\n",
      "Iteration 161, loss = 0.06667638\n",
      "Iteration 162, loss = 0.06632051\n",
      "Iteration 163, loss = 0.06589205\n",
      "Iteration 164, loss = 0.06554107\n",
      "Iteration 165, loss = 0.06511197\n",
      "Iteration 166, loss = 0.06477651\n",
      "Iteration 167, loss = 0.06436033\n",
      "Iteration 168, loss = 0.06405905\n",
      "Iteration 169, loss = 0.06363572\n",
      "Iteration 170, loss = 0.06331320\n",
      "Iteration 171, loss = 0.06292776\n",
      "Iteration 172, loss = 0.06254722\n",
      "Iteration 173, loss = 0.06216401\n",
      "Iteration 174, loss = 0.06178133\n",
      "Iteration 175, loss = 0.06149319\n",
      "Iteration 176, loss = 0.06110649\n",
      "Iteration 177, loss = 0.06086765\n",
      "Iteration 178, loss = 0.06045554\n",
      "Iteration 179, loss = 0.06011999\n",
      "Iteration 180, loss = 0.05978442\n",
      "Iteration 181, loss = 0.05946253\n",
      "Iteration 182, loss = 0.05908653\n",
      "Iteration 183, loss = 0.05883116\n",
      "Iteration 184, loss = 0.05843993\n",
      "Iteration 185, loss = 0.05818851\n",
      "Iteration 186, loss = 0.05778463\n",
      "Iteration 187, loss = 0.05751080\n",
      "Iteration 188, loss = 0.05723050\n",
      "Iteration 189, loss = 0.05694770\n",
      "Iteration 190, loss = 0.05659029\n",
      "Iteration 191, loss = 0.05627903\n",
      "Iteration 192, loss = 0.05597273\n",
      "Iteration 193, loss = 0.05568756\n",
      "Iteration 194, loss = 0.05537961\n",
      "Iteration 195, loss = 0.05509267\n",
      "Iteration 196, loss = 0.05481228\n",
      "Iteration 197, loss = 0.05443885\n",
      "Iteration 198, loss = 0.05427239\n",
      "Iteration 199, loss = 0.05388029\n",
      "Iteration 200, loss = 0.05366650\n",
      "[CV] END activation=relu, alpha=0.001, batch_size=100, hidden_layer_sizes=(100,), solver=sgd; total time= 1.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.44964550\n",
      "Iteration 2, loss = 0.66980401\n",
      "Iteration 3, loss = 0.50366093\n",
      "Iteration 4, loss = 0.43439066\n",
      "Iteration 5, loss = 0.39536721\n",
      "Iteration 6, loss = 0.36934709\n",
      "Iteration 7, loss = 0.35016673\n",
      "Iteration 8, loss = 0.33533478\n",
      "Iteration 9, loss = 0.32285110\n",
      "Iteration 10, loss = 0.31230235\n",
      "Iteration 11, loss = 0.30310794\n",
      "Iteration 12, loss = 0.29472919\n",
      "Iteration 13, loss = 0.28705449\n",
      "Iteration 14, loss = 0.28010597\n",
      "Iteration 15, loss = 0.27372255\n",
      "Iteration 16, loss = 0.26752481\n",
      "Iteration 17, loss = 0.26186716\n",
      "Iteration 18, loss = 0.25674834\n",
      "Iteration 19, loss = 0.25136816\n",
      "Iteration 20, loss = 0.24671663\n",
      "Iteration 21, loss = 0.24211311\n",
      "Iteration 22, loss = 0.23758114\n",
      "Iteration 23, loss = 0.23342081\n",
      "Iteration 24, loss = 0.22949266\n",
      "Iteration 25, loss = 0.22553764\n",
      "Iteration 26, loss = 0.22192418\n",
      "Iteration 27, loss = 0.21829688\n",
      "Iteration 28, loss = 0.21497517\n",
      "Iteration 29, loss = 0.21170579\n",
      "Iteration 30, loss = 0.20865359\n",
      "Iteration 31, loss = 0.20569864\n",
      "Iteration 32, loss = 0.20267162\n",
      "Iteration 33, loss = 0.19988133\n",
      "Iteration 34, loss = 0.19713781\n",
      "Iteration 35, loss = 0.19450164\n",
      "Iteration 36, loss = 0.19191914\n",
      "Iteration 37, loss = 0.18926621\n",
      "Iteration 38, loss = 0.18693134\n",
      "Iteration 39, loss = 0.18447553\n",
      "Iteration 40, loss = 0.18209632\n",
      "Iteration 41, loss = 0.17989076\n",
      "Iteration 42, loss = 0.17752452\n",
      "Iteration 43, loss = 0.17557235\n",
      "Iteration 44, loss = 0.17342033\n",
      "Iteration 45, loss = 0.17128476\n",
      "Iteration 46, loss = 0.16931059\n",
      "Iteration 47, loss = 0.16738361\n",
      "Iteration 48, loss = 0.16555194\n",
      "Iteration 49, loss = 0.16361795\n",
      "Iteration 50, loss = 0.16177507\n",
      "Iteration 51, loss = 0.15998134\n",
      "Iteration 52, loss = 0.15822119\n",
      "Iteration 53, loss = 0.15658086\n",
      "Iteration 54, loss = 0.15483312\n",
      "Iteration 55, loss = 0.15325383\n",
      "Iteration 56, loss = 0.15160757\n",
      "Iteration 57, loss = 0.14986213\n",
      "Iteration 58, loss = 0.14839198\n",
      "Iteration 59, loss = 0.14692345\n",
      "Iteration 60, loss = 0.14548429\n",
      "Iteration 61, loss = 0.14383512\n",
      "Iteration 62, loss = 0.14255184\n",
      "Iteration 63, loss = 0.14113422\n",
      "Iteration 64, loss = 0.13965412\n",
      "Iteration 65, loss = 0.13838523\n",
      "Iteration 66, loss = 0.13712655\n",
      "Iteration 67, loss = 0.13581083\n",
      "Iteration 68, loss = 0.13433530\n",
      "Iteration 69, loss = 0.13315936\n",
      "Iteration 70, loss = 0.13184660\n",
      "Iteration 71, loss = 0.13071626\n",
      "Iteration 72, loss = 0.12945571\n",
      "Iteration 73, loss = 0.12834264\n",
      "Iteration 74, loss = 0.12707722\n",
      "Iteration 75, loss = 0.12599825\n",
      "Iteration 76, loss = 0.12489777\n",
      "Iteration 77, loss = 0.12367400\n",
      "Iteration 78, loss = 0.12258478\n",
      "Iteration 79, loss = 0.12157875\n",
      "Iteration 80, loss = 0.12048735\n",
      "Iteration 81, loss = 0.11938555\n",
      "Iteration 82, loss = 0.11845068\n",
      "Iteration 83, loss = 0.11733542\n",
      "Iteration 84, loss = 0.11636720\n",
      "Iteration 85, loss = 0.11531873\n",
      "Iteration 86, loss = 0.11440410\n",
      "Iteration 87, loss = 0.11336875\n",
      "Iteration 88, loss = 0.11251954\n",
      "Iteration 89, loss = 0.11147078\n",
      "Iteration 90, loss = 0.11053665\n",
      "Iteration 91, loss = 0.10975747\n",
      "Iteration 92, loss = 0.10879586\n",
      "Iteration 93, loss = 0.10789159\n",
      "Iteration 94, loss = 0.10706827\n",
      "Iteration 95, loss = 0.10614860\n",
      "Iteration 96, loss = 0.10538221\n",
      "Iteration 97, loss = 0.10454689\n",
      "Iteration 98, loss = 0.10369258\n",
      "Iteration 99, loss = 0.10292225\n",
      "Iteration 100, loss = 0.10205480\n",
      "Iteration 101, loss = 0.10132882\n",
      "Iteration 102, loss = 0.10063451\n",
      "Iteration 103, loss = 0.09975500\n",
      "Iteration 104, loss = 0.09897455\n",
      "Iteration 105, loss = 0.09820419\n",
      "Iteration 106, loss = 0.09761596\n",
      "Iteration 107, loss = 0.09673989\n",
      "Iteration 108, loss = 0.09607073\n",
      "Iteration 109, loss = 0.09527687\n",
      "Iteration 110, loss = 0.09460002\n",
      "Iteration 111, loss = 0.09394312\n",
      "Iteration 112, loss = 0.09321661\n",
      "Iteration 113, loss = 0.09259275\n",
      "Iteration 114, loss = 0.09191009\n",
      "Iteration 115, loss = 0.09127133\n",
      "Iteration 116, loss = 0.09058928\n",
      "Iteration 117, loss = 0.08992937\n",
      "Iteration 118, loss = 0.08928295\n",
      "Iteration 119, loss = 0.08859296\n",
      "Iteration 120, loss = 0.08802042\n",
      "Iteration 121, loss = 0.08741078\n",
      "Iteration 122, loss = 0.08678474\n",
      "Iteration 123, loss = 0.08617800\n",
      "Iteration 124, loss = 0.08553361\n",
      "Iteration 125, loss = 0.08498064\n",
      "Iteration 126, loss = 0.08438331\n",
      "Iteration 127, loss = 0.08370787\n",
      "Iteration 128, loss = 0.08322299\n",
      "Iteration 129, loss = 0.08257210\n",
      "Iteration 130, loss = 0.08216960\n",
      "Iteration 131, loss = 0.08158020\n",
      "Iteration 132, loss = 0.08097372\n",
      "Iteration 133, loss = 0.08048375\n",
      "Iteration 134, loss = 0.07992329\n",
      "Iteration 135, loss = 0.07937586\n",
      "Iteration 136, loss = 0.07882984\n",
      "Iteration 137, loss = 0.07823917\n",
      "Iteration 138, loss = 0.07784176\n",
      "Iteration 139, loss = 0.07728597\n",
      "Iteration 140, loss = 0.07672776\n",
      "Iteration 141, loss = 0.07632159\n",
      "Iteration 142, loss = 0.07580149\n",
      "Iteration 143, loss = 0.07530079\n",
      "Iteration 144, loss = 0.07478647\n",
      "Iteration 145, loss = 0.07438490\n",
      "Iteration 146, loss = 0.07392329\n",
      "Iteration 147, loss = 0.07341436\n",
      "Iteration 148, loss = 0.07295550\n",
      "Iteration 149, loss = 0.07254096\n",
      "Iteration 150, loss = 0.07209384\n",
      "Iteration 151, loss = 0.07153825\n",
      "Iteration 152, loss = 0.07119278\n",
      "Iteration 153, loss = 0.07071830\n",
      "Iteration 154, loss = 0.07030049\n",
      "Iteration 155, loss = 0.06990275\n",
      "Iteration 156, loss = 0.06934420\n",
      "Iteration 157, loss = 0.06904172\n",
      "Iteration 158, loss = 0.06855456\n",
      "Iteration 159, loss = 0.06819785\n",
      "Iteration 160, loss = 0.06781168\n",
      "Iteration 161, loss = 0.06733914\n",
      "Iteration 162, loss = 0.06693842\n",
      "Iteration 163, loss = 0.06646899\n",
      "Iteration 164, loss = 0.06618823\n",
      "Iteration 165, loss = 0.06578634\n",
      "Iteration 166, loss = 0.06538530\n",
      "Iteration 167, loss = 0.06501201\n",
      "Iteration 168, loss = 0.06471554\n",
      "Iteration 169, loss = 0.06417914\n",
      "Iteration 170, loss = 0.06387112\n",
      "Iteration 171, loss = 0.06345021\n",
      "Iteration 172, loss = 0.06313865\n",
      "Iteration 173, loss = 0.06274460\n",
      "Iteration 174, loss = 0.06231672\n",
      "Iteration 175, loss = 0.06204865\n",
      "Iteration 176, loss = 0.06166716\n",
      "Iteration 177, loss = 0.06135329\n",
      "Iteration 178, loss = 0.06097646\n",
      "Iteration 179, loss = 0.06059484\n",
      "Iteration 180, loss = 0.06027392\n",
      "Iteration 181, loss = 0.06003199\n",
      "Iteration 182, loss = 0.05966054\n",
      "Iteration 183, loss = 0.05930930\n",
      "Iteration 184, loss = 0.05898879\n",
      "Iteration 185, loss = 0.05865564\n",
      "Iteration 186, loss = 0.05833873\n",
      "Iteration 187, loss = 0.05795034\n",
      "Iteration 188, loss = 0.05762693\n",
      "Iteration 189, loss = 0.05728454\n",
      "Iteration 190, loss = 0.05704127\n",
      "Iteration 191, loss = 0.05667590\n",
      "Iteration 192, loss = 0.05640513\n",
      "Iteration 193, loss = 0.05599405\n",
      "Iteration 194, loss = 0.05575832\n",
      "Iteration 195, loss = 0.05548527\n",
      "Iteration 196, loss = 0.05520163\n",
      "Iteration 197, loss = 0.05490830\n",
      "Iteration 198, loss = 0.05454613\n",
      "Iteration 199, loss = 0.05429164\n",
      "Iteration 200, loss = 0.05400370\n",
      "[CV] END activation=relu, alpha=0.001, batch_size=100, hidden_layer_sizes=(100,), solver=sgd; total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.30397345\n",
      "Iteration 2, loss = 0.12100643\n",
      "Iteration 3, loss = 0.07746082\n",
      "Iteration 4, loss = 0.05396997\n",
      "Iteration 5, loss = 0.04057925\n",
      "Iteration 6, loss = 0.03359926\n",
      "Iteration 7, loss = 0.02310770\n",
      "Iteration 8, loss = 0.01888735\n",
      "Iteration 9, loss = 0.02056890\n",
      "Iteration 10, loss = 0.01852193\n",
      "Iteration 11, loss = 0.02273487\n",
      "Iteration 12, loss = 0.01422945\n",
      "Iteration 13, loss = 0.01921644\n",
      "Iteration 14, loss = 0.01821667\n",
      "Iteration 15, loss = 0.01254133\n",
      "Iteration 16, loss = 0.01831467\n",
      "Iteration 17, loss = 0.01308442\n",
      "Iteration 18, loss = 0.02002765\n",
      "Iteration 19, loss = 0.01443270\n",
      "Iteration 20, loss = 0.01360217\n",
      "Iteration 21, loss = 0.01029498\n",
      "Iteration 22, loss = 0.01894736\n",
      "Iteration 23, loss = 0.01563659\n",
      "Iteration 24, loss = 0.01772468\n",
      "Iteration 25, loss = 0.01652191\n",
      "Iteration 26, loss = 0.01426834\n",
      "Iteration 27, loss = 0.01286886\n",
      "Iteration 28, loss = 0.01107401\n",
      "Iteration 29, loss = 0.01761408\n",
      "Iteration 30, loss = 0.01982957\n",
      "Iteration 31, loss = 0.01278841\n",
      "Iteration 32, loss = 0.00927056\n",
      "Iteration 33, loss = 0.00880373\n",
      "Iteration 34, loss = 0.00848813\n",
      "Iteration 35, loss = 0.00816318\n",
      "Iteration 36, loss = 0.00782395\n",
      "Iteration 37, loss = 0.00746790\n",
      "Iteration 38, loss = 0.00709483\n",
      "Iteration 39, loss = 0.00671565\n",
      "Iteration 40, loss = 0.02961103\n",
      "Iteration 41, loss = 0.02778461\n",
      "Iteration 42, loss = 0.01089026\n",
      "Iteration 43, loss = 0.00900636\n",
      "Iteration 44, loss = 0.00782406\n",
      "Iteration 45, loss = 0.00749962\n",
      "Iteration 46, loss = 0.00722179\n",
      "Iteration 47, loss = 0.00694316\n",
      "Iteration 48, loss = 0.00668063\n",
      "Iteration 49, loss = 0.00738093\n",
      "Iteration 50, loss = 0.03524846\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.001, batch_size=100, hidden_layer_sizes=(400, 100), solver=adam; total time= 3.5min\n",
      "Iteration 1, loss = 0.30928530\n",
      "Iteration 2, loss = 0.12274004\n",
      "Iteration 3, loss = 0.07717012\n",
      "Iteration 4, loss = 0.05397386\n",
      "Iteration 5, loss = 0.04138016\n",
      "Iteration 6, loss = 0.03189249\n",
      "Iteration 7, loss = 0.02536916\n",
      "Iteration 8, loss = 0.02154730\n",
      "Iteration 9, loss = 0.01863724\n",
      "Iteration 10, loss = 0.02244578\n",
      "Iteration 11, loss = 0.02090888\n",
      "Iteration 12, loss = 0.01731580\n",
      "Iteration 13, loss = 0.01450184\n",
      "Iteration 14, loss = 0.02153952\n",
      "Iteration 15, loss = 0.01932901\n",
      "Iteration 16, loss = 0.01190232\n",
      "Iteration 17, loss = 0.00877850\n",
      "Iteration 18, loss = 0.00822181\n",
      "Iteration 19, loss = 0.00848669\n",
      "Iteration 20, loss = 0.02664786\n",
      "Iteration 21, loss = 0.02407552\n",
      "Iteration 22, loss = 0.01251879\n",
      "Iteration 23, loss = 0.01425225\n",
      "Iteration 24, loss = 0.01574210\n",
      "Iteration 25, loss = 0.01524223\n",
      "Iteration 26, loss = 0.01567478\n",
      "Iteration 27, loss = 0.01569413\n",
      "Iteration 28, loss = 0.01118393\n",
      "Iteration 29, loss = 0.01342406\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.001, batch_size=100, hidden_layer_sizes=(400, 100), solver=adam; total time= 1.7min\n",
      "Iteration 1, loss = 0.30860755\n",
      "Iteration 2, loss = 0.11786731\n",
      "Iteration 3, loss = 0.07363286\n",
      "Iteration 4, loss = 0.05653851\n",
      "Iteration 5, loss = 0.03870041\n",
      "Iteration 6, loss = 0.03106462\n",
      "Iteration 7, loss = 0.02411888\n",
      "Iteration 8, loss = 0.02193104\n",
      "Iteration 9, loss = 0.02009408\n",
      "Iteration 10, loss = 0.01600030\n",
      "Iteration 11, loss = 0.02133220\n",
      "Iteration 12, loss = 0.01471369\n",
      "Iteration 13, loss = 0.01841417\n",
      "Iteration 14, loss = 0.02314690\n",
      "Iteration 15, loss = 0.01752929\n",
      "Iteration 16, loss = 0.01043538\n",
      "Iteration 17, loss = 0.01597931\n",
      "Iteration 18, loss = 0.01376297\n",
      "Iteration 19, loss = 0.01645314\n",
      "Iteration 20, loss = 0.01710457\n",
      "Iteration 21, loss = 0.01268312\n",
      "Iteration 22, loss = 0.01100864\n",
      "Iteration 23, loss = 0.01912540\n",
      "Iteration 24, loss = 0.02187040\n",
      "Iteration 25, loss = 0.01294497\n",
      "Iteration 26, loss = 0.01053061\n",
      "Iteration 27, loss = 0.01293476\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.001, batch_size=100, hidden_layer_sizes=(400, 100), solver=adam; total time= 1.6min\n",
      "Iteration 1, loss = 0.30885355\n",
      "Iteration 2, loss = 0.12034594\n",
      "Iteration 3, loss = 0.07866798\n",
      "Iteration 4, loss = 0.05605139\n",
      "Iteration 5, loss = 0.03954816\n",
      "Iteration 6, loss = 0.02844054\n",
      "Iteration 7, loss = 0.02605128\n",
      "Iteration 8, loss = 0.02210018\n",
      "Iteration 9, loss = 0.02129455\n",
      "Iteration 10, loss = 0.01689060\n",
      "Iteration 11, loss = 0.01945380\n",
      "Iteration 12, loss = 0.02097046\n",
      "Iteration 13, loss = 0.01497163\n",
      "Iteration 14, loss = 0.01481531\n",
      "Iteration 15, loss = 0.02105163\n",
      "Iteration 16, loss = 0.01617076\n",
      "Iteration 17, loss = 0.01595013\n",
      "Iteration 18, loss = 0.01632277\n",
      "Iteration 19, loss = 0.01240549\n",
      "Iteration 20, loss = 0.01892956\n",
      "Iteration 21, loss = 0.01685262\n",
      "Iteration 22, loss = 0.01441917\n",
      "Iteration 23, loss = 0.00967795\n",
      "Iteration 24, loss = 0.01143106\n",
      "Iteration 25, loss = 0.01656023\n",
      "Iteration 26, loss = 0.02142457\n",
      "Iteration 27, loss = 0.01244180\n",
      "Iteration 28, loss = 0.01202091\n",
      "Iteration 29, loss = 0.01472231\n",
      "Iteration 30, loss = 0.01952429\n",
      "Iteration 31, loss = 0.01563117\n",
      "Iteration 32, loss = 0.01457740\n",
      "Iteration 33, loss = 0.01179192\n",
      "Iteration 34, loss = 0.00991120\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.001, batch_size=100, hidden_layer_sizes=(400, 100), solver=adam; total time= 2.1min\n",
      "Iteration 1, loss = 0.31306954\n",
      "Iteration 2, loss = 0.11833642\n",
      "Iteration 3, loss = 0.07881114\n",
      "Iteration 4, loss = 0.05571398\n",
      "Iteration 5, loss = 0.04234058\n",
      "Iteration 6, loss = 0.03257748\n",
      "Iteration 7, loss = 0.02403856\n",
      "Iteration 8, loss = 0.02137945\n",
      "Iteration 9, loss = 0.02004370\n",
      "Iteration 10, loss = 0.01853991\n",
      "Iteration 11, loss = 0.01726065\n",
      "Iteration 12, loss = 0.02020135\n",
      "Iteration 13, loss = 0.01862162\n",
      "Iteration 14, loss = 0.01542974\n",
      "Iteration 15, loss = 0.02357932\n",
      "Iteration 16, loss = 0.01547094\n",
      "Iteration 17, loss = 0.01046024\n",
      "Iteration 18, loss = 0.00805739\n",
      "Iteration 19, loss = 0.00752705\n",
      "Iteration 20, loss = 0.00723287\n",
      "Iteration 21, loss = 0.00695259\n",
      "Iteration 22, loss = 0.00667209\n",
      "Iteration 23, loss = 0.00639111\n",
      "Iteration 24, loss = 0.00610700\n",
      "Iteration 25, loss = 0.01809474\n",
      "Iteration 26, loss = 0.04685867\n",
      "Iteration 27, loss = 0.01916075\n",
      "Iteration 28, loss = 0.01414959\n",
      "Iteration 29, loss = 0.01025648\n",
      "Iteration 30, loss = 0.01475541\n",
      "Iteration 31, loss = 0.01478151\n",
      "Iteration 32, loss = 0.01200254\n",
      "Iteration 33, loss = 0.01344100\n",
      "Iteration 34, loss = 0.01147552\n",
      "Iteration 35, loss = 0.00885563\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.001, batch_size=100, hidden_layer_sizes=(400, 100), solver=adam; total time= 2.1min\n",
      "Iteration 1, loss = 1.42358103\n",
      "Iteration 2, loss = 0.57570529\n",
      "Iteration 3, loss = 0.42647421\n",
      "Iteration 4, loss = 0.36768205\n",
      "Iteration 5, loss = 0.33327392\n",
      "Iteration 6, loss = 0.31019583\n",
      "Iteration 7, loss = 0.29214849\n",
      "Iteration 8, loss = 0.27723064\n",
      "Iteration 9, loss = 0.26487826\n",
      "Iteration 10, loss = 0.25344592\n",
      "Iteration 11, loss = 0.24354376\n",
      "Iteration 12, loss = 0.23438379\n",
      "Iteration 13, loss = 0.22577605\n",
      "Iteration 14, loss = 0.21828771\n",
      "Iteration 15, loss = 0.21096918\n",
      "Iteration 16, loss = 0.20381229\n",
      "Iteration 17, loss = 0.19761645\n",
      "Iteration 18, loss = 0.19169167\n",
      "Iteration 19, loss = 0.18593939\n",
      "Iteration 20, loss = 0.18043598\n",
      "Iteration 21, loss = 0.17552977\n",
      "Iteration 22, loss = 0.17071682\n",
      "Iteration 23, loss = 0.16572376\n",
      "Iteration 24, loss = 0.16143667\n",
      "Iteration 25, loss = 0.15712217\n",
      "Iteration 26, loss = 0.15287187\n",
      "Iteration 27, loss = 0.14909689\n",
      "Iteration 28, loss = 0.14542147\n",
      "Iteration 29, loss = 0.14174919\n",
      "Iteration 30, loss = 0.13823261\n",
      "Iteration 31, loss = 0.13473492\n",
      "Iteration 32, loss = 0.13163159\n",
      "Iteration 33, loss = 0.12851447\n",
      "Iteration 34, loss = 0.12545139\n",
      "Iteration 35, loss = 0.12272237\n",
      "Iteration 36, loss = 0.12000060\n",
      "Iteration 37, loss = 0.11700329\n",
      "Iteration 38, loss = 0.11451911\n",
      "Iteration 39, loss = 0.11213883\n",
      "Iteration 40, loss = 0.10950101\n",
      "Iteration 41, loss = 0.10742603\n",
      "Iteration 42, loss = 0.10492412\n",
      "Iteration 43, loss = 0.10279299\n",
      "Iteration 44, loss = 0.10033619\n",
      "Iteration 45, loss = 0.09844598\n",
      "Iteration 46, loss = 0.09612462\n",
      "Iteration 47, loss = 0.09411967\n",
      "Iteration 48, loss = 0.09212462\n",
      "Iteration 49, loss = 0.09040652\n",
      "Iteration 50, loss = 0.08856035\n",
      "Iteration 51, loss = 0.08669684\n",
      "Iteration 52, loss = 0.08493110\n",
      "Iteration 53, loss = 0.08321023\n",
      "Iteration 54, loss = 0.08150247\n",
      "Iteration 55, loss = 0.07976854\n",
      "Iteration 56, loss = 0.07843335\n",
      "Iteration 57, loss = 0.07670508\n",
      "Iteration 58, loss = 0.07516282\n",
      "Iteration 59, loss = 0.07389308\n",
      "Iteration 60, loss = 0.07242587\n",
      "Iteration 61, loss = 0.07086190\n",
      "Iteration 62, loss = 0.06970135\n",
      "Iteration 63, loss = 0.06831760\n",
      "Iteration 64, loss = 0.06712189\n",
      "Iteration 65, loss = 0.06579317\n",
      "Iteration 66, loss = 0.06455901\n",
      "Iteration 67, loss = 0.06326444\n",
      "Iteration 68, loss = 0.06219319\n",
      "Iteration 69, loss = 0.06098342\n",
      "Iteration 70, loss = 0.05983247\n",
      "Iteration 71, loss = 0.05870331\n",
      "Iteration 72, loss = 0.05782292\n",
      "Iteration 73, loss = 0.05656104\n",
      "Iteration 74, loss = 0.05570743\n",
      "Iteration 75, loss = 0.05470632\n",
      "Iteration 76, loss = 0.05376274\n",
      "Iteration 77, loss = 0.05289659\n",
      "Iteration 78, loss = 0.05188754\n",
      "Iteration 79, loss = 0.05094878\n",
      "Iteration 80, loss = 0.05016599\n",
      "Iteration 81, loss = 0.04941727\n",
      "Iteration 82, loss = 0.04842792\n",
      "Iteration 83, loss = 0.04765497\n",
      "Iteration 84, loss = 0.04681639\n",
      "Iteration 85, loss = 0.04598970\n",
      "Iteration 86, loss = 0.04527082\n",
      "Iteration 87, loss = 0.04450593\n",
      "Iteration 88, loss = 0.04371671\n",
      "Iteration 89, loss = 0.04308701\n",
      "Iteration 90, loss = 0.04232592\n",
      "Iteration 91, loss = 0.04176148\n",
      "Iteration 92, loss = 0.04093545\n",
      "Iteration 93, loss = 0.04044823\n",
      "Iteration 94, loss = 0.03968608\n",
      "Iteration 95, loss = 0.03907864\n",
      "Iteration 96, loss = 0.03848425\n",
      "Iteration 97, loss = 0.03777129\n",
      "Iteration 98, loss = 0.03712783\n",
      "Iteration 99, loss = 0.03672790\n",
      "Iteration 100, loss = 0.03613542\n",
      "Iteration 101, loss = 0.03549062\n",
      "Iteration 102, loss = 0.03488866\n",
      "Iteration 103, loss = 0.03460003\n",
      "Iteration 104, loss = 0.03394258\n",
      "Iteration 105, loss = 0.03345468\n",
      "Iteration 106, loss = 0.03287865\n",
      "Iteration 107, loss = 0.03248472\n",
      "Iteration 108, loss = 0.03197204\n",
      "Iteration 109, loss = 0.03151431\n",
      "Iteration 110, loss = 0.03101923\n",
      "Iteration 111, loss = 0.03055957\n",
      "Iteration 112, loss = 0.03027056\n",
      "Iteration 113, loss = 0.02974927\n",
      "Iteration 114, loss = 0.02922923\n",
      "Iteration 115, loss = 0.02881466\n",
      "Iteration 116, loss = 0.02849492\n",
      "Iteration 117, loss = 0.02803962\n",
      "Iteration 118, loss = 0.02761682\n",
      "Iteration 119, loss = 0.02727504\n",
      "Iteration 120, loss = 0.02694988\n",
      "Iteration 121, loss = 0.02650564\n",
      "Iteration 122, loss = 0.02613039\n",
      "Iteration 123, loss = 0.02578330\n",
      "Iteration 124, loss = 0.02546459\n",
      "Iteration 125, loss = 0.02516122\n",
      "Iteration 126, loss = 0.02479948\n",
      "Iteration 127, loss = 0.02445844\n",
      "Iteration 128, loss = 0.02419922\n",
      "Iteration 129, loss = 0.02373548\n",
      "Iteration 130, loss = 0.02351593\n",
      "Iteration 131, loss = 0.02320985\n",
      "Iteration 132, loss = 0.02293781\n",
      "Iteration 133, loss = 0.02264645\n",
      "Iteration 134, loss = 0.02231184\n",
      "Iteration 135, loss = 0.02215849\n",
      "Iteration 136, loss = 0.02175596\n",
      "Iteration 137, loss = 0.02159337\n",
      "Iteration 138, loss = 0.02127500\n",
      "Iteration 139, loss = 0.02100062\n",
      "Iteration 140, loss = 0.02073944\n",
      "Iteration 141, loss = 0.02054712\n",
      "Iteration 142, loss = 0.02022190\n",
      "Iteration 143, loss = 0.02004281\n",
      "Iteration 144, loss = 0.01980650\n",
      "Iteration 145, loss = 0.01959225\n",
      "Iteration 146, loss = 0.01941951\n",
      "Iteration 147, loss = 0.01916530\n",
      "Iteration 148, loss = 0.01900741\n",
      "Iteration 149, loss = 0.01877398\n",
      "Iteration 150, loss = 0.01857976\n",
      "Iteration 151, loss = 0.01838798\n",
      "Iteration 152, loss = 0.01818386\n",
      "Iteration 153, loss = 0.01794516\n",
      "Iteration 154, loss = 0.01781005\n",
      "Iteration 155, loss = 0.01758809\n",
      "Iteration 156, loss = 0.01740735\n",
      "Iteration 157, loss = 0.01725389\n",
      "Iteration 158, loss = 0.01705903\n",
      "Iteration 159, loss = 0.01691667\n",
      "Iteration 160, loss = 0.01674945\n",
      "Iteration 161, loss = 0.01659448\n",
      "Iteration 162, loss = 0.01639466\n",
      "Iteration 163, loss = 0.01625103\n",
      "Iteration 164, loss = 0.01613778\n",
      "Iteration 165, loss = 0.01594938\n",
      "Iteration 166, loss = 0.01584070\n",
      "Iteration 167, loss = 0.01566801\n",
      "Iteration 168, loss = 0.01549781\n",
      "Iteration 169, loss = 0.01542302\n",
      "Iteration 170, loss = 0.01524340\n",
      "Iteration 171, loss = 0.01510258\n",
      "Iteration 172, loss = 0.01498022\n",
      "Iteration 173, loss = 0.01482587\n",
      "Iteration 174, loss = 0.01467194\n",
      "Iteration 175, loss = 0.01461446\n",
      "Iteration 176, loss = 0.01447284\n",
      "Iteration 177, loss = 0.01437343\n",
      "Iteration 178, loss = 0.01423601\n",
      "Iteration 179, loss = 0.01414653\n",
      "Iteration 180, loss = 0.01403512\n",
      "Iteration 181, loss = 0.01389809\n",
      "Iteration 182, loss = 0.01377276\n",
      "Iteration 183, loss = 0.01367784\n",
      "Iteration 184, loss = 0.01358268\n",
      "Iteration 185, loss = 0.01349207\n",
      "Iteration 186, loss = 0.01335164\n",
      "Iteration 187, loss = 0.01324857\n",
      "Iteration 188, loss = 0.01316847\n",
      "Iteration 189, loss = 0.01307317\n",
      "Iteration 190, loss = 0.01296366\n",
      "Iteration 191, loss = 0.01288568\n",
      "Iteration 192, loss = 0.01273386\n",
      "Iteration 193, loss = 0.01267678\n",
      "Iteration 194, loss = 0.01260460\n",
      "Iteration 195, loss = 0.01247777\n",
      "Iteration 196, loss = 0.01240967\n",
      "Iteration 197, loss = 0.01234051\n",
      "Iteration 198, loss = 0.01224961\n",
      "Iteration 199, loss = 0.01217219\n",
      "Iteration 200, loss = 0.01207003\n",
      "[CV] END activation=relu, alpha=0.001, batch_size=100, hidden_layer_sizes=(400, 100), solver=sgd; total time= 6.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.33808895\n",
      "Iteration 2, loss = 0.54903058\n",
      "Iteration 3, loss = 0.41398379\n",
      "Iteration 4, loss = 0.35881408\n",
      "Iteration 5, loss = 0.32691438\n",
      "Iteration 6, loss = 0.30430003\n",
      "Iteration 7, loss = 0.28658408\n",
      "Iteration 8, loss = 0.27212718\n",
      "Iteration 9, loss = 0.25978942\n",
      "Iteration 10, loss = 0.24864512\n",
      "Iteration 11, loss = 0.23880794\n",
      "Iteration 12, loss = 0.22975800\n",
      "Iteration 13, loss = 0.22116492\n",
      "Iteration 14, loss = 0.21355789\n",
      "Iteration 15, loss = 0.20651353\n",
      "Iteration 16, loss = 0.19999847\n",
      "Iteration 17, loss = 0.19344079\n",
      "Iteration 18, loss = 0.18760328\n",
      "Iteration 19, loss = 0.18209337\n",
      "Iteration 20, loss = 0.17658723\n",
      "Iteration 21, loss = 0.17138437\n",
      "Iteration 22, loss = 0.16675525\n",
      "Iteration 23, loss = 0.16209359\n",
      "Iteration 24, loss = 0.15776674\n",
      "Iteration 25, loss = 0.15358868\n",
      "Iteration 26, loss = 0.14992854\n",
      "Iteration 27, loss = 0.14579991\n",
      "Iteration 28, loss = 0.14184428\n",
      "Iteration 29, loss = 0.13852202\n",
      "Iteration 30, loss = 0.13523983\n",
      "Iteration 31, loss = 0.13195733\n",
      "Iteration 32, loss = 0.12867837\n",
      "Iteration 33, loss = 0.12591478\n",
      "Iteration 34, loss = 0.12278227\n",
      "Iteration 35, loss = 0.12010549\n",
      "Iteration 36, loss = 0.11731948\n",
      "Iteration 37, loss = 0.11475567\n",
      "Iteration 38, loss = 0.11218593\n",
      "Iteration 39, loss = 0.10971125\n",
      "Iteration 40, loss = 0.10719100\n",
      "Iteration 41, loss = 0.10510146\n",
      "Iteration 42, loss = 0.10284747\n",
      "Iteration 43, loss = 0.10063084\n",
      "Iteration 44, loss = 0.09847737\n",
      "Iteration 45, loss = 0.09662904\n",
      "Iteration 46, loss = 0.09456438\n",
      "Iteration 47, loss = 0.09250791\n",
      "Iteration 48, loss = 0.09069745\n",
      "Iteration 49, loss = 0.08868411\n",
      "Iteration 50, loss = 0.08725200\n",
      "Iteration 51, loss = 0.08543181\n",
      "Iteration 52, loss = 0.08369779\n",
      "Iteration 53, loss = 0.08214844\n",
      "Iteration 54, loss = 0.08046061\n",
      "Iteration 55, loss = 0.07900308\n",
      "Iteration 56, loss = 0.07745061\n",
      "Iteration 57, loss = 0.07604510\n",
      "Iteration 58, loss = 0.07469337\n",
      "Iteration 59, loss = 0.07311784\n",
      "Iteration 60, loss = 0.07177478\n",
      "Iteration 61, loss = 0.07056653\n",
      "Iteration 62, loss = 0.06911552\n",
      "Iteration 63, loss = 0.06778916\n",
      "Iteration 64, loss = 0.06664744\n",
      "Iteration 65, loss = 0.06537447\n",
      "Iteration 66, loss = 0.06423184\n",
      "Iteration 67, loss = 0.06315478\n",
      "Iteration 68, loss = 0.06212421\n",
      "Iteration 69, loss = 0.06085322\n",
      "Iteration 70, loss = 0.05993411\n",
      "Iteration 71, loss = 0.05883364\n",
      "Iteration 72, loss = 0.05784883\n",
      "Iteration 73, loss = 0.05669824\n",
      "Iteration 74, loss = 0.05594521\n",
      "Iteration 75, loss = 0.05479441\n",
      "Iteration 76, loss = 0.05379704\n",
      "Iteration 77, loss = 0.05315790\n",
      "Iteration 78, loss = 0.05207799\n",
      "Iteration 79, loss = 0.05099600\n",
      "Iteration 80, loss = 0.05041446\n",
      "Iteration 81, loss = 0.04942377\n",
      "Iteration 82, loss = 0.04860570\n",
      "Iteration 83, loss = 0.04771511\n",
      "Iteration 84, loss = 0.04711066\n",
      "Iteration 85, loss = 0.04626809\n",
      "Iteration 86, loss = 0.04544106\n",
      "Iteration 87, loss = 0.04476941\n",
      "Iteration 88, loss = 0.04423893\n",
      "Iteration 89, loss = 0.04324999\n",
      "Iteration 90, loss = 0.04265334\n",
      "Iteration 91, loss = 0.04192851\n",
      "Iteration 92, loss = 0.04131353\n",
      "Iteration 93, loss = 0.04062573\n",
      "Iteration 94, loss = 0.04010779\n",
      "Iteration 95, loss = 0.03933241\n",
      "Iteration 96, loss = 0.03874809\n",
      "Iteration 97, loss = 0.03807734\n",
      "Iteration 98, loss = 0.03766308\n",
      "Iteration 99, loss = 0.03701922\n",
      "Iteration 100, loss = 0.03646284\n",
      "Iteration 101, loss = 0.03578115\n",
      "Iteration 102, loss = 0.03541118\n",
      "Iteration 103, loss = 0.03474928\n",
      "Iteration 104, loss = 0.03420132\n",
      "Iteration 105, loss = 0.03372834\n",
      "Iteration 106, loss = 0.03325794\n",
      "Iteration 107, loss = 0.03276910\n",
      "Iteration 108, loss = 0.03229033\n",
      "Iteration 109, loss = 0.03189282\n",
      "Iteration 110, loss = 0.03134316\n",
      "Iteration 111, loss = 0.03089170\n",
      "Iteration 112, loss = 0.03040882\n",
      "Iteration 113, loss = 0.03001593\n",
      "Iteration 114, loss = 0.02964435\n",
      "Iteration 115, loss = 0.02916375\n",
      "Iteration 116, loss = 0.02878427\n",
      "Iteration 117, loss = 0.02838414\n",
      "Iteration 118, loss = 0.02806206\n",
      "Iteration 119, loss = 0.02752264\n",
      "Iteration 120, loss = 0.02716351\n",
      "Iteration 121, loss = 0.02691377\n",
      "Iteration 122, loss = 0.02650683\n",
      "Iteration 123, loss = 0.02615480\n",
      "Iteration 124, loss = 0.02568509\n",
      "Iteration 125, loss = 0.02535398\n",
      "Iteration 126, loss = 0.02515700\n",
      "Iteration 127, loss = 0.02481706\n",
      "Iteration 128, loss = 0.02443390\n",
      "Iteration 129, loss = 0.02421202\n",
      "Iteration 130, loss = 0.02385863\n",
      "Iteration 131, loss = 0.02344066\n",
      "Iteration 132, loss = 0.02319061\n",
      "Iteration 133, loss = 0.02296016\n",
      "Iteration 134, loss = 0.02267987\n",
      "Iteration 135, loss = 0.02242883\n",
      "Iteration 136, loss = 0.02205505\n",
      "Iteration 137, loss = 0.02188284\n",
      "Iteration 138, loss = 0.02149777\n",
      "Iteration 139, loss = 0.02124258\n",
      "Iteration 140, loss = 0.02106249\n",
      "Iteration 141, loss = 0.02080764\n",
      "Iteration 142, loss = 0.02054577\n",
      "Iteration 143, loss = 0.02029423\n",
      "Iteration 144, loss = 0.02004208\n",
      "Iteration 145, loss = 0.01982462\n",
      "Iteration 146, loss = 0.01958759\n",
      "Iteration 147, loss = 0.01943085\n",
      "Iteration 148, loss = 0.01922572\n",
      "Iteration 149, loss = 0.01892120\n",
      "Iteration 150, loss = 0.01881176\n",
      "Iteration 151, loss = 0.01864447\n",
      "Iteration 152, loss = 0.01832915\n",
      "Iteration 153, loss = 0.01818600\n",
      "Iteration 154, loss = 0.01803679\n",
      "Iteration 155, loss = 0.01781720\n",
      "Iteration 156, loss = 0.01761435\n",
      "Iteration 157, loss = 0.01743233\n",
      "Iteration 158, loss = 0.01725367\n",
      "Iteration 159, loss = 0.01702987\n",
      "Iteration 160, loss = 0.01692455\n",
      "Iteration 161, loss = 0.01675915\n",
      "Iteration 162, loss = 0.01658901\n",
      "Iteration 163, loss = 0.01642602\n",
      "Iteration 164, loss = 0.01625317\n",
      "Iteration 165, loss = 0.01609454\n",
      "Iteration 166, loss = 0.01593255\n",
      "Iteration 167, loss = 0.01581006\n",
      "Iteration 168, loss = 0.01569517\n",
      "Iteration 169, loss = 0.01547176\n",
      "Iteration 170, loss = 0.01537096\n",
      "Iteration 171, loss = 0.01527801\n",
      "Iteration 172, loss = 0.01511490\n",
      "Iteration 173, loss = 0.01495770\n",
      "Iteration 174, loss = 0.01482737\n",
      "Iteration 175, loss = 0.01473175\n",
      "Iteration 176, loss = 0.01456404\n",
      "Iteration 177, loss = 0.01445312\n",
      "Iteration 178, loss = 0.01436279\n",
      "Iteration 179, loss = 0.01419192\n",
      "Iteration 180, loss = 0.01407536\n",
      "Iteration 181, loss = 0.01399580\n",
      "Iteration 182, loss = 0.01384377\n",
      "Iteration 183, loss = 0.01376119\n",
      "Iteration 184, loss = 0.01365558\n",
      "Iteration 185, loss = 0.01352995\n",
      "Iteration 186, loss = 0.01340542\n",
      "Iteration 187, loss = 0.01331045\n",
      "Iteration 188, loss = 0.01319232\n",
      "Iteration 189, loss = 0.01309833\n",
      "Iteration 190, loss = 0.01303200\n",
      "Iteration 191, loss = 0.01292535\n",
      "Iteration 192, loss = 0.01283527\n",
      "Iteration 193, loss = 0.01274910\n",
      "Iteration 194, loss = 0.01265063\n",
      "Iteration 195, loss = 0.01256075\n",
      "Iteration 196, loss = 0.01246285\n",
      "Iteration 197, loss = 0.01242477\n",
      "Iteration 198, loss = 0.01228071\n",
      "Iteration 199, loss = 0.01224875\n",
      "Iteration 200, loss = 0.01212529\n",
      "[CV] END activation=relu, alpha=0.001, batch_size=100, hidden_layer_sizes=(400, 100), solver=sgd; total time= 6.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.39035329\n",
      "Iteration 2, loss = 0.55372950\n",
      "Iteration 3, loss = 0.41315809\n",
      "Iteration 4, loss = 0.35819939\n",
      "Iteration 5, loss = 0.32608952\n",
      "Iteration 6, loss = 0.30351470\n",
      "Iteration 7, loss = 0.28572363\n",
      "Iteration 8, loss = 0.27127958\n",
      "Iteration 9, loss = 0.25878247\n",
      "Iteration 10, loss = 0.24761611\n",
      "Iteration 11, loss = 0.23737108\n",
      "Iteration 12, loss = 0.22822185\n",
      "Iteration 13, loss = 0.21978336\n",
      "Iteration 14, loss = 0.21198899\n",
      "Iteration 15, loss = 0.20429793\n",
      "Iteration 16, loss = 0.19728406\n",
      "Iteration 17, loss = 0.19049125\n",
      "Iteration 18, loss = 0.18478245\n",
      "Iteration 19, loss = 0.17886164\n",
      "Iteration 20, loss = 0.17367270\n",
      "Iteration 21, loss = 0.16846568\n",
      "Iteration 22, loss = 0.16375293\n",
      "Iteration 23, loss = 0.15896517\n",
      "Iteration 24, loss = 0.15452439\n",
      "Iteration 25, loss = 0.15016080\n",
      "Iteration 26, loss = 0.14637230\n",
      "Iteration 27, loss = 0.14221818\n",
      "Iteration 28, loss = 0.13878787\n",
      "Iteration 29, loss = 0.13510607\n",
      "Iteration 30, loss = 0.13184260\n",
      "Iteration 31, loss = 0.12861965\n",
      "Iteration 32, loss = 0.12552309\n",
      "Iteration 33, loss = 0.12240551\n",
      "Iteration 34, loss = 0.11976457\n",
      "Iteration 35, loss = 0.11695207\n",
      "Iteration 36, loss = 0.11406804\n",
      "Iteration 37, loss = 0.11155035\n",
      "Iteration 38, loss = 0.10876268\n",
      "Iteration 39, loss = 0.10653738\n",
      "Iteration 40, loss = 0.10431448\n",
      "Iteration 41, loss = 0.10184364\n",
      "Iteration 42, loss = 0.09952992\n",
      "Iteration 43, loss = 0.09751706\n",
      "Iteration 44, loss = 0.09549035\n",
      "Iteration 45, loss = 0.09333857\n",
      "Iteration 46, loss = 0.09139662\n",
      "Iteration 47, loss = 0.08950415\n",
      "Iteration 48, loss = 0.08771849\n",
      "Iteration 49, loss = 0.08591077\n",
      "Iteration 50, loss = 0.08417340\n",
      "Iteration 51, loss = 0.08237702\n",
      "Iteration 52, loss = 0.08077465\n",
      "Iteration 53, loss = 0.07903720\n",
      "Iteration 54, loss = 0.07766209\n",
      "Iteration 55, loss = 0.07608117\n",
      "Iteration 56, loss = 0.07472111\n",
      "Iteration 57, loss = 0.07322507\n",
      "Iteration 58, loss = 0.07186442\n",
      "Iteration 59, loss = 0.07055107\n",
      "Iteration 60, loss = 0.06915971\n",
      "Iteration 61, loss = 0.06779149\n",
      "Iteration 62, loss = 0.06654598\n",
      "Iteration 63, loss = 0.06533799\n",
      "Iteration 64, loss = 0.06409915\n",
      "Iteration 65, loss = 0.06282440\n",
      "Iteration 66, loss = 0.06178510\n",
      "Iteration 67, loss = 0.06057522\n",
      "Iteration 68, loss = 0.05957533\n",
      "Iteration 69, loss = 0.05858827\n",
      "Iteration 70, loss = 0.05750515\n",
      "Iteration 71, loss = 0.05646652\n",
      "Iteration 72, loss = 0.05545034\n",
      "Iteration 73, loss = 0.05465849\n",
      "Iteration 74, loss = 0.05357715\n",
      "Iteration 75, loss = 0.05263154\n",
      "Iteration 76, loss = 0.05167486\n",
      "Iteration 77, loss = 0.05085941\n",
      "Iteration 78, loss = 0.04989362\n",
      "Iteration 79, loss = 0.04920944\n",
      "Iteration 80, loss = 0.04825964\n",
      "Iteration 81, loss = 0.04750570\n",
      "Iteration 82, loss = 0.04667009\n",
      "Iteration 83, loss = 0.04598648\n",
      "Iteration 84, loss = 0.04530479\n",
      "Iteration 85, loss = 0.04441879\n",
      "Iteration 86, loss = 0.04372074\n",
      "Iteration 87, loss = 0.04306853\n",
      "Iteration 88, loss = 0.04235956\n",
      "Iteration 89, loss = 0.04160193\n",
      "Iteration 90, loss = 0.04101006\n",
      "Iteration 91, loss = 0.04035974\n",
      "Iteration 92, loss = 0.03975865\n",
      "Iteration 93, loss = 0.03915892\n",
      "Iteration 94, loss = 0.03846738\n",
      "Iteration 95, loss = 0.03795128\n",
      "Iteration 96, loss = 0.03729185\n",
      "Iteration 97, loss = 0.03677934\n",
      "Iteration 98, loss = 0.03620211\n",
      "Iteration 99, loss = 0.03562881\n",
      "Iteration 100, loss = 0.03511115\n",
      "Iteration 101, loss = 0.03460709\n",
      "Iteration 102, loss = 0.03399328\n",
      "Iteration 103, loss = 0.03348110\n",
      "Iteration 104, loss = 0.03296218\n",
      "Iteration 105, loss = 0.03258726\n",
      "Iteration 106, loss = 0.03212361\n",
      "Iteration 107, loss = 0.03163973\n",
      "Iteration 108, loss = 0.03123662\n",
      "Iteration 109, loss = 0.03071365\n",
      "Iteration 110, loss = 0.03026120\n",
      "Iteration 111, loss = 0.02983423\n",
      "Iteration 112, loss = 0.02940233\n",
      "Iteration 113, loss = 0.02906448\n",
      "Iteration 114, loss = 0.02863123\n",
      "Iteration 115, loss = 0.02825147\n",
      "Iteration 116, loss = 0.02783947\n",
      "Iteration 117, loss = 0.02738853\n",
      "Iteration 118, loss = 0.02708107\n",
      "Iteration 119, loss = 0.02675766\n",
      "Iteration 120, loss = 0.02638434\n",
      "Iteration 121, loss = 0.02601125\n",
      "Iteration 122, loss = 0.02569696\n",
      "Iteration 123, loss = 0.02533176\n",
      "Iteration 124, loss = 0.02503160\n",
      "Iteration 125, loss = 0.02468486\n",
      "Iteration 126, loss = 0.02440380\n",
      "Iteration 127, loss = 0.02410269\n",
      "Iteration 128, loss = 0.02373496\n",
      "Iteration 129, loss = 0.02344508\n",
      "Iteration 130, loss = 0.02315961\n",
      "Iteration 131, loss = 0.02297956\n",
      "Iteration 132, loss = 0.02255773\n",
      "Iteration 133, loss = 0.02235934\n",
      "Iteration 134, loss = 0.02204961\n",
      "Iteration 135, loss = 0.02179873\n",
      "Iteration 136, loss = 0.02152451\n",
      "Iteration 137, loss = 0.02132236\n",
      "Iteration 138, loss = 0.02104938\n",
      "Iteration 139, loss = 0.02082221\n",
      "Iteration 140, loss = 0.02057223\n",
      "Iteration 141, loss = 0.02032618\n",
      "Iteration 142, loss = 0.02005562\n",
      "Iteration 143, loss = 0.01989843\n",
      "Iteration 144, loss = 0.01970396\n",
      "Iteration 145, loss = 0.01944551\n",
      "Iteration 146, loss = 0.01922363\n",
      "Iteration 147, loss = 0.01901558\n",
      "Iteration 148, loss = 0.01877406\n",
      "Iteration 149, loss = 0.01864190\n",
      "Iteration 150, loss = 0.01838509\n",
      "Iteration 151, loss = 0.01823537\n",
      "Iteration 152, loss = 0.01804374\n",
      "Iteration 153, loss = 0.01784769\n",
      "Iteration 154, loss = 0.01765065\n",
      "Iteration 155, loss = 0.01754098\n",
      "Iteration 156, loss = 0.01732303\n",
      "Iteration 157, loss = 0.01714222\n",
      "Iteration 158, loss = 0.01701226\n",
      "Iteration 159, loss = 0.01683780\n",
      "Iteration 160, loss = 0.01668595\n",
      "Iteration 161, loss = 0.01646371\n",
      "Iteration 162, loss = 0.01630043\n",
      "Iteration 163, loss = 0.01616517\n",
      "Iteration 164, loss = 0.01603760\n",
      "Iteration 165, loss = 0.01590470\n",
      "Iteration 166, loss = 0.01574112\n",
      "Iteration 167, loss = 0.01564160\n",
      "Iteration 168, loss = 0.01543971\n",
      "Iteration 169, loss = 0.01534536\n",
      "Iteration 170, loss = 0.01519151\n",
      "Iteration 171, loss = 0.01507431\n",
      "Iteration 172, loss = 0.01495379\n",
      "Iteration 173, loss = 0.01480982\n",
      "Iteration 174, loss = 0.01469302\n",
      "Iteration 175, loss = 0.01457214\n",
      "Iteration 176, loss = 0.01441936\n",
      "Iteration 177, loss = 0.01428721\n",
      "Iteration 178, loss = 0.01418387\n",
      "Iteration 179, loss = 0.01406807\n",
      "Iteration 180, loss = 0.01397817\n",
      "Iteration 181, loss = 0.01382950\n",
      "Iteration 182, loss = 0.01374246\n",
      "Iteration 183, loss = 0.01361662\n",
      "Iteration 184, loss = 0.01351295\n",
      "Iteration 185, loss = 0.01342117\n",
      "Iteration 186, loss = 0.01335821\n",
      "Iteration 187, loss = 0.01321468\n",
      "Iteration 188, loss = 0.01313459\n",
      "Iteration 189, loss = 0.01303692\n",
      "Iteration 190, loss = 0.01292023\n",
      "Iteration 191, loss = 0.01285387\n",
      "Iteration 192, loss = 0.01275815\n",
      "Iteration 193, loss = 0.01266811\n",
      "Iteration 194, loss = 0.01255944\n",
      "Iteration 195, loss = 0.01247900\n",
      "Iteration 196, loss = 0.01241658\n",
      "Iteration 197, loss = 0.01231153\n",
      "Iteration 198, loss = 0.01222041\n",
      "Iteration 199, loss = 0.01215905\n",
      "Iteration 200, loss = 0.01207763\n",
      "[CV] END activation=relu, alpha=0.001, batch_size=100, hidden_layer_sizes=(400, 100), solver=sgd; total time= 6.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.34702245\n",
      "Iteration 2, loss = 0.54432816\n",
      "Iteration 3, loss = 0.40922532\n",
      "Iteration 4, loss = 0.35516545\n",
      "Iteration 5, loss = 0.32374840\n",
      "Iteration 6, loss = 0.30137472\n",
      "Iteration 7, loss = 0.28427349\n",
      "Iteration 8, loss = 0.26998805\n",
      "Iteration 9, loss = 0.25743491\n",
      "Iteration 10, loss = 0.24648881\n",
      "Iteration 11, loss = 0.23656742\n",
      "Iteration 12, loss = 0.22792159\n",
      "Iteration 13, loss = 0.21935247\n",
      "Iteration 14, loss = 0.21178737\n",
      "Iteration 15, loss = 0.20490921\n",
      "Iteration 16, loss = 0.19774021\n",
      "Iteration 17, loss = 0.19177956\n",
      "Iteration 18, loss = 0.18569395\n",
      "Iteration 19, loss = 0.18030687\n",
      "Iteration 20, loss = 0.17506061\n",
      "Iteration 21, loss = 0.17003368\n",
      "Iteration 22, loss = 0.16508091\n",
      "Iteration 23, loss = 0.16071464\n",
      "Iteration 24, loss = 0.15617778\n",
      "Iteration 25, loss = 0.15233708\n",
      "Iteration 26, loss = 0.14824223\n",
      "Iteration 27, loss = 0.14416915\n",
      "Iteration 28, loss = 0.14078156\n",
      "Iteration 29, loss = 0.13705400\n",
      "Iteration 30, loss = 0.13397086\n",
      "Iteration 31, loss = 0.13055421\n",
      "Iteration 32, loss = 0.12743472\n",
      "Iteration 33, loss = 0.12423841\n",
      "Iteration 34, loss = 0.12134371\n",
      "Iteration 35, loss = 0.11866331\n",
      "Iteration 36, loss = 0.11603356\n",
      "Iteration 37, loss = 0.11314704\n",
      "Iteration 38, loss = 0.11085568\n",
      "Iteration 39, loss = 0.10815056\n",
      "Iteration 40, loss = 0.10577721\n",
      "Iteration 41, loss = 0.10359185\n",
      "Iteration 42, loss = 0.10145897\n",
      "Iteration 43, loss = 0.09910677\n",
      "Iteration 44, loss = 0.09685618\n",
      "Iteration 45, loss = 0.09472304\n",
      "Iteration 46, loss = 0.09313408\n",
      "Iteration 47, loss = 0.09090086\n",
      "Iteration 48, loss = 0.08891362\n",
      "Iteration 49, loss = 0.08726702\n",
      "Iteration 50, loss = 0.08533103\n",
      "Iteration 51, loss = 0.08368035\n",
      "Iteration 52, loss = 0.08214337\n",
      "Iteration 53, loss = 0.08021352\n",
      "Iteration 54, loss = 0.07883052\n",
      "Iteration 55, loss = 0.07708723\n",
      "Iteration 56, loss = 0.07568596\n",
      "Iteration 57, loss = 0.07407658\n",
      "Iteration 58, loss = 0.07274752\n",
      "Iteration 59, loss = 0.07133400\n",
      "Iteration 60, loss = 0.06997895\n",
      "Iteration 61, loss = 0.06867841\n",
      "Iteration 62, loss = 0.06728909\n",
      "Iteration 63, loss = 0.06589181\n",
      "Iteration 64, loss = 0.06476973\n",
      "Iteration 65, loss = 0.06347381\n",
      "Iteration 66, loss = 0.06225133\n",
      "Iteration 67, loss = 0.06115724\n",
      "Iteration 68, loss = 0.05989618\n",
      "Iteration 69, loss = 0.05877622\n",
      "Iteration 70, loss = 0.05767562\n",
      "Iteration 71, loss = 0.05675861\n",
      "Iteration 72, loss = 0.05575049\n",
      "Iteration 73, loss = 0.05490463\n",
      "Iteration 74, loss = 0.05391516\n",
      "Iteration 75, loss = 0.05286820\n",
      "Iteration 76, loss = 0.05205156\n",
      "Iteration 77, loss = 0.05115658\n",
      "Iteration 78, loss = 0.05017572\n",
      "Iteration 79, loss = 0.04930538\n",
      "Iteration 80, loss = 0.04844824\n",
      "Iteration 81, loss = 0.04752359\n",
      "Iteration 82, loss = 0.04682556\n",
      "Iteration 83, loss = 0.04601996\n",
      "Iteration 84, loss = 0.04526795\n",
      "Iteration 85, loss = 0.04440590\n",
      "Iteration 86, loss = 0.04373942\n",
      "Iteration 87, loss = 0.04296631\n",
      "Iteration 88, loss = 0.04231934\n",
      "Iteration 89, loss = 0.04153487\n",
      "Iteration 90, loss = 0.04081089\n",
      "Iteration 91, loss = 0.04030347\n",
      "Iteration 92, loss = 0.03968425\n",
      "Iteration 93, loss = 0.03890797\n",
      "Iteration 94, loss = 0.03838064\n",
      "Iteration 95, loss = 0.03774435\n",
      "Iteration 96, loss = 0.03710734\n",
      "Iteration 97, loss = 0.03663582\n",
      "Iteration 98, loss = 0.03591303\n",
      "Iteration 99, loss = 0.03551716\n",
      "Iteration 100, loss = 0.03499548\n",
      "Iteration 101, loss = 0.03431069\n",
      "Iteration 102, loss = 0.03385873\n",
      "Iteration 103, loss = 0.03329566\n",
      "Iteration 104, loss = 0.03283286\n",
      "Iteration 105, loss = 0.03233481\n",
      "Iteration 106, loss = 0.03183158\n",
      "Iteration 107, loss = 0.03140510\n",
      "Iteration 108, loss = 0.03089874\n",
      "Iteration 109, loss = 0.03045602\n",
      "Iteration 110, loss = 0.03011668\n",
      "Iteration 111, loss = 0.02965797\n",
      "Iteration 112, loss = 0.02917933\n",
      "Iteration 113, loss = 0.02881984\n",
      "Iteration 114, loss = 0.02840218\n",
      "Iteration 115, loss = 0.02795769\n",
      "Iteration 116, loss = 0.02754974\n",
      "Iteration 117, loss = 0.02720034\n",
      "Iteration 118, loss = 0.02685777\n",
      "Iteration 119, loss = 0.02650733\n",
      "Iteration 120, loss = 0.02612002\n",
      "Iteration 121, loss = 0.02576236\n",
      "Iteration 122, loss = 0.02546283\n",
      "Iteration 123, loss = 0.02512236\n",
      "Iteration 124, loss = 0.02475796\n",
      "Iteration 125, loss = 0.02438227\n",
      "Iteration 126, loss = 0.02413091\n",
      "Iteration 127, loss = 0.02383542\n",
      "Iteration 128, loss = 0.02348628\n",
      "Iteration 129, loss = 0.02318484\n",
      "Iteration 130, loss = 0.02294144\n",
      "Iteration 131, loss = 0.02261589\n",
      "Iteration 132, loss = 0.02232945\n",
      "Iteration 133, loss = 0.02209842\n",
      "Iteration 134, loss = 0.02183112\n",
      "Iteration 135, loss = 0.02158840\n",
      "Iteration 136, loss = 0.02124251\n",
      "Iteration 137, loss = 0.02097222\n",
      "Iteration 138, loss = 0.02074266\n",
      "Iteration 139, loss = 0.02058908\n",
      "Iteration 140, loss = 0.02026222\n",
      "Iteration 141, loss = 0.02007073\n",
      "Iteration 142, loss = 0.01977625\n",
      "Iteration 143, loss = 0.01963012\n",
      "Iteration 144, loss = 0.01935626\n",
      "Iteration 145, loss = 0.01915247\n",
      "Iteration 146, loss = 0.01885532\n",
      "Iteration 147, loss = 0.01876042\n",
      "Iteration 148, loss = 0.01848924\n",
      "Iteration 149, loss = 0.01831312\n",
      "Iteration 150, loss = 0.01808816\n",
      "Iteration 151, loss = 0.01792458\n",
      "Iteration 152, loss = 0.01776881\n",
      "Iteration 153, loss = 0.01757564\n",
      "Iteration 154, loss = 0.01740727\n",
      "Iteration 155, loss = 0.01719789\n",
      "Iteration 156, loss = 0.01701423\n",
      "Iteration 157, loss = 0.01685508\n",
      "Iteration 158, loss = 0.01669157\n",
      "Iteration 159, loss = 0.01652053\n",
      "Iteration 160, loss = 0.01636801\n",
      "Iteration 161, loss = 0.01618093\n",
      "Iteration 162, loss = 0.01601375\n",
      "Iteration 163, loss = 0.01590069\n",
      "Iteration 164, loss = 0.01570373\n",
      "Iteration 165, loss = 0.01558683\n",
      "Iteration 166, loss = 0.01543477\n",
      "Iteration 167, loss = 0.01527140\n",
      "Iteration 168, loss = 0.01517762\n",
      "Iteration 169, loss = 0.01495702\n",
      "Iteration 170, loss = 0.01491984\n",
      "Iteration 171, loss = 0.01476017\n",
      "Iteration 172, loss = 0.01463006\n",
      "Iteration 173, loss = 0.01450403\n",
      "Iteration 174, loss = 0.01437323\n",
      "Iteration 175, loss = 0.01425912\n",
      "Iteration 176, loss = 0.01413834\n",
      "Iteration 177, loss = 0.01401773\n",
      "Iteration 178, loss = 0.01387077\n",
      "Iteration 179, loss = 0.01377410\n",
      "Iteration 180, loss = 0.01363528\n",
      "Iteration 181, loss = 0.01353592\n",
      "Iteration 182, loss = 0.01344803\n",
      "Iteration 183, loss = 0.01330676\n",
      "Iteration 184, loss = 0.01322661\n",
      "Iteration 185, loss = 0.01311450\n",
      "Iteration 186, loss = 0.01302860\n",
      "Iteration 187, loss = 0.01294915\n",
      "Iteration 188, loss = 0.01281761\n",
      "Iteration 189, loss = 0.01275880\n",
      "Iteration 190, loss = 0.01264814\n",
      "Iteration 191, loss = 0.01252777\n",
      "Iteration 192, loss = 0.01246036\n",
      "Iteration 193, loss = 0.01236837\n",
      "Iteration 194, loss = 0.01229282\n",
      "Iteration 195, loss = 0.01220451\n",
      "Iteration 196, loss = 0.01212128\n",
      "Iteration 197, loss = 0.01204349\n",
      "Iteration 198, loss = 0.01193571\n",
      "Iteration 199, loss = 0.01184982\n",
      "Iteration 200, loss = 0.01178806\n",
      "[CV] END activation=relu, alpha=0.001, batch_size=100, hidden_layer_sizes=(400, 100), solver=sgd; total time= 6.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.33169125\n",
      "Iteration 2, loss = 0.56377541\n",
      "Iteration 3, loss = 0.42270500\n",
      "Iteration 4, loss = 0.36345674\n",
      "Iteration 5, loss = 0.32871836\n",
      "Iteration 6, loss = 0.30479061\n",
      "Iteration 7, loss = 0.28653003\n",
      "Iteration 8, loss = 0.27122742\n",
      "Iteration 9, loss = 0.25889002\n",
      "Iteration 10, loss = 0.24771556\n",
      "Iteration 11, loss = 0.23749146\n",
      "Iteration 12, loss = 0.22844313\n",
      "Iteration 13, loss = 0.22005129\n",
      "Iteration 14, loss = 0.21240067\n",
      "Iteration 15, loss = 0.20526943\n",
      "Iteration 16, loss = 0.19866261\n",
      "Iteration 17, loss = 0.19229622\n",
      "Iteration 18, loss = 0.18631861\n",
      "Iteration 19, loss = 0.18073636\n",
      "Iteration 20, loss = 0.17540168\n",
      "Iteration 21, loss = 0.17045102\n",
      "Iteration 22, loss = 0.16550748\n",
      "Iteration 23, loss = 0.16096686\n",
      "Iteration 24, loss = 0.15656740\n",
      "Iteration 25, loss = 0.15256782\n",
      "Iteration 26, loss = 0.14835872\n",
      "Iteration 27, loss = 0.14436037\n",
      "Iteration 28, loss = 0.14090147\n",
      "Iteration 29, loss = 0.13716541\n",
      "Iteration 30, loss = 0.13376163\n",
      "Iteration 31, loss = 0.13049535\n",
      "Iteration 32, loss = 0.12742995\n",
      "Iteration 33, loss = 0.12457342\n",
      "Iteration 34, loss = 0.12142082\n",
      "Iteration 35, loss = 0.11849838\n",
      "Iteration 36, loss = 0.11589318\n",
      "Iteration 37, loss = 0.11334803\n",
      "Iteration 38, loss = 0.11075629\n",
      "Iteration 39, loss = 0.10837069\n",
      "Iteration 40, loss = 0.10605037\n",
      "Iteration 41, loss = 0.10363493\n",
      "Iteration 42, loss = 0.10135174\n",
      "Iteration 43, loss = 0.09931904\n",
      "Iteration 44, loss = 0.09743204\n",
      "Iteration 45, loss = 0.09500696\n",
      "Iteration 46, loss = 0.09313325\n",
      "Iteration 47, loss = 0.09129041\n",
      "Iteration 48, loss = 0.08933123\n",
      "Iteration 49, loss = 0.08753656\n",
      "Iteration 50, loss = 0.08570511\n",
      "Iteration 51, loss = 0.08402766\n",
      "Iteration 52, loss = 0.08236577\n",
      "Iteration 53, loss = 0.08089670\n",
      "Iteration 54, loss = 0.07934162\n",
      "Iteration 55, loss = 0.07752442\n",
      "Iteration 56, loss = 0.07611147\n",
      "Iteration 57, loss = 0.07462540\n",
      "Iteration 58, loss = 0.07325577\n",
      "Iteration 59, loss = 0.07190362\n",
      "Iteration 60, loss = 0.07040933\n",
      "Iteration 61, loss = 0.06916374\n",
      "Iteration 62, loss = 0.06795275\n",
      "Iteration 63, loss = 0.06650135\n",
      "Iteration 64, loss = 0.06536842\n",
      "Iteration 65, loss = 0.06427251\n",
      "Iteration 66, loss = 0.06297405\n",
      "Iteration 67, loss = 0.06186879\n",
      "Iteration 68, loss = 0.06066186\n",
      "Iteration 69, loss = 0.05988662\n",
      "Iteration 70, loss = 0.05868824\n",
      "Iteration 71, loss = 0.05771351\n",
      "Iteration 72, loss = 0.05665205\n",
      "Iteration 73, loss = 0.05566834\n",
      "Iteration 74, loss = 0.05476248\n",
      "Iteration 75, loss = 0.05373453\n",
      "Iteration 76, loss = 0.05289067\n",
      "Iteration 77, loss = 0.05193416\n",
      "Iteration 78, loss = 0.05117105\n",
      "Iteration 79, loss = 0.05019621\n",
      "Iteration 80, loss = 0.04936183\n",
      "Iteration 81, loss = 0.04851524\n",
      "Iteration 82, loss = 0.04781703\n",
      "Iteration 83, loss = 0.04687914\n",
      "Iteration 84, loss = 0.04614793\n",
      "Iteration 85, loss = 0.04542367\n",
      "Iteration 86, loss = 0.04485408\n",
      "Iteration 87, loss = 0.04396210\n",
      "Iteration 88, loss = 0.04316540\n",
      "Iteration 89, loss = 0.04255227\n",
      "Iteration 90, loss = 0.04197269\n",
      "Iteration 91, loss = 0.04117254\n",
      "Iteration 92, loss = 0.04039765\n",
      "Iteration 93, loss = 0.03989267\n",
      "Iteration 94, loss = 0.03921560\n",
      "Iteration 95, loss = 0.03856115\n",
      "Iteration 96, loss = 0.03810184\n",
      "Iteration 97, loss = 0.03750840\n",
      "Iteration 98, loss = 0.03691989\n",
      "Iteration 99, loss = 0.03629238\n",
      "Iteration 100, loss = 0.03579314\n",
      "Iteration 101, loss = 0.03526443\n",
      "Iteration 102, loss = 0.03468812\n",
      "Iteration 103, loss = 0.03419197\n",
      "Iteration 104, loss = 0.03355128\n",
      "Iteration 105, loss = 0.03327009\n",
      "Iteration 106, loss = 0.03269300\n",
      "Iteration 107, loss = 0.03209270\n",
      "Iteration 108, loss = 0.03184708\n",
      "Iteration 109, loss = 0.03127420\n",
      "Iteration 110, loss = 0.03078734\n",
      "Iteration 111, loss = 0.03037403\n",
      "Iteration 112, loss = 0.02991998\n",
      "Iteration 113, loss = 0.02958461\n",
      "Iteration 114, loss = 0.02911446\n",
      "Iteration 115, loss = 0.02882219\n",
      "Iteration 116, loss = 0.02830120\n",
      "Iteration 117, loss = 0.02796062\n",
      "Iteration 118, loss = 0.02765292\n",
      "Iteration 119, loss = 0.02717943\n",
      "Iteration 120, loss = 0.02684521\n",
      "Iteration 121, loss = 0.02648732\n",
      "Iteration 122, loss = 0.02615002\n",
      "Iteration 123, loss = 0.02576228\n",
      "Iteration 124, loss = 0.02552722\n",
      "Iteration 125, loss = 0.02514581\n",
      "Iteration 126, loss = 0.02476183\n",
      "Iteration 127, loss = 0.02455398\n",
      "Iteration 128, loss = 0.02411792\n",
      "Iteration 129, loss = 0.02388566\n",
      "Iteration 130, loss = 0.02358113\n",
      "Iteration 131, loss = 0.02329875\n",
      "Iteration 132, loss = 0.02296880\n",
      "Iteration 133, loss = 0.02275829\n",
      "Iteration 134, loss = 0.02242146\n",
      "Iteration 135, loss = 0.02222883\n",
      "Iteration 136, loss = 0.02194774\n",
      "Iteration 137, loss = 0.02168892\n",
      "Iteration 138, loss = 0.02136612\n",
      "Iteration 139, loss = 0.02115594\n",
      "Iteration 140, loss = 0.02092887\n",
      "Iteration 141, loss = 0.02071045\n",
      "Iteration 142, loss = 0.02044589\n",
      "Iteration 143, loss = 0.02023773\n",
      "Iteration 144, loss = 0.02000901\n",
      "Iteration 145, loss = 0.01979231\n",
      "Iteration 146, loss = 0.01955451\n",
      "Iteration 147, loss = 0.01934156\n",
      "Iteration 148, loss = 0.01911929\n",
      "Iteration 149, loss = 0.01892584\n",
      "Iteration 150, loss = 0.01871030\n",
      "Iteration 151, loss = 0.01857495\n",
      "Iteration 152, loss = 0.01835856\n",
      "Iteration 153, loss = 0.01812967\n",
      "Iteration 154, loss = 0.01794233\n",
      "Iteration 155, loss = 0.01775491\n",
      "Iteration 156, loss = 0.01759600\n",
      "Iteration 157, loss = 0.01744943\n",
      "Iteration 158, loss = 0.01725204\n",
      "Iteration 159, loss = 0.01708736\n",
      "Iteration 160, loss = 0.01693595\n",
      "Iteration 161, loss = 0.01675702\n",
      "Iteration 162, loss = 0.01661703\n",
      "Iteration 163, loss = 0.01647728\n",
      "Iteration 164, loss = 0.01633123\n",
      "Iteration 165, loss = 0.01615423\n",
      "Iteration 166, loss = 0.01596361\n",
      "Iteration 167, loss = 0.01584516\n",
      "Iteration 168, loss = 0.01570981\n",
      "Iteration 169, loss = 0.01556865\n",
      "Iteration 170, loss = 0.01538865\n",
      "Iteration 171, loss = 0.01529268\n",
      "Iteration 172, loss = 0.01517081\n",
      "Iteration 173, loss = 0.01501043\n",
      "Iteration 174, loss = 0.01490459\n",
      "Iteration 175, loss = 0.01476616\n",
      "Iteration 176, loss = 0.01464875\n",
      "Iteration 177, loss = 0.01453182\n",
      "Iteration 178, loss = 0.01442172\n",
      "Iteration 179, loss = 0.01430714\n",
      "Iteration 180, loss = 0.01418125\n",
      "Iteration 181, loss = 0.01408431\n",
      "Iteration 182, loss = 0.01397633\n",
      "Iteration 183, loss = 0.01383699\n",
      "Iteration 184, loss = 0.01375073\n",
      "Iteration 185, loss = 0.01363923\n",
      "Iteration 186, loss = 0.01354076\n",
      "Iteration 187, loss = 0.01340926\n",
      "Iteration 188, loss = 0.01332895\n",
      "Iteration 189, loss = 0.01323314\n",
      "Iteration 190, loss = 0.01315584\n",
      "Iteration 191, loss = 0.01306679\n",
      "Iteration 192, loss = 0.01294629\n",
      "Iteration 193, loss = 0.01286360\n",
      "Iteration 194, loss = 0.01277052\n",
      "Iteration 195, loss = 0.01267788\n",
      "Iteration 196, loss = 0.01261142\n",
      "Iteration 197, loss = 0.01250058\n",
      "Iteration 198, loss = 0.01242823\n",
      "Iteration 199, loss = 0.01233926\n",
      "Iteration 200, loss = 0.01224802\n",
      "[CV] END activation=relu, alpha=0.001, batch_size=100, hidden_layer_sizes=(400, 100), solver=sgd; total time= 6.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.39843328\n",
      "Iteration 2, loss = 0.16135449\n",
      "Iteration 3, loss = 0.11078089\n",
      "Iteration 4, loss = 0.08402816\n",
      "Iteration 5, loss = 0.06732735\n",
      "Iteration 6, loss = 0.05464418\n",
      "Iteration 7, loss = 0.04356243\n",
      "Iteration 8, loss = 0.03378586\n",
      "Iteration 9, loss = 0.02883442\n",
      "Iteration 10, loss = 0.02692841\n",
      "Iteration 11, loss = 0.02292335\n",
      "Iteration 12, loss = 0.02671473\n",
      "Iteration 13, loss = 0.01918842\n",
      "Iteration 14, loss = 0.01496177\n",
      "Iteration 15, loss = 0.01774784\n",
      "Iteration 16, loss = 0.02227367\n",
      "Iteration 17, loss = 0.01620945\n",
      "Iteration 18, loss = 0.02093755\n",
      "Iteration 19, loss = 0.01792922\n",
      "Iteration 20, loss = 0.01092680\n",
      "Iteration 21, loss = 0.00926218\n",
      "Iteration 22, loss = 0.01844361\n",
      "Iteration 23, loss = 0.01717964\n",
      "Iteration 24, loss = 0.01568280\n",
      "Iteration 25, loss = 0.01000228\n",
      "Iteration 26, loss = 0.00825844\n",
      "Iteration 27, loss = 0.02146900\n",
      "Iteration 28, loss = 0.01463792\n",
      "Iteration 29, loss = 0.00943024\n",
      "Iteration 30, loss = 0.00750635\n",
      "Iteration 31, loss = 0.01536041\n",
      "Iteration 32, loss = 0.01778418\n",
      "Iteration 33, loss = 0.01658934\n",
      "Iteration 34, loss = 0.01246482\n",
      "Iteration 35, loss = 0.01355775\n",
      "Iteration 36, loss = 0.00736540\n",
      "Iteration 37, loss = 0.00679580\n",
      "Iteration 38, loss = 0.00592980\n",
      "Iteration 39, loss = 0.00559873\n",
      "Iteration 40, loss = 0.00546475\n",
      "Iteration 41, loss = 0.00535796\n",
      "Iteration 42, loss = 0.00524587\n",
      "Iteration 43, loss = 0.00512409\n",
      "Iteration 44, loss = 0.00499275\n",
      "Iteration 45, loss = 0.00485053\n",
      "Iteration 46, loss = 0.00469784\n",
      "Iteration 47, loss = 0.00453627\n",
      "Iteration 48, loss = 0.00436935\n",
      "Iteration 49, loss = 0.02107016\n",
      "Iteration 50, loss = 0.04557429\n",
      "Iteration 51, loss = 0.01123734\n",
      "Iteration 52, loss = 0.01318889\n",
      "Iteration 53, loss = 0.01123038\n",
      "Iteration 54, loss = 0.01474918\n",
      "Iteration 55, loss = 0.00901206\n",
      "Iteration 56, loss = 0.00915436\n",
      "Iteration 57, loss = 0.00540131\n",
      "Iteration 58, loss = 0.00513538\n",
      "Iteration 59, loss = 0.00503297\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.001, batch_size=100, hidden_layer_sizes=(100, 100, 100), solver=adam; total time= 1.7min\n",
      "Iteration 1, loss = 0.40653160\n",
      "Iteration 2, loss = 0.15821606\n",
      "Iteration 3, loss = 0.11155987\n",
      "Iteration 4, loss = 0.08286357\n",
      "Iteration 5, loss = 0.06770806\n",
      "Iteration 6, loss = 0.05235187\n",
      "Iteration 7, loss = 0.04280801\n",
      "Iteration 8, loss = 0.03590388\n",
      "Iteration 9, loss = 0.03325929\n",
      "Iteration 10, loss = 0.02895668\n",
      "Iteration 11, loss = 0.02959224\n",
      "Iteration 12, loss = 0.01909612\n",
      "Iteration 13, loss = 0.02025847\n",
      "Iteration 14, loss = 0.02119741\n",
      "Iteration 15, loss = 0.01573960\n",
      "Iteration 16, loss = 0.02169169\n",
      "Iteration 17, loss = 0.01825563\n",
      "Iteration 18, loss = 0.01854213\n",
      "Iteration 19, loss = 0.01581506\n",
      "Iteration 20, loss = 0.01319165\n",
      "Iteration 21, loss = 0.02355469\n",
      "Iteration 22, loss = 0.01284504\n",
      "Iteration 23, loss = 0.00947299\n",
      "Iteration 24, loss = 0.01733497\n",
      "Iteration 25, loss = 0.01679251\n",
      "Iteration 26, loss = 0.01878983\n",
      "Iteration 27, loss = 0.01009352\n",
      "Iteration 28, loss = 0.01423631\n",
      "Iteration 29, loss = 0.01288006\n",
      "Iteration 30, loss = 0.01569902\n",
      "Iteration 31, loss = 0.01432778\n",
      "Iteration 32, loss = 0.01057963\n",
      "Iteration 33, loss = 0.00904491\n",
      "Iteration 34, loss = 0.02260531\n",
      "Iteration 35, loss = 0.01534336\n",
      "Iteration 36, loss = 0.00957304\n",
      "Iteration 37, loss = 0.00693767\n",
      "Iteration 38, loss = 0.00767188\n",
      "Iteration 39, loss = 0.01425729\n",
      "Iteration 40, loss = 0.02149205\n",
      "Iteration 41, loss = 0.01411329\n",
      "Iteration 42, loss = 0.00796311\n",
      "Iteration 43, loss = 0.00706080\n",
      "Iteration 44, loss = 0.00633565\n",
      "Iteration 45, loss = 0.00613491\n",
      "Iteration 46, loss = 0.00609151\n",
      "Iteration 47, loss = 0.00577032\n",
      "Iteration 48, loss = 0.00552773\n",
      "Iteration 49, loss = 0.00539265\n",
      "Iteration 50, loss = 0.00524938\n",
      "Iteration 51, loss = 0.00509550\n",
      "Iteration 52, loss = 0.00493104\n",
      "Iteration 53, loss = 0.00475699\n",
      "Iteration 54, loss = 0.00457056\n",
      "Iteration 55, loss = 0.04156781\n",
      "Iteration 56, loss = 0.02189511\n",
      "Iteration 57, loss = 0.01237577\n",
      "Iteration 58, loss = 0.00859567\n",
      "Iteration 59, loss = 0.00917783\n",
      "Iteration 60, loss = 0.01269139\n",
      "Iteration 61, loss = 0.01226561\n",
      "Iteration 62, loss = 0.01205797\n",
      "Iteration 63, loss = 0.00830982\n",
      "Iteration 64, loss = 0.01008413\n",
      "Iteration 65, loss = 0.01478178\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.001, batch_size=100, hidden_layer_sizes=(100, 100, 100), solver=adam; total time= 1.9min\n",
      "Iteration 1, loss = 0.40727117\n",
      "Iteration 2, loss = 0.15824260\n",
      "Iteration 3, loss = 0.11341411\n",
      "Iteration 4, loss = 0.08637358\n",
      "Iteration 5, loss = 0.06709528\n",
      "Iteration 6, loss = 0.05545340\n",
      "Iteration 7, loss = 0.04715680\n",
      "Iteration 8, loss = 0.03792925\n",
      "Iteration 9, loss = 0.02956400\n",
      "Iteration 10, loss = 0.02710879\n",
      "Iteration 11, loss = 0.02512816\n",
      "Iteration 12, loss = 0.02162971\n",
      "Iteration 13, loss = 0.02380210\n",
      "Iteration 14, loss = 0.01834001\n",
      "Iteration 15, loss = 0.01735818\n",
      "Iteration 16, loss = 0.01997519\n",
      "Iteration 17, loss = 0.01648840\n",
      "Iteration 18, loss = 0.02148940\n",
      "Iteration 19, loss = 0.01991157\n",
      "Iteration 20, loss = 0.01641813\n",
      "Iteration 21, loss = 0.01269758\n",
      "Iteration 22, loss = 0.01433417\n",
      "Iteration 23, loss = 0.01888839\n",
      "Iteration 24, loss = 0.01971597\n",
      "Iteration 25, loss = 0.01219751\n",
      "Iteration 26, loss = 0.00641083\n",
      "Iteration 27, loss = 0.00587611\n",
      "Iteration 28, loss = 0.02098707\n",
      "Iteration 29, loss = 0.02067990\n",
      "Iteration 30, loss = 0.01208196\n",
      "Iteration 31, loss = 0.01174629\n",
      "Iteration 32, loss = 0.00943440\n",
      "Iteration 33, loss = 0.02033404\n",
      "Iteration 34, loss = 0.01534383\n",
      "Iteration 35, loss = 0.01354089\n",
      "Iteration 36, loss = 0.01107224\n",
      "Iteration 37, loss = 0.00732823\n",
      "Iteration 38, loss = 0.01564398\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.001, batch_size=100, hidden_layer_sizes=(100, 100, 100), solver=adam; total time=  48.6s\n",
      "Iteration 1, loss = 0.40806072\n",
      "Iteration 2, loss = 0.15477372\n",
      "Iteration 3, loss = 0.10667168\n",
      "Iteration 4, loss = 0.07827232\n",
      "Iteration 5, loss = 0.06563598\n",
      "Iteration 6, loss = 0.04897810\n",
      "Iteration 7, loss = 0.04070016\n",
      "Iteration 8, loss = 0.03396119\n",
      "Iteration 9, loss = 0.03169531\n",
      "Iteration 10, loss = 0.02879907\n",
      "Iteration 11, loss = 0.02266593\n",
      "Iteration 12, loss = 0.02108575\n",
      "Iteration 13, loss = 0.01679715\n",
      "Iteration 14, loss = 0.01827539\n",
      "Iteration 15, loss = 0.02450222\n",
      "Iteration 16, loss = 0.01920887\n",
      "Iteration 17, loss = 0.01715658\n",
      "Iteration 18, loss = 0.01371534\n",
      "Iteration 19, loss = 0.01729007\n",
      "Iteration 20, loss = 0.01713439\n",
      "Iteration 21, loss = 0.01382993\n",
      "Iteration 22, loss = 0.01324396\n",
      "Iteration 23, loss = 0.01596541\n",
      "Iteration 24, loss = 0.01208158\n",
      "Iteration 25, loss = 0.01950281\n",
      "Iteration 26, loss = 0.01799211\n",
      "Iteration 27, loss = 0.01158931\n",
      "Iteration 28, loss = 0.01004588\n",
      "Iteration 29, loss = 0.01667745\n",
      "Iteration 30, loss = 0.00936574\n",
      "Iteration 31, loss = 0.01290603\n",
      "Iteration 32, loss = 0.01716232\n",
      "Iteration 33, loss = 0.00960079\n",
      "Iteration 34, loss = 0.01820622\n",
      "Iteration 35, loss = 0.01435579\n",
      "Iteration 36, loss = 0.00975548\n",
      "Iteration 37, loss = 0.00718117\n",
      "Iteration 38, loss = 0.01969002\n",
      "Iteration 39, loss = 0.01230138\n",
      "Iteration 40, loss = 0.00927997\n",
      "Iteration 41, loss = 0.01143543\n",
      "Iteration 42, loss = 0.01684479\n",
      "Iteration 43, loss = 0.01016596\n",
      "Iteration 44, loss = 0.00747683\n",
      "Iteration 45, loss = 0.01731308\n",
      "Iteration 46, loss = 0.01675347\n",
      "Iteration 47, loss = 0.01238964\n",
      "Iteration 48, loss = 0.00955767\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.001, batch_size=100, hidden_layer_sizes=(100, 100, 100), solver=adam; total time= 1.2min\n",
      "Iteration 1, loss = 0.41064277\n",
      "Iteration 2, loss = 0.16468583\n",
      "Iteration 3, loss = 0.11420759\n",
      "Iteration 4, loss = 0.08871267\n",
      "Iteration 5, loss = 0.06806775\n",
      "Iteration 6, loss = 0.05661410\n",
      "Iteration 7, loss = 0.04575320\n",
      "Iteration 8, loss = 0.03855131\n",
      "Iteration 9, loss = 0.02876320\n",
      "Iteration 10, loss = 0.02837932\n",
      "Iteration 11, loss = 0.02621583\n",
      "Iteration 12, loss = 0.02582878\n",
      "Iteration 13, loss = 0.02273327\n",
      "Iteration 14, loss = 0.01699330\n",
      "Iteration 15, loss = 0.01780166\n",
      "Iteration 16, loss = 0.01725087\n",
      "Iteration 17, loss = 0.02320586\n",
      "Iteration 18, loss = 0.01503683\n",
      "Iteration 19, loss = 0.01832778\n",
      "Iteration 20, loss = 0.01667129\n",
      "Iteration 21, loss = 0.01407821\n",
      "Iteration 22, loss = 0.01405821\n",
      "Iteration 23, loss = 0.01950977\n",
      "Iteration 24, loss = 0.01382953\n",
      "Iteration 25, loss = 0.01867808\n",
      "Iteration 26, loss = 0.01967168\n",
      "Iteration 27, loss = 0.01241542\n",
      "Iteration 28, loss = 0.00926661\n",
      "Iteration 29, loss = 0.01069873\n",
      "Iteration 30, loss = 0.02001562\n",
      "Iteration 31, loss = 0.01471969\n",
      "Iteration 32, loss = 0.01204424\n",
      "Iteration 33, loss = 0.01386807\n",
      "Iteration 34, loss = 0.00658065\n",
      "Iteration 35, loss = 0.01428248\n",
      "Iteration 36, loss = 0.01867350\n",
      "Iteration 37, loss = 0.01140739\n",
      "Iteration 38, loss = 0.00799011\n",
      "Iteration 39, loss = 0.01100233\n",
      "Iteration 40, loss = 0.02222095\n",
      "Iteration 41, loss = 0.01133317\n",
      "Iteration 42, loss = 0.01493129\n",
      "Iteration 43, loss = 0.01183986\n",
      "Iteration 44, loss = 0.01346009\n",
      "Iteration 45, loss = 0.01012875\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.001, batch_size=100, hidden_layer_sizes=(100, 100, 100), solver=adam; total time= 1.1min\n",
      "Iteration 1, loss = 1.82164091\n",
      "Iteration 2, loss = 0.72130275\n",
      "Iteration 3, loss = 0.45859774\n",
      "Iteration 4, loss = 0.37270534\n",
      "Iteration 5, loss = 0.32776195\n",
      "Iteration 6, loss = 0.30002455\n",
      "Iteration 7, loss = 0.27803892\n",
      "Iteration 8, loss = 0.26181588\n",
      "Iteration 9, loss = 0.24729919\n",
      "Iteration 10, loss = 0.23581432\n",
      "Iteration 11, loss = 0.22475730\n",
      "Iteration 12, loss = 0.21495056\n",
      "Iteration 13, loss = 0.20644342\n",
      "Iteration 14, loss = 0.19828333\n",
      "Iteration 15, loss = 0.19094769\n",
      "Iteration 16, loss = 0.18472703\n",
      "Iteration 17, loss = 0.17836367\n",
      "Iteration 18, loss = 0.17243382\n",
      "Iteration 19, loss = 0.16690528\n",
      "Iteration 20, loss = 0.16212918\n",
      "Iteration 21, loss = 0.15708655\n",
      "Iteration 22, loss = 0.15285912\n",
      "Iteration 23, loss = 0.14789176\n",
      "Iteration 24, loss = 0.14424350\n",
      "Iteration 25, loss = 0.14035806\n",
      "Iteration 26, loss = 0.13648980\n",
      "Iteration 27, loss = 0.13304052\n",
      "Iteration 28, loss = 0.12937824\n",
      "Iteration 29, loss = 0.12644258\n",
      "Iteration 30, loss = 0.12301436\n",
      "Iteration 31, loss = 0.11986713\n",
      "Iteration 32, loss = 0.11668509\n",
      "Iteration 33, loss = 0.11399089\n",
      "Iteration 34, loss = 0.11157543\n",
      "Iteration 35, loss = 0.10878904\n",
      "Iteration 36, loss = 0.10556882\n",
      "Iteration 37, loss = 0.10363952\n",
      "Iteration 38, loss = 0.10128178\n",
      "Iteration 39, loss = 0.09858183\n",
      "Iteration 40, loss = 0.09706087\n",
      "Iteration 41, loss = 0.09403441\n",
      "Iteration 42, loss = 0.09223507\n",
      "Iteration 43, loss = 0.08959571\n",
      "Iteration 44, loss = 0.08795172\n",
      "Iteration 45, loss = 0.08585643\n",
      "Iteration 46, loss = 0.08406838\n",
      "Iteration 47, loss = 0.08214244\n",
      "Iteration 48, loss = 0.08032440\n",
      "Iteration 49, loss = 0.07853970\n",
      "Iteration 50, loss = 0.07727968\n",
      "Iteration 51, loss = 0.07524493\n",
      "Iteration 52, loss = 0.07391767\n",
      "Iteration 53, loss = 0.07216333\n",
      "Iteration 54, loss = 0.07053788\n",
      "Iteration 55, loss = 0.06906318\n",
      "Iteration 56, loss = 0.06750007\n",
      "Iteration 57, loss = 0.06623829\n",
      "Iteration 58, loss = 0.06480497\n",
      "Iteration 59, loss = 0.06326820\n",
      "Iteration 60, loss = 0.06205177\n",
      "Iteration 61, loss = 0.06041170\n",
      "Iteration 62, loss = 0.05954817\n",
      "Iteration 63, loss = 0.05813384\n",
      "Iteration 64, loss = 0.05690270\n",
      "Iteration 65, loss = 0.05581125\n",
      "Iteration 66, loss = 0.05440904\n",
      "Iteration 67, loss = 0.05356077\n",
      "Iteration 68, loss = 0.05196153\n",
      "Iteration 69, loss = 0.05128676\n",
      "Iteration 70, loss = 0.04991859\n",
      "Iteration 71, loss = 0.04901881\n",
      "Iteration 72, loss = 0.04821288\n",
      "Iteration 73, loss = 0.04703024\n",
      "Iteration 74, loss = 0.04606653\n",
      "Iteration 75, loss = 0.04524062\n",
      "Iteration 76, loss = 0.04455297\n",
      "Iteration 77, loss = 0.04358216\n",
      "Iteration 78, loss = 0.04273050\n",
      "Iteration 79, loss = 0.04210133\n",
      "Iteration 80, loss = 0.04111863\n",
      "Iteration 81, loss = 0.04022620\n",
      "Iteration 82, loss = 0.03938785\n",
      "Iteration 83, loss = 0.03861565\n",
      "Iteration 84, loss = 0.03777202\n",
      "Iteration 85, loss = 0.03712205\n",
      "Iteration 86, loss = 0.03639365\n",
      "Iteration 87, loss = 0.03542524\n",
      "Iteration 88, loss = 0.03489107\n",
      "Iteration 89, loss = 0.03425417\n",
      "Iteration 90, loss = 0.03367906\n",
      "Iteration 91, loss = 0.03321961\n",
      "Iteration 92, loss = 0.03223623\n",
      "Iteration 93, loss = 0.03156490\n",
      "Iteration 94, loss = 0.03113423\n",
      "Iteration 95, loss = 0.03036641\n",
      "Iteration 96, loss = 0.02987379\n",
      "Iteration 97, loss = 0.02926257\n",
      "Iteration 98, loss = 0.02868566\n",
      "Iteration 99, loss = 0.02837817\n",
      "Iteration 100, loss = 0.02772946\n",
      "Iteration 101, loss = 0.02707667\n",
      "Iteration 102, loss = 0.02662853\n",
      "Iteration 103, loss = 0.02601742\n",
      "Iteration 104, loss = 0.02563345\n",
      "Iteration 105, loss = 0.02521154\n",
      "Iteration 106, loss = 0.02472647\n",
      "Iteration 107, loss = 0.02424223\n",
      "Iteration 108, loss = 0.02381789\n",
      "Iteration 109, loss = 0.02328008\n",
      "Iteration 110, loss = 0.02281554\n",
      "Iteration 111, loss = 0.02241045\n",
      "Iteration 112, loss = 0.02199581\n",
      "Iteration 113, loss = 0.02159265\n",
      "Iteration 114, loss = 0.02126484\n",
      "Iteration 115, loss = 0.02067001\n",
      "Iteration 116, loss = 0.02036798\n",
      "Iteration 117, loss = 0.01991151\n",
      "Iteration 118, loss = 0.01964739\n",
      "Iteration 119, loss = 0.01943777\n",
      "Iteration 120, loss = 0.01892256\n",
      "Iteration 121, loss = 0.01871232\n",
      "Iteration 122, loss = 0.01836632\n",
      "Iteration 123, loss = 0.01791801\n",
      "Iteration 124, loss = 0.01769490\n",
      "Iteration 125, loss = 0.01736530\n",
      "Iteration 126, loss = 0.01701527\n",
      "Iteration 127, loss = 0.01675369\n",
      "Iteration 128, loss = 0.01638871\n",
      "Iteration 129, loss = 0.01616339\n",
      "Iteration 130, loss = 0.01580897\n",
      "Iteration 131, loss = 0.01560701\n",
      "Iteration 132, loss = 0.01528648\n",
      "Iteration 133, loss = 0.01506987\n",
      "Iteration 134, loss = 0.01483888\n",
      "Iteration 135, loss = 0.01453694\n",
      "Iteration 136, loss = 0.01437908\n",
      "Iteration 137, loss = 0.01412122\n",
      "Iteration 138, loss = 0.01388466\n",
      "Iteration 139, loss = 0.01365894\n",
      "Iteration 140, loss = 0.01339777\n",
      "Iteration 141, loss = 0.01324279\n",
      "Iteration 142, loss = 0.01299394\n",
      "Iteration 143, loss = 0.01279784\n",
      "Iteration 144, loss = 0.01257455\n",
      "Iteration 145, loss = 0.01246616\n",
      "Iteration 146, loss = 0.01226890\n",
      "Iteration 147, loss = 0.01201159\n",
      "Iteration 148, loss = 0.01186479\n",
      "Iteration 149, loss = 0.01169096\n",
      "Iteration 150, loss = 0.01150599\n",
      "Iteration 151, loss = 0.01130681\n",
      "Iteration 152, loss = 0.01117188\n",
      "Iteration 153, loss = 0.01094054\n",
      "Iteration 154, loss = 0.01085323\n",
      "Iteration 155, loss = 0.01072266\n",
      "Iteration 156, loss = 0.01054258\n",
      "Iteration 157, loss = 0.01037744\n",
      "Iteration 158, loss = 0.01022615\n",
      "Iteration 159, loss = 0.01007779\n",
      "Iteration 160, loss = 0.00996022\n",
      "Iteration 161, loss = 0.00984896\n",
      "Iteration 162, loss = 0.00973556\n",
      "Iteration 163, loss = 0.00961024\n",
      "Iteration 164, loss = 0.00948834\n",
      "Iteration 165, loss = 0.00935043\n",
      "Iteration 166, loss = 0.00928037\n",
      "Iteration 167, loss = 0.00916204\n",
      "Iteration 168, loss = 0.00902735\n",
      "Iteration 169, loss = 0.00896403\n",
      "Iteration 170, loss = 0.00886534\n",
      "Iteration 171, loss = 0.00867324\n",
      "Iteration 172, loss = 0.00862881\n",
      "Iteration 173, loss = 0.00853029\n",
      "Iteration 174, loss = 0.00839216\n",
      "Iteration 175, loss = 0.00836158\n",
      "Iteration 176, loss = 0.00824437\n",
      "Iteration 177, loss = 0.00821270\n",
      "Iteration 178, loss = 0.00807915\n",
      "Iteration 179, loss = 0.00805105\n",
      "Iteration 180, loss = 0.00792489\n",
      "Iteration 181, loss = 0.00785423\n",
      "Iteration 182, loss = 0.00776714\n",
      "Iteration 183, loss = 0.00774078\n",
      "Iteration 184, loss = 0.00764920\n",
      "Iteration 185, loss = 0.00758555\n",
      "Iteration 186, loss = 0.00754064\n",
      "Iteration 187, loss = 0.00747654\n",
      "Iteration 188, loss = 0.00739806\n",
      "Iteration 189, loss = 0.00731090\n",
      "Iteration 190, loss = 0.00723653\n",
      "Iteration 191, loss = 0.00719505\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.001, batch_size=100, hidden_layer_sizes=(100, 100, 100), solver=sgd; total time= 2.7min\n",
      "Iteration 1, loss = 1.77577788\n",
      "Iteration 2, loss = 0.67149757\n",
      "Iteration 3, loss = 0.44491796\n",
      "Iteration 4, loss = 0.37024087\n",
      "Iteration 5, loss = 0.33097337\n",
      "Iteration 6, loss = 0.30385687\n",
      "Iteration 7, loss = 0.28350804\n",
      "Iteration 8, loss = 0.26727091\n",
      "Iteration 9, loss = 0.25296228\n",
      "Iteration 10, loss = 0.24121143\n",
      "Iteration 11, loss = 0.22933378\n",
      "Iteration 12, loss = 0.21977098\n",
      "Iteration 13, loss = 0.21038603\n",
      "Iteration 14, loss = 0.20239487\n",
      "Iteration 15, loss = 0.19424092\n",
      "Iteration 16, loss = 0.18763960\n",
      "Iteration 17, loss = 0.18114351\n",
      "Iteration 18, loss = 0.17466570\n",
      "Iteration 19, loss = 0.16876151\n",
      "Iteration 20, loss = 0.16350593\n",
      "Iteration 21, loss = 0.15832042\n",
      "Iteration 22, loss = 0.15350078\n",
      "Iteration 23, loss = 0.14829290\n",
      "Iteration 24, loss = 0.14432224\n",
      "Iteration 25, loss = 0.13999975\n",
      "Iteration 26, loss = 0.13634411\n",
      "Iteration 27, loss = 0.13183701\n",
      "Iteration 28, loss = 0.12822858\n",
      "Iteration 29, loss = 0.12479537\n",
      "Iteration 30, loss = 0.12190276\n",
      "Iteration 31, loss = 0.11849618\n",
      "Iteration 32, loss = 0.11562479\n",
      "Iteration 33, loss = 0.11243958\n",
      "Iteration 34, loss = 0.10950892\n",
      "Iteration 35, loss = 0.10682377\n",
      "Iteration 36, loss = 0.10396909\n",
      "Iteration 37, loss = 0.10144634\n",
      "Iteration 38, loss = 0.09896291\n",
      "Iteration 39, loss = 0.09691616\n",
      "Iteration 40, loss = 0.09485853\n",
      "Iteration 41, loss = 0.09205293\n",
      "Iteration 42, loss = 0.09011368\n",
      "Iteration 43, loss = 0.08812115\n",
      "Iteration 44, loss = 0.08621434\n",
      "Iteration 45, loss = 0.08386772\n",
      "Iteration 46, loss = 0.08226199\n",
      "Iteration 47, loss = 0.08016604\n",
      "Iteration 48, loss = 0.07818098\n",
      "Iteration 49, loss = 0.07644249\n",
      "Iteration 50, loss = 0.07516925\n",
      "Iteration 51, loss = 0.07320241\n",
      "Iteration 52, loss = 0.07130513\n",
      "Iteration 53, loss = 0.06939761\n",
      "Iteration 54, loss = 0.06815238\n",
      "Iteration 55, loss = 0.06694462\n",
      "Iteration 56, loss = 0.06527853\n",
      "Iteration 57, loss = 0.06372463\n",
      "Iteration 58, loss = 0.06233085\n",
      "Iteration 59, loss = 0.06079700\n",
      "Iteration 60, loss = 0.05963323\n",
      "Iteration 61, loss = 0.05839001\n",
      "Iteration 62, loss = 0.05716273\n",
      "Iteration 63, loss = 0.05579049\n",
      "Iteration 64, loss = 0.05438521\n",
      "Iteration 65, loss = 0.05331847\n",
      "Iteration 66, loss = 0.05210485\n",
      "Iteration 67, loss = 0.05096802\n",
      "Iteration 68, loss = 0.04985032\n",
      "Iteration 69, loss = 0.04880215\n",
      "Iteration 70, loss = 0.04756859\n",
      "Iteration 71, loss = 0.04660396\n",
      "Iteration 72, loss = 0.04552700\n",
      "Iteration 73, loss = 0.04471339\n",
      "Iteration 74, loss = 0.04344789\n",
      "Iteration 75, loss = 0.04278972\n",
      "Iteration 76, loss = 0.04205731\n",
      "Iteration 77, loss = 0.04098531\n",
      "Iteration 78, loss = 0.04005746\n",
      "Iteration 79, loss = 0.03909482\n",
      "Iteration 80, loss = 0.03812017\n",
      "Iteration 81, loss = 0.03756790\n",
      "Iteration 82, loss = 0.03668857\n",
      "Iteration 83, loss = 0.03578575\n",
      "Iteration 84, loss = 0.03516954\n",
      "Iteration 85, loss = 0.03432550\n",
      "Iteration 86, loss = 0.03364915\n",
      "Iteration 87, loss = 0.03283597\n",
      "Iteration 88, loss = 0.03227493\n",
      "Iteration 89, loss = 0.03150056\n",
      "Iteration 90, loss = 0.03101348\n",
      "Iteration 91, loss = 0.03030559\n",
      "Iteration 92, loss = 0.02977118\n",
      "Iteration 93, loss = 0.02913359\n",
      "Iteration 94, loss = 0.02845635\n",
      "Iteration 95, loss = 0.02781661\n",
      "Iteration 96, loss = 0.02734091\n",
      "Iteration 97, loss = 0.02670126\n",
      "Iteration 98, loss = 0.02619139\n",
      "Iteration 99, loss = 0.02564873\n",
      "Iteration 100, loss = 0.02495893\n",
      "Iteration 101, loss = 0.02457391\n",
      "Iteration 102, loss = 0.02401521\n",
      "Iteration 103, loss = 0.02366155\n",
      "Iteration 104, loss = 0.02311079\n",
      "Iteration 105, loss = 0.02270562\n",
      "Iteration 106, loss = 0.02233204\n",
      "Iteration 107, loss = 0.02164056\n",
      "Iteration 108, loss = 0.02133437\n",
      "Iteration 109, loss = 0.02091727\n",
      "Iteration 110, loss = 0.02039445\n",
      "Iteration 111, loss = 0.02017695\n",
      "Iteration 112, loss = 0.01971980\n",
      "Iteration 113, loss = 0.01919357\n",
      "Iteration 114, loss = 0.01909512\n",
      "Iteration 115, loss = 0.01858501\n",
      "Iteration 116, loss = 0.01818838\n",
      "Iteration 117, loss = 0.01801377\n",
      "Iteration 118, loss = 0.01747847\n",
      "Iteration 119, loss = 0.01705537\n",
      "Iteration 120, loss = 0.01692955\n",
      "Iteration 121, loss = 0.01652770\n",
      "Iteration 122, loss = 0.01624354\n",
      "Iteration 123, loss = 0.01581005\n",
      "Iteration 124, loss = 0.01568367\n",
      "Iteration 125, loss = 0.01539914\n",
      "Iteration 126, loss = 0.01504301\n",
      "Iteration 127, loss = 0.01479398\n",
      "Iteration 128, loss = 0.01457835\n",
      "Iteration 129, loss = 0.01439203\n",
      "Iteration 130, loss = 0.01400661\n",
      "Iteration 131, loss = 0.01371146\n",
      "Iteration 132, loss = 0.01357269\n",
      "Iteration 133, loss = 0.01329169\n",
      "Iteration 134, loss = 0.01308586\n",
      "Iteration 135, loss = 0.01283243\n",
      "Iteration 136, loss = 0.01259585\n",
      "Iteration 137, loss = 0.01237774\n",
      "Iteration 138, loss = 0.01225764\n",
      "Iteration 139, loss = 0.01205071\n",
      "Iteration 140, loss = 0.01194885\n",
      "Iteration 141, loss = 0.01158235\n",
      "Iteration 142, loss = 0.01143726\n",
      "Iteration 143, loss = 0.01122970\n",
      "Iteration 144, loss = 0.01112776\n",
      "Iteration 145, loss = 0.01086155\n",
      "Iteration 146, loss = 0.01068962\n",
      "Iteration 147, loss = 0.01055599\n",
      "Iteration 148, loss = 0.01040688\n",
      "Iteration 149, loss = 0.01023213\n",
      "Iteration 150, loss = 0.01006216\n",
      "Iteration 151, loss = 0.01005713\n",
      "Iteration 152, loss = 0.00982495\n",
      "Iteration 153, loss = 0.00967262\n",
      "Iteration 154, loss = 0.00955153\n",
      "Iteration 155, loss = 0.00950549\n",
      "Iteration 156, loss = 0.00936081\n",
      "Iteration 157, loss = 0.00927886\n",
      "Iteration 158, loss = 0.00912770\n",
      "Iteration 159, loss = 0.00900090\n",
      "Iteration 160, loss = 0.00885885\n",
      "Iteration 161, loss = 0.00876088\n",
      "Iteration 162, loss = 0.00866421\n",
      "Iteration 163, loss = 0.00853074\n",
      "Iteration 164, loss = 0.00848701\n",
      "Iteration 165, loss = 0.00835455\n",
      "Iteration 166, loss = 0.00829656\n",
      "Iteration 167, loss = 0.00818782\n",
      "Iteration 168, loss = 0.00807996\n",
      "Iteration 169, loss = 0.00796904\n",
      "Iteration 170, loss = 0.00790718\n",
      "Iteration 171, loss = 0.00775273\n",
      "Iteration 172, loss = 0.00770402\n",
      "Iteration 173, loss = 0.00767236\n",
      "Iteration 174, loss = 0.00756136\n",
      "Iteration 175, loss = 0.00749560\n",
      "Iteration 176, loss = 0.00744195\n",
      "Iteration 177, loss = 0.00735311\n",
      "Iteration 178, loss = 0.00728809\n",
      "Iteration 179, loss = 0.00724108\n",
      "Iteration 180, loss = 0.00717177\n",
      "Iteration 181, loss = 0.00708113\n",
      "Iteration 182, loss = 0.00702062\n",
      "Iteration 183, loss = 0.00697847\n",
      "Iteration 184, loss = 0.00693433\n",
      "Iteration 185, loss = 0.00683726\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.001, batch_size=100, hidden_layer_sizes=(100, 100, 100), solver=sgd; total time= 2.6min\n",
      "Iteration 1, loss = 1.72422719\n",
      "Iteration 2, loss = 0.68829909\n",
      "Iteration 3, loss = 0.44827903\n",
      "Iteration 4, loss = 0.37146106\n",
      "Iteration 5, loss = 0.33276633\n",
      "Iteration 6, loss = 0.30593280\n",
      "Iteration 7, loss = 0.28569616\n",
      "Iteration 8, loss = 0.26887031\n",
      "Iteration 9, loss = 0.25507931\n",
      "Iteration 10, loss = 0.24233301\n",
      "Iteration 11, loss = 0.23100245\n",
      "Iteration 12, loss = 0.22062216\n",
      "Iteration 13, loss = 0.21107662\n",
      "Iteration 14, loss = 0.20287149\n",
      "Iteration 15, loss = 0.19407294\n",
      "Iteration 16, loss = 0.18707973\n",
      "Iteration 17, loss = 0.18001866\n",
      "Iteration 18, loss = 0.17338854\n",
      "Iteration 19, loss = 0.16724511\n",
      "Iteration 20, loss = 0.16162221\n",
      "Iteration 21, loss = 0.15632640\n",
      "Iteration 22, loss = 0.15129184\n",
      "Iteration 23, loss = 0.14636748\n",
      "Iteration 24, loss = 0.14172893\n",
      "Iteration 25, loss = 0.13784235\n",
      "Iteration 26, loss = 0.13411698\n",
      "Iteration 27, loss = 0.13018633\n",
      "Iteration 28, loss = 0.12662244\n",
      "Iteration 29, loss = 0.12289203\n",
      "Iteration 30, loss = 0.12002679\n",
      "Iteration 31, loss = 0.11647368\n",
      "Iteration 32, loss = 0.11335013\n",
      "Iteration 33, loss = 0.11068675\n",
      "Iteration 34, loss = 0.10771631\n",
      "Iteration 35, loss = 0.10500968\n",
      "Iteration 36, loss = 0.10222787\n",
      "Iteration 37, loss = 0.09975645\n",
      "Iteration 38, loss = 0.09742991\n",
      "Iteration 39, loss = 0.09491479\n",
      "Iteration 40, loss = 0.09274067\n",
      "Iteration 41, loss = 0.09098657\n",
      "Iteration 42, loss = 0.08851053\n",
      "Iteration 43, loss = 0.08670053\n",
      "Iteration 44, loss = 0.08463080\n",
      "Iteration 45, loss = 0.08265413\n",
      "Iteration 46, loss = 0.08018651\n",
      "Iteration 47, loss = 0.07902376\n",
      "Iteration 48, loss = 0.07691179\n",
      "Iteration 49, loss = 0.07496068\n",
      "Iteration 50, loss = 0.07349209\n",
      "Iteration 51, loss = 0.07209873\n",
      "Iteration 52, loss = 0.07038370\n",
      "Iteration 53, loss = 0.06869286\n",
      "Iteration 54, loss = 0.06719826\n",
      "Iteration 55, loss = 0.06563639\n",
      "Iteration 56, loss = 0.06394739\n",
      "Iteration 57, loss = 0.06273294\n",
      "Iteration 58, loss = 0.06171482\n",
      "Iteration 59, loss = 0.05998914\n",
      "Iteration 60, loss = 0.05872025\n",
      "Iteration 61, loss = 0.05737606\n",
      "Iteration 62, loss = 0.05637825\n",
      "Iteration 63, loss = 0.05497527\n",
      "Iteration 64, loss = 0.05382822\n",
      "Iteration 65, loss = 0.05250327\n",
      "Iteration 66, loss = 0.05137494\n",
      "Iteration 67, loss = 0.05026472\n",
      "Iteration 68, loss = 0.04905667\n",
      "Iteration 69, loss = 0.04801064\n",
      "Iteration 70, loss = 0.04694801\n",
      "Iteration 71, loss = 0.04599534\n",
      "Iteration 72, loss = 0.04489425\n",
      "Iteration 73, loss = 0.04396086\n",
      "Iteration 74, loss = 0.04331569\n",
      "Iteration 75, loss = 0.04227244\n",
      "Iteration 76, loss = 0.04130697\n",
      "Iteration 77, loss = 0.04022648\n",
      "Iteration 78, loss = 0.03952183\n",
      "Iteration 79, loss = 0.03868052\n",
      "Iteration 80, loss = 0.03767824\n",
      "Iteration 81, loss = 0.03689077\n",
      "Iteration 82, loss = 0.03618851\n",
      "Iteration 83, loss = 0.03554234\n",
      "Iteration 84, loss = 0.03470779\n",
      "Iteration 85, loss = 0.03390161\n",
      "Iteration 86, loss = 0.03299243\n",
      "Iteration 87, loss = 0.03232626\n",
      "Iteration 88, loss = 0.03175216\n",
      "Iteration 89, loss = 0.03107402\n",
      "Iteration 90, loss = 0.03028776\n",
      "Iteration 91, loss = 0.02978653\n",
      "Iteration 92, loss = 0.02916552\n",
      "Iteration 93, loss = 0.02857290\n",
      "Iteration 94, loss = 0.02806268\n",
      "Iteration 95, loss = 0.02750582\n",
      "Iteration 96, loss = 0.02684999\n",
      "Iteration 97, loss = 0.02613569\n",
      "Iteration 98, loss = 0.02569564\n",
      "Iteration 99, loss = 0.02523009\n",
      "Iteration 100, loss = 0.02460782\n",
      "Iteration 101, loss = 0.02414293\n",
      "Iteration 102, loss = 0.02369539\n",
      "Iteration 103, loss = 0.02317723\n",
      "Iteration 104, loss = 0.02263288\n",
      "Iteration 105, loss = 0.02222536\n",
      "Iteration 106, loss = 0.02186993\n",
      "Iteration 107, loss = 0.02140925\n",
      "Iteration 108, loss = 0.02094275\n",
      "Iteration 109, loss = 0.02057888\n",
      "Iteration 110, loss = 0.02010995\n",
      "Iteration 111, loss = 0.01973758\n",
      "Iteration 112, loss = 0.01942167\n",
      "Iteration 113, loss = 0.01912701\n",
      "Iteration 114, loss = 0.01875355\n",
      "Iteration 115, loss = 0.01848645\n",
      "Iteration 116, loss = 0.01796873\n",
      "Iteration 117, loss = 0.01782527\n",
      "Iteration 118, loss = 0.01735480\n",
      "Iteration 119, loss = 0.01703274\n",
      "Iteration 120, loss = 0.01676250\n",
      "Iteration 121, loss = 0.01641281\n",
      "Iteration 122, loss = 0.01603685\n",
      "Iteration 123, loss = 0.01586415\n",
      "Iteration 124, loss = 0.01557745\n",
      "Iteration 125, loss = 0.01532028\n",
      "Iteration 126, loss = 0.01507792\n",
      "Iteration 127, loss = 0.01494230\n",
      "Iteration 128, loss = 0.01463925\n",
      "Iteration 129, loss = 0.01429809\n",
      "Iteration 130, loss = 0.01417596\n",
      "Iteration 131, loss = 0.01394095\n",
      "Iteration 132, loss = 0.01370195\n",
      "Iteration 133, loss = 0.01348774\n",
      "Iteration 134, loss = 0.01330363\n",
      "Iteration 135, loss = 0.01308437\n",
      "Iteration 136, loss = 0.01283445\n",
      "Iteration 137, loss = 0.01263932\n",
      "Iteration 138, loss = 0.01246665\n",
      "Iteration 139, loss = 0.01225262\n",
      "Iteration 140, loss = 0.01211375\n",
      "Iteration 141, loss = 0.01197742\n",
      "Iteration 142, loss = 0.01172670\n",
      "Iteration 143, loss = 0.01162925\n",
      "Iteration 144, loss = 0.01144309\n",
      "Iteration 145, loss = 0.01126013\n",
      "Iteration 146, loss = 0.01110325\n",
      "Iteration 147, loss = 0.01099350\n",
      "Iteration 148, loss = 0.01082569\n",
      "Iteration 149, loss = 0.01068080\n",
      "Iteration 150, loss = 0.01054498\n",
      "Iteration 151, loss = 0.01043353\n",
      "Iteration 152, loss = 0.01027320\n",
      "Iteration 153, loss = 0.01013209\n",
      "Iteration 154, loss = 0.01000667\n",
      "Iteration 155, loss = 0.00990039\n",
      "Iteration 156, loss = 0.00976998\n",
      "Iteration 157, loss = 0.00964843\n",
      "Iteration 158, loss = 0.00953968\n",
      "Iteration 159, loss = 0.00943054\n",
      "Iteration 160, loss = 0.00932735\n",
      "Iteration 161, loss = 0.00920657\n",
      "Iteration 162, loss = 0.00908723\n",
      "Iteration 163, loss = 0.00901041\n",
      "Iteration 164, loss = 0.00892921\n",
      "Iteration 165, loss = 0.00877710\n",
      "Iteration 166, loss = 0.00874417\n",
      "Iteration 167, loss = 0.00860322\n",
      "Iteration 168, loss = 0.00852494\n",
      "Iteration 169, loss = 0.00844271\n",
      "Iteration 170, loss = 0.00830974\n",
      "Iteration 171, loss = 0.00829253\n",
      "Iteration 172, loss = 0.00818128\n",
      "Iteration 173, loss = 0.00808087\n",
      "Iteration 174, loss = 0.00800545\n",
      "Iteration 175, loss = 0.00792249\n",
      "Iteration 176, loss = 0.00785463\n",
      "Iteration 177, loss = 0.00779138\n",
      "Iteration 178, loss = 0.00770075\n",
      "Iteration 179, loss = 0.00763266\n",
      "Iteration 180, loss = 0.00754552\n",
      "Iteration 181, loss = 0.00748912\n",
      "Iteration 182, loss = 0.00738466\n",
      "Iteration 183, loss = 0.00732439\n",
      "Iteration 184, loss = 0.00725763\n",
      "Iteration 185, loss = 0.00719658\n",
      "Iteration 186, loss = 0.00715251\n",
      "Iteration 187, loss = 0.00703362\n",
      "Iteration 188, loss = 0.00702461\n",
      "Iteration 189, loss = 0.00695995\n",
      "Iteration 190, loss = 0.00686954\n",
      "Iteration 191, loss = 0.00685565\n",
      "Iteration 192, loss = 0.00676412\n",
      "Iteration 193, loss = 0.00671846\n",
      "Iteration 194, loss = 0.00668544\n",
      "Iteration 195, loss = 0.00663922\n",
      "Iteration 196, loss = 0.00656448\n",
      "Iteration 197, loss = 0.00649959\n",
      "Iteration 198, loss = 0.00647441\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.001, batch_size=100, hidden_layer_sizes=(100, 100, 100), solver=sgd; total time= 2.8min\n",
      "Iteration 1, loss = 1.84123080\n",
      "Iteration 2, loss = 0.70240428\n",
      "Iteration 3, loss = 0.44283557\n",
      "Iteration 4, loss = 0.37095504\n",
      "Iteration 5, loss = 0.33268685\n",
      "Iteration 6, loss = 0.30594326\n",
      "Iteration 7, loss = 0.28554787\n",
      "Iteration 8, loss = 0.26926511\n",
      "Iteration 9, loss = 0.25499393\n",
      "Iteration 10, loss = 0.24274150\n",
      "Iteration 11, loss = 0.23178583\n",
      "Iteration 12, loss = 0.22163535\n",
      "Iteration 13, loss = 0.21275660\n",
      "Iteration 14, loss = 0.20417568\n",
      "Iteration 15, loss = 0.19592023\n",
      "Iteration 16, loss = 0.18946774\n",
      "Iteration 17, loss = 0.18183118\n",
      "Iteration 18, loss = 0.17588125\n",
      "Iteration 19, loss = 0.16939747\n",
      "Iteration 20, loss = 0.16366550\n",
      "Iteration 21, loss = 0.15840086\n",
      "Iteration 22, loss = 0.15321156\n",
      "Iteration 23, loss = 0.14885356\n",
      "Iteration 24, loss = 0.14432548\n",
      "Iteration 25, loss = 0.13982994\n",
      "Iteration 26, loss = 0.13576194\n",
      "Iteration 27, loss = 0.13178759\n",
      "Iteration 28, loss = 0.12757455\n",
      "Iteration 29, loss = 0.12408811\n",
      "Iteration 30, loss = 0.12095210\n",
      "Iteration 31, loss = 0.11744112\n",
      "Iteration 32, loss = 0.11438856\n",
      "Iteration 33, loss = 0.11115000\n",
      "Iteration 34, loss = 0.10826530\n",
      "Iteration 35, loss = 0.10562657\n",
      "Iteration 36, loss = 0.10272955\n",
      "Iteration 37, loss = 0.09999102\n",
      "Iteration 38, loss = 0.09762510\n",
      "Iteration 39, loss = 0.09489001\n",
      "Iteration 40, loss = 0.09241809\n",
      "Iteration 41, loss = 0.09009499\n",
      "Iteration 42, loss = 0.08753246\n",
      "Iteration 43, loss = 0.08577973\n",
      "Iteration 44, loss = 0.08345023\n",
      "Iteration 45, loss = 0.08126840\n",
      "Iteration 46, loss = 0.07943527\n",
      "Iteration 47, loss = 0.07762660\n",
      "Iteration 48, loss = 0.07568950\n",
      "Iteration 49, loss = 0.07401996\n",
      "Iteration 50, loss = 0.07212810\n",
      "Iteration 51, loss = 0.07003776\n",
      "Iteration 52, loss = 0.06864731\n",
      "Iteration 53, loss = 0.06699237\n",
      "Iteration 54, loss = 0.06524436\n",
      "Iteration 55, loss = 0.06371513\n",
      "Iteration 56, loss = 0.06234251\n",
      "Iteration 57, loss = 0.06097747\n",
      "Iteration 58, loss = 0.05963883\n",
      "Iteration 59, loss = 0.05778473\n",
      "Iteration 60, loss = 0.05680429\n",
      "Iteration 61, loss = 0.05493018\n",
      "Iteration 62, loss = 0.05399104\n",
      "Iteration 63, loss = 0.05323948\n",
      "Iteration 64, loss = 0.05129163\n",
      "Iteration 65, loss = 0.05040139\n",
      "Iteration 66, loss = 0.04892036\n",
      "Iteration 67, loss = 0.04793953\n",
      "Iteration 68, loss = 0.04671820\n",
      "Iteration 69, loss = 0.04579841\n",
      "Iteration 70, loss = 0.04440647\n",
      "Iteration 71, loss = 0.04362971\n",
      "Iteration 72, loss = 0.04252378\n",
      "Iteration 73, loss = 0.04173112\n",
      "Iteration 74, loss = 0.04060242\n",
      "Iteration 75, loss = 0.04003062\n",
      "Iteration 76, loss = 0.03883315\n",
      "Iteration 77, loss = 0.03809113\n",
      "Iteration 78, loss = 0.03728277\n",
      "Iteration 79, loss = 0.03617664\n",
      "Iteration 80, loss = 0.03553426\n",
      "Iteration 81, loss = 0.03468946\n",
      "Iteration 82, loss = 0.03388707\n",
      "Iteration 83, loss = 0.03300794\n",
      "Iteration 84, loss = 0.03248047\n",
      "Iteration 85, loss = 0.03156481\n",
      "Iteration 86, loss = 0.03065759\n",
      "Iteration 87, loss = 0.03007905\n",
      "Iteration 88, loss = 0.02972981\n",
      "Iteration 89, loss = 0.02887981\n",
      "Iteration 90, loss = 0.02822049\n",
      "Iteration 91, loss = 0.02776240\n",
      "Iteration 92, loss = 0.02716419\n",
      "Iteration 93, loss = 0.02637606\n",
      "Iteration 94, loss = 0.02572292\n",
      "Iteration 95, loss = 0.02547085\n",
      "Iteration 96, loss = 0.02455899\n",
      "Iteration 97, loss = 0.02398506\n",
      "Iteration 98, loss = 0.02376600\n",
      "Iteration 99, loss = 0.02294655\n",
      "Iteration 100, loss = 0.02260844\n",
      "Iteration 101, loss = 0.02201358\n",
      "Iteration 102, loss = 0.02170765\n",
      "Iteration 103, loss = 0.02121444\n",
      "Iteration 104, loss = 0.02074303\n",
      "Iteration 105, loss = 0.02041648\n",
      "Iteration 106, loss = 0.01998967\n",
      "Iteration 107, loss = 0.01947871\n",
      "Iteration 108, loss = 0.01910832\n",
      "Iteration 109, loss = 0.01872358\n",
      "Iteration 110, loss = 0.01836591\n",
      "Iteration 111, loss = 0.01805149\n",
      "Iteration 112, loss = 0.01765142\n",
      "Iteration 113, loss = 0.01732644\n",
      "Iteration 114, loss = 0.01696800\n",
      "Iteration 115, loss = 0.01672580\n",
      "Iteration 116, loss = 0.01639381\n",
      "Iteration 117, loss = 0.01618425\n",
      "Iteration 118, loss = 0.01569867\n",
      "Iteration 119, loss = 0.01545685\n",
      "Iteration 120, loss = 0.01515579\n",
      "Iteration 121, loss = 0.01495856\n",
      "Iteration 122, loss = 0.01458298\n",
      "Iteration 123, loss = 0.01431292\n",
      "Iteration 124, loss = 0.01396793\n",
      "Iteration 125, loss = 0.01384983\n",
      "Iteration 126, loss = 0.01357142\n",
      "Iteration 127, loss = 0.01343336\n",
      "Iteration 128, loss = 0.01316488\n",
      "Iteration 129, loss = 0.01294260\n",
      "Iteration 130, loss = 0.01272172\n",
      "Iteration 131, loss = 0.01249952\n",
      "Iteration 132, loss = 0.01231370\n",
      "Iteration 133, loss = 0.01221186\n",
      "Iteration 134, loss = 0.01190712\n",
      "Iteration 135, loss = 0.01180318\n",
      "Iteration 136, loss = 0.01149332\n",
      "Iteration 137, loss = 0.01143859\n",
      "Iteration 138, loss = 0.01126660\n",
      "Iteration 139, loss = 0.01108853\n",
      "Iteration 140, loss = 0.01103238\n",
      "Iteration 141, loss = 0.01073912\n",
      "Iteration 142, loss = 0.01067841\n",
      "Iteration 143, loss = 0.01046176\n",
      "Iteration 144, loss = 0.01035550\n",
      "Iteration 145, loss = 0.01020597\n",
      "Iteration 146, loss = 0.01005008\n",
      "Iteration 147, loss = 0.00992602\n",
      "Iteration 148, loss = 0.00985829\n",
      "Iteration 149, loss = 0.00966884\n",
      "Iteration 150, loss = 0.00949591\n",
      "Iteration 151, loss = 0.00943854\n",
      "Iteration 152, loss = 0.00932259\n",
      "Iteration 153, loss = 0.00920517\n",
      "Iteration 154, loss = 0.00900606\n",
      "Iteration 155, loss = 0.00894231\n",
      "Iteration 156, loss = 0.00883648\n",
      "Iteration 157, loss = 0.00871729\n",
      "Iteration 158, loss = 0.00862799\n",
      "Iteration 159, loss = 0.00858696\n",
      "Iteration 160, loss = 0.00842598\n",
      "Iteration 161, loss = 0.00834117\n",
      "Iteration 162, loss = 0.00825154\n",
      "Iteration 163, loss = 0.00814557\n",
      "Iteration 164, loss = 0.00806755\n",
      "Iteration 165, loss = 0.00796686\n",
      "Iteration 166, loss = 0.00784855\n",
      "Iteration 167, loss = 0.00779015\n",
      "Iteration 168, loss = 0.00773895\n",
      "Iteration 169, loss = 0.00761509\n",
      "Iteration 170, loss = 0.00755613\n",
      "Iteration 171, loss = 0.00748075\n",
      "Iteration 172, loss = 0.00739796\n",
      "Iteration 173, loss = 0.00730605\n",
      "Iteration 174, loss = 0.00727479\n",
      "Iteration 175, loss = 0.00718260\n",
      "Iteration 176, loss = 0.00712962\n",
      "Iteration 177, loss = 0.00705550\n",
      "Iteration 178, loss = 0.00697615\n",
      "Iteration 179, loss = 0.00691212\n",
      "Iteration 180, loss = 0.00688801\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.001, batch_size=100, hidden_layer_sizes=(100, 100, 100), solver=sgd; total time= 2.6min\n",
      "Iteration 1, loss = 1.86440843\n",
      "Iteration 2, loss = 0.71208057\n",
      "Iteration 3, loss = 0.44646677\n",
      "Iteration 4, loss = 0.37501311\n",
      "Iteration 5, loss = 0.33794245\n",
      "Iteration 6, loss = 0.31364641\n",
      "Iteration 7, loss = 0.29531277\n",
      "Iteration 8, loss = 0.27955706\n",
      "Iteration 9, loss = 0.26728894\n",
      "Iteration 10, loss = 0.25600382\n",
      "Iteration 11, loss = 0.24520887\n",
      "Iteration 12, loss = 0.23580849\n",
      "Iteration 13, loss = 0.22650403\n",
      "Iteration 14, loss = 0.21846767\n",
      "Iteration 15, loss = 0.21018079\n",
      "Iteration 16, loss = 0.20329301\n",
      "Iteration 17, loss = 0.19582618\n",
      "Iteration 18, loss = 0.18897916\n",
      "Iteration 19, loss = 0.18273983\n",
      "Iteration 20, loss = 0.17701510\n",
      "Iteration 21, loss = 0.17124715\n",
      "Iteration 22, loss = 0.16554542\n",
      "Iteration 23, loss = 0.16082716\n",
      "Iteration 24, loss = 0.15571798\n",
      "Iteration 25, loss = 0.15151046\n",
      "Iteration 26, loss = 0.14717311\n",
      "Iteration 27, loss = 0.14252382\n",
      "Iteration 28, loss = 0.13911053\n",
      "Iteration 29, loss = 0.13532838\n",
      "Iteration 30, loss = 0.13174840\n",
      "Iteration 31, loss = 0.12840831\n",
      "Iteration 32, loss = 0.12453623\n",
      "Iteration 33, loss = 0.12191251\n",
      "Iteration 34, loss = 0.11862155\n",
      "Iteration 35, loss = 0.11574675\n",
      "Iteration 36, loss = 0.11283557\n",
      "Iteration 37, loss = 0.10976045\n",
      "Iteration 38, loss = 0.10727846\n",
      "Iteration 39, loss = 0.10488677\n",
      "Iteration 40, loss = 0.10254033\n",
      "Iteration 41, loss = 0.09998667\n",
      "Iteration 42, loss = 0.09732054\n",
      "Iteration 43, loss = 0.09499670\n",
      "Iteration 44, loss = 0.09272929\n",
      "Iteration 45, loss = 0.09037736\n",
      "Iteration 46, loss = 0.08848292\n",
      "Iteration 47, loss = 0.08664114\n",
      "Iteration 48, loss = 0.08399510\n",
      "Iteration 49, loss = 0.08236456\n",
      "Iteration 50, loss = 0.08058659\n",
      "Iteration 51, loss = 0.07819350\n",
      "Iteration 52, loss = 0.07661780\n",
      "Iteration 53, loss = 0.07471885\n",
      "Iteration 54, loss = 0.07303287\n",
      "Iteration 55, loss = 0.07160907\n",
      "Iteration 56, loss = 0.07004555\n",
      "Iteration 57, loss = 0.06821752\n",
      "Iteration 58, loss = 0.06653885\n",
      "Iteration 59, loss = 0.06490931\n",
      "Iteration 60, loss = 0.06363644\n",
      "Iteration 61, loss = 0.06217564\n",
      "Iteration 62, loss = 0.06071965\n",
      "Iteration 63, loss = 0.05923431\n",
      "Iteration 64, loss = 0.05784867\n",
      "Iteration 65, loss = 0.05668688\n",
      "Iteration 66, loss = 0.05478800\n",
      "Iteration 67, loss = 0.05402334\n",
      "Iteration 68, loss = 0.05275095\n",
      "Iteration 69, loss = 0.05150224\n",
      "Iteration 70, loss = 0.05027433\n",
      "Iteration 71, loss = 0.04940159\n",
      "Iteration 72, loss = 0.04784565\n",
      "Iteration 73, loss = 0.04705941\n",
      "Iteration 74, loss = 0.04598771\n",
      "Iteration 75, loss = 0.04495035\n",
      "Iteration 76, loss = 0.04386908\n",
      "Iteration 77, loss = 0.04304380\n",
      "Iteration 78, loss = 0.04201866\n",
      "Iteration 79, loss = 0.04105973\n",
      "Iteration 80, loss = 0.04034268\n",
      "Iteration 81, loss = 0.03906621\n",
      "Iteration 82, loss = 0.03820147\n",
      "Iteration 83, loss = 0.03780100\n",
      "Iteration 84, loss = 0.03672509\n",
      "Iteration 85, loss = 0.03594663\n",
      "Iteration 86, loss = 0.03519788\n",
      "Iteration 87, loss = 0.03434371\n",
      "Iteration 88, loss = 0.03366216\n",
      "Iteration 89, loss = 0.03286236\n",
      "Iteration 90, loss = 0.03216620\n",
      "Iteration 91, loss = 0.03139848\n",
      "Iteration 92, loss = 0.03102445\n",
      "Iteration 93, loss = 0.03013001\n",
      "Iteration 94, loss = 0.02948052\n",
      "Iteration 95, loss = 0.02885397\n",
      "Iteration 96, loss = 0.02826190\n",
      "Iteration 97, loss = 0.02756643\n",
      "Iteration 98, loss = 0.02671707\n",
      "Iteration 99, loss = 0.02638773\n",
      "Iteration 100, loss = 0.02589187\n",
      "Iteration 101, loss = 0.02536714\n",
      "Iteration 102, loss = 0.02476700\n",
      "Iteration 103, loss = 0.02449462\n",
      "Iteration 104, loss = 0.02373599\n",
      "Iteration 105, loss = 0.02339587\n",
      "Iteration 106, loss = 0.02269423\n",
      "Iteration 107, loss = 0.02248534\n",
      "Iteration 108, loss = 0.02194080\n",
      "Iteration 109, loss = 0.02130844\n",
      "Iteration 110, loss = 0.02096556\n",
      "Iteration 111, loss = 0.02059935\n",
      "Iteration 112, loss = 0.02010739\n",
      "Iteration 113, loss = 0.01975322\n",
      "Iteration 114, loss = 0.01935821\n",
      "Iteration 115, loss = 0.01893660\n",
      "Iteration 116, loss = 0.01864034\n",
      "Iteration 117, loss = 0.01830401\n",
      "Iteration 118, loss = 0.01781374\n",
      "Iteration 119, loss = 0.01759879\n",
      "Iteration 120, loss = 0.01720366\n",
      "Iteration 121, loss = 0.01700368\n",
      "Iteration 122, loss = 0.01665071\n",
      "Iteration 123, loss = 0.01641495\n",
      "Iteration 124, loss = 0.01601403\n",
      "Iteration 125, loss = 0.01572844\n",
      "Iteration 126, loss = 0.01542349\n",
      "Iteration 127, loss = 0.01520550\n",
      "Iteration 128, loss = 0.01496825\n",
      "Iteration 129, loss = 0.01464190\n",
      "Iteration 130, loss = 0.01448805\n",
      "Iteration 131, loss = 0.01418653\n",
      "Iteration 132, loss = 0.01394526\n",
      "Iteration 133, loss = 0.01369782\n",
      "Iteration 134, loss = 0.01347727\n",
      "Iteration 135, loss = 0.01322510\n",
      "Iteration 136, loss = 0.01298543\n",
      "Iteration 137, loss = 0.01275988\n",
      "Iteration 138, loss = 0.01260106\n",
      "Iteration 139, loss = 0.01242902\n",
      "Iteration 140, loss = 0.01221074\n",
      "Iteration 141, loss = 0.01194462\n",
      "Iteration 142, loss = 0.01183343\n",
      "Iteration 143, loss = 0.01171469\n",
      "Iteration 144, loss = 0.01145937\n",
      "Iteration 145, loss = 0.01128351\n",
      "Iteration 146, loss = 0.01115927\n",
      "Iteration 147, loss = 0.01092779\n",
      "Iteration 148, loss = 0.01078047\n",
      "Iteration 149, loss = 0.01060201\n",
      "Iteration 150, loss = 0.01041942\n",
      "Iteration 151, loss = 0.01032569\n",
      "Iteration 152, loss = 0.01018293\n",
      "Iteration 153, loss = 0.00999111\n",
      "Iteration 154, loss = 0.00985067\n",
      "Iteration 155, loss = 0.00970090\n",
      "Iteration 156, loss = 0.00959197\n",
      "Iteration 157, loss = 0.00948875\n",
      "Iteration 158, loss = 0.00939570\n",
      "Iteration 159, loss = 0.00925041\n",
      "Iteration 160, loss = 0.00906092\n",
      "Iteration 161, loss = 0.00899300\n",
      "Iteration 162, loss = 0.00890669\n",
      "Iteration 163, loss = 0.00874328\n",
      "Iteration 164, loss = 0.00863951\n",
      "Iteration 165, loss = 0.00850145\n",
      "Iteration 166, loss = 0.00847350\n",
      "Iteration 167, loss = 0.00834011\n",
      "Iteration 168, loss = 0.00824239\n",
      "Iteration 169, loss = 0.00816516\n",
      "Iteration 170, loss = 0.00804066\n",
      "Iteration 171, loss = 0.00801881\n",
      "Iteration 172, loss = 0.00789724\n",
      "Iteration 173, loss = 0.00781257\n",
      "Iteration 174, loss = 0.00775267\n",
      "Iteration 175, loss = 0.00768435\n",
      "Iteration 176, loss = 0.00758269\n",
      "Iteration 177, loss = 0.00749343\n",
      "Iteration 178, loss = 0.00742408\n",
      "Iteration 179, loss = 0.00738128\n",
      "Iteration 180, loss = 0.00729637\n",
      "Iteration 181, loss = 0.00721314\n",
      "Iteration 182, loss = 0.00713923\n",
      "Iteration 183, loss = 0.00713492\n",
      "Iteration 184, loss = 0.00702105\n",
      "Iteration 185, loss = 0.00695851\n",
      "Iteration 186, loss = 0.00692505\n",
      "Iteration 187, loss = 0.00685494\n",
      "Iteration 188, loss = 0.00680502\n",
      "Iteration 189, loss = 0.00674809\n",
      "Iteration 190, loss = 0.00669385\n",
      "Iteration 191, loss = 0.00667501\n",
      "Iteration 192, loss = 0.00661982\n",
      "Iteration 193, loss = 0.00654532\n",
      "Iteration 194, loss = 0.00649680\n",
      "Iteration 195, loss = 0.00646377\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.001, batch_size=100, hidden_layer_sizes=(100, 100, 100), solver=sgd; total time= 2.8min\n",
      "Iteration 1, loss = 0.53206881\n",
      "Iteration 2, loss = 0.25282503\n",
      "Iteration 3, loss = 0.19686689\n",
      "Iteration 4, loss = 0.16277039\n",
      "Iteration 5, loss = 0.13811118\n",
      "Iteration 6, loss = 0.11715353\n",
      "Iteration 7, loss = 0.10298044\n",
      "Iteration 8, loss = 0.09075729\n",
      "Iteration 9, loss = 0.08059859\n",
      "Iteration 10, loss = 0.07156547\n",
      "Iteration 11, loss = 0.06413254\n",
      "Iteration 12, loss = 0.05872920\n",
      "Iteration 13, loss = 0.05200149\n",
      "Iteration 14, loss = 0.04644161\n",
      "Iteration 15, loss = 0.04311831\n",
      "Iteration 16, loss = 0.03835679\n",
      "Iteration 17, loss = 0.03443303\n",
      "Iteration 18, loss = 0.03117867\n",
      "Iteration 19, loss = 0.02822861\n",
      "Iteration 20, loss = 0.02609472\n",
      "Iteration 21, loss = 0.02287512\n",
      "Iteration 22, loss = 0.02065629\n",
      "Iteration 23, loss = 0.01948826\n",
      "Iteration 24, loss = 0.01750718\n",
      "Iteration 25, loss = 0.01616021\n",
      "Iteration 26, loss = 0.01486976\n",
      "Iteration 27, loss = 0.01424964\n",
      "Iteration 28, loss = 0.01228331\n",
      "Iteration 29, loss = 0.01129722\n",
      "Iteration 30, loss = 0.01049362\n",
      "Iteration 31, loss = 0.00986133\n",
      "Iteration 32, loss = 0.00895412\n",
      "Iteration 33, loss = 0.00839373\n",
      "Iteration 34, loss = 0.00798796\n",
      "Iteration 35, loss = 0.00763306\n",
      "Iteration 36, loss = 0.00737307\n",
      "Iteration 37, loss = 0.00683430\n",
      "Iteration 38, loss = 0.00629023\n",
      "Iteration 39, loss = 0.00603215\n",
      "Iteration 40, loss = 0.00570979\n",
      "Iteration 41, loss = 0.00546634\n",
      "Iteration 42, loss = 0.00537947\n",
      "Iteration 43, loss = 0.00509075\n",
      "Iteration 44, loss = 0.00496471\n",
      "Iteration 45, loss = 0.00487016\n",
      "Iteration 46, loss = 0.00483697\n",
      "Iteration 47, loss = 0.01711445\n",
      "Iteration 48, loss = 0.00982975\n",
      "Iteration 49, loss = 0.00629162\n",
      "Iteration 50, loss = 0.00456578\n",
      "Iteration 51, loss = 0.00431496\n",
      "Iteration 52, loss = 0.00421279\n",
      "Iteration 53, loss = 0.00411610\n",
      "Iteration 54, loss = 0.00405816\n",
      "Iteration 55, loss = 0.00402261\n",
      "Iteration 56, loss = 0.00395444\n",
      "Iteration 57, loss = 0.00388663\n",
      "Iteration 58, loss = 0.00389290\n",
      "Iteration 59, loss = 0.00383151\n",
      "Iteration 60, loss = 0.00376736\n",
      "Iteration 61, loss = 0.00374361\n",
      "Iteration 62, loss = 0.00388835\n",
      "Iteration 63, loss = 0.01566794\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.001, batch_size=200, hidden_layer_sizes=(100,), solver=adam; total time=  33.3s\n",
      "Iteration 1, loss = 0.56391179\n",
      "Iteration 2, loss = 0.24662549\n",
      "Iteration 3, loss = 0.19342940\n",
      "Iteration 4, loss = 0.15940930\n",
      "Iteration 5, loss = 0.13526396\n",
      "Iteration 6, loss = 0.11632766\n",
      "Iteration 7, loss = 0.10051367\n",
      "Iteration 8, loss = 0.08990821\n",
      "Iteration 9, loss = 0.07843574\n",
      "Iteration 10, loss = 0.06970353\n",
      "Iteration 11, loss = 0.06260268\n",
      "Iteration 12, loss = 0.05661533\n",
      "Iteration 13, loss = 0.05039342\n",
      "Iteration 14, loss = 0.04553253\n",
      "Iteration 15, loss = 0.04038680\n",
      "Iteration 16, loss = 0.03674465\n",
      "Iteration 17, loss = 0.03354239\n",
      "Iteration 18, loss = 0.03014762\n",
      "Iteration 19, loss = 0.02756854\n",
      "Iteration 20, loss = 0.02471542\n",
      "Iteration 21, loss = 0.02286602\n",
      "Iteration 22, loss = 0.02088218\n",
      "Iteration 23, loss = 0.01895194\n",
      "Iteration 24, loss = 0.01760967\n",
      "Iteration 25, loss = 0.01674042\n",
      "Iteration 26, loss = 0.01471944\n",
      "Iteration 27, loss = 0.01343469\n",
      "Iteration 28, loss = 0.01271792\n",
      "Iteration 29, loss = 0.01165675\n",
      "Iteration 30, loss = 0.01055785\n",
      "Iteration 31, loss = 0.01004644\n",
      "Iteration 32, loss = 0.00920032\n",
      "Iteration 33, loss = 0.00872554\n",
      "Iteration 34, loss = 0.00809318\n",
      "Iteration 35, loss = 0.00788257\n",
      "Iteration 36, loss = 0.00724089\n",
      "Iteration 37, loss = 0.00683278\n",
      "Iteration 38, loss = 0.00636936\n",
      "Iteration 39, loss = 0.00622071\n",
      "Iteration 40, loss = 0.00617899\n",
      "Iteration 41, loss = 0.00584324\n",
      "Iteration 42, loss = 0.00537156\n",
      "Iteration 43, loss = 0.00535148\n",
      "Iteration 44, loss = 0.00520387\n",
      "Iteration 45, loss = 0.00525716\n",
      "Iteration 46, loss = 0.00468808\n",
      "Iteration 47, loss = 0.00456482\n",
      "Iteration 48, loss = 0.00455420\n",
      "Iteration 49, loss = 0.00447408\n",
      "Iteration 50, loss = 0.00428164\n",
      "Iteration 51, loss = 0.00422679\n",
      "Iteration 52, loss = 0.00411201\n",
      "Iteration 53, loss = 0.00410000\n",
      "Iteration 54, loss = 0.00453438\n",
      "Iteration 55, loss = 0.02276864\n",
      "Iteration 56, loss = 0.00983275\n",
      "Iteration 57, loss = 0.00467562\n",
      "Iteration 58, loss = 0.00416737\n",
      "Iteration 59, loss = 0.00400934\n",
      "Iteration 60, loss = 0.00393296\n",
      "Iteration 61, loss = 0.00386505\n",
      "Iteration 62, loss = 0.00381524\n",
      "Iteration 63, loss = 0.00377359\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.001, batch_size=200, hidden_layer_sizes=(100,), solver=adam; total time=  32.3s\n",
      "Iteration 1, loss = 0.55003869\n",
      "Iteration 2, loss = 0.24385509\n",
      "Iteration 3, loss = 0.19161683\n",
      "Iteration 4, loss = 0.15702601\n",
      "Iteration 5, loss = 0.13361050\n",
      "Iteration 6, loss = 0.11547499\n",
      "Iteration 7, loss = 0.10086460\n",
      "Iteration 8, loss = 0.08875494\n",
      "Iteration 9, loss = 0.07866352\n",
      "Iteration 10, loss = 0.07098155\n",
      "Iteration 11, loss = 0.06335391\n",
      "Iteration 12, loss = 0.05700483\n",
      "Iteration 13, loss = 0.05082264\n",
      "Iteration 14, loss = 0.04597409\n",
      "Iteration 15, loss = 0.04128679\n",
      "Iteration 16, loss = 0.03709819\n",
      "Iteration 17, loss = 0.03397429\n",
      "Iteration 18, loss = 0.02997099\n",
      "Iteration 19, loss = 0.02799489\n",
      "Iteration 20, loss = 0.02520197\n",
      "Iteration 21, loss = 0.02217337\n",
      "Iteration 22, loss = 0.02052201\n",
      "Iteration 23, loss = 0.01889018\n",
      "Iteration 24, loss = 0.01709339\n",
      "Iteration 25, loss = 0.01593555\n",
      "Iteration 26, loss = 0.01418012\n",
      "Iteration 27, loss = 0.01301081\n",
      "Iteration 28, loss = 0.01214727\n",
      "Iteration 29, loss = 0.01136715\n",
      "Iteration 30, loss = 0.01043036\n",
      "Iteration 31, loss = 0.00950287\n",
      "Iteration 32, loss = 0.00893458\n",
      "Iteration 33, loss = 0.00848363\n",
      "Iteration 34, loss = 0.00783627\n",
      "Iteration 35, loss = 0.00816049\n",
      "Iteration 36, loss = 0.00737140\n",
      "Iteration 37, loss = 0.00673014\n",
      "Iteration 38, loss = 0.00620940\n",
      "Iteration 39, loss = 0.00593837\n",
      "Iteration 40, loss = 0.00560630\n",
      "Iteration 41, loss = 0.00533546\n",
      "Iteration 42, loss = 0.00524374\n",
      "Iteration 43, loss = 0.00520581\n",
      "Iteration 44, loss = 0.00512911\n",
      "Iteration 45, loss = 0.00493585\n",
      "Iteration 46, loss = 0.00472263\n",
      "Iteration 47, loss = 0.00462084\n",
      "Iteration 48, loss = 0.00441700\n",
      "Iteration 49, loss = 0.00439183\n",
      "Iteration 50, loss = 0.00422619\n",
      "Iteration 51, loss = 0.00422473\n",
      "Iteration 52, loss = 0.00433249\n",
      "Iteration 53, loss = 0.00524820\n",
      "Iteration 54, loss = 0.01805105\n",
      "Iteration 55, loss = 0.00943390\n",
      "Iteration 56, loss = 0.00502423\n",
      "Iteration 57, loss = 0.00421328\n",
      "Iteration 58, loss = 0.00415848\n",
      "Iteration 59, loss = 0.00386971\n",
      "Iteration 60, loss = 0.00381599\n",
      "Iteration 61, loss = 0.00374999\n",
      "Iteration 62, loss = 0.00369966\n",
      "Iteration 63, loss = 0.00365224\n",
      "Iteration 64, loss = 0.00361332\n",
      "Iteration 65, loss = 0.00358165\n",
      "Iteration 66, loss = 0.00354365\n",
      "Iteration 67, loss = 0.00351962\n",
      "Iteration 68, loss = 0.00348877\n",
      "Iteration 69, loss = 0.00345112\n",
      "Iteration 70, loss = 0.00344188\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.001, batch_size=200, hidden_layer_sizes=(100,), solver=adam; total time=  35.6s\n",
      "Iteration 1, loss = 0.55798746\n",
      "Iteration 2, loss = 0.26121006\n",
      "Iteration 3, loss = 0.20548575\n",
      "Iteration 4, loss = 0.16968731\n",
      "Iteration 5, loss = 0.14397140\n",
      "Iteration 6, loss = 0.12439575\n",
      "Iteration 7, loss = 0.10776673\n",
      "Iteration 8, loss = 0.09551397\n",
      "Iteration 9, loss = 0.08380402\n",
      "Iteration 10, loss = 0.07445969\n",
      "Iteration 11, loss = 0.06649521\n",
      "Iteration 12, loss = 0.05935952\n",
      "Iteration 13, loss = 0.05331768\n",
      "Iteration 14, loss = 0.04804227\n",
      "Iteration 15, loss = 0.04323690\n",
      "Iteration 16, loss = 0.03906451\n",
      "Iteration 17, loss = 0.03510082\n",
      "Iteration 18, loss = 0.03177255\n",
      "Iteration 19, loss = 0.02902817\n",
      "Iteration 20, loss = 0.02674565\n",
      "Iteration 21, loss = 0.02342451\n",
      "Iteration 22, loss = 0.02180146\n",
      "Iteration 23, loss = 0.02014804\n",
      "Iteration 24, loss = 0.01865334\n",
      "Iteration 25, loss = 0.01669371\n",
      "Iteration 26, loss = 0.01554263\n",
      "Iteration 27, loss = 0.01394603\n",
      "Iteration 28, loss = 0.01329687\n",
      "Iteration 29, loss = 0.01235043\n",
      "Iteration 30, loss = 0.01089238\n",
      "Iteration 31, loss = 0.01045500\n",
      "Iteration 32, loss = 0.00967815\n",
      "Iteration 33, loss = 0.00869758\n",
      "Iteration 34, loss = 0.00840422\n",
      "Iteration 35, loss = 0.00836355\n",
      "Iteration 36, loss = 0.00752750\n",
      "Iteration 37, loss = 0.00676301\n",
      "Iteration 38, loss = 0.00691001\n",
      "Iteration 39, loss = 0.00732129\n",
      "Iteration 40, loss = 0.00604807\n",
      "Iteration 41, loss = 0.00593184\n",
      "Iteration 42, loss = 0.00567396\n",
      "Iteration 43, loss = 0.00532049\n",
      "Iteration 44, loss = 0.00535356\n",
      "Iteration 45, loss = 0.00504172\n",
      "Iteration 46, loss = 0.00474919\n",
      "Iteration 47, loss = 0.00470287\n",
      "Iteration 48, loss = 0.00467448\n",
      "Iteration 49, loss = 0.00441652\n",
      "Iteration 50, loss = 0.00437815\n",
      "Iteration 51, loss = 0.00433861\n",
      "Iteration 52, loss = 0.01407772\n",
      "Iteration 53, loss = 0.01360221\n",
      "Iteration 54, loss = 0.00563310\n",
      "Iteration 55, loss = 0.00444216\n",
      "Iteration 56, loss = 0.00419498\n",
      "Iteration 57, loss = 0.00410815\n",
      "Iteration 58, loss = 0.00403087\n",
      "Iteration 59, loss = 0.00393824\n",
      "Iteration 60, loss = 0.00389712\n",
      "Iteration 61, loss = 0.00383989\n",
      "Iteration 62, loss = 0.00379285\n",
      "Iteration 63, loss = 0.00376490\n",
      "Iteration 64, loss = 0.00373528\n",
      "Iteration 65, loss = 0.00368446\n",
      "Iteration 66, loss = 0.00364999\n",
      "Iteration 67, loss = 0.00363142\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.001, batch_size=200, hidden_layer_sizes=(100,), solver=adam; total time=  33.9s\n",
      "Iteration 1, loss = 0.56122027\n",
      "Iteration 2, loss = 0.26272208\n",
      "Iteration 3, loss = 0.20717275\n",
      "Iteration 4, loss = 0.16935162\n",
      "Iteration 5, loss = 0.14151024\n",
      "Iteration 6, loss = 0.12145363\n",
      "Iteration 7, loss = 0.10686144\n",
      "Iteration 8, loss = 0.09342148\n",
      "Iteration 9, loss = 0.08237500\n",
      "Iteration 10, loss = 0.07406993\n",
      "Iteration 11, loss = 0.06681133\n",
      "Iteration 12, loss = 0.05904359\n",
      "Iteration 13, loss = 0.05368705\n",
      "Iteration 14, loss = 0.04813534\n",
      "Iteration 15, loss = 0.04328994\n",
      "Iteration 16, loss = 0.03988318\n",
      "Iteration 17, loss = 0.03603794\n",
      "Iteration 18, loss = 0.03183381\n",
      "Iteration 19, loss = 0.02913978\n",
      "Iteration 20, loss = 0.02650211\n",
      "Iteration 21, loss = 0.02404167\n",
      "Iteration 22, loss = 0.02149370\n",
      "Iteration 23, loss = 0.01941754\n",
      "Iteration 24, loss = 0.01812513\n",
      "Iteration 25, loss = 0.01630087\n",
      "Iteration 26, loss = 0.01575261\n",
      "Iteration 27, loss = 0.01427920\n",
      "Iteration 28, loss = 0.01267904\n",
      "Iteration 29, loss = 0.01202452\n",
      "Iteration 30, loss = 0.01075371\n",
      "Iteration 31, loss = 0.01013376\n",
      "Iteration 32, loss = 0.00944352\n",
      "Iteration 33, loss = 0.00901656\n",
      "Iteration 34, loss = 0.00886482\n",
      "Iteration 35, loss = 0.00809735\n",
      "Iteration 36, loss = 0.00762798\n",
      "Iteration 37, loss = 0.00702013\n",
      "Iteration 38, loss = 0.00685301\n",
      "Iteration 39, loss = 0.00634462\n",
      "Iteration 40, loss = 0.00602141\n",
      "Iteration 41, loss = 0.00566870\n",
      "Iteration 42, loss = 0.00575926\n",
      "Iteration 43, loss = 0.00533727\n",
      "Iteration 44, loss = 0.00507542\n",
      "Iteration 45, loss = 0.00488380\n",
      "Iteration 46, loss = 0.00485497\n",
      "Iteration 47, loss = 0.00468948\n",
      "Iteration 48, loss = 0.00479362\n",
      "Iteration 49, loss = 0.00490913\n",
      "Iteration 50, loss = 0.01743776\n",
      "Iteration 51, loss = 0.01088376\n",
      "Iteration 52, loss = 0.00504947\n",
      "Iteration 53, loss = 0.00443095\n",
      "Iteration 54, loss = 0.00425232\n",
      "Iteration 55, loss = 0.00413468\n",
      "Iteration 56, loss = 0.00405334\n",
      "Iteration 57, loss = 0.00400579\n",
      "Iteration 58, loss = 0.00393469\n",
      "Iteration 59, loss = 0.00388004\n",
      "Iteration 60, loss = 0.00385298\n",
      "Iteration 61, loss = 0.00380154\n",
      "Iteration 62, loss = 0.00376382\n",
      "Iteration 63, loss = 0.00372082\n",
      "Iteration 64, loss = 0.00367333\n",
      "Iteration 65, loss = 0.00367869\n",
      "Iteration 66, loss = 0.00368604\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.001, batch_size=200, hidden_layer_sizes=(100,), solver=adam; total time=  33.9s\n",
      "Iteration 1, loss = 1.82728696\n",
      "Iteration 2, loss = 1.07277218\n",
      "Iteration 3, loss = 0.76597204\n",
      "Iteration 4, loss = 0.62519222\n",
      "Iteration 5, loss = 0.54641236\n",
      "Iteration 6, loss = 0.49591870\n",
      "Iteration 7, loss = 0.46055943\n",
      "Iteration 8, loss = 0.43423965\n",
      "Iteration 9, loss = 0.41376400\n",
      "Iteration 10, loss = 0.39724623\n",
      "Iteration 11, loss = 0.38368242\n",
      "Iteration 12, loss = 0.37191370\n",
      "Iteration 13, loss = 0.36197023\n",
      "Iteration 14, loss = 0.35326283\n",
      "Iteration 15, loss = 0.34544072\n",
      "Iteration 16, loss = 0.33847518\n",
      "Iteration 17, loss = 0.33206430\n",
      "Iteration 18, loss = 0.32616600\n",
      "Iteration 19, loss = 0.32069794\n",
      "Iteration 20, loss = 0.31566256\n",
      "Iteration 21, loss = 0.31088610\n",
      "Iteration 22, loss = 0.30639763\n",
      "Iteration 23, loss = 0.30212668\n",
      "Iteration 24, loss = 0.29813887\n",
      "Iteration 25, loss = 0.29423406\n",
      "Iteration 26, loss = 0.29056516\n",
      "Iteration 27, loss = 0.28700908\n",
      "Iteration 28, loss = 0.28355091\n",
      "Iteration 29, loss = 0.28024434\n",
      "Iteration 30, loss = 0.27706894\n",
      "Iteration 31, loss = 0.27399946\n",
      "Iteration 32, loss = 0.27098014\n",
      "Iteration 33, loss = 0.26812671\n",
      "Iteration 34, loss = 0.26537997\n",
      "Iteration 35, loss = 0.26279046\n",
      "Iteration 36, loss = 0.26013413\n",
      "Iteration 37, loss = 0.25763104\n",
      "Iteration 38, loss = 0.25522160\n",
      "Iteration 39, loss = 0.25287084\n",
      "Iteration 40, loss = 0.25055537\n",
      "Iteration 41, loss = 0.24828068\n",
      "Iteration 42, loss = 0.24609293\n",
      "Iteration 43, loss = 0.24385581\n",
      "Iteration 44, loss = 0.24178129\n",
      "Iteration 45, loss = 0.23973582\n",
      "Iteration 46, loss = 0.23770092\n",
      "Iteration 47, loss = 0.23574258\n",
      "Iteration 48, loss = 0.23379946\n",
      "Iteration 49, loss = 0.23198453\n",
      "Iteration 50, loss = 0.23003691\n",
      "Iteration 51, loss = 0.22824540\n",
      "Iteration 52, loss = 0.22648091\n",
      "Iteration 53, loss = 0.22468139\n",
      "Iteration 54, loss = 0.22289963\n",
      "Iteration 55, loss = 0.22122743\n",
      "Iteration 56, loss = 0.21953058\n",
      "Iteration 57, loss = 0.21794801\n",
      "Iteration 58, loss = 0.21643940\n",
      "Iteration 59, loss = 0.21476972\n",
      "Iteration 60, loss = 0.21315870\n",
      "Iteration 61, loss = 0.21160606\n",
      "Iteration 62, loss = 0.21009295\n",
      "Iteration 63, loss = 0.20858892\n",
      "Iteration 64, loss = 0.20709573\n",
      "Iteration 65, loss = 0.20569196\n",
      "Iteration 66, loss = 0.20421976\n",
      "Iteration 67, loss = 0.20286930\n",
      "Iteration 68, loss = 0.20146265\n",
      "Iteration 69, loss = 0.20002038\n",
      "Iteration 70, loss = 0.19870820\n",
      "Iteration 71, loss = 0.19734730\n",
      "Iteration 72, loss = 0.19603444\n",
      "Iteration 73, loss = 0.19469377\n",
      "Iteration 74, loss = 0.19347836\n",
      "Iteration 75, loss = 0.19224258\n",
      "Iteration 76, loss = 0.19097143\n",
      "Iteration 77, loss = 0.18973122\n",
      "Iteration 78, loss = 0.18842387\n",
      "Iteration 79, loss = 0.18737732\n",
      "Iteration 80, loss = 0.18613401\n",
      "Iteration 81, loss = 0.18498417\n",
      "Iteration 82, loss = 0.18389348\n",
      "Iteration 83, loss = 0.18268814\n",
      "Iteration 84, loss = 0.18160190\n",
      "Iteration 85, loss = 0.18047759\n",
      "Iteration 86, loss = 0.17950457\n",
      "Iteration 87, loss = 0.17831882\n",
      "Iteration 88, loss = 0.17726377\n",
      "Iteration 89, loss = 0.17619951\n",
      "Iteration 90, loss = 0.17514612\n",
      "Iteration 91, loss = 0.17415053\n",
      "Iteration 92, loss = 0.17321155\n",
      "Iteration 93, loss = 0.17217080\n",
      "Iteration 94, loss = 0.17110521\n",
      "Iteration 95, loss = 0.17022448\n",
      "Iteration 96, loss = 0.16928933\n",
      "Iteration 97, loss = 0.16833720\n",
      "Iteration 98, loss = 0.16739260\n",
      "Iteration 99, loss = 0.16639937\n",
      "Iteration 100, loss = 0.16549811\n",
      "Iteration 101, loss = 0.16456284\n",
      "Iteration 102, loss = 0.16372692\n",
      "Iteration 103, loss = 0.16277527\n",
      "Iteration 104, loss = 0.16189783\n",
      "Iteration 105, loss = 0.16099769\n",
      "Iteration 106, loss = 0.16020837\n",
      "Iteration 107, loss = 0.15929287\n",
      "Iteration 108, loss = 0.15848297\n",
      "Iteration 109, loss = 0.15762104\n",
      "Iteration 110, loss = 0.15674867\n",
      "Iteration 111, loss = 0.15590676\n",
      "Iteration 112, loss = 0.15504874\n",
      "Iteration 113, loss = 0.15437269\n",
      "Iteration 114, loss = 0.15355981\n",
      "Iteration 115, loss = 0.15271267\n",
      "Iteration 116, loss = 0.15193588\n",
      "Iteration 117, loss = 0.15115496\n",
      "Iteration 118, loss = 0.15042089\n",
      "Iteration 119, loss = 0.14950369\n",
      "Iteration 120, loss = 0.14882659\n",
      "Iteration 121, loss = 0.14809514\n",
      "Iteration 122, loss = 0.14732625\n",
      "Iteration 123, loss = 0.14659884\n",
      "Iteration 124, loss = 0.14586838\n",
      "Iteration 125, loss = 0.14521698\n",
      "Iteration 126, loss = 0.14437211\n",
      "Iteration 127, loss = 0.14359610\n",
      "Iteration 128, loss = 0.14301104\n",
      "Iteration 129, loss = 0.14225891\n",
      "Iteration 130, loss = 0.14154635\n",
      "Iteration 131, loss = 0.14091751\n",
      "Iteration 132, loss = 0.14021111\n",
      "Iteration 133, loss = 0.13942383\n",
      "Iteration 134, loss = 0.13885020\n",
      "Iteration 135, loss = 0.13814862\n",
      "Iteration 136, loss = 0.13744522\n",
      "Iteration 137, loss = 0.13686121\n",
      "Iteration 138, loss = 0.13615064\n",
      "Iteration 139, loss = 0.13558023\n",
      "Iteration 140, loss = 0.13494739\n",
      "Iteration 141, loss = 0.13424469\n",
      "Iteration 142, loss = 0.13358722\n",
      "Iteration 143, loss = 0.13293918\n",
      "Iteration 144, loss = 0.13238549\n",
      "Iteration 145, loss = 0.13175473\n",
      "Iteration 146, loss = 0.13114422\n",
      "Iteration 147, loss = 0.13051696\n",
      "Iteration 148, loss = 0.12992436\n",
      "Iteration 149, loss = 0.12931085\n",
      "Iteration 150, loss = 0.12877328\n",
      "Iteration 151, loss = 0.12808525\n",
      "Iteration 152, loss = 0.12758012\n",
      "Iteration 153, loss = 0.12698806\n",
      "Iteration 154, loss = 0.12639851\n",
      "Iteration 155, loss = 0.12580879\n",
      "Iteration 156, loss = 0.12530288\n",
      "Iteration 157, loss = 0.12470523\n",
      "Iteration 158, loss = 0.12419063\n",
      "Iteration 159, loss = 0.12356770\n",
      "Iteration 160, loss = 0.12312827\n",
      "Iteration 161, loss = 0.12253014\n",
      "Iteration 162, loss = 0.12195692\n",
      "Iteration 163, loss = 0.12143886\n",
      "Iteration 164, loss = 0.12086965\n",
      "Iteration 165, loss = 0.12037053\n",
      "Iteration 166, loss = 0.11981436\n",
      "Iteration 167, loss = 0.11934979\n",
      "Iteration 168, loss = 0.11875037\n",
      "Iteration 169, loss = 0.11829040\n",
      "Iteration 170, loss = 0.11772715\n",
      "Iteration 171, loss = 0.11727814\n",
      "Iteration 172, loss = 0.11687178\n",
      "Iteration 173, loss = 0.11628245\n",
      "Iteration 174, loss = 0.11574597\n",
      "Iteration 175, loss = 0.11522198\n",
      "Iteration 176, loss = 0.11480541\n",
      "Iteration 177, loss = 0.11435142\n",
      "Iteration 178, loss = 0.11378375\n",
      "Iteration 179, loss = 0.11330085\n",
      "Iteration 180, loss = 0.11285458\n",
      "Iteration 181, loss = 0.11240317\n",
      "Iteration 182, loss = 0.11191929\n",
      "Iteration 183, loss = 0.11139455\n",
      "Iteration 184, loss = 0.11104250\n",
      "Iteration 185, loss = 0.11052279\n",
      "Iteration 186, loss = 0.11005631\n",
      "Iteration 187, loss = 0.10961958\n",
      "Iteration 188, loss = 0.10916237\n",
      "Iteration 189, loss = 0.10874274\n",
      "Iteration 190, loss = 0.10824569\n",
      "Iteration 191, loss = 0.10784754\n",
      "Iteration 192, loss = 0.10740551\n",
      "Iteration 193, loss = 0.10697996\n",
      "Iteration 194, loss = 0.10648760\n",
      "Iteration 195, loss = 0.10616296\n",
      "Iteration 196, loss = 0.10571354\n",
      "Iteration 197, loss = 0.10524680\n",
      "Iteration 198, loss = 0.10483269\n",
      "Iteration 199, loss = 0.10439386\n",
      "Iteration 200, loss = 0.10399592\n",
      "[CV] END activation=relu, alpha=0.001, batch_size=200, hidden_layer_sizes=(100,), solver=sgd; total time= 1.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.86329292\n",
      "Iteration 2, loss = 1.08516724\n",
      "Iteration 3, loss = 0.75934066\n",
      "Iteration 4, loss = 0.61837789\n",
      "Iteration 5, loss = 0.54105636\n",
      "Iteration 6, loss = 0.49173472\n",
      "Iteration 7, loss = 0.45710213\n",
      "Iteration 8, loss = 0.43157303\n",
      "Iteration 9, loss = 0.41154671\n",
      "Iteration 10, loss = 0.39556697\n",
      "Iteration 11, loss = 0.38229312\n",
      "Iteration 12, loss = 0.37099227\n",
      "Iteration 13, loss = 0.36129493\n",
      "Iteration 14, loss = 0.35279125\n",
      "Iteration 15, loss = 0.34498841\n",
      "Iteration 16, loss = 0.33829109\n",
      "Iteration 17, loss = 0.33189768\n",
      "Iteration 18, loss = 0.32623109\n",
      "Iteration 19, loss = 0.32096183\n",
      "Iteration 20, loss = 0.31593687\n",
      "Iteration 21, loss = 0.31129723\n",
      "Iteration 22, loss = 0.30696412\n",
      "Iteration 23, loss = 0.30285342\n",
      "Iteration 24, loss = 0.29899669\n",
      "Iteration 25, loss = 0.29525469\n",
      "Iteration 26, loss = 0.29170903\n",
      "Iteration 27, loss = 0.28830517\n",
      "Iteration 28, loss = 0.28495376\n",
      "Iteration 29, loss = 0.28190327\n",
      "Iteration 30, loss = 0.27875858\n",
      "Iteration 31, loss = 0.27574887\n",
      "Iteration 32, loss = 0.27284397\n",
      "Iteration 33, loss = 0.26995367\n",
      "Iteration 34, loss = 0.26713912\n",
      "Iteration 35, loss = 0.26448577\n",
      "Iteration 36, loss = 0.26184312\n",
      "Iteration 37, loss = 0.25930639\n",
      "Iteration 38, loss = 0.25687773\n",
      "Iteration 39, loss = 0.25440258\n",
      "Iteration 40, loss = 0.25201372\n",
      "Iteration 41, loss = 0.24973218\n",
      "Iteration 42, loss = 0.24744469\n",
      "Iteration 43, loss = 0.24526872\n",
      "Iteration 44, loss = 0.24296982\n",
      "Iteration 45, loss = 0.24093218\n",
      "Iteration 46, loss = 0.23879957\n",
      "Iteration 47, loss = 0.23672508\n",
      "Iteration 48, loss = 0.23473660\n",
      "Iteration 49, loss = 0.23273493\n",
      "Iteration 50, loss = 0.23080045\n",
      "Iteration 51, loss = 0.22897368\n",
      "Iteration 52, loss = 0.22709548\n",
      "Iteration 53, loss = 0.22528143\n",
      "Iteration 54, loss = 0.22356107\n",
      "Iteration 55, loss = 0.22174721\n",
      "Iteration 56, loss = 0.22000517\n",
      "Iteration 57, loss = 0.21839030\n",
      "Iteration 58, loss = 0.21676160\n",
      "Iteration 59, loss = 0.21505591\n",
      "Iteration 60, loss = 0.21343359\n",
      "Iteration 61, loss = 0.21188798\n",
      "Iteration 62, loss = 0.21030585\n",
      "Iteration 63, loss = 0.20887687\n",
      "Iteration 64, loss = 0.20735000\n",
      "Iteration 65, loss = 0.20588947\n",
      "Iteration 66, loss = 0.20438407\n",
      "Iteration 67, loss = 0.20301537\n",
      "Iteration 68, loss = 0.20161103\n",
      "Iteration 69, loss = 0.20015436\n",
      "Iteration 70, loss = 0.19882264\n",
      "Iteration 71, loss = 0.19751096\n",
      "Iteration 72, loss = 0.19620724\n",
      "Iteration 73, loss = 0.19484301\n",
      "Iteration 74, loss = 0.19352138\n",
      "Iteration 75, loss = 0.19224160\n",
      "Iteration 76, loss = 0.19100587\n",
      "Iteration 77, loss = 0.18978915\n",
      "Iteration 78, loss = 0.18857094\n",
      "Iteration 79, loss = 0.18732108\n",
      "Iteration 80, loss = 0.18612504\n",
      "Iteration 81, loss = 0.18491497\n",
      "Iteration 82, loss = 0.18385299\n",
      "Iteration 83, loss = 0.18269381\n",
      "Iteration 84, loss = 0.18156782\n",
      "Iteration 85, loss = 0.18032061\n",
      "Iteration 86, loss = 0.17926860\n",
      "Iteration 87, loss = 0.17819531\n",
      "Iteration 88, loss = 0.17709908\n",
      "Iteration 89, loss = 0.17602707\n",
      "Iteration 90, loss = 0.17497397\n",
      "Iteration 91, loss = 0.17395341\n",
      "Iteration 92, loss = 0.17289500\n",
      "Iteration 93, loss = 0.17187561\n",
      "Iteration 94, loss = 0.17092025\n",
      "Iteration 95, loss = 0.16988362\n",
      "Iteration 96, loss = 0.16883829\n",
      "Iteration 97, loss = 0.16793876\n",
      "Iteration 98, loss = 0.16699857\n",
      "Iteration 99, loss = 0.16597230\n",
      "Iteration 100, loss = 0.16513055\n",
      "Iteration 101, loss = 0.16416960\n",
      "Iteration 102, loss = 0.16321122\n",
      "Iteration 103, loss = 0.16220202\n",
      "Iteration 104, loss = 0.16136571\n",
      "Iteration 105, loss = 0.16048334\n",
      "Iteration 106, loss = 0.15965913\n",
      "Iteration 107, loss = 0.15873011\n",
      "Iteration 108, loss = 0.15781294\n",
      "Iteration 109, loss = 0.15700386\n",
      "Iteration 110, loss = 0.15622135\n",
      "Iteration 111, loss = 0.15532993\n",
      "Iteration 112, loss = 0.15453686\n",
      "Iteration 113, loss = 0.15370945\n",
      "Iteration 114, loss = 0.15288204\n",
      "Iteration 115, loss = 0.15205228\n",
      "Iteration 116, loss = 0.15136162\n",
      "Iteration 117, loss = 0.15050758\n",
      "Iteration 118, loss = 0.14968541\n",
      "Iteration 119, loss = 0.14888088\n",
      "Iteration 120, loss = 0.14818876\n",
      "Iteration 121, loss = 0.14745072\n",
      "Iteration 122, loss = 0.14665454\n",
      "Iteration 123, loss = 0.14592724\n",
      "Iteration 124, loss = 0.14521746\n",
      "Iteration 125, loss = 0.14445363\n",
      "Iteration 126, loss = 0.14379097\n",
      "Iteration 127, loss = 0.14299784\n",
      "Iteration 128, loss = 0.14239731\n",
      "Iteration 129, loss = 0.14164721\n",
      "Iteration 130, loss = 0.14094199\n",
      "Iteration 131, loss = 0.14026748\n",
      "Iteration 132, loss = 0.13958437\n",
      "Iteration 133, loss = 0.13889446\n",
      "Iteration 134, loss = 0.13824313\n",
      "Iteration 135, loss = 0.13752834\n",
      "Iteration 136, loss = 0.13694543\n",
      "Iteration 137, loss = 0.13626539\n",
      "Iteration 138, loss = 0.13565007\n",
      "Iteration 139, loss = 0.13496083\n",
      "Iteration 140, loss = 0.13428525\n",
      "Iteration 141, loss = 0.13370820\n",
      "Iteration 142, loss = 0.13311207\n",
      "Iteration 143, loss = 0.13245513\n",
      "Iteration 144, loss = 0.13188632\n",
      "Iteration 145, loss = 0.13126708\n",
      "Iteration 146, loss = 0.13059603\n",
      "Iteration 147, loss = 0.13000300\n",
      "Iteration 148, loss = 0.12947532\n",
      "Iteration 149, loss = 0.12891001\n",
      "Iteration 150, loss = 0.12822947\n",
      "Iteration 151, loss = 0.12778410\n",
      "Iteration 152, loss = 0.12707026\n",
      "Iteration 153, loss = 0.12657460\n",
      "Iteration 154, loss = 0.12601449\n",
      "Iteration 155, loss = 0.12541124\n",
      "Iteration 156, loss = 0.12488428\n",
      "Iteration 157, loss = 0.12432248\n",
      "Iteration 158, loss = 0.12377805\n",
      "Iteration 159, loss = 0.12325377\n",
      "Iteration 160, loss = 0.12268129\n",
      "Iteration 161, loss = 0.12216760\n",
      "Iteration 162, loss = 0.12161783\n",
      "Iteration 163, loss = 0.12108789\n",
      "Iteration 164, loss = 0.12061662\n",
      "Iteration 165, loss = 0.12004310\n",
      "Iteration 166, loss = 0.11956271\n",
      "Iteration 167, loss = 0.11902672\n",
      "Iteration 168, loss = 0.11853761\n",
      "Iteration 169, loss = 0.11807505\n",
      "Iteration 170, loss = 0.11756598\n",
      "Iteration 171, loss = 0.11709694\n",
      "Iteration 172, loss = 0.11656117\n",
      "Iteration 173, loss = 0.11607214\n",
      "Iteration 174, loss = 0.11569980\n",
      "Iteration 175, loss = 0.11514681\n",
      "Iteration 176, loss = 0.11461972\n",
      "Iteration 177, loss = 0.11420698\n",
      "Iteration 178, loss = 0.11375180\n",
      "Iteration 179, loss = 0.11323607\n",
      "Iteration 180, loss = 0.11278941\n",
      "Iteration 181, loss = 0.11233828\n",
      "Iteration 182, loss = 0.11190444\n",
      "Iteration 183, loss = 0.11143245\n",
      "Iteration 184, loss = 0.11099892\n",
      "Iteration 185, loss = 0.11056478\n",
      "Iteration 186, loss = 0.11010464\n",
      "Iteration 187, loss = 0.10963657\n",
      "Iteration 188, loss = 0.10923179\n",
      "Iteration 189, loss = 0.10877318\n",
      "Iteration 190, loss = 0.10833694\n",
      "Iteration 191, loss = 0.10791526\n",
      "Iteration 192, loss = 0.10753866\n",
      "Iteration 193, loss = 0.10707632\n",
      "Iteration 194, loss = 0.10665874\n",
      "Iteration 195, loss = 0.10625072\n",
      "Iteration 196, loss = 0.10583446\n",
      "Iteration 197, loss = 0.10545589\n",
      "Iteration 198, loss = 0.10499439\n",
      "Iteration 199, loss = 0.10459956\n",
      "Iteration 200, loss = 0.10420066\n",
      "[CV] END activation=relu, alpha=0.001, batch_size=200, hidden_layer_sizes=(100,), solver=sgd; total time= 1.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.87752431\n",
      "Iteration 2, loss = 1.08650229\n",
      "Iteration 3, loss = 0.75915013\n",
      "Iteration 4, loss = 0.61845747\n",
      "Iteration 5, loss = 0.54145452\n",
      "Iteration 6, loss = 0.49225184\n",
      "Iteration 7, loss = 0.45785730\n",
      "Iteration 8, loss = 0.43193507\n",
      "Iteration 9, loss = 0.41158374\n",
      "Iteration 10, loss = 0.39506927\n",
      "Iteration 11, loss = 0.38116111\n",
      "Iteration 12, loss = 0.36944734\n",
      "Iteration 13, loss = 0.35918504\n",
      "Iteration 14, loss = 0.35009881\n",
      "Iteration 15, loss = 0.34211285\n",
      "Iteration 16, loss = 0.33482207\n",
      "Iteration 17, loss = 0.32822912\n",
      "Iteration 18, loss = 0.32225503\n",
      "Iteration 19, loss = 0.31667198\n",
      "Iteration 20, loss = 0.31164673\n",
      "Iteration 21, loss = 0.30666949\n",
      "Iteration 22, loss = 0.30217641\n",
      "Iteration 23, loss = 0.29781732\n",
      "Iteration 24, loss = 0.29401383\n",
      "Iteration 25, loss = 0.29013802\n",
      "Iteration 26, loss = 0.28634024\n",
      "Iteration 27, loss = 0.28313816\n",
      "Iteration 28, loss = 0.27968880\n",
      "Iteration 29, loss = 0.27662786\n",
      "Iteration 30, loss = 0.27343100\n",
      "Iteration 31, loss = 0.27050898\n",
      "Iteration 32, loss = 0.26776219\n",
      "Iteration 33, loss = 0.26515679\n",
      "Iteration 34, loss = 0.26232674\n",
      "Iteration 35, loss = 0.25990357\n",
      "Iteration 36, loss = 0.25734926\n",
      "Iteration 37, loss = 0.25502798\n",
      "Iteration 38, loss = 0.25270028\n",
      "Iteration 39, loss = 0.25044059\n",
      "Iteration 40, loss = 0.24819924\n",
      "Iteration 41, loss = 0.24597856\n",
      "Iteration 42, loss = 0.24387203\n",
      "Iteration 43, loss = 0.24176897\n",
      "Iteration 44, loss = 0.23973573\n",
      "Iteration 45, loss = 0.23789303\n",
      "Iteration 46, loss = 0.23585210\n",
      "Iteration 47, loss = 0.23398623\n",
      "Iteration 48, loss = 0.23217035\n",
      "Iteration 49, loss = 0.23028122\n",
      "Iteration 50, loss = 0.22848100\n",
      "Iteration 51, loss = 0.22674439\n",
      "Iteration 52, loss = 0.22510066\n",
      "Iteration 53, loss = 0.22329652\n",
      "Iteration 54, loss = 0.22173058\n",
      "Iteration 55, loss = 0.22010404\n",
      "Iteration 56, loss = 0.21844607\n",
      "Iteration 57, loss = 0.21697635\n",
      "Iteration 58, loss = 0.21534868\n",
      "Iteration 59, loss = 0.21379430\n",
      "Iteration 60, loss = 0.21231007\n",
      "Iteration 61, loss = 0.21078367\n",
      "Iteration 62, loss = 0.20931580\n",
      "Iteration 63, loss = 0.20791513\n",
      "Iteration 64, loss = 0.20644394\n",
      "Iteration 65, loss = 0.20506302\n",
      "Iteration 66, loss = 0.20369767\n",
      "Iteration 67, loss = 0.20233051\n",
      "Iteration 68, loss = 0.20088797\n",
      "Iteration 69, loss = 0.19957918\n",
      "Iteration 70, loss = 0.19825282\n",
      "Iteration 71, loss = 0.19695838\n",
      "Iteration 72, loss = 0.19575981\n",
      "Iteration 73, loss = 0.19449009\n",
      "Iteration 74, loss = 0.19323504\n",
      "Iteration 75, loss = 0.19196836\n",
      "Iteration 76, loss = 0.19076953\n",
      "Iteration 77, loss = 0.18955785\n",
      "Iteration 78, loss = 0.18835894\n",
      "Iteration 79, loss = 0.18717627\n",
      "Iteration 80, loss = 0.18601167\n",
      "Iteration 81, loss = 0.18487806\n",
      "Iteration 82, loss = 0.18376386\n",
      "Iteration 83, loss = 0.18266254\n",
      "Iteration 84, loss = 0.18149892\n",
      "Iteration 85, loss = 0.18031406\n",
      "Iteration 86, loss = 0.17933535\n",
      "Iteration 87, loss = 0.17822741\n",
      "Iteration 88, loss = 0.17712581\n",
      "Iteration 89, loss = 0.17618524\n",
      "Iteration 90, loss = 0.17506523\n",
      "Iteration 91, loss = 0.17408892\n",
      "Iteration 92, loss = 0.17306539\n",
      "Iteration 93, loss = 0.17203290\n",
      "Iteration 94, loss = 0.17100861\n",
      "Iteration 95, loss = 0.17003894\n",
      "Iteration 96, loss = 0.16907367\n",
      "Iteration 97, loss = 0.16810984\n",
      "Iteration 98, loss = 0.16713317\n",
      "Iteration 99, loss = 0.16621347\n",
      "Iteration 100, loss = 0.16519983\n",
      "Iteration 101, loss = 0.16431432\n",
      "Iteration 102, loss = 0.16341454\n",
      "Iteration 103, loss = 0.16243739\n",
      "Iteration 104, loss = 0.16160222\n",
      "Iteration 105, loss = 0.16078763\n",
      "Iteration 106, loss = 0.15980929\n",
      "Iteration 107, loss = 0.15893926\n",
      "Iteration 108, loss = 0.15809425\n",
      "Iteration 109, loss = 0.15724423\n",
      "Iteration 110, loss = 0.15639048\n",
      "Iteration 111, loss = 0.15559351\n",
      "Iteration 112, loss = 0.15469764\n",
      "Iteration 113, loss = 0.15396108\n",
      "Iteration 114, loss = 0.15309068\n",
      "Iteration 115, loss = 0.15223067\n",
      "Iteration 116, loss = 0.15147597\n",
      "Iteration 117, loss = 0.15069652\n",
      "Iteration 118, loss = 0.14984220\n",
      "Iteration 119, loss = 0.14909365\n",
      "Iteration 120, loss = 0.14830321\n",
      "Iteration 121, loss = 0.14754620\n",
      "Iteration 122, loss = 0.14673889\n",
      "Iteration 123, loss = 0.14607075\n",
      "Iteration 124, loss = 0.14527689\n",
      "Iteration 125, loss = 0.14456435\n",
      "Iteration 126, loss = 0.14379055\n",
      "Iteration 127, loss = 0.14305877\n",
      "Iteration 128, loss = 0.14238310\n",
      "Iteration 129, loss = 0.14151388\n",
      "Iteration 130, loss = 0.14093899\n",
      "Iteration 131, loss = 0.14022194\n",
      "Iteration 132, loss = 0.13952940\n",
      "Iteration 133, loss = 0.13884768\n",
      "Iteration 134, loss = 0.13818841\n",
      "Iteration 135, loss = 0.13751565\n",
      "Iteration 136, loss = 0.13681277\n",
      "Iteration 137, loss = 0.13619135\n",
      "Iteration 138, loss = 0.13549960\n",
      "Iteration 139, loss = 0.13488488\n",
      "Iteration 140, loss = 0.13414098\n",
      "Iteration 141, loss = 0.13351229\n",
      "Iteration 142, loss = 0.13289689\n",
      "Iteration 143, loss = 0.13230799\n",
      "Iteration 144, loss = 0.13169946\n",
      "Iteration 145, loss = 0.13102258\n",
      "Iteration 146, loss = 0.13042609\n",
      "Iteration 147, loss = 0.12976613\n",
      "Iteration 148, loss = 0.12919502\n",
      "Iteration 149, loss = 0.12856016\n",
      "Iteration 150, loss = 0.12798628\n",
      "Iteration 151, loss = 0.12738175\n",
      "Iteration 152, loss = 0.12688987\n",
      "Iteration 153, loss = 0.12627394\n",
      "Iteration 154, loss = 0.12569291\n",
      "Iteration 155, loss = 0.12504852\n",
      "Iteration 156, loss = 0.12452151\n",
      "Iteration 157, loss = 0.12403670\n",
      "Iteration 158, loss = 0.12344836\n",
      "Iteration 159, loss = 0.12282620\n",
      "Iteration 160, loss = 0.12237495\n",
      "Iteration 161, loss = 0.12179391\n",
      "Iteration 162, loss = 0.12116653\n",
      "Iteration 163, loss = 0.12071891\n",
      "Iteration 164, loss = 0.12013009\n",
      "Iteration 165, loss = 0.11960971\n",
      "Iteration 166, loss = 0.11910786\n",
      "Iteration 167, loss = 0.11857457\n",
      "Iteration 168, loss = 0.11804172\n",
      "Iteration 169, loss = 0.11746306\n",
      "Iteration 170, loss = 0.11701199\n",
      "Iteration 171, loss = 0.11645891\n",
      "Iteration 172, loss = 0.11596457\n",
      "Iteration 173, loss = 0.11548005\n",
      "Iteration 174, loss = 0.11499696\n",
      "Iteration 175, loss = 0.11444645\n",
      "Iteration 176, loss = 0.11403136\n",
      "Iteration 177, loss = 0.11350498\n",
      "Iteration 178, loss = 0.11300594\n",
      "Iteration 179, loss = 0.11260104\n",
      "Iteration 180, loss = 0.11202222\n",
      "Iteration 181, loss = 0.11155998\n",
      "Iteration 182, loss = 0.11110114\n",
      "Iteration 183, loss = 0.11057740\n",
      "Iteration 184, loss = 0.11020008\n",
      "Iteration 185, loss = 0.10971762\n",
      "Iteration 186, loss = 0.10922168\n",
      "Iteration 187, loss = 0.10878629\n",
      "Iteration 188, loss = 0.10831643\n",
      "Iteration 189, loss = 0.10784993\n",
      "Iteration 190, loss = 0.10735250\n",
      "Iteration 191, loss = 0.10699719\n",
      "Iteration 192, loss = 0.10653864\n",
      "Iteration 193, loss = 0.10614633\n",
      "Iteration 194, loss = 0.10568876\n",
      "Iteration 195, loss = 0.10517919\n",
      "Iteration 196, loss = 0.10476908\n",
      "Iteration 197, loss = 0.10435789\n",
      "Iteration 198, loss = 0.10396199\n",
      "Iteration 199, loss = 0.10349598\n",
      "Iteration 200, loss = 0.10310072\n",
      "[CV] END activation=relu, alpha=0.001, batch_size=200, hidden_layer_sizes=(100,), solver=sgd; total time= 1.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.74275140\n",
      "Iteration 2, loss = 0.98775732\n",
      "Iteration 3, loss = 0.71435829\n",
      "Iteration 4, loss = 0.59194419\n",
      "Iteration 5, loss = 0.52271217\n",
      "Iteration 6, loss = 0.47788967\n",
      "Iteration 7, loss = 0.44619743\n",
      "Iteration 8, loss = 0.42241757\n",
      "Iteration 9, loss = 0.40362494\n",
      "Iteration 10, loss = 0.38834454\n",
      "Iteration 11, loss = 0.37562190\n",
      "Iteration 12, loss = 0.36461577\n",
      "Iteration 13, loss = 0.35512701\n",
      "Iteration 14, loss = 0.34667607\n",
      "Iteration 15, loss = 0.33921210\n",
      "Iteration 16, loss = 0.33230809\n",
      "Iteration 17, loss = 0.32598793\n",
      "Iteration 18, loss = 0.32022121\n",
      "Iteration 19, loss = 0.31501049\n",
      "Iteration 20, loss = 0.30975742\n",
      "Iteration 21, loss = 0.30527583\n",
      "Iteration 22, loss = 0.30074559\n",
      "Iteration 23, loss = 0.29664731\n",
      "Iteration 24, loss = 0.29256556\n",
      "Iteration 25, loss = 0.28887225\n",
      "Iteration 26, loss = 0.28520156\n",
      "Iteration 27, loss = 0.28178638\n",
      "Iteration 28, loss = 0.27840792\n",
      "Iteration 29, loss = 0.27530620\n",
      "Iteration 30, loss = 0.27210338\n",
      "Iteration 31, loss = 0.26910453\n",
      "Iteration 32, loss = 0.26611030\n",
      "Iteration 33, loss = 0.26332719\n",
      "Iteration 34, loss = 0.26057671\n",
      "Iteration 35, loss = 0.25799170\n",
      "Iteration 36, loss = 0.25526982\n",
      "Iteration 37, loss = 0.25281054\n",
      "Iteration 38, loss = 0.25040552\n",
      "Iteration 39, loss = 0.24794560\n",
      "Iteration 40, loss = 0.24557232\n",
      "Iteration 41, loss = 0.24322795\n",
      "Iteration 42, loss = 0.24101618\n",
      "Iteration 43, loss = 0.23878800\n",
      "Iteration 44, loss = 0.23657187\n",
      "Iteration 45, loss = 0.23439007\n",
      "Iteration 46, loss = 0.23232190\n",
      "Iteration 47, loss = 0.23037566\n",
      "Iteration 48, loss = 0.22819622\n",
      "Iteration 49, loss = 0.22634980\n",
      "Iteration 50, loss = 0.22441826\n",
      "Iteration 51, loss = 0.22242553\n",
      "Iteration 52, loss = 0.22058208\n",
      "Iteration 53, loss = 0.21882053\n",
      "Iteration 54, loss = 0.21695210\n",
      "Iteration 55, loss = 0.21517904\n",
      "Iteration 56, loss = 0.21343459\n",
      "Iteration 57, loss = 0.21179440\n",
      "Iteration 58, loss = 0.20999138\n",
      "Iteration 59, loss = 0.20838119\n",
      "Iteration 60, loss = 0.20672877\n",
      "Iteration 61, loss = 0.20512042\n",
      "Iteration 62, loss = 0.20347745\n",
      "Iteration 63, loss = 0.20199845\n",
      "Iteration 64, loss = 0.20039435\n",
      "Iteration 65, loss = 0.19890157\n",
      "Iteration 66, loss = 0.19746601\n",
      "Iteration 67, loss = 0.19604683\n",
      "Iteration 68, loss = 0.19452368\n",
      "Iteration 69, loss = 0.19307300\n",
      "Iteration 70, loss = 0.19166146\n",
      "Iteration 71, loss = 0.19025685\n",
      "Iteration 72, loss = 0.18892498\n",
      "Iteration 73, loss = 0.18756102\n",
      "Iteration 74, loss = 0.18626416\n",
      "Iteration 75, loss = 0.18496488\n",
      "Iteration 76, loss = 0.18369496\n",
      "Iteration 77, loss = 0.18244444\n",
      "Iteration 78, loss = 0.18109539\n",
      "Iteration 79, loss = 0.17993029\n",
      "Iteration 80, loss = 0.17870112\n",
      "Iteration 81, loss = 0.17754726\n",
      "Iteration 82, loss = 0.17627364\n",
      "Iteration 83, loss = 0.17518118\n",
      "Iteration 84, loss = 0.17403035\n",
      "Iteration 85, loss = 0.17289422\n",
      "Iteration 86, loss = 0.17180454\n",
      "Iteration 87, loss = 0.17070622\n",
      "Iteration 88, loss = 0.16950893\n",
      "Iteration 89, loss = 0.16853805\n",
      "Iteration 90, loss = 0.16750107\n",
      "Iteration 91, loss = 0.16645122\n",
      "Iteration 92, loss = 0.16538760\n",
      "Iteration 93, loss = 0.16438611\n",
      "Iteration 94, loss = 0.16338830\n",
      "Iteration 95, loss = 0.16242327\n",
      "Iteration 96, loss = 0.16145424\n",
      "Iteration 97, loss = 0.16050961\n",
      "Iteration 98, loss = 0.15950774\n",
      "Iteration 99, loss = 0.15858119\n",
      "Iteration 100, loss = 0.15764070\n",
      "Iteration 101, loss = 0.15677022\n",
      "Iteration 102, loss = 0.15585445\n",
      "Iteration 103, loss = 0.15496491\n",
      "Iteration 104, loss = 0.15410077\n",
      "Iteration 105, loss = 0.15327769\n",
      "Iteration 106, loss = 0.15233604\n",
      "Iteration 107, loss = 0.15150376\n",
      "Iteration 108, loss = 0.15065655\n",
      "Iteration 109, loss = 0.14989223\n",
      "Iteration 110, loss = 0.14903485\n",
      "Iteration 111, loss = 0.14825030\n",
      "Iteration 112, loss = 0.14751593\n",
      "Iteration 113, loss = 0.14660849\n",
      "Iteration 114, loss = 0.14587900\n",
      "Iteration 115, loss = 0.14514604\n",
      "Iteration 116, loss = 0.14430809\n",
      "Iteration 117, loss = 0.14353637\n",
      "Iteration 118, loss = 0.14283373\n",
      "Iteration 119, loss = 0.14212929\n",
      "Iteration 120, loss = 0.14130318\n",
      "Iteration 121, loss = 0.14066634\n",
      "Iteration 122, loss = 0.13993537\n",
      "Iteration 123, loss = 0.13917230\n",
      "Iteration 124, loss = 0.13852868\n",
      "Iteration 125, loss = 0.13778587\n",
      "Iteration 126, loss = 0.13715731\n",
      "Iteration 127, loss = 0.13648444\n",
      "Iteration 128, loss = 0.13578233\n",
      "Iteration 129, loss = 0.13509422\n",
      "Iteration 130, loss = 0.13447330\n",
      "Iteration 131, loss = 0.13373540\n",
      "Iteration 132, loss = 0.13311586\n",
      "Iteration 133, loss = 0.13251865\n",
      "Iteration 134, loss = 0.13189684\n",
      "Iteration 135, loss = 0.13125092\n",
      "Iteration 136, loss = 0.13059204\n",
      "Iteration 137, loss = 0.12998959\n",
      "Iteration 138, loss = 0.12939295\n",
      "Iteration 139, loss = 0.12878826\n",
      "Iteration 140, loss = 0.12818983\n",
      "Iteration 141, loss = 0.12755528\n",
      "Iteration 142, loss = 0.12701306\n",
      "Iteration 143, loss = 0.12639602\n",
      "Iteration 144, loss = 0.12581446\n",
      "Iteration 145, loss = 0.12520329\n",
      "Iteration 146, loss = 0.12467655\n",
      "Iteration 147, loss = 0.12412030\n",
      "Iteration 148, loss = 0.12354900\n",
      "Iteration 149, loss = 0.12292720\n",
      "Iteration 150, loss = 0.12246219\n",
      "Iteration 151, loss = 0.12186950\n",
      "Iteration 152, loss = 0.12137914\n",
      "Iteration 153, loss = 0.12076480\n",
      "Iteration 154, loss = 0.12030001\n",
      "Iteration 155, loss = 0.11976696\n",
      "Iteration 156, loss = 0.11917286\n",
      "Iteration 157, loss = 0.11877711\n",
      "Iteration 158, loss = 0.11823003\n",
      "Iteration 159, loss = 0.11768664\n",
      "Iteration 160, loss = 0.11717340\n",
      "Iteration 161, loss = 0.11666882\n",
      "Iteration 162, loss = 0.11614931\n",
      "Iteration 163, loss = 0.11564873\n",
      "Iteration 164, loss = 0.11515924\n",
      "Iteration 165, loss = 0.11467663\n",
      "Iteration 166, loss = 0.11416746\n",
      "Iteration 167, loss = 0.11372493\n",
      "Iteration 168, loss = 0.11327335\n",
      "Iteration 169, loss = 0.11276158\n",
      "Iteration 170, loss = 0.11230516\n",
      "Iteration 171, loss = 0.11180734\n",
      "Iteration 172, loss = 0.11139283\n",
      "Iteration 173, loss = 0.11095269\n",
      "Iteration 174, loss = 0.11049364\n",
      "Iteration 175, loss = 0.10999760\n",
      "Iteration 176, loss = 0.10952825\n",
      "Iteration 177, loss = 0.10908852\n",
      "Iteration 178, loss = 0.10864236\n",
      "Iteration 179, loss = 0.10821233\n",
      "Iteration 180, loss = 0.10777829\n",
      "Iteration 181, loss = 0.10735914\n",
      "Iteration 182, loss = 0.10697286\n",
      "Iteration 183, loss = 0.10647333\n",
      "Iteration 184, loss = 0.10614428\n",
      "Iteration 185, loss = 0.10565765\n",
      "Iteration 186, loss = 0.10523057\n",
      "Iteration 187, loss = 0.10483389\n",
      "Iteration 188, loss = 0.10439980\n",
      "Iteration 189, loss = 0.10401115\n",
      "Iteration 190, loss = 0.10357053\n",
      "Iteration 191, loss = 0.10319325\n",
      "Iteration 192, loss = 0.10274802\n",
      "Iteration 193, loss = 0.10238149\n",
      "Iteration 194, loss = 0.10200393\n",
      "Iteration 195, loss = 0.10158294\n",
      "Iteration 196, loss = 0.10117779\n",
      "Iteration 197, loss = 0.10075181\n",
      "Iteration 198, loss = 0.10042481\n",
      "Iteration 199, loss = 0.10006308\n",
      "Iteration 200, loss = 0.09964890\n",
      "[CV] END activation=relu, alpha=0.001, batch_size=200, hidden_layer_sizes=(100,), solver=sgd; total time= 1.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.78289628\n",
      "Iteration 2, loss = 1.03869390\n",
      "Iteration 3, loss = 0.73932696\n",
      "Iteration 4, loss = 0.60637747\n",
      "Iteration 5, loss = 0.53252420\n",
      "Iteration 6, loss = 0.48520095\n",
      "Iteration 7, loss = 0.45199208\n",
      "Iteration 8, loss = 0.42696382\n",
      "Iteration 9, loss = 0.40755594\n",
      "Iteration 10, loss = 0.39177875\n",
      "Iteration 11, loss = 0.37861784\n",
      "Iteration 12, loss = 0.36754388\n",
      "Iteration 13, loss = 0.35799939\n",
      "Iteration 14, loss = 0.34942782\n",
      "Iteration 15, loss = 0.34165950\n",
      "Iteration 16, loss = 0.33492447\n",
      "Iteration 17, loss = 0.32851952\n",
      "Iteration 18, loss = 0.32286418\n",
      "Iteration 19, loss = 0.31753444\n",
      "Iteration 20, loss = 0.31246592\n",
      "Iteration 21, loss = 0.30783600\n",
      "Iteration 22, loss = 0.30340227\n",
      "Iteration 23, loss = 0.29922450\n",
      "Iteration 24, loss = 0.29527030\n",
      "Iteration 25, loss = 0.29145285\n",
      "Iteration 26, loss = 0.28787915\n",
      "Iteration 27, loss = 0.28444727\n",
      "Iteration 28, loss = 0.28102491\n",
      "Iteration 29, loss = 0.27778000\n",
      "Iteration 30, loss = 0.27466702\n",
      "Iteration 31, loss = 0.27170603\n",
      "Iteration 32, loss = 0.26873353\n",
      "Iteration 33, loss = 0.26591260\n",
      "Iteration 34, loss = 0.26321453\n",
      "Iteration 35, loss = 0.26054001\n",
      "Iteration 36, loss = 0.25790980\n",
      "Iteration 37, loss = 0.25528705\n",
      "Iteration 38, loss = 0.25284772\n",
      "Iteration 39, loss = 0.25045944\n",
      "Iteration 40, loss = 0.24811162\n",
      "Iteration 41, loss = 0.24564267\n",
      "Iteration 42, loss = 0.24349999\n",
      "Iteration 43, loss = 0.24122471\n",
      "Iteration 44, loss = 0.23913137\n",
      "Iteration 45, loss = 0.23694782\n",
      "Iteration 46, loss = 0.23497384\n",
      "Iteration 47, loss = 0.23293284\n",
      "Iteration 48, loss = 0.23095232\n",
      "Iteration 49, loss = 0.22900805\n",
      "Iteration 50, loss = 0.22706437\n",
      "Iteration 51, loss = 0.22520305\n",
      "Iteration 52, loss = 0.22329636\n",
      "Iteration 53, loss = 0.22145081\n",
      "Iteration 54, loss = 0.21974364\n",
      "Iteration 55, loss = 0.21804597\n",
      "Iteration 56, loss = 0.21634636\n",
      "Iteration 57, loss = 0.21467152\n",
      "Iteration 58, loss = 0.21298703\n",
      "Iteration 59, loss = 0.21121657\n",
      "Iteration 60, loss = 0.20970542\n",
      "Iteration 61, loss = 0.20808847\n",
      "Iteration 62, loss = 0.20657563\n",
      "Iteration 63, loss = 0.20500858\n",
      "Iteration 64, loss = 0.20347620\n",
      "Iteration 65, loss = 0.20200007\n",
      "Iteration 66, loss = 0.20048404\n",
      "Iteration 67, loss = 0.19908714\n",
      "Iteration 68, loss = 0.19764690\n",
      "Iteration 69, loss = 0.19623694\n",
      "Iteration 70, loss = 0.19480892\n",
      "Iteration 71, loss = 0.19349443\n",
      "Iteration 72, loss = 0.19212002\n",
      "Iteration 73, loss = 0.19088655\n",
      "Iteration 74, loss = 0.18952912\n",
      "Iteration 75, loss = 0.18817145\n",
      "Iteration 76, loss = 0.18702246\n",
      "Iteration 77, loss = 0.18574820\n",
      "Iteration 78, loss = 0.18446951\n",
      "Iteration 79, loss = 0.18331094\n",
      "Iteration 80, loss = 0.18211772\n",
      "Iteration 81, loss = 0.18097696\n",
      "Iteration 82, loss = 0.17977430\n",
      "Iteration 83, loss = 0.17860514\n",
      "Iteration 84, loss = 0.17751939\n",
      "Iteration 85, loss = 0.17638993\n",
      "Iteration 86, loss = 0.17518266\n",
      "Iteration 87, loss = 0.17415176\n",
      "Iteration 88, loss = 0.17308144\n",
      "Iteration 89, loss = 0.17207632\n",
      "Iteration 90, loss = 0.17097210\n",
      "Iteration 91, loss = 0.16992965\n",
      "Iteration 92, loss = 0.16887439\n",
      "Iteration 93, loss = 0.16795900\n",
      "Iteration 94, loss = 0.16689857\n",
      "Iteration 95, loss = 0.16590341\n",
      "Iteration 96, loss = 0.16495113\n",
      "Iteration 97, loss = 0.16395452\n",
      "Iteration 98, loss = 0.16304026\n",
      "Iteration 99, loss = 0.16199831\n",
      "Iteration 100, loss = 0.16107895\n",
      "Iteration 101, loss = 0.16016071\n",
      "Iteration 102, loss = 0.15928123\n",
      "Iteration 103, loss = 0.15838874\n",
      "Iteration 104, loss = 0.15745911\n",
      "Iteration 105, loss = 0.15660808\n",
      "Iteration 106, loss = 0.15570637\n",
      "Iteration 107, loss = 0.15482709\n",
      "Iteration 108, loss = 0.15400456\n",
      "Iteration 109, loss = 0.15316746\n",
      "Iteration 110, loss = 0.15228128\n",
      "Iteration 111, loss = 0.15140881\n",
      "Iteration 112, loss = 0.15061631\n",
      "Iteration 113, loss = 0.14981449\n",
      "Iteration 114, loss = 0.14894741\n",
      "Iteration 115, loss = 0.14811490\n",
      "Iteration 116, loss = 0.14748116\n",
      "Iteration 117, loss = 0.14659731\n",
      "Iteration 118, loss = 0.14587078\n",
      "Iteration 119, loss = 0.14507776\n",
      "Iteration 120, loss = 0.14435146\n",
      "Iteration 121, loss = 0.14358062\n",
      "Iteration 122, loss = 0.14282739\n",
      "Iteration 123, loss = 0.14211880\n",
      "Iteration 124, loss = 0.14132616\n",
      "Iteration 125, loss = 0.14059789\n",
      "Iteration 126, loss = 0.13991882\n",
      "Iteration 127, loss = 0.13915348\n",
      "Iteration 128, loss = 0.13845408\n",
      "Iteration 129, loss = 0.13784625\n",
      "Iteration 130, loss = 0.13712439\n",
      "Iteration 131, loss = 0.13640521\n",
      "Iteration 132, loss = 0.13571723\n",
      "Iteration 133, loss = 0.13509336\n",
      "Iteration 134, loss = 0.13431075\n",
      "Iteration 135, loss = 0.13374760\n",
      "Iteration 136, loss = 0.13310446\n",
      "Iteration 137, loss = 0.13240328\n",
      "Iteration 138, loss = 0.13182731\n",
      "Iteration 139, loss = 0.13116426\n",
      "Iteration 140, loss = 0.13047560\n",
      "Iteration 141, loss = 0.12984082\n",
      "Iteration 142, loss = 0.12923564\n",
      "Iteration 143, loss = 0.12860784\n",
      "Iteration 144, loss = 0.12805941\n",
      "Iteration 145, loss = 0.12738092\n",
      "Iteration 146, loss = 0.12675173\n",
      "Iteration 147, loss = 0.12617474\n",
      "Iteration 148, loss = 0.12557228\n",
      "Iteration 149, loss = 0.12493312\n",
      "Iteration 150, loss = 0.12439976\n",
      "Iteration 151, loss = 0.12378265\n",
      "Iteration 152, loss = 0.12324791\n",
      "Iteration 153, loss = 0.12272106\n",
      "Iteration 154, loss = 0.12213644\n",
      "Iteration 155, loss = 0.12159278\n",
      "Iteration 156, loss = 0.12100263\n",
      "Iteration 157, loss = 0.12043970\n",
      "Iteration 158, loss = 0.11982312\n",
      "Iteration 159, loss = 0.11940460\n",
      "Iteration 160, loss = 0.11877736\n",
      "Iteration 161, loss = 0.11826533\n",
      "Iteration 162, loss = 0.11779074\n",
      "Iteration 163, loss = 0.11724758\n",
      "Iteration 164, loss = 0.11668095\n",
      "Iteration 165, loss = 0.11621947\n",
      "Iteration 166, loss = 0.11570230\n",
      "Iteration 167, loss = 0.11518847\n",
      "Iteration 168, loss = 0.11459823\n",
      "Iteration 169, loss = 0.11411513\n",
      "Iteration 170, loss = 0.11363047\n",
      "Iteration 171, loss = 0.11316761\n",
      "Iteration 172, loss = 0.11265072\n",
      "Iteration 173, loss = 0.11213367\n",
      "Iteration 174, loss = 0.11169186\n",
      "Iteration 175, loss = 0.11119501\n",
      "Iteration 176, loss = 0.11075518\n",
      "Iteration 177, loss = 0.11022446\n",
      "Iteration 178, loss = 0.10976912\n",
      "Iteration 179, loss = 0.10932108\n",
      "Iteration 180, loss = 0.10887331\n",
      "Iteration 181, loss = 0.10838131\n",
      "Iteration 182, loss = 0.10789996\n",
      "Iteration 183, loss = 0.10751197\n",
      "Iteration 184, loss = 0.10700569\n",
      "Iteration 185, loss = 0.10658092\n",
      "Iteration 186, loss = 0.10609700\n",
      "Iteration 187, loss = 0.10564913\n",
      "Iteration 188, loss = 0.10518129\n",
      "Iteration 189, loss = 0.10480907\n",
      "Iteration 190, loss = 0.10439651\n",
      "Iteration 191, loss = 0.10388859\n",
      "Iteration 192, loss = 0.10345285\n",
      "Iteration 193, loss = 0.10307267\n",
      "Iteration 194, loss = 0.10264168\n",
      "Iteration 195, loss = 0.10219345\n",
      "Iteration 196, loss = 0.10182651\n",
      "Iteration 197, loss = 0.10145383\n",
      "Iteration 198, loss = 0.10099935\n",
      "Iteration 199, loss = 0.10063835\n",
      "Iteration 200, loss = 0.10019117\n",
      "[CV] END activation=relu, alpha=0.001, batch_size=200, hidden_layer_sizes=(100,), solver=sgd; total time= 1.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.37586750\n",
      "Iteration 2, loss = 0.14143693\n",
      "Iteration 3, loss = 0.09351892\n",
      "Iteration 4, loss = 0.06570541\n",
      "Iteration 5, loss = 0.04866987\n",
      "Iteration 6, loss = 0.03555339\n",
      "Iteration 7, loss = 0.02571050\n",
      "Iteration 8, loss = 0.02087945\n",
      "Iteration 9, loss = 0.01447783\n",
      "Iteration 10, loss = 0.01035831\n",
      "Iteration 11, loss = 0.00843623\n",
      "Iteration 12, loss = 0.00635568\n",
      "Iteration 13, loss = 0.00592952\n",
      "Iteration 14, loss = 0.01860142\n",
      "Iteration 15, loss = 0.02400557\n",
      "Iteration 16, loss = 0.01120631\n",
      "Iteration 17, loss = 0.00591030\n",
      "Iteration 18, loss = 0.00465210\n",
      "Iteration 19, loss = 0.00380703\n",
      "Iteration 20, loss = 0.00350083\n",
      "Iteration 21, loss = 0.00338949\n",
      "Iteration 22, loss = 0.00331826\n",
      "Iteration 23, loss = 0.00324193\n",
      "Iteration 24, loss = 0.00318629\n",
      "Iteration 25, loss = 0.00313056\n",
      "Iteration 26, loss = 0.00307850\n",
      "Iteration 27, loss = 0.00302121\n",
      "Iteration 28, loss = 0.00297005\n",
      "Iteration 29, loss = 0.00292039\n",
      "Iteration 30, loss = 0.00287360\n",
      "Iteration 31, loss = 0.00282463\n",
      "Iteration 32, loss = 0.00277500\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.001, batch_size=200, hidden_layer_sizes=(400, 100), solver=adam; total time= 1.0min\n",
      "Iteration 1, loss = 0.38414706\n",
      "Iteration 2, loss = 0.14236910\n",
      "Iteration 3, loss = 0.09602284\n",
      "Iteration 4, loss = 0.06608777\n",
      "Iteration 5, loss = 0.04801866\n",
      "Iteration 6, loss = 0.03540609\n",
      "Iteration 7, loss = 0.02787903\n",
      "Iteration 8, loss = 0.02170699\n",
      "Iteration 9, loss = 0.01740223\n",
      "Iteration 10, loss = 0.01239450\n",
      "Iteration 11, loss = 0.00905386\n",
      "Iteration 12, loss = 0.00708172\n",
      "Iteration 13, loss = 0.00541539\n",
      "Iteration 14, loss = 0.00427091\n",
      "Iteration 15, loss = 0.00386007\n",
      "Iteration 16, loss = 0.00401968\n",
      "Iteration 17, loss = 0.00346034\n",
      "Iteration 18, loss = 0.00328653\n",
      "Iteration 19, loss = 0.00323340\n",
      "Iteration 20, loss = 0.00323440\n",
      "Iteration 21, loss = 0.00306776\n",
      "Iteration 22, loss = 0.00295735\n",
      "Iteration 23, loss = 0.00289955\n",
      "Iteration 24, loss = 0.00284462\n",
      "Iteration 25, loss = 0.00278674\n",
      "Iteration 26, loss = 0.00274078\n",
      "Iteration 27, loss = 0.00269733\n",
      "Iteration 28, loss = 0.00265130\n",
      "Iteration 29, loss = 0.00260321\n",
      "Iteration 30, loss = 0.00256340\n",
      "Iteration 31, loss = 0.00251952\n",
      "Iteration 32, loss = 0.04439983\n",
      "Iteration 33, loss = 0.04046127\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.001, batch_size=200, hidden_layer_sizes=(400, 100), solver=adam; total time= 1.0min\n",
      "Iteration 1, loss = 0.39310713\n",
      "Iteration 2, loss = 0.14373415\n",
      "Iteration 3, loss = 0.09727576\n",
      "Iteration 4, loss = 0.06749347\n",
      "Iteration 5, loss = 0.04971114\n",
      "Iteration 6, loss = 0.03559398\n",
      "Iteration 7, loss = 0.02718812\n",
      "Iteration 8, loss = 0.02146735\n",
      "Iteration 9, loss = 0.01743067\n",
      "Iteration 10, loss = 0.01248921\n",
      "Iteration 11, loss = 0.01055188\n",
      "Iteration 12, loss = 0.01061005\n",
      "Iteration 13, loss = 0.00945432\n",
      "Iteration 14, loss = 0.00918384\n",
      "Iteration 15, loss = 0.00556149\n",
      "Iteration 16, loss = 0.00384674\n",
      "Iteration 17, loss = 0.00356957\n",
      "Iteration 18, loss = 0.00339037\n",
      "Iteration 19, loss = 0.00330586\n",
      "Iteration 20, loss = 0.00320641\n",
      "Iteration 21, loss = 0.00313462\n",
      "Iteration 22, loss = 0.00305121\n",
      "Iteration 23, loss = 0.00299767\n",
      "Iteration 24, loss = 0.00294522\n",
      "Iteration 25, loss = 0.00289210\n",
      "Iteration 26, loss = 0.00283589\n",
      "Iteration 27, loss = 0.00278723\n",
      "Iteration 28, loss = 0.00274008\n",
      "Iteration 29, loss = 0.00268816\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.001, batch_size=200, hidden_layer_sizes=(400, 100), solver=adam; total time=  54.7s\n",
      "Iteration 1, loss = 0.37431811\n",
      "Iteration 2, loss = 0.14426730\n",
      "Iteration 3, loss = 0.09155561\n",
      "Iteration 4, loss = 0.06595918\n",
      "Iteration 5, loss = 0.04778883\n",
      "Iteration 6, loss = 0.03801767\n",
      "Iteration 7, loss = 0.02566785\n",
      "Iteration 8, loss = 0.02085869\n",
      "Iteration 9, loss = 0.01503317\n",
      "Iteration 10, loss = 0.01225654\n",
      "Iteration 11, loss = 0.00893499\n",
      "Iteration 12, loss = 0.00911982\n",
      "Iteration 13, loss = 0.00871496\n",
      "Iteration 14, loss = 0.00525991\n",
      "Iteration 15, loss = 0.00470266\n",
      "Iteration 16, loss = 0.00397234\n",
      "Iteration 17, loss = 0.00349203\n",
      "Iteration 18, loss = 0.00331259\n",
      "Iteration 19, loss = 0.00321464\n",
      "Iteration 20, loss = 0.00311993\n",
      "Iteration 21, loss = 0.00306884\n",
      "Iteration 22, loss = 0.00300496\n",
      "Iteration 23, loss = 0.00293918\n",
      "Iteration 24, loss = 0.00288668\n",
      "Iteration 25, loss = 0.00283188\n",
      "Iteration 26, loss = 0.00277605\n",
      "Iteration 27, loss = 0.00274058\n",
      "Iteration 28, loss = 0.00268363\n",
      "Iteration 29, loss = 0.00264668\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.001, batch_size=200, hidden_layer_sizes=(400, 100), solver=adam; total time=  52.3s\n",
      "Iteration 1, loss = 0.38984484\n",
      "Iteration 2, loss = 0.14140587\n",
      "Iteration 3, loss = 0.09073626\n",
      "Iteration 4, loss = 0.06524311\n",
      "Iteration 5, loss = 0.04834000\n",
      "Iteration 6, loss = 0.03508098\n",
      "Iteration 7, loss = 0.02613733\n",
      "Iteration 8, loss = 0.01947791\n",
      "Iteration 9, loss = 0.01499178\n",
      "Iteration 10, loss = 0.01088937\n",
      "Iteration 11, loss = 0.00929755\n",
      "Iteration 12, loss = 0.00625745\n",
      "Iteration 13, loss = 0.00617859\n",
      "Iteration 14, loss = 0.00850340\n",
      "Iteration 15, loss = 0.02373901\n",
      "Iteration 16, loss = 0.01418642\n",
      "Iteration 17, loss = 0.00686214\n",
      "Iteration 18, loss = 0.00457523\n",
      "Iteration 19, loss = 0.00379963\n",
      "Iteration 20, loss = 0.00346899\n",
      "Iteration 21, loss = 0.00337144\n",
      "Iteration 22, loss = 0.00329969\n",
      "Iteration 23, loss = 0.00323727\n",
      "Iteration 24, loss = 0.00318229\n",
      "Iteration 25, loss = 0.00312227\n",
      "Iteration 26, loss = 0.00307242\n",
      "Iteration 27, loss = 0.00301837\n",
      "Iteration 28, loss = 0.00296676\n",
      "Iteration 29, loss = 0.00291796\n",
      "Iteration 30, loss = 0.00286828\n",
      "Iteration 31, loss = 0.00282049\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.001, batch_size=200, hidden_layer_sizes=(400, 100), solver=adam; total time=  58.1s\n",
      "Iteration 1, loss = 1.72333924\n",
      "Iteration 2, loss = 0.83605191\n",
      "Iteration 3, loss = 0.56420618\n",
      "Iteration 4, loss = 0.46408034\n",
      "Iteration 5, loss = 0.41192700\n",
      "Iteration 6, loss = 0.37905955\n",
      "Iteration 7, loss = 0.35542141\n",
      "Iteration 8, loss = 0.33777073\n",
      "Iteration 9, loss = 0.32313256\n",
      "Iteration 10, loss = 0.31097324\n",
      "Iteration 11, loss = 0.30046741\n",
      "Iteration 12, loss = 0.29083256\n",
      "Iteration 13, loss = 0.28268746\n",
      "Iteration 14, loss = 0.27498325\n",
      "Iteration 15, loss = 0.26779700\n",
      "Iteration 16, loss = 0.26116831\n",
      "Iteration 17, loss = 0.25520822\n",
      "Iteration 18, loss = 0.24924051\n",
      "Iteration 19, loss = 0.24358993\n",
      "Iteration 20, loss = 0.23860298\n",
      "Iteration 21, loss = 0.23351108\n",
      "Iteration 22, loss = 0.22888362\n",
      "Iteration 23, loss = 0.22409641\n",
      "Iteration 24, loss = 0.22000147\n",
      "Iteration 25, loss = 0.21565848\n",
      "Iteration 26, loss = 0.21170855\n",
      "Iteration 27, loss = 0.20790247\n",
      "Iteration 28, loss = 0.20395741\n",
      "Iteration 29, loss = 0.20033462\n",
      "Iteration 30, loss = 0.19679402\n",
      "Iteration 31, loss = 0.19343170\n",
      "Iteration 32, loss = 0.19004773\n",
      "Iteration 33, loss = 0.18698945\n",
      "Iteration 34, loss = 0.18364174\n",
      "Iteration 35, loss = 0.18052224\n",
      "Iteration 36, loss = 0.17758660\n",
      "Iteration 37, loss = 0.17484147\n",
      "Iteration 38, loss = 0.17212699\n",
      "Iteration 39, loss = 0.16920408\n",
      "Iteration 40, loss = 0.16663801\n",
      "Iteration 41, loss = 0.16404491\n",
      "Iteration 42, loss = 0.16153256\n",
      "Iteration 43, loss = 0.15923596\n",
      "Iteration 44, loss = 0.15668245\n",
      "Iteration 45, loss = 0.15444426\n",
      "Iteration 46, loss = 0.15211317\n",
      "Iteration 47, loss = 0.14986281\n",
      "Iteration 48, loss = 0.14771684\n",
      "Iteration 49, loss = 0.14559326\n",
      "Iteration 50, loss = 0.14360720\n",
      "Iteration 51, loss = 0.14152686\n",
      "Iteration 52, loss = 0.13955946\n",
      "Iteration 53, loss = 0.13773210\n",
      "Iteration 54, loss = 0.13563149\n",
      "Iteration 55, loss = 0.13397320\n",
      "Iteration 56, loss = 0.13205616\n",
      "Iteration 57, loss = 0.13029082\n",
      "Iteration 58, loss = 0.12864902\n",
      "Iteration 59, loss = 0.12690443\n",
      "Iteration 60, loss = 0.12524214\n",
      "Iteration 61, loss = 0.12358440\n",
      "Iteration 62, loss = 0.12197111\n",
      "Iteration 63, loss = 0.12041326\n",
      "Iteration 64, loss = 0.11881083\n",
      "Iteration 65, loss = 0.11748003\n",
      "Iteration 66, loss = 0.11605854\n",
      "Iteration 67, loss = 0.11444912\n",
      "Iteration 68, loss = 0.11300911\n",
      "Iteration 69, loss = 0.11157480\n",
      "Iteration 70, loss = 0.11031343\n",
      "Iteration 71, loss = 0.10887683\n",
      "Iteration 72, loss = 0.10773942\n",
      "Iteration 73, loss = 0.10628687\n",
      "Iteration 74, loss = 0.10501455\n",
      "Iteration 75, loss = 0.10382598\n",
      "Iteration 76, loss = 0.10255427\n",
      "Iteration 77, loss = 0.10138425\n",
      "Iteration 78, loss = 0.10022606\n",
      "Iteration 79, loss = 0.09896186\n",
      "Iteration 80, loss = 0.09786656\n",
      "Iteration 81, loss = 0.09673655\n",
      "Iteration 82, loss = 0.09557651\n",
      "Iteration 83, loss = 0.09455072\n",
      "Iteration 84, loss = 0.09342574\n",
      "Iteration 85, loss = 0.09243158\n",
      "Iteration 86, loss = 0.09132725\n",
      "Iteration 87, loss = 0.09031481\n",
      "Iteration 88, loss = 0.08915229\n",
      "Iteration 89, loss = 0.08829477\n",
      "Iteration 90, loss = 0.08728253\n",
      "Iteration 91, loss = 0.08626168\n",
      "Iteration 92, loss = 0.08544517\n",
      "Iteration 93, loss = 0.08438713\n",
      "Iteration 94, loss = 0.08347086\n",
      "Iteration 95, loss = 0.08266395\n",
      "Iteration 96, loss = 0.08167477\n",
      "Iteration 97, loss = 0.08077995\n",
      "Iteration 98, loss = 0.07995881\n",
      "Iteration 99, loss = 0.07908897\n",
      "Iteration 100, loss = 0.07837567\n",
      "Iteration 101, loss = 0.07737769\n",
      "Iteration 102, loss = 0.07651585\n",
      "Iteration 103, loss = 0.07582189\n",
      "Iteration 104, loss = 0.07499448\n",
      "Iteration 105, loss = 0.07417991\n",
      "Iteration 106, loss = 0.07344289\n",
      "Iteration 107, loss = 0.07265068\n",
      "Iteration 108, loss = 0.07195030\n",
      "Iteration 109, loss = 0.07122826\n",
      "Iteration 110, loss = 0.07052451\n",
      "Iteration 111, loss = 0.06964111\n",
      "Iteration 112, loss = 0.06898366\n",
      "Iteration 113, loss = 0.06822592\n",
      "Iteration 114, loss = 0.06752273\n",
      "Iteration 115, loss = 0.06692642\n",
      "Iteration 116, loss = 0.06611347\n",
      "Iteration 117, loss = 0.06550580\n",
      "Iteration 118, loss = 0.06494086\n",
      "Iteration 119, loss = 0.06407292\n",
      "Iteration 120, loss = 0.06355667\n",
      "Iteration 121, loss = 0.06296090\n",
      "Iteration 122, loss = 0.06235914\n",
      "Iteration 123, loss = 0.06171296\n",
      "Iteration 124, loss = 0.06104992\n",
      "Iteration 125, loss = 0.06053467\n",
      "Iteration 126, loss = 0.05998540\n",
      "Iteration 127, loss = 0.05944834\n",
      "Iteration 128, loss = 0.05869751\n",
      "Iteration 129, loss = 0.05812089\n",
      "Iteration 130, loss = 0.05761868\n",
      "Iteration 131, loss = 0.05706613\n",
      "Iteration 132, loss = 0.05650517\n",
      "Iteration 133, loss = 0.05589540\n",
      "Iteration 134, loss = 0.05541561\n",
      "Iteration 135, loss = 0.05502364\n",
      "Iteration 136, loss = 0.05437417\n",
      "Iteration 137, loss = 0.05378794\n",
      "Iteration 138, loss = 0.05338543\n",
      "Iteration 139, loss = 0.05280765\n",
      "Iteration 140, loss = 0.05231363\n",
      "Iteration 141, loss = 0.05187155\n",
      "Iteration 142, loss = 0.05135511\n",
      "Iteration 143, loss = 0.05087456\n",
      "Iteration 144, loss = 0.05032374\n",
      "Iteration 145, loss = 0.04993763\n",
      "Iteration 146, loss = 0.04935689\n",
      "Iteration 147, loss = 0.04887468\n",
      "Iteration 148, loss = 0.04847557\n",
      "Iteration 149, loss = 0.04809788\n",
      "Iteration 150, loss = 0.04763184\n",
      "Iteration 151, loss = 0.04710178\n",
      "Iteration 152, loss = 0.04672935\n",
      "Iteration 153, loss = 0.04629271\n",
      "Iteration 154, loss = 0.04587461\n",
      "Iteration 155, loss = 0.04548600\n",
      "Iteration 156, loss = 0.04506600\n",
      "Iteration 157, loss = 0.04461269\n",
      "Iteration 158, loss = 0.04426443\n",
      "Iteration 159, loss = 0.04375686\n",
      "Iteration 160, loss = 0.04340201\n",
      "Iteration 161, loss = 0.04303978\n",
      "Iteration 162, loss = 0.04266155\n",
      "Iteration 163, loss = 0.04222509\n",
      "Iteration 164, loss = 0.04180884\n",
      "Iteration 165, loss = 0.04143156\n",
      "Iteration 166, loss = 0.04111720\n",
      "Iteration 167, loss = 0.04074418\n",
      "Iteration 168, loss = 0.04036386\n",
      "Iteration 169, loss = 0.03995859\n",
      "Iteration 170, loss = 0.03964522\n",
      "Iteration 171, loss = 0.03936823\n",
      "Iteration 172, loss = 0.03902231\n",
      "Iteration 173, loss = 0.03867526\n",
      "Iteration 174, loss = 0.03834405\n",
      "Iteration 175, loss = 0.03799133\n",
      "Iteration 176, loss = 0.03762754\n",
      "Iteration 177, loss = 0.03738112\n",
      "Iteration 178, loss = 0.03692156\n",
      "Iteration 179, loss = 0.03662733\n",
      "Iteration 180, loss = 0.03641644\n",
      "Iteration 181, loss = 0.03607730\n",
      "Iteration 182, loss = 0.03571154\n",
      "Iteration 183, loss = 0.03540939\n",
      "Iteration 184, loss = 0.03512622\n",
      "Iteration 185, loss = 0.03479443\n",
      "Iteration 186, loss = 0.03453728\n",
      "Iteration 187, loss = 0.03421449\n",
      "Iteration 188, loss = 0.03396558\n",
      "Iteration 189, loss = 0.03363704\n",
      "Iteration 190, loss = 0.03335598\n",
      "Iteration 191, loss = 0.03309086\n",
      "Iteration 192, loss = 0.03276766\n",
      "Iteration 193, loss = 0.03251997\n",
      "Iteration 194, loss = 0.03226444\n",
      "Iteration 195, loss = 0.03201479\n",
      "Iteration 196, loss = 0.03172613\n",
      "Iteration 197, loss = 0.03150737\n",
      "Iteration 198, loss = 0.03128980\n",
      "Iteration 199, loss = 0.03096646\n",
      "Iteration 200, loss = 0.03070824\n",
      "[CV] END activation=relu, alpha=0.001, batch_size=200, hidden_layer_sizes=(400, 100), solver=sgd; total time= 3.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.83717084\n",
      "Iteration 2, loss = 0.94295699\n",
      "Iteration 3, loss = 0.61526864\n",
      "Iteration 4, loss = 0.49433781\n",
      "Iteration 5, loss = 0.43252527\n",
      "Iteration 6, loss = 0.39425679\n",
      "Iteration 7, loss = 0.36768687\n",
      "Iteration 8, loss = 0.34747300\n",
      "Iteration 9, loss = 0.33111510\n",
      "Iteration 10, loss = 0.31765955\n",
      "Iteration 11, loss = 0.30648385\n",
      "Iteration 12, loss = 0.29615248\n",
      "Iteration 13, loss = 0.28730148\n",
      "Iteration 14, loss = 0.27903554\n",
      "Iteration 15, loss = 0.27149409\n",
      "Iteration 16, loss = 0.26449233\n",
      "Iteration 17, loss = 0.25806221\n",
      "Iteration 18, loss = 0.25210200\n",
      "Iteration 19, loss = 0.24648058\n",
      "Iteration 20, loss = 0.24103756\n",
      "Iteration 21, loss = 0.23595820\n",
      "Iteration 22, loss = 0.23139404\n",
      "Iteration 23, loss = 0.22661732\n",
      "Iteration 24, loss = 0.22218488\n",
      "Iteration 25, loss = 0.21788042\n",
      "Iteration 26, loss = 0.21384483\n",
      "Iteration 27, loss = 0.20986025\n",
      "Iteration 28, loss = 0.20617576\n",
      "Iteration 29, loss = 0.20255363\n",
      "Iteration 30, loss = 0.19901041\n",
      "Iteration 31, loss = 0.19570620\n",
      "Iteration 32, loss = 0.19231513\n",
      "Iteration 33, loss = 0.18927027\n",
      "Iteration 34, loss = 0.18613520\n",
      "Iteration 35, loss = 0.18304453\n",
      "Iteration 36, loss = 0.18038657\n",
      "Iteration 37, loss = 0.17731488\n",
      "Iteration 38, loss = 0.17481710\n",
      "Iteration 39, loss = 0.17245917\n",
      "Iteration 40, loss = 0.16945291\n",
      "Iteration 41, loss = 0.16712735\n",
      "Iteration 42, loss = 0.16458794\n",
      "Iteration 43, loss = 0.16224219\n",
      "Iteration 44, loss = 0.15998148\n",
      "Iteration 45, loss = 0.15758446\n",
      "Iteration 46, loss = 0.15557312\n",
      "Iteration 47, loss = 0.15332675\n",
      "Iteration 48, loss = 0.15118172\n",
      "Iteration 49, loss = 0.14911424\n",
      "Iteration 50, loss = 0.14744096\n",
      "Iteration 51, loss = 0.14523951\n",
      "Iteration 52, loss = 0.14331495\n",
      "Iteration 53, loss = 0.14160164\n",
      "Iteration 54, loss = 0.13963089\n",
      "Iteration 55, loss = 0.13778260\n",
      "Iteration 56, loss = 0.13610856\n",
      "Iteration 57, loss = 0.13455388\n",
      "Iteration 58, loss = 0.13276280\n",
      "Iteration 59, loss = 0.13095555\n",
      "Iteration 60, loss = 0.12934868\n",
      "Iteration 61, loss = 0.12787300\n",
      "Iteration 62, loss = 0.12628703\n",
      "Iteration 63, loss = 0.12481396\n",
      "Iteration 64, loss = 0.12316576\n",
      "Iteration 65, loss = 0.12183148\n",
      "Iteration 66, loss = 0.12021878\n",
      "Iteration 67, loss = 0.11883055\n",
      "Iteration 68, loss = 0.11744902\n",
      "Iteration 69, loss = 0.11618083\n",
      "Iteration 70, loss = 0.11469178\n",
      "Iteration 71, loss = 0.11343754\n",
      "Iteration 72, loss = 0.11204146\n",
      "Iteration 73, loss = 0.11085626\n",
      "Iteration 74, loss = 0.10957937\n",
      "Iteration 75, loss = 0.10836839\n",
      "Iteration 76, loss = 0.10701550\n",
      "Iteration 77, loss = 0.10575043\n",
      "Iteration 78, loss = 0.10476434\n",
      "Iteration 79, loss = 0.10343156\n",
      "Iteration 80, loss = 0.10233612\n",
      "Iteration 81, loss = 0.10130360\n",
      "Iteration 82, loss = 0.10003324\n",
      "Iteration 83, loss = 0.09895728\n",
      "Iteration 84, loss = 0.09787628\n",
      "Iteration 85, loss = 0.09676430\n",
      "Iteration 86, loss = 0.09577756\n",
      "Iteration 87, loss = 0.09473920\n",
      "Iteration 88, loss = 0.09360627\n",
      "Iteration 89, loss = 0.09264342\n",
      "Iteration 90, loss = 0.09163208\n",
      "Iteration 91, loss = 0.09081578\n",
      "Iteration 92, loss = 0.08972590\n",
      "Iteration 93, loss = 0.08873305\n",
      "Iteration 94, loss = 0.08786722\n",
      "Iteration 95, loss = 0.08693327\n",
      "Iteration 96, loss = 0.08604657\n",
      "Iteration 97, loss = 0.08519827\n",
      "Iteration 98, loss = 0.08414145\n",
      "Iteration 99, loss = 0.08335267\n",
      "Iteration 100, loss = 0.08240456\n",
      "Iteration 101, loss = 0.08164608\n",
      "Iteration 102, loss = 0.08072533\n",
      "Iteration 103, loss = 0.07997002\n",
      "Iteration 104, loss = 0.07910751\n",
      "Iteration 105, loss = 0.07833456\n",
      "Iteration 106, loss = 0.07757333\n",
      "Iteration 107, loss = 0.07675218\n",
      "Iteration 108, loss = 0.07598333\n",
      "Iteration 109, loss = 0.07521025\n",
      "Iteration 110, loss = 0.07445708\n",
      "Iteration 111, loss = 0.07365282\n",
      "Iteration 112, loss = 0.07297358\n",
      "Iteration 113, loss = 0.07221949\n",
      "Iteration 114, loss = 0.07155747\n",
      "Iteration 115, loss = 0.07087708\n",
      "Iteration 116, loss = 0.07000358\n",
      "Iteration 117, loss = 0.06947779\n",
      "Iteration 118, loss = 0.06867506\n",
      "Iteration 119, loss = 0.06814021\n",
      "Iteration 120, loss = 0.06749393\n",
      "Iteration 121, loss = 0.06674964\n",
      "Iteration 122, loss = 0.06618502\n",
      "Iteration 123, loss = 0.06546363\n",
      "Iteration 124, loss = 0.06476989\n",
      "Iteration 125, loss = 0.06421920\n",
      "Iteration 126, loss = 0.06363091\n",
      "Iteration 127, loss = 0.06302186\n",
      "Iteration 128, loss = 0.06245105\n",
      "Iteration 129, loss = 0.06175796\n",
      "Iteration 130, loss = 0.06124799\n",
      "Iteration 131, loss = 0.06065886\n",
      "Iteration 132, loss = 0.06014263\n",
      "Iteration 133, loss = 0.05956561\n",
      "Iteration 134, loss = 0.05889611\n",
      "Iteration 135, loss = 0.05849613\n",
      "Iteration 136, loss = 0.05791348\n",
      "Iteration 137, loss = 0.05729737\n",
      "Iteration 138, loss = 0.05682724\n",
      "Iteration 139, loss = 0.05639525\n",
      "Iteration 140, loss = 0.05594843\n",
      "Iteration 141, loss = 0.05526227\n",
      "Iteration 142, loss = 0.05474874\n",
      "Iteration 143, loss = 0.05427412\n",
      "Iteration 144, loss = 0.05377775\n",
      "Iteration 145, loss = 0.05327753\n",
      "Iteration 146, loss = 0.05283006\n",
      "Iteration 147, loss = 0.05242724\n",
      "Iteration 148, loss = 0.05182562\n",
      "Iteration 149, loss = 0.05147827\n",
      "Iteration 150, loss = 0.05097914\n",
      "Iteration 151, loss = 0.05051896\n",
      "Iteration 152, loss = 0.05010570\n",
      "Iteration 153, loss = 0.04957232\n",
      "Iteration 154, loss = 0.04918811\n",
      "Iteration 155, loss = 0.04865387\n",
      "Iteration 156, loss = 0.04833204\n",
      "Iteration 157, loss = 0.04784788\n",
      "Iteration 158, loss = 0.04744428\n",
      "Iteration 159, loss = 0.04707442\n",
      "Iteration 160, loss = 0.04660466\n",
      "Iteration 161, loss = 0.04625607\n",
      "Iteration 162, loss = 0.04583259\n",
      "Iteration 163, loss = 0.04529484\n",
      "Iteration 164, loss = 0.04505881\n",
      "Iteration 165, loss = 0.04463159\n",
      "Iteration 166, loss = 0.04426825\n",
      "Iteration 167, loss = 0.04390918\n",
      "Iteration 168, loss = 0.04354750\n",
      "Iteration 169, loss = 0.04316933\n",
      "Iteration 170, loss = 0.04276791\n",
      "Iteration 171, loss = 0.04248075\n",
      "Iteration 172, loss = 0.04197654\n",
      "Iteration 173, loss = 0.04169866\n",
      "Iteration 174, loss = 0.04136843\n",
      "Iteration 175, loss = 0.04096532\n",
      "Iteration 176, loss = 0.04066446\n",
      "Iteration 177, loss = 0.04029206\n",
      "Iteration 178, loss = 0.03998226\n",
      "Iteration 179, loss = 0.03963250\n",
      "Iteration 180, loss = 0.03932069\n",
      "Iteration 181, loss = 0.03905277\n",
      "Iteration 182, loss = 0.03865867\n",
      "Iteration 183, loss = 0.03834788\n",
      "Iteration 184, loss = 0.03803642\n",
      "Iteration 185, loss = 0.03766942\n",
      "Iteration 186, loss = 0.03745670\n",
      "Iteration 187, loss = 0.03710795\n",
      "Iteration 188, loss = 0.03681738\n",
      "Iteration 189, loss = 0.03651780\n",
      "Iteration 190, loss = 0.03616721\n",
      "Iteration 191, loss = 0.03590580\n",
      "Iteration 192, loss = 0.03560994\n",
      "Iteration 193, loss = 0.03535338\n",
      "Iteration 194, loss = 0.03509964\n",
      "Iteration 195, loss = 0.03477782\n",
      "Iteration 196, loss = 0.03452853\n",
      "Iteration 197, loss = 0.03416998\n",
      "Iteration 198, loss = 0.03390158\n",
      "Iteration 199, loss = 0.03369602\n",
      "Iteration 200, loss = 0.03340472\n",
      "[CV] END activation=relu, alpha=0.001, batch_size=200, hidden_layer_sizes=(400, 100), solver=sgd; total time= 3.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.73395543\n",
      "Iteration 2, loss = 0.87481285\n",
      "Iteration 3, loss = 0.59264249\n",
      "Iteration 4, loss = 0.48440569\n",
      "Iteration 5, loss = 0.42598466\n",
      "Iteration 6, loss = 0.38906182\n",
      "Iteration 7, loss = 0.36250718\n",
      "Iteration 8, loss = 0.34252594\n",
      "Iteration 9, loss = 0.32654560\n",
      "Iteration 10, loss = 0.31333587\n",
      "Iteration 11, loss = 0.30192598\n",
      "Iteration 12, loss = 0.29204243\n",
      "Iteration 13, loss = 0.28303827\n",
      "Iteration 14, loss = 0.27518682\n",
      "Iteration 15, loss = 0.26788785\n",
      "Iteration 16, loss = 0.26103258\n",
      "Iteration 17, loss = 0.25472573\n",
      "Iteration 18, loss = 0.24876310\n",
      "Iteration 19, loss = 0.24334031\n",
      "Iteration 20, loss = 0.23798200\n",
      "Iteration 21, loss = 0.23307345\n",
      "Iteration 22, loss = 0.22812267\n",
      "Iteration 23, loss = 0.22364854\n",
      "Iteration 24, loss = 0.21925258\n",
      "Iteration 25, loss = 0.21494777\n",
      "Iteration 26, loss = 0.21111815\n",
      "Iteration 27, loss = 0.20710701\n",
      "Iteration 28, loss = 0.20335377\n",
      "Iteration 29, loss = 0.19979097\n",
      "Iteration 30, loss = 0.19615697\n",
      "Iteration 31, loss = 0.19278773\n",
      "Iteration 32, loss = 0.18941926\n",
      "Iteration 33, loss = 0.18639036\n",
      "Iteration 34, loss = 0.18320894\n",
      "Iteration 35, loss = 0.18003714\n",
      "Iteration 36, loss = 0.17726314\n",
      "Iteration 37, loss = 0.17434192\n",
      "Iteration 38, loss = 0.17152750\n",
      "Iteration 39, loss = 0.16887079\n",
      "Iteration 40, loss = 0.16629070\n",
      "Iteration 41, loss = 0.16350310\n",
      "Iteration 42, loss = 0.16112216\n",
      "Iteration 43, loss = 0.15863220\n",
      "Iteration 44, loss = 0.15637388\n",
      "Iteration 45, loss = 0.15405307\n",
      "Iteration 46, loss = 0.15167867\n",
      "Iteration 47, loss = 0.14948325\n",
      "Iteration 48, loss = 0.14733227\n",
      "Iteration 49, loss = 0.14502643\n",
      "Iteration 50, loss = 0.14305867\n",
      "Iteration 51, loss = 0.14108383\n",
      "Iteration 52, loss = 0.13893280\n",
      "Iteration 53, loss = 0.13725181\n",
      "Iteration 54, loss = 0.13511806\n",
      "Iteration 55, loss = 0.13331502\n",
      "Iteration 56, loss = 0.13153991\n",
      "Iteration 57, loss = 0.12971615\n",
      "Iteration 58, loss = 0.12784737\n",
      "Iteration 59, loss = 0.12626429\n",
      "Iteration 60, loss = 0.12471960\n",
      "Iteration 61, loss = 0.12289340\n",
      "Iteration 62, loss = 0.12130252\n",
      "Iteration 63, loss = 0.11980011\n",
      "Iteration 64, loss = 0.11803247\n",
      "Iteration 65, loss = 0.11647739\n",
      "Iteration 66, loss = 0.11496383\n",
      "Iteration 67, loss = 0.11356153\n",
      "Iteration 68, loss = 0.11210739\n",
      "Iteration 69, loss = 0.11081305\n",
      "Iteration 70, loss = 0.10941474\n",
      "Iteration 71, loss = 0.10806894\n",
      "Iteration 72, loss = 0.10665242\n",
      "Iteration 73, loss = 0.10528116\n",
      "Iteration 74, loss = 0.10407149\n",
      "Iteration 75, loss = 0.10275827\n",
      "Iteration 76, loss = 0.10155849\n",
      "Iteration 77, loss = 0.10037160\n",
      "Iteration 78, loss = 0.09910097\n",
      "Iteration 79, loss = 0.09793687\n",
      "Iteration 80, loss = 0.09689470\n",
      "Iteration 81, loss = 0.09565388\n",
      "Iteration 82, loss = 0.09450380\n",
      "Iteration 83, loss = 0.09342838\n",
      "Iteration 84, loss = 0.09230383\n",
      "Iteration 85, loss = 0.09128764\n",
      "Iteration 86, loss = 0.09025099\n",
      "Iteration 87, loss = 0.08924570\n",
      "Iteration 88, loss = 0.08820578\n",
      "Iteration 89, loss = 0.08710855\n",
      "Iteration 90, loss = 0.08611678\n",
      "Iteration 91, loss = 0.08517642\n",
      "Iteration 92, loss = 0.08413115\n",
      "Iteration 93, loss = 0.08333621\n",
      "Iteration 94, loss = 0.08238334\n",
      "Iteration 95, loss = 0.08143574\n",
      "Iteration 96, loss = 0.08064829\n",
      "Iteration 97, loss = 0.07953190\n",
      "Iteration 98, loss = 0.07885297\n",
      "Iteration 99, loss = 0.07791234\n",
      "Iteration 100, loss = 0.07726811\n",
      "Iteration 101, loss = 0.07639171\n",
      "Iteration 102, loss = 0.07553256\n",
      "Iteration 103, loss = 0.07462071\n",
      "Iteration 104, loss = 0.07387118\n",
      "Iteration 105, loss = 0.07303185\n",
      "Iteration 106, loss = 0.07219528\n",
      "Iteration 107, loss = 0.07149603\n",
      "Iteration 108, loss = 0.07082896\n",
      "Iteration 109, loss = 0.07004380\n",
      "Iteration 110, loss = 0.06932181\n",
      "Iteration 111, loss = 0.06855701\n",
      "Iteration 112, loss = 0.06791789\n",
      "Iteration 113, loss = 0.06729790\n",
      "Iteration 114, loss = 0.06646129\n",
      "Iteration 115, loss = 0.06587937\n",
      "Iteration 116, loss = 0.06511518\n",
      "Iteration 117, loss = 0.06453116\n",
      "Iteration 118, loss = 0.06378664\n",
      "Iteration 119, loss = 0.06312468\n",
      "Iteration 120, loss = 0.06259790\n",
      "Iteration 121, loss = 0.06184234\n",
      "Iteration 122, loss = 0.06134080\n",
      "Iteration 123, loss = 0.06071429\n",
      "Iteration 124, loss = 0.05995017\n",
      "Iteration 125, loss = 0.05950965\n",
      "Iteration 126, loss = 0.05882671\n",
      "Iteration 127, loss = 0.05831934\n",
      "Iteration 128, loss = 0.05781225\n",
      "Iteration 129, loss = 0.05713576\n",
      "Iteration 130, loss = 0.05656273\n",
      "Iteration 131, loss = 0.05603699\n",
      "Iteration 132, loss = 0.05553924\n",
      "Iteration 133, loss = 0.05496424\n",
      "Iteration 134, loss = 0.05437170\n",
      "Iteration 135, loss = 0.05393892\n",
      "Iteration 136, loss = 0.05343589\n",
      "Iteration 137, loss = 0.05288534\n",
      "Iteration 138, loss = 0.05229233\n",
      "Iteration 139, loss = 0.05179588\n",
      "Iteration 140, loss = 0.05134512\n",
      "Iteration 141, loss = 0.05086561\n",
      "Iteration 142, loss = 0.05033088\n",
      "Iteration 143, loss = 0.04993349\n",
      "Iteration 144, loss = 0.04954021\n",
      "Iteration 145, loss = 0.04905140\n",
      "Iteration 146, loss = 0.04847793\n",
      "Iteration 147, loss = 0.04806539\n",
      "Iteration 148, loss = 0.04755968\n",
      "Iteration 149, loss = 0.04725737\n",
      "Iteration 150, loss = 0.04674073\n",
      "Iteration 151, loss = 0.04624566\n",
      "Iteration 152, loss = 0.04583596\n",
      "Iteration 153, loss = 0.04548774\n",
      "Iteration 154, loss = 0.04512355\n",
      "Iteration 155, loss = 0.04467407\n",
      "Iteration 156, loss = 0.04423386\n",
      "Iteration 157, loss = 0.04374185\n",
      "Iteration 158, loss = 0.04339122\n",
      "Iteration 159, loss = 0.04303761\n",
      "Iteration 160, loss = 0.04266850\n",
      "Iteration 161, loss = 0.04223776\n",
      "Iteration 162, loss = 0.04186916\n",
      "Iteration 163, loss = 0.04145575\n",
      "Iteration 164, loss = 0.04121028\n",
      "Iteration 165, loss = 0.04078029\n",
      "Iteration 166, loss = 0.04033553\n",
      "Iteration 167, loss = 0.04005042\n",
      "Iteration 168, loss = 0.03972335\n",
      "Iteration 169, loss = 0.03931854\n",
      "Iteration 170, loss = 0.03900855\n",
      "Iteration 171, loss = 0.03865649\n",
      "Iteration 172, loss = 0.03831453\n",
      "Iteration 173, loss = 0.03795420\n",
      "Iteration 174, loss = 0.03765596\n",
      "Iteration 175, loss = 0.03729670\n",
      "Iteration 176, loss = 0.03694852\n",
      "Iteration 177, loss = 0.03665524\n",
      "Iteration 178, loss = 0.03638562\n",
      "Iteration 179, loss = 0.03604251\n",
      "Iteration 180, loss = 0.03574889\n",
      "Iteration 181, loss = 0.03541597\n",
      "Iteration 182, loss = 0.03506033\n",
      "Iteration 183, loss = 0.03482065\n",
      "Iteration 184, loss = 0.03444662\n",
      "Iteration 185, loss = 0.03423295\n",
      "Iteration 186, loss = 0.03394932\n",
      "Iteration 187, loss = 0.03366876\n",
      "Iteration 188, loss = 0.03336239\n",
      "Iteration 189, loss = 0.03309991\n",
      "Iteration 190, loss = 0.03278783\n",
      "Iteration 191, loss = 0.03245364\n",
      "Iteration 192, loss = 0.03226962\n",
      "Iteration 193, loss = 0.03194660\n",
      "Iteration 194, loss = 0.03169385\n",
      "Iteration 195, loss = 0.03147383\n",
      "Iteration 196, loss = 0.03118710\n",
      "Iteration 197, loss = 0.03097824\n",
      "Iteration 198, loss = 0.03069475\n",
      "Iteration 199, loss = 0.03050937\n",
      "Iteration 200, loss = 0.03020421\n",
      "[CV] END activation=relu, alpha=0.001, batch_size=200, hidden_layer_sizes=(400, 100), solver=sgd; total time= 3.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.81590487\n",
      "Iteration 2, loss = 0.93406552\n",
      "Iteration 3, loss = 0.60799224\n",
      "Iteration 4, loss = 0.48724452\n",
      "Iteration 5, loss = 0.42539936\n",
      "Iteration 6, loss = 0.38751614\n",
      "Iteration 7, loss = 0.36099751\n",
      "Iteration 8, loss = 0.34123355\n",
      "Iteration 9, loss = 0.32514117\n",
      "Iteration 10, loss = 0.31194526\n",
      "Iteration 11, loss = 0.30065760\n",
      "Iteration 12, loss = 0.29097040\n",
      "Iteration 13, loss = 0.28227677\n",
      "Iteration 14, loss = 0.27410988\n",
      "Iteration 15, loss = 0.26673647\n",
      "Iteration 16, loss = 0.26022419\n",
      "Iteration 17, loss = 0.25383144\n",
      "Iteration 18, loss = 0.24801127\n",
      "Iteration 19, loss = 0.24254070\n",
      "Iteration 20, loss = 0.23735496\n",
      "Iteration 21, loss = 0.23239227\n",
      "Iteration 22, loss = 0.22781571\n",
      "Iteration 23, loss = 0.22329474\n",
      "Iteration 24, loss = 0.21885922\n",
      "Iteration 25, loss = 0.21473511\n",
      "Iteration 26, loss = 0.21101172\n",
      "Iteration 27, loss = 0.20725030\n",
      "Iteration 28, loss = 0.20356991\n",
      "Iteration 29, loss = 0.20004456\n",
      "Iteration 30, loss = 0.19644813\n",
      "Iteration 31, loss = 0.19332275\n",
      "Iteration 32, loss = 0.19017168\n",
      "Iteration 33, loss = 0.18706593\n",
      "Iteration 34, loss = 0.18398134\n",
      "Iteration 35, loss = 0.18108492\n",
      "Iteration 36, loss = 0.17847005\n",
      "Iteration 37, loss = 0.17558047\n",
      "Iteration 38, loss = 0.17297427\n",
      "Iteration 39, loss = 0.17034206\n",
      "Iteration 40, loss = 0.16779728\n",
      "Iteration 41, loss = 0.16550798\n",
      "Iteration 42, loss = 0.16297602\n",
      "Iteration 43, loss = 0.16076317\n",
      "Iteration 44, loss = 0.15855888\n",
      "Iteration 45, loss = 0.15619804\n",
      "Iteration 46, loss = 0.15418331\n",
      "Iteration 47, loss = 0.15209733\n",
      "Iteration 48, loss = 0.15003841\n",
      "Iteration 49, loss = 0.14814758\n",
      "Iteration 50, loss = 0.14584121\n",
      "Iteration 51, loss = 0.14400424\n",
      "Iteration 52, loss = 0.14219192\n",
      "Iteration 53, loss = 0.14024342\n",
      "Iteration 54, loss = 0.13851343\n",
      "Iteration 55, loss = 0.13675457\n",
      "Iteration 56, loss = 0.13514242\n",
      "Iteration 57, loss = 0.13331159\n",
      "Iteration 58, loss = 0.13186131\n",
      "Iteration 59, loss = 0.13008997\n",
      "Iteration 60, loss = 0.12863014\n",
      "Iteration 61, loss = 0.12705060\n",
      "Iteration 62, loss = 0.12539971\n",
      "Iteration 63, loss = 0.12408189\n",
      "Iteration 64, loss = 0.12251379\n",
      "Iteration 65, loss = 0.12108530\n",
      "Iteration 66, loss = 0.11971092\n",
      "Iteration 67, loss = 0.11833910\n",
      "Iteration 68, loss = 0.11699627\n",
      "Iteration 69, loss = 0.11548055\n",
      "Iteration 70, loss = 0.11416979\n",
      "Iteration 71, loss = 0.11311532\n",
      "Iteration 72, loss = 0.11162045\n",
      "Iteration 73, loss = 0.11026429\n",
      "Iteration 74, loss = 0.10919079\n",
      "Iteration 75, loss = 0.10797072\n",
      "Iteration 76, loss = 0.10677253\n",
      "Iteration 77, loss = 0.10567071\n",
      "Iteration 78, loss = 0.10448141\n",
      "Iteration 79, loss = 0.10341538\n",
      "Iteration 80, loss = 0.10220608\n",
      "Iteration 81, loss = 0.10109627\n",
      "Iteration 82, loss = 0.10006096\n",
      "Iteration 83, loss = 0.09879070\n",
      "Iteration 84, loss = 0.09805502\n",
      "Iteration 85, loss = 0.09680397\n",
      "Iteration 86, loss = 0.09558457\n",
      "Iteration 87, loss = 0.09494218\n",
      "Iteration 88, loss = 0.09391509\n",
      "Iteration 89, loss = 0.09289101\n",
      "Iteration 90, loss = 0.09197500\n",
      "Iteration 91, loss = 0.09094341\n",
      "Iteration 92, loss = 0.09016372\n",
      "Iteration 93, loss = 0.08910675\n",
      "Iteration 94, loss = 0.08818404\n",
      "Iteration 95, loss = 0.08720763\n",
      "Iteration 96, loss = 0.08641971\n",
      "Iteration 97, loss = 0.08554827\n",
      "Iteration 98, loss = 0.08457988\n",
      "Iteration 99, loss = 0.08382372\n",
      "Iteration 100, loss = 0.08306180\n",
      "Iteration 101, loss = 0.08218285\n",
      "Iteration 102, loss = 0.08131370\n",
      "Iteration 103, loss = 0.08045098\n",
      "Iteration 104, loss = 0.07964382\n",
      "Iteration 105, loss = 0.07887733\n",
      "Iteration 106, loss = 0.07804955\n",
      "Iteration 107, loss = 0.07731545\n",
      "Iteration 108, loss = 0.07650735\n",
      "Iteration 109, loss = 0.07578601\n",
      "Iteration 110, loss = 0.07498678\n",
      "Iteration 111, loss = 0.07423817\n",
      "Iteration 112, loss = 0.07358645\n",
      "Iteration 113, loss = 0.07278561\n",
      "Iteration 114, loss = 0.07221782\n",
      "Iteration 115, loss = 0.07155385\n",
      "Iteration 116, loss = 0.07076072\n",
      "Iteration 117, loss = 0.07020258\n",
      "Iteration 118, loss = 0.06938105\n",
      "Iteration 119, loss = 0.06879944\n",
      "Iteration 120, loss = 0.06821007\n",
      "Iteration 121, loss = 0.06746496\n",
      "Iteration 122, loss = 0.06683311\n",
      "Iteration 123, loss = 0.06626825\n",
      "Iteration 124, loss = 0.06560602\n",
      "Iteration 125, loss = 0.06489883\n",
      "Iteration 126, loss = 0.06434496\n",
      "Iteration 127, loss = 0.06371159\n",
      "Iteration 128, loss = 0.06317420\n",
      "Iteration 129, loss = 0.06259005\n",
      "Iteration 130, loss = 0.06192972\n",
      "Iteration 131, loss = 0.06141184\n",
      "Iteration 132, loss = 0.06090746\n",
      "Iteration 133, loss = 0.06014663\n",
      "Iteration 134, loss = 0.05977990\n",
      "Iteration 135, loss = 0.05912263\n",
      "Iteration 136, loss = 0.05861625\n",
      "Iteration 137, loss = 0.05814210\n",
      "Iteration 138, loss = 0.05751116\n",
      "Iteration 139, loss = 0.05699566\n",
      "Iteration 140, loss = 0.05659942\n",
      "Iteration 141, loss = 0.05602705\n",
      "Iteration 142, loss = 0.05544894\n",
      "Iteration 143, loss = 0.05496109\n",
      "Iteration 144, loss = 0.05437625\n",
      "Iteration 145, loss = 0.05400460\n",
      "Iteration 146, loss = 0.05346445\n",
      "Iteration 147, loss = 0.05304891\n",
      "Iteration 148, loss = 0.05250985\n",
      "Iteration 149, loss = 0.05206993\n",
      "Iteration 150, loss = 0.05156012\n",
      "Iteration 151, loss = 0.05110969\n",
      "Iteration 152, loss = 0.05061805\n",
      "Iteration 153, loss = 0.05018696\n",
      "Iteration 154, loss = 0.04980364\n",
      "Iteration 155, loss = 0.04933468\n",
      "Iteration 156, loss = 0.04889072\n",
      "Iteration 157, loss = 0.04842063\n",
      "Iteration 158, loss = 0.04795474\n",
      "Iteration 159, loss = 0.04760044\n",
      "Iteration 160, loss = 0.04723878\n",
      "Iteration 161, loss = 0.04683295\n",
      "Iteration 162, loss = 0.04631966\n",
      "Iteration 163, loss = 0.04606485\n",
      "Iteration 164, loss = 0.04546437\n",
      "Iteration 165, loss = 0.04514476\n",
      "Iteration 166, loss = 0.04488949\n",
      "Iteration 167, loss = 0.04435652\n",
      "Iteration 168, loss = 0.04398053\n",
      "Iteration 169, loss = 0.04358127\n",
      "Iteration 170, loss = 0.04330401\n",
      "Iteration 171, loss = 0.04282691\n",
      "Iteration 172, loss = 0.04250504\n",
      "Iteration 173, loss = 0.04211470\n",
      "Iteration 174, loss = 0.04178687\n",
      "Iteration 175, loss = 0.04140144\n",
      "Iteration 176, loss = 0.04099886\n",
      "Iteration 177, loss = 0.04076989\n",
      "Iteration 178, loss = 0.04037276\n",
      "Iteration 179, loss = 0.04006092\n",
      "Iteration 180, loss = 0.03976034\n",
      "Iteration 181, loss = 0.03929530\n",
      "Iteration 182, loss = 0.03902831\n",
      "Iteration 183, loss = 0.03875798\n",
      "Iteration 184, loss = 0.03838639\n",
      "Iteration 185, loss = 0.03808152\n",
      "Iteration 186, loss = 0.03776940\n",
      "Iteration 187, loss = 0.03745458\n",
      "Iteration 188, loss = 0.03720953\n",
      "Iteration 189, loss = 0.03675060\n",
      "Iteration 190, loss = 0.03655268\n",
      "Iteration 191, loss = 0.03625423\n",
      "Iteration 192, loss = 0.03592523\n",
      "Iteration 193, loss = 0.03566579\n",
      "Iteration 194, loss = 0.03535354\n",
      "Iteration 195, loss = 0.03505227\n",
      "Iteration 196, loss = 0.03478998\n",
      "Iteration 197, loss = 0.03454818\n",
      "Iteration 198, loss = 0.03423553\n",
      "Iteration 199, loss = 0.03406279\n",
      "Iteration 200, loss = 0.03368738\n",
      "[CV] END activation=relu, alpha=0.001, batch_size=200, hidden_layer_sizes=(400, 100), solver=sgd; total time= 3.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.71538437\n",
      "Iteration 2, loss = 0.85909336\n",
      "Iteration 3, loss = 0.59000961\n",
      "Iteration 4, loss = 0.48387095\n",
      "Iteration 5, loss = 0.42633579\n",
      "Iteration 6, loss = 0.38947789\n",
      "Iteration 7, loss = 0.36397477\n",
      "Iteration 8, loss = 0.34416913\n",
      "Iteration 9, loss = 0.32844387\n",
      "Iteration 10, loss = 0.31544091\n",
      "Iteration 11, loss = 0.30410370\n",
      "Iteration 12, loss = 0.29422167\n",
      "Iteration 13, loss = 0.28543256\n",
      "Iteration 14, loss = 0.27759062\n",
      "Iteration 15, loss = 0.27035057\n",
      "Iteration 16, loss = 0.26333509\n",
      "Iteration 17, loss = 0.25717109\n",
      "Iteration 18, loss = 0.25136138\n",
      "Iteration 19, loss = 0.24590566\n",
      "Iteration 20, loss = 0.24030412\n",
      "Iteration 21, loss = 0.23536210\n",
      "Iteration 22, loss = 0.23060585\n",
      "Iteration 23, loss = 0.22612466\n",
      "Iteration 24, loss = 0.22152901\n",
      "Iteration 25, loss = 0.21720435\n",
      "Iteration 26, loss = 0.21309307\n",
      "Iteration 27, loss = 0.20936166\n",
      "Iteration 28, loss = 0.20560607\n",
      "Iteration 29, loss = 0.20182933\n",
      "Iteration 30, loss = 0.19816641\n",
      "Iteration 31, loss = 0.19485014\n",
      "Iteration 32, loss = 0.19145290\n",
      "Iteration 33, loss = 0.18810189\n",
      "Iteration 34, loss = 0.18494923\n",
      "Iteration 35, loss = 0.18195456\n",
      "Iteration 36, loss = 0.17894135\n",
      "Iteration 37, loss = 0.17603332\n",
      "Iteration 38, loss = 0.17333136\n",
      "Iteration 39, loss = 0.17037427\n",
      "Iteration 40, loss = 0.16784243\n",
      "Iteration 41, loss = 0.16525734\n",
      "Iteration 42, loss = 0.16271762\n",
      "Iteration 43, loss = 0.16021236\n",
      "Iteration 44, loss = 0.15782301\n",
      "Iteration 45, loss = 0.15553128\n",
      "Iteration 46, loss = 0.15303691\n",
      "Iteration 47, loss = 0.15084369\n",
      "Iteration 48, loss = 0.14878652\n",
      "Iteration 49, loss = 0.14678010\n",
      "Iteration 50, loss = 0.14441833\n",
      "Iteration 51, loss = 0.14241767\n",
      "Iteration 52, loss = 0.14057452\n",
      "Iteration 53, loss = 0.13874178\n",
      "Iteration 54, loss = 0.13682101\n",
      "Iteration 55, loss = 0.13486560\n",
      "Iteration 56, loss = 0.13311205\n",
      "Iteration 57, loss = 0.13141925\n",
      "Iteration 58, loss = 0.12971684\n",
      "Iteration 59, loss = 0.12796765\n",
      "Iteration 60, loss = 0.12639125\n",
      "Iteration 61, loss = 0.12481672\n",
      "Iteration 62, loss = 0.12322823\n",
      "Iteration 63, loss = 0.12167302\n",
      "Iteration 64, loss = 0.12008736\n",
      "Iteration 65, loss = 0.11861170\n",
      "Iteration 66, loss = 0.11716928\n",
      "Iteration 67, loss = 0.11559335\n",
      "Iteration 68, loss = 0.11439657\n",
      "Iteration 69, loss = 0.11300261\n",
      "Iteration 70, loss = 0.11168958\n",
      "Iteration 71, loss = 0.11037132\n",
      "Iteration 72, loss = 0.10890719\n",
      "Iteration 73, loss = 0.10786750\n",
      "Iteration 74, loss = 0.10647984\n",
      "Iteration 75, loss = 0.10524795\n",
      "Iteration 76, loss = 0.10395987\n",
      "Iteration 77, loss = 0.10296484\n",
      "Iteration 78, loss = 0.10161560\n",
      "Iteration 79, loss = 0.10063542\n",
      "Iteration 80, loss = 0.09952321\n",
      "Iteration 81, loss = 0.09837234\n",
      "Iteration 82, loss = 0.09732724\n",
      "Iteration 83, loss = 0.09628327\n",
      "Iteration 84, loss = 0.09514035\n",
      "Iteration 85, loss = 0.09396411\n",
      "Iteration 86, loss = 0.09321313\n",
      "Iteration 87, loss = 0.09214550\n",
      "Iteration 88, loss = 0.09110495\n",
      "Iteration 89, loss = 0.09015858\n",
      "Iteration 90, loss = 0.08934119\n",
      "Iteration 91, loss = 0.08839484\n",
      "Iteration 92, loss = 0.08745502\n",
      "Iteration 93, loss = 0.08642143\n",
      "Iteration 94, loss = 0.08554950\n",
      "Iteration 95, loss = 0.08460273\n",
      "Iteration 96, loss = 0.08385411\n",
      "Iteration 97, loss = 0.08283696\n",
      "Iteration 98, loss = 0.08204252\n",
      "Iteration 99, loss = 0.08118845\n",
      "Iteration 100, loss = 0.08047931\n",
      "Iteration 101, loss = 0.07960878\n",
      "Iteration 102, loss = 0.07884241\n",
      "Iteration 103, loss = 0.07796365\n",
      "Iteration 104, loss = 0.07712531\n",
      "Iteration 105, loss = 0.07647447\n",
      "Iteration 106, loss = 0.07564285\n",
      "Iteration 107, loss = 0.07483619\n",
      "Iteration 108, loss = 0.07420720\n",
      "Iteration 109, loss = 0.07340667\n",
      "Iteration 110, loss = 0.07269009\n",
      "Iteration 111, loss = 0.07189144\n",
      "Iteration 112, loss = 0.07125459\n",
      "Iteration 113, loss = 0.07050824\n",
      "Iteration 114, loss = 0.06993231\n",
      "Iteration 115, loss = 0.06915941\n",
      "Iteration 116, loss = 0.06857676\n",
      "Iteration 117, loss = 0.06790900\n",
      "Iteration 118, loss = 0.06719001\n",
      "Iteration 119, loss = 0.06645386\n",
      "Iteration 120, loss = 0.06594915\n",
      "Iteration 121, loss = 0.06522681\n",
      "Iteration 122, loss = 0.06468720\n",
      "Iteration 123, loss = 0.06406610\n",
      "Iteration 124, loss = 0.06345651\n",
      "Iteration 125, loss = 0.06311652\n",
      "Iteration 126, loss = 0.06220671\n",
      "Iteration 127, loss = 0.06161521\n",
      "Iteration 128, loss = 0.06102794\n",
      "Iteration 129, loss = 0.06048468\n",
      "Iteration 130, loss = 0.05999315\n",
      "Iteration 131, loss = 0.05945130\n",
      "Iteration 132, loss = 0.05893225\n",
      "Iteration 133, loss = 0.05828060\n",
      "Iteration 134, loss = 0.05780003\n",
      "Iteration 135, loss = 0.05726350\n",
      "Iteration 136, loss = 0.05661500\n",
      "Iteration 137, loss = 0.05628596\n",
      "Iteration 138, loss = 0.05561918\n",
      "Iteration 139, loss = 0.05505440\n",
      "Iteration 140, loss = 0.05470198\n",
      "Iteration 141, loss = 0.05415366\n",
      "Iteration 142, loss = 0.05356596\n",
      "Iteration 143, loss = 0.05318522\n",
      "Iteration 144, loss = 0.05267532\n",
      "Iteration 145, loss = 0.05219106\n",
      "Iteration 146, loss = 0.05180743\n",
      "Iteration 147, loss = 0.05137758\n",
      "Iteration 148, loss = 0.05073382\n",
      "Iteration 149, loss = 0.05035668\n",
      "Iteration 150, loss = 0.05001385\n",
      "Iteration 151, loss = 0.04947698\n",
      "Iteration 152, loss = 0.04900816\n",
      "Iteration 153, loss = 0.04854778\n",
      "Iteration 154, loss = 0.04815372\n",
      "Iteration 155, loss = 0.04773257\n",
      "Iteration 156, loss = 0.04740448\n",
      "Iteration 157, loss = 0.04692391\n",
      "Iteration 158, loss = 0.04652706\n",
      "Iteration 159, loss = 0.04615323\n",
      "Iteration 160, loss = 0.04578590\n",
      "Iteration 161, loss = 0.04530128\n",
      "Iteration 162, loss = 0.04476137\n",
      "Iteration 163, loss = 0.04454375\n",
      "Iteration 164, loss = 0.04419548\n",
      "Iteration 165, loss = 0.04374624\n",
      "Iteration 166, loss = 0.04338498\n",
      "Iteration 167, loss = 0.04293196\n",
      "Iteration 168, loss = 0.04261056\n",
      "Iteration 169, loss = 0.04218681\n",
      "Iteration 170, loss = 0.04193538\n",
      "Iteration 171, loss = 0.04153521\n",
      "Iteration 172, loss = 0.04119664\n",
      "Iteration 173, loss = 0.04086911\n",
      "Iteration 174, loss = 0.04057679\n",
      "Iteration 175, loss = 0.04018532\n",
      "Iteration 176, loss = 0.03974612\n",
      "Iteration 177, loss = 0.03953241\n",
      "Iteration 178, loss = 0.03927303\n",
      "Iteration 179, loss = 0.03874736\n",
      "Iteration 180, loss = 0.03851021\n",
      "Iteration 181, loss = 0.03825084\n",
      "Iteration 182, loss = 0.03787428\n",
      "Iteration 183, loss = 0.03756247\n",
      "Iteration 184, loss = 0.03719271\n",
      "Iteration 185, loss = 0.03691262\n",
      "Iteration 186, loss = 0.03665412\n",
      "Iteration 187, loss = 0.03640864\n",
      "Iteration 188, loss = 0.03603658\n",
      "Iteration 189, loss = 0.03570492\n",
      "Iteration 190, loss = 0.03547590\n",
      "Iteration 191, loss = 0.03518092\n",
      "Iteration 192, loss = 0.03494306\n",
      "Iteration 193, loss = 0.03467193\n",
      "Iteration 194, loss = 0.03430687\n",
      "Iteration 195, loss = 0.03407571\n",
      "Iteration 196, loss = 0.03379233\n",
      "Iteration 197, loss = 0.03349396\n",
      "Iteration 198, loss = 0.03321226\n",
      "Iteration 199, loss = 0.03302176\n",
      "Iteration 200, loss = 0.03266549\n",
      "[CV] END activation=relu, alpha=0.001, batch_size=200, hidden_layer_sizes=(400, 100), solver=sgd; total time= 3.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.52880042\n",
      "Iteration 2, loss = 0.18712151\n",
      "Iteration 3, loss = 0.13491321\n",
      "Iteration 4, loss = 0.09994455\n",
      "Iteration 5, loss = 0.07840539\n",
      "Iteration 6, loss = 0.06208487\n",
      "Iteration 7, loss = 0.05240604\n",
      "Iteration 8, loss = 0.04465913\n",
      "Iteration 9, loss = 0.03610320\n",
      "Iteration 10, loss = 0.02893378\n",
      "Iteration 11, loss = 0.02469821\n",
      "Iteration 12, loss = 0.02623187\n",
      "Iteration 13, loss = 0.01859856\n",
      "Iteration 14, loss = 0.01454477\n",
      "Iteration 15, loss = 0.01348345\n",
      "Iteration 16, loss = 0.01130801\n",
      "Iteration 17, loss = 0.01894020\n",
      "Iteration 18, loss = 0.01093770\n",
      "Iteration 19, loss = 0.01431172\n",
      "Iteration 20, loss = 0.00861821\n",
      "Iteration 21, loss = 0.00473372\n",
      "Iteration 22, loss = 0.00313020\n",
      "Iteration 23, loss = 0.01285300\n",
      "Iteration 24, loss = 0.01697369\n",
      "Iteration 25, loss = 0.01679722\n",
      "Iteration 26, loss = 0.00749280\n",
      "Iteration 27, loss = 0.01164988\n",
      "Iteration 28, loss = 0.00697205\n",
      "Iteration 29, loss = 0.00414403\n",
      "Iteration 30, loss = 0.00640637\n",
      "Iteration 31, loss = 0.00330532\n",
      "Iteration 32, loss = 0.00967127\n",
      "Iteration 33, loss = 0.02095207\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.001, batch_size=200, hidden_layer_sizes=(100, 100, 100), solver=adam; total time=  25.3s\n",
      "Iteration 1, loss = 0.52502763\n",
      "Iteration 2, loss = 0.19447964\n",
      "Iteration 3, loss = 0.14097870\n",
      "Iteration 4, loss = 0.11252385\n",
      "Iteration 5, loss = 0.08793720\n",
      "Iteration 6, loss = 0.07198101\n",
      "Iteration 7, loss = 0.06145372\n",
      "Iteration 8, loss = 0.05072562\n",
      "Iteration 9, loss = 0.04078045\n",
      "Iteration 10, loss = 0.03193674\n",
      "Iteration 11, loss = 0.02859261\n",
      "Iteration 12, loss = 0.02438384\n",
      "Iteration 13, loss = 0.02265713\n",
      "Iteration 14, loss = 0.01726551\n",
      "Iteration 15, loss = 0.01720598\n",
      "Iteration 16, loss = 0.01672788\n",
      "Iteration 17, loss = 0.01535411\n",
      "Iteration 18, loss = 0.01370132\n",
      "Iteration 19, loss = 0.00988243\n",
      "Iteration 20, loss = 0.00890987\n",
      "Iteration 21, loss = 0.00841413\n",
      "Iteration 22, loss = 0.01410178\n",
      "Iteration 23, loss = 0.01432602\n",
      "Iteration 24, loss = 0.01824169\n",
      "Iteration 25, loss = 0.01154503\n",
      "Iteration 26, loss = 0.00793697\n",
      "Iteration 27, loss = 0.00676841\n",
      "Iteration 28, loss = 0.00472100\n",
      "Iteration 29, loss = 0.00266073\n",
      "Iteration 30, loss = 0.00243747\n",
      "Iteration 31, loss = 0.00237504\n",
      "Iteration 32, loss = 0.00233284\n",
      "Iteration 33, loss = 0.00229775\n",
      "Iteration 34, loss = 0.00227013\n",
      "Iteration 35, loss = 0.00224302\n",
      "Iteration 36, loss = 0.00221905\n",
      "Iteration 37, loss = 0.00219517\n",
      "Iteration 38, loss = 0.00217208\n",
      "Iteration 39, loss = 0.00214981\n",
      "Iteration 40, loss = 0.00212579\n",
      "Iteration 41, loss = 0.00210497\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.001, batch_size=200, hidden_layer_sizes=(100, 100, 100), solver=adam; total time=  33.0s\n",
      "Iteration 1, loss = 0.51470600\n",
      "Iteration 2, loss = 0.17852415\n",
      "Iteration 3, loss = 0.12909472\n",
      "Iteration 4, loss = 0.09925262\n",
      "Iteration 5, loss = 0.07749631\n",
      "Iteration 6, loss = 0.06420315\n",
      "Iteration 7, loss = 0.05368723\n",
      "Iteration 8, loss = 0.04304479\n",
      "Iteration 9, loss = 0.03460298\n",
      "Iteration 10, loss = 0.03043075\n",
      "Iteration 11, loss = 0.02318745\n",
      "Iteration 12, loss = 0.01933860\n",
      "Iteration 13, loss = 0.01938299\n",
      "Iteration 14, loss = 0.02106549\n",
      "Iteration 15, loss = 0.01242669\n",
      "Iteration 16, loss = 0.00992353\n",
      "Iteration 17, loss = 0.01410401\n",
      "Iteration 18, loss = 0.00918880\n",
      "Iteration 19, loss = 0.00997223\n",
      "Iteration 20, loss = 0.01788843\n",
      "Iteration 21, loss = 0.01444340\n",
      "Iteration 22, loss = 0.01051332\n",
      "Iteration 23, loss = 0.00524867\n",
      "Iteration 24, loss = 0.00336185\n",
      "Iteration 25, loss = 0.00245587\n",
      "Iteration 26, loss = 0.00234473\n",
      "Iteration 27, loss = 0.00227568\n",
      "Iteration 28, loss = 0.00219978\n",
      "Iteration 29, loss = 0.00215782\n",
      "Iteration 30, loss = 0.00213155\n",
      "Iteration 31, loss = 0.00210252\n",
      "Iteration 32, loss = 0.00207450\n",
      "Iteration 33, loss = 0.00204896\n",
      "Iteration 34, loss = 0.00202929\n",
      "Iteration 35, loss = 0.00200972\n",
      "Iteration 36, loss = 0.00198277\n",
      "Iteration 37, loss = 0.00196272\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.001, batch_size=200, hidden_layer_sizes=(100, 100, 100), solver=adam; total time=  30.4s\n",
      "Iteration 1, loss = 0.50645661\n",
      "Iteration 2, loss = 0.18229431\n",
      "Iteration 3, loss = 0.12743118\n",
      "Iteration 4, loss = 0.10049123\n",
      "Iteration 5, loss = 0.07845373\n",
      "Iteration 6, loss = 0.06419374\n",
      "Iteration 7, loss = 0.05587848\n",
      "Iteration 8, loss = 0.04177622\n",
      "Iteration 9, loss = 0.03498665\n",
      "Iteration 10, loss = 0.02958068\n",
      "Iteration 11, loss = 0.02363374\n",
      "Iteration 12, loss = 0.01924190\n",
      "Iteration 13, loss = 0.01418275\n",
      "Iteration 14, loss = 0.01334023\n",
      "Iteration 15, loss = 0.01520870\n",
      "Iteration 16, loss = 0.01648905\n",
      "Iteration 17, loss = 0.01359606\n",
      "Iteration 18, loss = 0.01408087\n",
      "Iteration 19, loss = 0.01137917\n",
      "Iteration 20, loss = 0.00751816\n",
      "Iteration 21, loss = 0.00565099\n",
      "Iteration 22, loss = 0.00323240\n",
      "Iteration 23, loss = 0.00247794\n",
      "Iteration 24, loss = 0.00232390\n",
      "Iteration 25, loss = 0.00224793\n",
      "Iteration 26, loss = 0.00218542\n",
      "Iteration 27, loss = 0.00214685\n",
      "Iteration 28, loss = 0.00211058\n",
      "Iteration 29, loss = 0.00208591\n",
      "Iteration 30, loss = 0.00205715\n",
      "Iteration 31, loss = 0.00202894\n",
      "Iteration 32, loss = 0.00200829\n",
      "Iteration 33, loss = 0.00198278\n",
      "Iteration 34, loss = 0.00196173\n",
      "Iteration 35, loss = 0.00194615\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.001, batch_size=200, hidden_layer_sizes=(100, 100, 100), solver=adam; total time=  28.4s\n",
      "Iteration 1, loss = 0.51851896\n",
      "Iteration 2, loss = 0.19271161\n",
      "Iteration 3, loss = 0.14022642\n",
      "Iteration 4, loss = 0.11165083\n",
      "Iteration 5, loss = 0.08863353\n",
      "Iteration 6, loss = 0.07222795\n",
      "Iteration 7, loss = 0.06051761\n",
      "Iteration 8, loss = 0.05098049\n",
      "Iteration 9, loss = 0.04212754\n",
      "Iteration 10, loss = 0.03247324\n",
      "Iteration 11, loss = 0.02908314\n",
      "Iteration 12, loss = 0.02082424\n",
      "Iteration 13, loss = 0.01974610\n",
      "Iteration 14, loss = 0.02118282\n",
      "Iteration 15, loss = 0.01666466\n",
      "Iteration 16, loss = 0.01329398\n",
      "Iteration 17, loss = 0.00983057\n",
      "Iteration 18, loss = 0.01210261\n",
      "Iteration 19, loss = 0.01296764\n",
      "Iteration 20, loss = 0.00825333\n",
      "Iteration 21, loss = 0.01454489\n",
      "Iteration 22, loss = 0.01481288\n",
      "Iteration 23, loss = 0.00776858\n",
      "Iteration 24, loss = 0.00611168\n",
      "Iteration 25, loss = 0.01026532\n",
      "Iteration 26, loss = 0.01450016\n",
      "Iteration 27, loss = 0.00570699\n",
      "Iteration 28, loss = 0.00358090\n",
      "Iteration 29, loss = 0.00272678\n",
      "Iteration 30, loss = 0.00234696\n",
      "Iteration 31, loss = 0.00228960\n",
      "Iteration 32, loss = 0.00225237\n",
      "Iteration 33, loss = 0.00222340\n",
      "Iteration 34, loss = 0.00219763\n",
      "Iteration 35, loss = 0.00217282\n",
      "Iteration 36, loss = 0.00215051\n",
      "Iteration 37, loss = 0.00212987\n",
      "Iteration 38, loss = 0.00210837\n",
      "Iteration 39, loss = 0.00208623\n",
      "Iteration 40, loss = 0.00206583\n",
      "Iteration 41, loss = 0.00204605\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.001, batch_size=200, hidden_layer_sizes=(100, 100, 100), solver=adam; total time=  34.6s\n",
      "Iteration 1, loss = 2.10766994\n",
      "Iteration 2, loss = 1.34216551\n",
      "Iteration 3, loss = 0.74734769\n",
      "Iteration 4, loss = 0.54732914\n",
      "Iteration 5, loss = 0.45906821\n",
      "Iteration 6, loss = 0.40902863\n",
      "Iteration 7, loss = 0.37671340\n",
      "Iteration 8, loss = 0.35337071\n",
      "Iteration 9, loss = 0.33508576\n",
      "Iteration 10, loss = 0.32033941\n",
      "Iteration 11, loss = 0.30785365\n",
      "Iteration 12, loss = 0.29653737\n",
      "Iteration 13, loss = 0.28681789\n",
      "Iteration 14, loss = 0.27779891\n",
      "Iteration 15, loss = 0.26956707\n",
      "Iteration 16, loss = 0.26190204\n",
      "Iteration 17, loss = 0.25482594\n",
      "Iteration 18, loss = 0.24815399\n",
      "Iteration 19, loss = 0.24177227\n",
      "Iteration 20, loss = 0.23584597\n",
      "Iteration 21, loss = 0.23037853\n",
      "Iteration 22, loss = 0.22523788\n",
      "Iteration 23, loss = 0.22018460\n",
      "Iteration 24, loss = 0.21486496\n",
      "Iteration 25, loss = 0.21021254\n",
      "Iteration 26, loss = 0.20577710\n",
      "Iteration 27, loss = 0.20127667\n",
      "Iteration 28, loss = 0.19723193\n",
      "Iteration 29, loss = 0.19328794\n",
      "Iteration 30, loss = 0.18924145\n",
      "Iteration 31, loss = 0.18578063\n",
      "Iteration 32, loss = 0.18209995\n",
      "Iteration 33, loss = 0.17857874\n",
      "Iteration 34, loss = 0.17518756\n",
      "Iteration 35, loss = 0.17187360\n",
      "Iteration 36, loss = 0.16869415\n",
      "Iteration 37, loss = 0.16561612\n",
      "Iteration 38, loss = 0.16307944\n",
      "Iteration 39, loss = 0.15963582\n",
      "Iteration 40, loss = 0.15718119\n",
      "Iteration 41, loss = 0.15417918\n",
      "Iteration 42, loss = 0.15169798\n",
      "Iteration 43, loss = 0.14902608\n",
      "Iteration 44, loss = 0.14645840\n",
      "Iteration 45, loss = 0.14407677\n",
      "Iteration 46, loss = 0.14186017\n",
      "Iteration 47, loss = 0.13969803\n",
      "Iteration 48, loss = 0.13728504\n",
      "Iteration 49, loss = 0.13516348\n",
      "Iteration 50, loss = 0.13273832\n",
      "Iteration 51, loss = 0.13121462\n",
      "Iteration 52, loss = 0.12853744\n",
      "Iteration 53, loss = 0.12681982\n",
      "Iteration 54, loss = 0.12490070\n",
      "Iteration 55, loss = 0.12257627\n",
      "Iteration 56, loss = 0.12122808\n",
      "Iteration 57, loss = 0.11932943\n",
      "Iteration 58, loss = 0.11748602\n",
      "Iteration 59, loss = 0.11562566\n",
      "Iteration 60, loss = 0.11389994\n",
      "Iteration 61, loss = 0.11241551\n",
      "Iteration 62, loss = 0.11059524\n",
      "Iteration 63, loss = 0.10897030\n",
      "Iteration 64, loss = 0.10753357\n",
      "Iteration 65, loss = 0.10565245\n",
      "Iteration 66, loss = 0.10435103\n",
      "Iteration 67, loss = 0.10299091\n",
      "Iteration 68, loss = 0.10156604\n",
      "Iteration 69, loss = 0.10048508\n",
      "Iteration 70, loss = 0.09862180\n",
      "Iteration 71, loss = 0.09741187\n",
      "Iteration 72, loss = 0.09631654\n",
      "Iteration 73, loss = 0.09487480\n",
      "Iteration 74, loss = 0.09367749\n",
      "Iteration 75, loss = 0.09247301\n",
      "Iteration 76, loss = 0.09124080\n",
      "Iteration 77, loss = 0.09005899\n",
      "Iteration 78, loss = 0.08874780\n",
      "Iteration 79, loss = 0.08772990\n",
      "Iteration 80, loss = 0.08634920\n",
      "Iteration 81, loss = 0.08499683\n",
      "Iteration 82, loss = 0.08401451\n",
      "Iteration 83, loss = 0.08287942\n",
      "Iteration 84, loss = 0.08222302\n",
      "Iteration 85, loss = 0.08079021\n",
      "Iteration 86, loss = 0.07995515\n",
      "Iteration 87, loss = 0.07908843\n",
      "Iteration 88, loss = 0.07795694\n",
      "Iteration 89, loss = 0.07709869\n",
      "Iteration 90, loss = 0.07584970\n",
      "Iteration 91, loss = 0.07532810\n",
      "Iteration 92, loss = 0.07416091\n",
      "Iteration 93, loss = 0.07329153\n",
      "Iteration 94, loss = 0.07226538\n",
      "Iteration 95, loss = 0.07139529\n",
      "Iteration 96, loss = 0.07082109\n",
      "Iteration 97, loss = 0.06963284\n",
      "Iteration 98, loss = 0.06887001\n",
      "Iteration 99, loss = 0.06788124\n",
      "Iteration 100, loss = 0.06709830\n",
      "Iteration 101, loss = 0.06627179\n",
      "Iteration 102, loss = 0.06537201\n",
      "Iteration 103, loss = 0.06469837\n",
      "Iteration 104, loss = 0.06398118\n",
      "Iteration 105, loss = 0.06317535\n",
      "Iteration 106, loss = 0.06201734\n",
      "Iteration 107, loss = 0.06166702\n",
      "Iteration 108, loss = 0.06058298\n",
      "Iteration 109, loss = 0.05988483\n",
      "Iteration 110, loss = 0.05921816\n",
      "Iteration 111, loss = 0.05851897\n",
      "Iteration 112, loss = 0.05781264\n",
      "Iteration 113, loss = 0.05691959\n",
      "Iteration 114, loss = 0.05637407\n",
      "Iteration 115, loss = 0.05573932\n",
      "Iteration 116, loss = 0.05508956\n",
      "Iteration 117, loss = 0.05445130\n",
      "Iteration 118, loss = 0.05369515\n",
      "Iteration 119, loss = 0.05307475\n",
      "Iteration 120, loss = 0.05246154\n",
      "Iteration 121, loss = 0.05157915\n",
      "Iteration 122, loss = 0.05116486\n",
      "Iteration 123, loss = 0.05045933\n",
      "Iteration 124, loss = 0.04966357\n",
      "Iteration 125, loss = 0.04923242\n",
      "Iteration 126, loss = 0.04867230\n",
      "Iteration 127, loss = 0.04809424\n",
      "Iteration 128, loss = 0.04737142\n",
      "Iteration 129, loss = 0.04706961\n",
      "Iteration 130, loss = 0.04648155\n",
      "Iteration 131, loss = 0.04588543\n",
      "Iteration 132, loss = 0.04523424\n",
      "Iteration 133, loss = 0.04461899\n",
      "Iteration 134, loss = 0.04417970\n",
      "Iteration 135, loss = 0.04376300\n",
      "Iteration 136, loss = 0.04325150\n",
      "Iteration 137, loss = 0.04270433\n",
      "Iteration 138, loss = 0.04229689\n",
      "Iteration 139, loss = 0.04164317\n",
      "Iteration 140, loss = 0.04096794\n",
      "Iteration 141, loss = 0.04040837\n",
      "Iteration 142, loss = 0.04007319\n",
      "Iteration 143, loss = 0.03982542\n",
      "Iteration 144, loss = 0.03913334\n",
      "Iteration 145, loss = 0.03876581\n",
      "Iteration 146, loss = 0.03830555\n",
      "Iteration 147, loss = 0.03801545\n",
      "Iteration 148, loss = 0.03756840\n",
      "Iteration 149, loss = 0.03708278\n",
      "Iteration 150, loss = 0.03661491\n",
      "Iteration 151, loss = 0.03601105\n",
      "Iteration 152, loss = 0.03576796\n",
      "Iteration 153, loss = 0.03515173\n",
      "Iteration 154, loss = 0.03486742\n",
      "Iteration 155, loss = 0.03436387\n",
      "Iteration 156, loss = 0.03400355\n",
      "Iteration 157, loss = 0.03366597\n",
      "Iteration 158, loss = 0.03306982\n",
      "Iteration 159, loss = 0.03294693\n",
      "Iteration 160, loss = 0.03233871\n",
      "Iteration 161, loss = 0.03207315\n",
      "Iteration 162, loss = 0.03168337\n",
      "Iteration 163, loss = 0.03127615\n",
      "Iteration 164, loss = 0.03091745\n",
      "Iteration 165, loss = 0.03051604\n",
      "Iteration 166, loss = 0.03025525\n",
      "Iteration 167, loss = 0.02983960\n",
      "Iteration 168, loss = 0.02948708\n",
      "Iteration 169, loss = 0.02930519\n",
      "Iteration 170, loss = 0.02884401\n",
      "Iteration 171, loss = 0.02852902\n",
      "Iteration 172, loss = 0.02810026\n",
      "Iteration 173, loss = 0.02784530\n",
      "Iteration 174, loss = 0.02744558\n",
      "Iteration 175, loss = 0.02716172\n",
      "Iteration 176, loss = 0.02675892\n",
      "Iteration 177, loss = 0.02661483\n",
      "Iteration 178, loss = 0.02613804\n",
      "Iteration 179, loss = 0.02600928\n",
      "Iteration 180, loss = 0.02558107\n",
      "Iteration 181, loss = 0.02531505\n",
      "Iteration 182, loss = 0.02507566\n",
      "Iteration 183, loss = 0.02486711\n",
      "Iteration 184, loss = 0.02438634\n",
      "Iteration 185, loss = 0.02419177\n",
      "Iteration 186, loss = 0.02383622\n",
      "Iteration 187, loss = 0.02362175\n",
      "Iteration 188, loss = 0.02335455\n",
      "Iteration 189, loss = 0.02308763\n",
      "Iteration 190, loss = 0.02299112\n",
      "Iteration 191, loss = 0.02254325\n",
      "Iteration 192, loss = 0.02240134\n",
      "Iteration 193, loss = 0.02207967\n",
      "Iteration 194, loss = 0.02181065\n",
      "Iteration 195, loss = 0.02153505\n",
      "Iteration 196, loss = 0.02121934\n",
      "Iteration 197, loss = 0.02106498\n",
      "Iteration 198, loss = 0.02071518\n",
      "Iteration 199, loss = 0.02052874\n",
      "Iteration 200, loss = 0.02037065\n",
      "[CV] END activation=relu, alpha=0.001, batch_size=200, hidden_layer_sizes=(100, 100, 100), solver=sgd; total time= 1.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.13944365\n",
      "Iteration 2, loss = 1.53173445\n",
      "Iteration 3, loss = 0.89409389\n",
      "Iteration 4, loss = 0.60851020\n",
      "Iteration 5, loss = 0.49344895\n",
      "Iteration 6, loss = 0.43380340\n",
      "Iteration 7, loss = 0.39594125\n",
      "Iteration 8, loss = 0.36992797\n",
      "Iteration 9, loss = 0.34902561\n",
      "Iteration 10, loss = 0.33266017\n",
      "Iteration 11, loss = 0.31879107\n",
      "Iteration 12, loss = 0.30741174\n",
      "Iteration 13, loss = 0.29716846\n",
      "Iteration 14, loss = 0.28816718\n",
      "Iteration 15, loss = 0.27962614\n",
      "Iteration 16, loss = 0.27241201\n",
      "Iteration 17, loss = 0.26527066\n",
      "Iteration 18, loss = 0.25922641\n",
      "Iteration 19, loss = 0.25276430\n",
      "Iteration 20, loss = 0.24695386\n",
      "Iteration 21, loss = 0.24221480\n",
      "Iteration 22, loss = 0.23685189\n",
      "Iteration 23, loss = 0.23154342\n",
      "Iteration 24, loss = 0.22747173\n",
      "Iteration 25, loss = 0.22279943\n",
      "Iteration 26, loss = 0.21845568\n",
      "Iteration 27, loss = 0.21441150\n",
      "Iteration 28, loss = 0.21055537\n",
      "Iteration 29, loss = 0.20696917\n",
      "Iteration 30, loss = 0.20346098\n",
      "Iteration 31, loss = 0.20005872\n",
      "Iteration 32, loss = 0.19629858\n",
      "Iteration 33, loss = 0.19318595\n",
      "Iteration 34, loss = 0.19000661\n",
      "Iteration 35, loss = 0.18733071\n",
      "Iteration 36, loss = 0.18411424\n",
      "Iteration 37, loss = 0.18133496\n",
      "Iteration 38, loss = 0.17850550\n",
      "Iteration 39, loss = 0.17578687\n",
      "Iteration 40, loss = 0.17318147\n",
      "Iteration 41, loss = 0.17067260\n",
      "Iteration 42, loss = 0.16814511\n",
      "Iteration 43, loss = 0.16567728\n",
      "Iteration 44, loss = 0.16348401\n",
      "Iteration 45, loss = 0.16102332\n",
      "Iteration 46, loss = 0.15896327\n",
      "Iteration 47, loss = 0.15647342\n",
      "Iteration 48, loss = 0.15456206\n",
      "Iteration 49, loss = 0.15230887\n",
      "Iteration 50, loss = 0.15011099\n",
      "Iteration 51, loss = 0.14831488\n",
      "Iteration 52, loss = 0.14601738\n",
      "Iteration 53, loss = 0.14435369\n",
      "Iteration 54, loss = 0.14227457\n",
      "Iteration 55, loss = 0.14036215\n",
      "Iteration 56, loss = 0.13852589\n",
      "Iteration 57, loss = 0.13689570\n",
      "Iteration 58, loss = 0.13492467\n",
      "Iteration 59, loss = 0.13320572\n",
      "Iteration 60, loss = 0.13169143\n",
      "Iteration 61, loss = 0.13015926\n",
      "Iteration 62, loss = 0.12816184\n",
      "Iteration 63, loss = 0.12684538\n",
      "Iteration 64, loss = 0.12541813\n",
      "Iteration 65, loss = 0.12345909\n",
      "Iteration 66, loss = 0.12178224\n",
      "Iteration 67, loss = 0.12042772\n",
      "Iteration 68, loss = 0.11899755\n",
      "Iteration 69, loss = 0.11745398\n",
      "Iteration 70, loss = 0.11586846\n",
      "Iteration 71, loss = 0.11454279\n",
      "Iteration 72, loss = 0.11341290\n",
      "Iteration 73, loss = 0.11181019\n",
      "Iteration 74, loss = 0.11041055\n",
      "Iteration 75, loss = 0.10900553\n",
      "Iteration 76, loss = 0.10773899\n",
      "Iteration 77, loss = 0.10640613\n",
      "Iteration 78, loss = 0.10525884\n",
      "Iteration 79, loss = 0.10388941\n",
      "Iteration 80, loss = 0.10279020\n",
      "Iteration 81, loss = 0.10127390\n",
      "Iteration 82, loss = 0.09999160\n",
      "Iteration 83, loss = 0.09878805\n",
      "Iteration 84, loss = 0.09768000\n",
      "Iteration 85, loss = 0.09661742\n",
      "Iteration 86, loss = 0.09567128\n",
      "Iteration 87, loss = 0.09431791\n",
      "Iteration 88, loss = 0.09336384\n",
      "Iteration 89, loss = 0.09209530\n",
      "Iteration 90, loss = 0.09117867\n",
      "Iteration 91, loss = 0.09005874\n",
      "Iteration 92, loss = 0.08905297\n",
      "Iteration 93, loss = 0.08821040\n",
      "Iteration 94, loss = 0.08683765\n",
      "Iteration 95, loss = 0.08605317\n",
      "Iteration 96, loss = 0.08490261\n",
      "Iteration 97, loss = 0.08398098\n",
      "Iteration 98, loss = 0.08292608\n",
      "Iteration 99, loss = 0.08200977\n",
      "Iteration 100, loss = 0.08121906\n",
      "Iteration 101, loss = 0.08023365\n",
      "Iteration 102, loss = 0.07905295\n",
      "Iteration 103, loss = 0.07824982\n",
      "Iteration 104, loss = 0.07736559\n",
      "Iteration 105, loss = 0.07671405\n",
      "Iteration 106, loss = 0.07594431\n",
      "Iteration 107, loss = 0.07477657\n",
      "Iteration 108, loss = 0.07417797\n",
      "Iteration 109, loss = 0.07332265\n",
      "Iteration 110, loss = 0.07254939\n",
      "Iteration 111, loss = 0.07155996\n",
      "Iteration 112, loss = 0.07079490\n",
      "Iteration 113, loss = 0.07018300\n",
      "Iteration 114, loss = 0.06929141\n",
      "Iteration 115, loss = 0.06835511\n",
      "Iteration 116, loss = 0.06775145\n",
      "Iteration 117, loss = 0.06707512\n",
      "Iteration 118, loss = 0.06619352\n",
      "Iteration 119, loss = 0.06561207\n",
      "Iteration 120, loss = 0.06490839\n",
      "Iteration 121, loss = 0.06436538\n",
      "Iteration 122, loss = 0.06346821\n",
      "Iteration 123, loss = 0.06279928\n",
      "Iteration 124, loss = 0.06222605\n",
      "Iteration 125, loss = 0.06135840\n",
      "Iteration 126, loss = 0.06098503\n",
      "Iteration 127, loss = 0.05991297\n",
      "Iteration 128, loss = 0.05930505\n",
      "Iteration 129, loss = 0.05862352\n",
      "Iteration 130, loss = 0.05813404\n",
      "Iteration 131, loss = 0.05735474\n",
      "Iteration 132, loss = 0.05711686\n",
      "Iteration 133, loss = 0.05644351\n",
      "Iteration 134, loss = 0.05551375\n",
      "Iteration 135, loss = 0.05506887\n",
      "Iteration 136, loss = 0.05436188\n",
      "Iteration 137, loss = 0.05374751\n",
      "Iteration 138, loss = 0.05317920\n",
      "Iteration 139, loss = 0.05273724\n",
      "Iteration 140, loss = 0.05220598\n",
      "Iteration 141, loss = 0.05145276\n",
      "Iteration 142, loss = 0.05107988\n",
      "Iteration 143, loss = 0.05049216\n",
      "Iteration 144, loss = 0.04991968\n",
      "Iteration 145, loss = 0.04936345\n",
      "Iteration 146, loss = 0.04873916\n",
      "Iteration 147, loss = 0.04825702\n",
      "Iteration 148, loss = 0.04789098\n",
      "Iteration 149, loss = 0.04702794\n",
      "Iteration 150, loss = 0.04684541\n",
      "Iteration 151, loss = 0.04622070\n",
      "Iteration 152, loss = 0.04597360\n",
      "Iteration 153, loss = 0.04499807\n",
      "Iteration 154, loss = 0.04480081\n",
      "Iteration 155, loss = 0.04412307\n",
      "Iteration 156, loss = 0.04389409\n",
      "Iteration 157, loss = 0.04336035\n",
      "Iteration 158, loss = 0.04299811\n",
      "Iteration 159, loss = 0.04242746\n",
      "Iteration 160, loss = 0.04192372\n",
      "Iteration 161, loss = 0.04158615\n",
      "Iteration 162, loss = 0.04111968\n",
      "Iteration 163, loss = 0.04068224\n",
      "Iteration 164, loss = 0.04012145\n",
      "Iteration 165, loss = 0.03996517\n",
      "Iteration 166, loss = 0.03931739\n",
      "Iteration 167, loss = 0.03904499\n",
      "Iteration 168, loss = 0.03852373\n",
      "Iteration 169, loss = 0.03807347\n",
      "Iteration 170, loss = 0.03774302\n",
      "Iteration 171, loss = 0.03723550\n",
      "Iteration 172, loss = 0.03694588\n",
      "Iteration 173, loss = 0.03644544\n",
      "Iteration 174, loss = 0.03616549\n",
      "Iteration 175, loss = 0.03568522\n",
      "Iteration 176, loss = 0.03543288\n",
      "Iteration 177, loss = 0.03497412\n",
      "Iteration 178, loss = 0.03453198\n",
      "Iteration 179, loss = 0.03427112\n",
      "Iteration 180, loss = 0.03381664\n",
      "Iteration 181, loss = 0.03335682\n",
      "Iteration 182, loss = 0.03299370\n",
      "Iteration 183, loss = 0.03272836\n",
      "Iteration 184, loss = 0.03254884\n",
      "Iteration 185, loss = 0.03225300\n",
      "Iteration 186, loss = 0.03164020\n",
      "Iteration 187, loss = 0.03140431\n",
      "Iteration 188, loss = 0.03111787\n",
      "Iteration 189, loss = 0.03075071\n",
      "Iteration 190, loss = 0.03045508\n",
      "Iteration 191, loss = 0.02997761\n",
      "Iteration 192, loss = 0.02974167\n",
      "Iteration 193, loss = 0.02945238\n",
      "Iteration 194, loss = 0.02913258\n",
      "Iteration 195, loss = 0.02872693\n",
      "Iteration 196, loss = 0.02844381\n",
      "Iteration 197, loss = 0.02818215\n",
      "Iteration 198, loss = 0.02782875\n",
      "Iteration 199, loss = 0.02757546\n",
      "Iteration 200, loss = 0.02721386\n",
      "[CV] END activation=relu, alpha=0.001, batch_size=200, hidden_layer_sizes=(100, 100, 100), solver=sgd; total time= 1.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.21406662\n",
      "Iteration 2, loss = 1.64478531\n",
      "Iteration 3, loss = 0.87818511\n",
      "Iteration 4, loss = 0.58557363\n",
      "Iteration 5, loss = 0.47483868\n",
      "Iteration 6, loss = 0.41704156\n",
      "Iteration 7, loss = 0.38084384\n",
      "Iteration 8, loss = 0.35533156\n",
      "Iteration 9, loss = 0.33492676\n",
      "Iteration 10, loss = 0.31981041\n",
      "Iteration 11, loss = 0.30609916\n",
      "Iteration 12, loss = 0.29450116\n",
      "Iteration 13, loss = 0.28483694\n",
      "Iteration 14, loss = 0.27556637\n",
      "Iteration 15, loss = 0.26757774\n",
      "Iteration 16, loss = 0.25992627\n",
      "Iteration 17, loss = 0.25312524\n",
      "Iteration 18, loss = 0.24665580\n",
      "Iteration 19, loss = 0.24055197\n",
      "Iteration 20, loss = 0.23471640\n",
      "Iteration 21, loss = 0.22927689\n",
      "Iteration 22, loss = 0.22411252\n",
      "Iteration 23, loss = 0.21911193\n",
      "Iteration 24, loss = 0.21430540\n",
      "Iteration 25, loss = 0.21032785\n",
      "Iteration 26, loss = 0.20573326\n",
      "Iteration 27, loss = 0.20178606\n",
      "Iteration 28, loss = 0.19776624\n",
      "Iteration 29, loss = 0.19391839\n",
      "Iteration 30, loss = 0.19038181\n",
      "Iteration 31, loss = 0.18723156\n",
      "Iteration 32, loss = 0.18339990\n",
      "Iteration 33, loss = 0.18039067\n",
      "Iteration 34, loss = 0.17705417\n",
      "Iteration 35, loss = 0.17374068\n",
      "Iteration 36, loss = 0.17068841\n",
      "Iteration 37, loss = 0.16839487\n",
      "Iteration 38, loss = 0.16511615\n",
      "Iteration 39, loss = 0.16255726\n",
      "Iteration 40, loss = 0.16011740\n",
      "Iteration 41, loss = 0.15723943\n",
      "Iteration 42, loss = 0.15471768\n",
      "Iteration 43, loss = 0.15239240\n",
      "Iteration 44, loss = 0.14986481\n",
      "Iteration 45, loss = 0.14762843\n",
      "Iteration 46, loss = 0.14510248\n",
      "Iteration 47, loss = 0.14302844\n",
      "Iteration 48, loss = 0.14097298\n",
      "Iteration 49, loss = 0.13890285\n",
      "Iteration 50, loss = 0.13676735\n",
      "Iteration 51, loss = 0.13484879\n",
      "Iteration 52, loss = 0.13256024\n",
      "Iteration 53, loss = 0.13080258\n",
      "Iteration 54, loss = 0.12883474\n",
      "Iteration 55, loss = 0.12680305\n",
      "Iteration 56, loss = 0.12523861\n",
      "Iteration 57, loss = 0.12309317\n",
      "Iteration 58, loss = 0.12114624\n",
      "Iteration 59, loss = 0.11957916\n",
      "Iteration 60, loss = 0.11798055\n",
      "Iteration 61, loss = 0.11605114\n",
      "Iteration 62, loss = 0.11473427\n",
      "Iteration 63, loss = 0.11291266\n",
      "Iteration 64, loss = 0.11140234\n",
      "Iteration 65, loss = 0.10964670\n",
      "Iteration 66, loss = 0.10836181\n",
      "Iteration 67, loss = 0.10670876\n",
      "Iteration 68, loss = 0.10525020\n",
      "Iteration 69, loss = 0.10366817\n",
      "Iteration 70, loss = 0.10230017\n",
      "Iteration 71, loss = 0.10076730\n",
      "Iteration 72, loss = 0.09973962\n",
      "Iteration 73, loss = 0.09845114\n",
      "Iteration 74, loss = 0.09702451\n",
      "Iteration 75, loss = 0.09563012\n",
      "Iteration 76, loss = 0.09460623\n",
      "Iteration 77, loss = 0.09311435\n",
      "Iteration 78, loss = 0.09213817\n",
      "Iteration 79, loss = 0.09090474\n",
      "Iteration 80, loss = 0.08963769\n",
      "Iteration 81, loss = 0.08868171\n",
      "Iteration 82, loss = 0.08751824\n",
      "Iteration 83, loss = 0.08653184\n",
      "Iteration 84, loss = 0.08541585\n",
      "Iteration 85, loss = 0.08419390\n",
      "Iteration 86, loss = 0.08319673\n",
      "Iteration 87, loss = 0.08198731\n",
      "Iteration 88, loss = 0.08084195\n",
      "Iteration 89, loss = 0.08005878\n",
      "Iteration 90, loss = 0.07895668\n",
      "Iteration 91, loss = 0.07781492\n",
      "Iteration 92, loss = 0.07715185\n",
      "Iteration 93, loss = 0.07589910\n",
      "Iteration 94, loss = 0.07511316\n",
      "Iteration 95, loss = 0.07443431\n",
      "Iteration 96, loss = 0.07321986\n",
      "Iteration 97, loss = 0.07232707\n",
      "Iteration 98, loss = 0.07154959\n",
      "Iteration 99, loss = 0.07080683\n",
      "Iteration 100, loss = 0.06980415\n",
      "Iteration 101, loss = 0.06923436\n",
      "Iteration 102, loss = 0.06812018\n",
      "Iteration 103, loss = 0.06727046\n",
      "Iteration 104, loss = 0.06659712\n",
      "Iteration 105, loss = 0.06561098\n",
      "Iteration 106, loss = 0.06507476\n",
      "Iteration 107, loss = 0.06417195\n",
      "Iteration 108, loss = 0.06342602\n",
      "Iteration 109, loss = 0.06277365\n",
      "Iteration 110, loss = 0.06215589\n",
      "Iteration 111, loss = 0.06134918\n",
      "Iteration 112, loss = 0.06043695\n",
      "Iteration 113, loss = 0.06017373\n",
      "Iteration 114, loss = 0.05920362\n",
      "Iteration 115, loss = 0.05862879\n",
      "Iteration 116, loss = 0.05793759\n",
      "Iteration 117, loss = 0.05699992\n",
      "Iteration 118, loss = 0.05643342\n",
      "Iteration 119, loss = 0.05572321\n",
      "Iteration 120, loss = 0.05502010\n",
      "Iteration 121, loss = 0.05452084\n",
      "Iteration 122, loss = 0.05397798\n",
      "Iteration 123, loss = 0.05340744\n",
      "Iteration 124, loss = 0.05288176\n",
      "Iteration 125, loss = 0.05244784\n",
      "Iteration 126, loss = 0.05176408\n",
      "Iteration 127, loss = 0.05080988\n",
      "Iteration 128, loss = 0.05033711\n",
      "Iteration 129, loss = 0.04978347\n",
      "Iteration 130, loss = 0.04931864\n",
      "Iteration 131, loss = 0.04875331\n",
      "Iteration 132, loss = 0.04814475\n",
      "Iteration 133, loss = 0.04756267\n",
      "Iteration 134, loss = 0.04704287\n",
      "Iteration 135, loss = 0.04660362\n",
      "Iteration 136, loss = 0.04590627\n",
      "Iteration 137, loss = 0.04544809\n",
      "Iteration 138, loss = 0.04479121\n",
      "Iteration 139, loss = 0.04455359\n",
      "Iteration 140, loss = 0.04402372\n",
      "Iteration 141, loss = 0.04364876\n",
      "Iteration 142, loss = 0.04296630\n",
      "Iteration 143, loss = 0.04257330\n",
      "Iteration 144, loss = 0.04197757\n",
      "Iteration 145, loss = 0.04139570\n",
      "Iteration 146, loss = 0.04121251\n",
      "Iteration 147, loss = 0.04086557\n",
      "Iteration 148, loss = 0.04020219\n",
      "Iteration 149, loss = 0.03958820\n",
      "Iteration 150, loss = 0.03918382\n",
      "Iteration 151, loss = 0.03870130\n",
      "Iteration 152, loss = 0.03818402\n",
      "Iteration 153, loss = 0.03793240\n",
      "Iteration 154, loss = 0.03758121\n",
      "Iteration 155, loss = 0.03696767\n",
      "Iteration 156, loss = 0.03687787\n",
      "Iteration 157, loss = 0.03617017\n",
      "Iteration 158, loss = 0.03594891\n",
      "Iteration 159, loss = 0.03550507\n",
      "Iteration 160, loss = 0.03528110\n",
      "Iteration 161, loss = 0.03457104\n",
      "Iteration 162, loss = 0.03441829\n",
      "Iteration 163, loss = 0.03375974\n",
      "Iteration 164, loss = 0.03367685\n",
      "Iteration 165, loss = 0.03321902\n",
      "Iteration 166, loss = 0.03272096\n",
      "Iteration 167, loss = 0.03244424\n",
      "Iteration 168, loss = 0.03203894\n",
      "Iteration 169, loss = 0.03170382\n",
      "Iteration 170, loss = 0.03141838\n",
      "Iteration 171, loss = 0.03095000\n",
      "Iteration 172, loss = 0.03072894\n",
      "Iteration 173, loss = 0.03025937\n",
      "Iteration 174, loss = 0.02995460\n",
      "Iteration 175, loss = 0.02966277\n",
      "Iteration 176, loss = 0.02936849\n",
      "Iteration 177, loss = 0.02894923\n",
      "Iteration 178, loss = 0.02858531\n",
      "Iteration 179, loss = 0.02837812\n",
      "Iteration 180, loss = 0.02799474\n",
      "Iteration 181, loss = 0.02768092\n",
      "Iteration 182, loss = 0.02742329\n",
      "Iteration 183, loss = 0.02702913\n",
      "Iteration 184, loss = 0.02674486\n",
      "Iteration 185, loss = 0.02650039\n",
      "Iteration 186, loss = 0.02601730\n",
      "Iteration 187, loss = 0.02599012\n",
      "Iteration 188, loss = 0.02554461\n",
      "Iteration 189, loss = 0.02533146\n",
      "Iteration 190, loss = 0.02499111\n",
      "Iteration 191, loss = 0.02476920\n",
      "Iteration 192, loss = 0.02459099\n",
      "Iteration 193, loss = 0.02428044\n",
      "Iteration 194, loss = 0.02409310\n",
      "Iteration 195, loss = 0.02370812\n",
      "Iteration 196, loss = 0.02349893\n",
      "Iteration 197, loss = 0.02324096\n",
      "Iteration 198, loss = 0.02286100\n",
      "Iteration 199, loss = 0.02265654\n",
      "Iteration 200, loss = 0.02254556\n",
      "[CV] END activation=relu, alpha=0.001, batch_size=200, hidden_layer_sizes=(100, 100, 100), solver=sgd; total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.11157147\n",
      "Iteration 2, loss = 1.37735099\n",
      "Iteration 3, loss = 0.81184921\n",
      "Iteration 4, loss = 0.59841693\n",
      "Iteration 5, loss = 0.49493854\n",
      "Iteration 6, loss = 0.43516544\n",
      "Iteration 7, loss = 0.39645456\n",
      "Iteration 8, loss = 0.36963624\n",
      "Iteration 9, loss = 0.34928377\n",
      "Iteration 10, loss = 0.33255369\n",
      "Iteration 11, loss = 0.31965788\n",
      "Iteration 12, loss = 0.30787312\n",
      "Iteration 13, loss = 0.29786094\n",
      "Iteration 14, loss = 0.28859333\n",
      "Iteration 15, loss = 0.28079733\n",
      "Iteration 16, loss = 0.27337569\n",
      "Iteration 17, loss = 0.26614906\n",
      "Iteration 18, loss = 0.25899660\n",
      "Iteration 19, loss = 0.25329260\n",
      "Iteration 20, loss = 0.24778471\n",
      "Iteration 21, loss = 0.24185631\n",
      "Iteration 22, loss = 0.23659040\n",
      "Iteration 23, loss = 0.23124123\n",
      "Iteration 24, loss = 0.22614300\n",
      "Iteration 25, loss = 0.22144776\n",
      "Iteration 26, loss = 0.21682189\n",
      "Iteration 27, loss = 0.21252269\n",
      "Iteration 28, loss = 0.20852953\n",
      "Iteration 29, loss = 0.20448537\n",
      "Iteration 30, loss = 0.20011576\n",
      "Iteration 31, loss = 0.19632736\n",
      "Iteration 32, loss = 0.19275920\n",
      "Iteration 33, loss = 0.18879083\n",
      "Iteration 34, loss = 0.18563617\n",
      "Iteration 35, loss = 0.18219117\n",
      "Iteration 36, loss = 0.17896395\n",
      "Iteration 37, loss = 0.17589828\n",
      "Iteration 38, loss = 0.17258229\n",
      "Iteration 39, loss = 0.16942480\n",
      "Iteration 40, loss = 0.16664757\n",
      "Iteration 41, loss = 0.16444401\n",
      "Iteration 42, loss = 0.16131508\n",
      "Iteration 43, loss = 0.15843746\n",
      "Iteration 44, loss = 0.15628882\n",
      "Iteration 45, loss = 0.15375106\n",
      "Iteration 46, loss = 0.15108041\n",
      "Iteration 47, loss = 0.14891216\n",
      "Iteration 48, loss = 0.14669202\n",
      "Iteration 49, loss = 0.14415718\n",
      "Iteration 50, loss = 0.14190302\n",
      "Iteration 51, loss = 0.13988301\n",
      "Iteration 52, loss = 0.13784382\n",
      "Iteration 53, loss = 0.13573999\n",
      "Iteration 54, loss = 0.13360281\n",
      "Iteration 55, loss = 0.13173804\n",
      "Iteration 56, loss = 0.12953028\n",
      "Iteration 57, loss = 0.12807270\n",
      "Iteration 58, loss = 0.12587109\n",
      "Iteration 59, loss = 0.12452701\n",
      "Iteration 60, loss = 0.12290827\n",
      "Iteration 61, loss = 0.12069383\n",
      "Iteration 62, loss = 0.11927831\n",
      "Iteration 63, loss = 0.11758977\n",
      "Iteration 64, loss = 0.11599242\n",
      "Iteration 65, loss = 0.11452764\n",
      "Iteration 66, loss = 0.11278781\n",
      "Iteration 67, loss = 0.11136429\n",
      "Iteration 68, loss = 0.10989820\n",
      "Iteration 69, loss = 0.10873979\n",
      "Iteration 70, loss = 0.10718227\n",
      "Iteration 71, loss = 0.10588155\n",
      "Iteration 72, loss = 0.10466677\n",
      "Iteration 73, loss = 0.10312665\n",
      "Iteration 74, loss = 0.10159527\n",
      "Iteration 75, loss = 0.10054202\n",
      "Iteration 76, loss = 0.09932902\n",
      "Iteration 77, loss = 0.09791947\n",
      "Iteration 78, loss = 0.09691653\n",
      "Iteration 79, loss = 0.09579025\n",
      "Iteration 80, loss = 0.09441448\n",
      "Iteration 81, loss = 0.09300354\n",
      "Iteration 82, loss = 0.09200737\n",
      "Iteration 83, loss = 0.09099135\n",
      "Iteration 84, loss = 0.08973175\n",
      "Iteration 85, loss = 0.08870912\n",
      "Iteration 86, loss = 0.08745638\n",
      "Iteration 87, loss = 0.08640327\n",
      "Iteration 88, loss = 0.08526100\n",
      "Iteration 89, loss = 0.08437813\n",
      "Iteration 90, loss = 0.08335719\n",
      "Iteration 91, loss = 0.08213080\n",
      "Iteration 92, loss = 0.08151900\n",
      "Iteration 93, loss = 0.08062003\n",
      "Iteration 94, loss = 0.07944542\n",
      "Iteration 95, loss = 0.07839842\n",
      "Iteration 96, loss = 0.07760556\n",
      "Iteration 97, loss = 0.07643990\n",
      "Iteration 98, loss = 0.07573735\n",
      "Iteration 99, loss = 0.07455992\n",
      "Iteration 100, loss = 0.07398635\n",
      "Iteration 101, loss = 0.07308598\n",
      "Iteration 102, loss = 0.07218606\n",
      "Iteration 103, loss = 0.07147420\n",
      "Iteration 104, loss = 0.07038778\n",
      "Iteration 105, loss = 0.06948470\n",
      "Iteration 106, loss = 0.06877090\n",
      "Iteration 107, loss = 0.06803372\n",
      "Iteration 108, loss = 0.06738540\n",
      "Iteration 109, loss = 0.06644126\n",
      "Iteration 110, loss = 0.06581618\n",
      "Iteration 111, loss = 0.06521841\n",
      "Iteration 112, loss = 0.06415401\n",
      "Iteration 113, loss = 0.06341924\n",
      "Iteration 114, loss = 0.06242989\n",
      "Iteration 115, loss = 0.06227224\n",
      "Iteration 116, loss = 0.06120986\n",
      "Iteration 117, loss = 0.06061622\n",
      "Iteration 118, loss = 0.05985858\n",
      "Iteration 119, loss = 0.05927798\n",
      "Iteration 120, loss = 0.05870799\n",
      "Iteration 121, loss = 0.05804336\n",
      "Iteration 122, loss = 0.05711120\n",
      "Iteration 123, loss = 0.05667598\n",
      "Iteration 124, loss = 0.05597065\n",
      "Iteration 125, loss = 0.05523977\n",
      "Iteration 126, loss = 0.05477967\n",
      "Iteration 127, loss = 0.05407210\n",
      "Iteration 128, loss = 0.05344239\n",
      "Iteration 129, loss = 0.05301804\n",
      "Iteration 130, loss = 0.05218517\n",
      "Iteration 131, loss = 0.05184720\n",
      "Iteration 132, loss = 0.05105238\n",
      "Iteration 133, loss = 0.05054667\n",
      "Iteration 134, loss = 0.05001370\n",
      "Iteration 135, loss = 0.04948662\n",
      "Iteration 136, loss = 0.04890169\n",
      "Iteration 137, loss = 0.04847478\n",
      "Iteration 138, loss = 0.04784622\n",
      "Iteration 139, loss = 0.04748967\n",
      "Iteration 140, loss = 0.04687016\n",
      "Iteration 141, loss = 0.04631520\n",
      "Iteration 142, loss = 0.04569276\n",
      "Iteration 143, loss = 0.04531743\n",
      "Iteration 144, loss = 0.04458222\n",
      "Iteration 145, loss = 0.04439867\n",
      "Iteration 146, loss = 0.04375147\n",
      "Iteration 147, loss = 0.04329189\n",
      "Iteration 148, loss = 0.04291289\n",
      "Iteration 149, loss = 0.04246924\n",
      "Iteration 150, loss = 0.04200528\n",
      "Iteration 151, loss = 0.04134660\n",
      "Iteration 152, loss = 0.04104342\n",
      "Iteration 153, loss = 0.04050459\n",
      "Iteration 154, loss = 0.04003855\n",
      "Iteration 155, loss = 0.03959296\n",
      "Iteration 156, loss = 0.03922657\n",
      "Iteration 157, loss = 0.03897494\n",
      "Iteration 158, loss = 0.03843386\n",
      "Iteration 159, loss = 0.03805119\n",
      "Iteration 160, loss = 0.03751023\n",
      "Iteration 161, loss = 0.03714064\n",
      "Iteration 162, loss = 0.03670395\n",
      "Iteration 163, loss = 0.03636183\n",
      "Iteration 164, loss = 0.03594857\n",
      "Iteration 165, loss = 0.03561857\n",
      "Iteration 166, loss = 0.03518055\n",
      "Iteration 167, loss = 0.03480148\n",
      "Iteration 168, loss = 0.03454820\n",
      "Iteration 169, loss = 0.03406602\n",
      "Iteration 170, loss = 0.03368015\n",
      "Iteration 171, loss = 0.03333521\n",
      "Iteration 172, loss = 0.03300283\n",
      "Iteration 173, loss = 0.03256601\n",
      "Iteration 174, loss = 0.03231334\n",
      "Iteration 175, loss = 0.03174439\n",
      "Iteration 176, loss = 0.03150866\n",
      "Iteration 177, loss = 0.03111129\n",
      "Iteration 178, loss = 0.03091613\n",
      "Iteration 179, loss = 0.03055549\n",
      "Iteration 180, loss = 0.03015230\n",
      "Iteration 181, loss = 0.02981155\n",
      "Iteration 182, loss = 0.02958509\n",
      "Iteration 183, loss = 0.02926520\n",
      "Iteration 184, loss = 0.02898344\n",
      "Iteration 185, loss = 0.02839616\n",
      "Iteration 186, loss = 0.02836233\n",
      "Iteration 187, loss = 0.02790142\n",
      "Iteration 188, loss = 0.02759027\n",
      "Iteration 189, loss = 0.02731007\n",
      "Iteration 190, loss = 0.02696560\n",
      "Iteration 191, loss = 0.02680140\n",
      "Iteration 192, loss = 0.02640903\n",
      "Iteration 193, loss = 0.02610849\n",
      "Iteration 194, loss = 0.02586504\n",
      "Iteration 195, loss = 0.02553210\n",
      "Iteration 196, loss = 0.02520600\n",
      "Iteration 197, loss = 0.02501948\n",
      "Iteration 198, loss = 0.02466748\n",
      "Iteration 199, loss = 0.02433058\n",
      "Iteration 200, loss = 0.02422774\n",
      "[CV] END activation=relu, alpha=0.001, batch_size=200, hidden_layer_sizes=(100, 100, 100), solver=sgd; total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.12034047\n",
      "Iteration 2, loss = 1.38557204\n",
      "Iteration 3, loss = 0.79941994\n",
      "Iteration 4, loss = 0.59456366\n",
      "Iteration 5, loss = 0.49724686\n",
      "Iteration 6, loss = 0.43691901\n",
      "Iteration 7, loss = 0.39670134\n",
      "Iteration 8, loss = 0.36793524\n",
      "Iteration 9, loss = 0.34669238\n",
      "Iteration 10, loss = 0.32994793\n",
      "Iteration 11, loss = 0.31598981\n",
      "Iteration 12, loss = 0.30396655\n",
      "Iteration 13, loss = 0.29394885\n",
      "Iteration 14, loss = 0.28482491\n",
      "Iteration 15, loss = 0.27680130\n",
      "Iteration 16, loss = 0.26892537\n",
      "Iteration 17, loss = 0.26179932\n",
      "Iteration 18, loss = 0.25537421\n",
      "Iteration 19, loss = 0.24889727\n",
      "Iteration 20, loss = 0.24347567\n",
      "Iteration 21, loss = 0.23766284\n",
      "Iteration 22, loss = 0.23207570\n",
      "Iteration 23, loss = 0.22717938\n",
      "Iteration 24, loss = 0.22206449\n",
      "Iteration 25, loss = 0.21755567\n",
      "Iteration 26, loss = 0.21283137\n",
      "Iteration 27, loss = 0.20835881\n",
      "Iteration 28, loss = 0.20406768\n",
      "Iteration 29, loss = 0.20006347\n",
      "Iteration 30, loss = 0.19622482\n",
      "Iteration 31, loss = 0.19239799\n",
      "Iteration 32, loss = 0.18873324\n",
      "Iteration 33, loss = 0.18484895\n",
      "Iteration 34, loss = 0.18154231\n",
      "Iteration 35, loss = 0.17811572\n",
      "Iteration 36, loss = 0.17465227\n",
      "Iteration 37, loss = 0.17150084\n",
      "Iteration 38, loss = 0.16868007\n",
      "Iteration 39, loss = 0.16534365\n",
      "Iteration 40, loss = 0.16277823\n",
      "Iteration 41, loss = 0.15994171\n",
      "Iteration 42, loss = 0.15702331\n",
      "Iteration 43, loss = 0.15473631\n",
      "Iteration 44, loss = 0.15193364\n",
      "Iteration 45, loss = 0.14948551\n",
      "Iteration 46, loss = 0.14679491\n",
      "Iteration 47, loss = 0.14445830\n",
      "Iteration 48, loss = 0.14225216\n",
      "Iteration 49, loss = 0.13972686\n",
      "Iteration 50, loss = 0.13774243\n",
      "Iteration 51, loss = 0.13530181\n",
      "Iteration 52, loss = 0.13335096\n",
      "Iteration 53, loss = 0.13126610\n",
      "Iteration 54, loss = 0.12902905\n",
      "Iteration 55, loss = 0.12747495\n",
      "Iteration 56, loss = 0.12523130\n",
      "Iteration 57, loss = 0.12330269\n",
      "Iteration 58, loss = 0.12144596\n",
      "Iteration 59, loss = 0.11988555\n",
      "Iteration 60, loss = 0.11782331\n",
      "Iteration 61, loss = 0.11611560\n",
      "Iteration 62, loss = 0.11439281\n",
      "Iteration 63, loss = 0.11274405\n",
      "Iteration 64, loss = 0.11096929\n",
      "Iteration 65, loss = 0.10952901\n",
      "Iteration 66, loss = 0.10777645\n",
      "Iteration 67, loss = 0.10610915\n",
      "Iteration 68, loss = 0.10467834\n",
      "Iteration 69, loss = 0.10300805\n",
      "Iteration 70, loss = 0.10166761\n",
      "Iteration 71, loss = 0.10007642\n",
      "Iteration 72, loss = 0.09891421\n",
      "Iteration 73, loss = 0.09736313\n",
      "Iteration 74, loss = 0.09601947\n",
      "Iteration 75, loss = 0.09472391\n",
      "Iteration 76, loss = 0.09334523\n",
      "Iteration 77, loss = 0.09199219\n",
      "Iteration 78, loss = 0.09078415\n",
      "Iteration 79, loss = 0.08967381\n",
      "Iteration 80, loss = 0.08818222\n",
      "Iteration 81, loss = 0.08683607\n",
      "Iteration 82, loss = 0.08589965\n",
      "Iteration 83, loss = 0.08474566\n",
      "Iteration 84, loss = 0.08339193\n",
      "Iteration 85, loss = 0.08247759\n",
      "Iteration 86, loss = 0.08121450\n",
      "Iteration 87, loss = 0.08008092\n",
      "Iteration 88, loss = 0.07899601\n",
      "Iteration 89, loss = 0.07783046\n",
      "Iteration 90, loss = 0.07709096\n",
      "Iteration 91, loss = 0.07586962\n",
      "Iteration 92, loss = 0.07491552\n",
      "Iteration 93, loss = 0.07387943\n",
      "Iteration 94, loss = 0.07301453\n",
      "Iteration 95, loss = 0.07193106\n",
      "Iteration 96, loss = 0.07102672\n",
      "Iteration 97, loss = 0.07021261\n",
      "Iteration 98, loss = 0.06892375\n",
      "Iteration 99, loss = 0.06799278\n",
      "Iteration 100, loss = 0.06744850\n",
      "Iteration 101, loss = 0.06656692\n",
      "Iteration 102, loss = 0.06559473\n",
      "Iteration 103, loss = 0.06462201\n",
      "Iteration 104, loss = 0.06384624\n",
      "Iteration 105, loss = 0.06301160\n",
      "Iteration 106, loss = 0.06234901\n",
      "Iteration 107, loss = 0.06140808\n",
      "Iteration 108, loss = 0.06063098\n",
      "Iteration 109, loss = 0.05971644\n",
      "Iteration 110, loss = 0.05926345\n",
      "Iteration 111, loss = 0.05832716\n",
      "Iteration 112, loss = 0.05743448\n",
      "Iteration 113, loss = 0.05687075\n",
      "Iteration 114, loss = 0.05582624\n",
      "Iteration 115, loss = 0.05556349\n",
      "Iteration 116, loss = 0.05455476\n",
      "Iteration 117, loss = 0.05387077\n",
      "Iteration 118, loss = 0.05339660\n",
      "Iteration 119, loss = 0.05283079\n",
      "Iteration 120, loss = 0.05204141\n",
      "Iteration 121, loss = 0.05142525\n",
      "Iteration 122, loss = 0.05065880\n",
      "Iteration 123, loss = 0.05013175\n",
      "Iteration 124, loss = 0.04934727\n",
      "Iteration 125, loss = 0.04894906\n",
      "Iteration 126, loss = 0.04822873\n",
      "Iteration 127, loss = 0.04755424\n",
      "Iteration 128, loss = 0.04694078\n",
      "Iteration 129, loss = 0.04651483\n",
      "Iteration 130, loss = 0.04579421\n",
      "Iteration 131, loss = 0.04517485\n",
      "Iteration 132, loss = 0.04469169\n",
      "Iteration 133, loss = 0.04412308\n",
      "Iteration 134, loss = 0.04363536\n",
      "Iteration 135, loss = 0.04307459\n",
      "Iteration 136, loss = 0.04259240\n",
      "Iteration 137, loss = 0.04192067\n",
      "Iteration 138, loss = 0.04156657\n",
      "Iteration 139, loss = 0.04095663\n",
      "Iteration 140, loss = 0.04045130\n",
      "Iteration 141, loss = 0.04015609\n",
      "Iteration 142, loss = 0.03938953\n",
      "Iteration 143, loss = 0.03891731\n",
      "Iteration 144, loss = 0.03854948\n",
      "Iteration 145, loss = 0.03804508\n",
      "Iteration 146, loss = 0.03730362\n",
      "Iteration 147, loss = 0.03714229\n",
      "Iteration 148, loss = 0.03669412\n",
      "Iteration 149, loss = 0.03615254\n",
      "Iteration 150, loss = 0.03575639\n",
      "Iteration 151, loss = 0.03529908\n",
      "Iteration 152, loss = 0.03496272\n",
      "Iteration 153, loss = 0.03440717\n",
      "Iteration 154, loss = 0.03399788\n",
      "Iteration 155, loss = 0.03371746\n",
      "Iteration 156, loss = 0.03326066\n",
      "Iteration 157, loss = 0.03282835\n",
      "Iteration 158, loss = 0.03242657\n",
      "Iteration 159, loss = 0.03213135\n",
      "Iteration 160, loss = 0.03170675\n",
      "Iteration 161, loss = 0.03133703\n",
      "Iteration 162, loss = 0.03087642\n",
      "Iteration 163, loss = 0.03054970\n",
      "Iteration 164, loss = 0.03004245\n",
      "Iteration 165, loss = 0.02983007\n",
      "Iteration 166, loss = 0.02930923\n",
      "Iteration 167, loss = 0.02909827\n",
      "Iteration 168, loss = 0.02870087\n",
      "Iteration 169, loss = 0.02845240\n",
      "Iteration 170, loss = 0.02800495\n",
      "Iteration 171, loss = 0.02773062\n",
      "Iteration 172, loss = 0.02732396\n",
      "Iteration 173, loss = 0.02708510\n",
      "Iteration 174, loss = 0.02678583\n",
      "Iteration 175, loss = 0.02643772\n",
      "Iteration 176, loss = 0.02613732\n",
      "Iteration 177, loss = 0.02587115\n",
      "Iteration 178, loss = 0.02538050\n",
      "Iteration 179, loss = 0.02531670\n",
      "Iteration 180, loss = 0.02494306\n",
      "Iteration 181, loss = 0.02481815\n",
      "Iteration 182, loss = 0.02435749\n",
      "Iteration 183, loss = 0.02402832\n",
      "Iteration 184, loss = 0.02378101\n",
      "Iteration 185, loss = 0.02358569\n",
      "Iteration 186, loss = 0.02318336\n",
      "Iteration 187, loss = 0.02298974\n",
      "Iteration 188, loss = 0.02261889\n",
      "Iteration 189, loss = 0.02241455\n",
      "Iteration 190, loss = 0.02227280\n",
      "Iteration 191, loss = 0.02198228\n",
      "Iteration 192, loss = 0.02166049\n",
      "Iteration 193, loss = 0.02134105\n",
      "Iteration 194, loss = 0.02114990\n",
      "Iteration 195, loss = 0.02095154\n",
      "Iteration 196, loss = 0.02068857\n",
      "Iteration 197, loss = 0.02038634\n",
      "Iteration 198, loss = 0.02037218\n",
      "Iteration 199, loss = 0.01994644\n",
      "Iteration 200, loss = 0.01980756\n",
      "[CV] END activation=relu, alpha=0.001, batch_size=200, hidden_layer_sizes=(100, 100, 100), solver=sgd; total time= 1.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.39223090\n",
      "Iteration 2, loss = 0.20590676\n",
      "Iteration 3, loss = 0.16039470\n",
      "Iteration 4, loss = 0.13452574\n",
      "Iteration 5, loss = 0.11912595\n",
      "Iteration 6, loss = 0.10640981\n",
      "Iteration 7, loss = 0.09994949\n",
      "Iteration 8, loss = 0.09330557\n",
      "Iteration 9, loss = 0.08753579\n",
      "Iteration 10, loss = 0.08384291\n",
      "Iteration 11, loss = 0.08068945\n",
      "Iteration 12, loss = 0.07812712\n",
      "Iteration 13, loss = 0.07663813\n",
      "Iteration 14, loss = 0.07420138\n",
      "Iteration 15, loss = 0.07204103\n",
      "Iteration 16, loss = 0.06996447\n",
      "Iteration 17, loss = 0.07206379\n",
      "Iteration 18, loss = 0.07034712\n",
      "Iteration 19, loss = 0.06775501\n",
      "Iteration 20, loss = 0.06804052\n",
      "Iteration 21, loss = 0.06650793\n",
      "Iteration 22, loss = 0.06673999\n",
      "Iteration 23, loss = 0.06698496\n",
      "Iteration 24, loss = 0.06588714\n",
      "Iteration 25, loss = 0.06530336\n",
      "Iteration 26, loss = 0.06528821\n",
      "Iteration 27, loss = 0.06597685\n",
      "Iteration 28, loss = 0.06303159\n",
      "Iteration 29, loss = 0.06399393\n",
      "Iteration 30, loss = 0.06362390\n",
      "Iteration 31, loss = 0.06212127\n",
      "Iteration 32, loss = 0.06326279\n",
      "Iteration 33, loss = 0.06392377\n",
      "Iteration 34, loss = 0.06307210\n",
      "Iteration 35, loss = 0.06036815\n",
      "Iteration 36, loss = 0.06194474\n",
      "Iteration 37, loss = 0.06210639\n",
      "Iteration 38, loss = 0.06234946\n",
      "Iteration 39, loss = 0.06107174\n",
      "Iteration 40, loss = 0.06037705\n",
      "Iteration 41, loss = 0.06100478\n",
      "Iteration 42, loss = 0.06114921\n",
      "Iteration 43, loss = 0.06340631\n",
      "Iteration 44, loss = 0.05993143\n",
      "Iteration 45, loss = 0.06006272\n",
      "Iteration 46, loss = 0.06255061\n",
      "Iteration 47, loss = 0.05986651\n",
      "Iteration 48, loss = 0.06112881\n",
      "Iteration 49, loss = 0.06218761\n",
      "Iteration 50, loss = 0.06097982\n",
      "Iteration 51, loss = 0.05869196\n",
      "Iteration 52, loss = 0.06047319\n",
      "Iteration 53, loss = 0.06133161\n",
      "Iteration 54, loss = 0.06008025\n",
      "Iteration 55, loss = 0.05963067\n",
      "Iteration 56, loss = 0.06018144\n",
      "Iteration 57, loss = 0.05927365\n",
      "Iteration 58, loss = 0.05872097\n",
      "Iteration 59, loss = 0.06148419\n",
      "Iteration 60, loss = 0.05780629\n",
      "Iteration 61, loss = 0.06003210\n",
      "Iteration 62, loss = 0.05899217\n",
      "Iteration 63, loss = 0.06140460\n",
      "Iteration 64, loss = 0.06010276\n",
      "Iteration 65, loss = 0.05847438\n",
      "Iteration 66, loss = 0.05759547\n",
      "Iteration 67, loss = 0.05930276\n",
      "Iteration 68, loss = 0.05952625\n",
      "Iteration 69, loss = 0.05778574\n",
      "Iteration 70, loss = 0.05863259\n",
      "Iteration 71, loss = 0.06101984\n",
      "Iteration 72, loss = 0.05800780\n",
      "Iteration 73, loss = 0.05625988\n",
      "Iteration 74, loss = 0.06053702\n",
      "Iteration 75, loss = 0.05794081\n",
      "Iteration 76, loss = 0.05730523\n",
      "Iteration 77, loss = 0.05925338\n",
      "Iteration 78, loss = 0.05883569\n",
      "Iteration 79, loss = 0.05832384\n",
      "Iteration 80, loss = 0.05859064\n",
      "Iteration 81, loss = 0.05830594\n",
      "Iteration 82, loss = 0.05641046\n",
      "Iteration 83, loss = 0.05784805\n",
      "Iteration 84, loss = 0.05923560\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.01, batch_size=50, hidden_layer_sizes=(100,), solver=adam; total time= 4.1min\n",
      "Iteration 1, loss = 0.40213431\n",
      "Iteration 2, loss = 0.21574309\n",
      "Iteration 3, loss = 0.16762535\n",
      "Iteration 4, loss = 0.14171728\n",
      "Iteration 5, loss = 0.12463530\n",
      "Iteration 6, loss = 0.11161607\n",
      "Iteration 7, loss = 0.10260827\n",
      "Iteration 8, loss = 0.09687514\n",
      "Iteration 9, loss = 0.09038664\n",
      "Iteration 10, loss = 0.08708243\n",
      "Iteration 11, loss = 0.08285056\n",
      "Iteration 12, loss = 0.08063279\n",
      "Iteration 13, loss = 0.07816037\n",
      "Iteration 14, loss = 0.07534266\n",
      "Iteration 15, loss = 0.07553622\n",
      "Iteration 16, loss = 0.07290478\n",
      "Iteration 17, loss = 0.07168480\n",
      "Iteration 18, loss = 0.06989982\n",
      "Iteration 19, loss = 0.07109236\n",
      "Iteration 20, loss = 0.06863587\n",
      "Iteration 21, loss = 0.06771908\n",
      "Iteration 22, loss = 0.06664262\n",
      "Iteration 23, loss = 0.06637706\n",
      "Iteration 24, loss = 0.06759460\n",
      "Iteration 25, loss = 0.06700743\n",
      "Iteration 26, loss = 0.06445169\n",
      "Iteration 27, loss = 0.06528187\n",
      "Iteration 28, loss = 0.06379735\n",
      "Iteration 29, loss = 0.06604957\n",
      "Iteration 30, loss = 0.06323897\n",
      "Iteration 31, loss = 0.06402010\n",
      "Iteration 32, loss = 0.06512464\n",
      "Iteration 33, loss = 0.06287100\n",
      "Iteration 34, loss = 0.06265169\n",
      "Iteration 35, loss = 0.06360787\n",
      "Iteration 36, loss = 0.06299920\n",
      "Iteration 37, loss = 0.06221100\n",
      "Iteration 38, loss = 0.06221265\n",
      "Iteration 39, loss = 0.06361696\n",
      "Iteration 40, loss = 0.06261185\n",
      "Iteration 41, loss = 0.06183655\n",
      "Iteration 42, loss = 0.06045261\n",
      "Iteration 43, loss = 0.06307198\n",
      "Iteration 44, loss = 0.06264673\n",
      "Iteration 45, loss = 0.06004045\n",
      "Iteration 46, loss = 0.06142922\n",
      "Iteration 47, loss = 0.06064040\n",
      "Iteration 48, loss = 0.06188018\n",
      "Iteration 49, loss = 0.06082253\n",
      "Iteration 50, loss = 0.06190392\n",
      "Iteration 51, loss = 0.06022124\n",
      "Iteration 52, loss = 0.06028654\n",
      "Iteration 53, loss = 0.06255733\n",
      "Iteration 54, loss = 0.06072685\n",
      "Iteration 55, loss = 0.05926866\n",
      "Iteration 56, loss = 0.06095377\n",
      "Iteration 57, loss = 0.06009416\n",
      "Iteration 58, loss = 0.06225714\n",
      "Iteration 59, loss = 0.05926777\n",
      "Iteration 60, loss = 0.05958115\n",
      "Iteration 61, loss = 0.05868345\n",
      "Iteration 62, loss = 0.06081710\n",
      "Iteration 63, loss = 0.06145972\n",
      "Iteration 64, loss = 0.05984367\n",
      "Iteration 65, loss = 0.05820891\n",
      "Iteration 66, loss = 0.05929019\n",
      "Iteration 67, loss = 0.06176299\n",
      "Iteration 68, loss = 0.06035948\n",
      "Iteration 69, loss = 0.05903051\n",
      "Iteration 70, loss = 0.05822827\n",
      "Iteration 71, loss = 0.05841536\n",
      "Iteration 72, loss = 0.05892115\n",
      "Iteration 73, loss = 0.06090827\n",
      "Iteration 74, loss = 0.05693763\n",
      "Iteration 75, loss = 0.06161560\n",
      "Iteration 76, loss = 0.05826538\n",
      "Iteration 77, loss = 0.05866835\n",
      "Iteration 78, loss = 0.05985553\n",
      "Iteration 79, loss = 0.05836152\n",
      "Iteration 80, loss = 0.05895306\n",
      "Iteration 81, loss = 0.05842098\n",
      "Iteration 82, loss = 0.05930834\n",
      "Iteration 83, loss = 0.05814609\n",
      "Iteration 84, loss = 0.05797386\n",
      "Iteration 85, loss = 0.06044089\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.01, batch_size=50, hidden_layer_sizes=(100,), solver=adam; total time= 3.7min\n",
      "Iteration 1, loss = 0.38567422\n",
      "Iteration 2, loss = 0.20130381\n",
      "Iteration 3, loss = 0.16122728\n",
      "Iteration 4, loss = 0.13908201\n",
      "Iteration 5, loss = 0.12149491\n",
      "Iteration 6, loss = 0.10998005\n",
      "Iteration 7, loss = 0.10133397\n",
      "Iteration 8, loss = 0.09517863\n",
      "Iteration 9, loss = 0.08960643\n",
      "Iteration 10, loss = 0.08510825\n",
      "Iteration 11, loss = 0.08136277\n",
      "Iteration 12, loss = 0.07995085\n",
      "Iteration 13, loss = 0.07639368\n",
      "Iteration 14, loss = 0.07577972\n",
      "Iteration 15, loss = 0.07378599\n",
      "Iteration 16, loss = 0.07177622\n",
      "Iteration 17, loss = 0.06955590\n",
      "Iteration 18, loss = 0.07068908\n",
      "Iteration 19, loss = 0.06957232\n",
      "Iteration 20, loss = 0.06698563\n",
      "Iteration 21, loss = 0.06927857\n",
      "Iteration 22, loss = 0.06711621\n",
      "Iteration 23, loss = 0.06706851\n",
      "Iteration 24, loss = 0.06487030\n",
      "Iteration 25, loss = 0.06532730\n",
      "Iteration 26, loss = 0.06538451\n",
      "Iteration 27, loss = 0.06395790\n",
      "Iteration 28, loss = 0.06453161\n",
      "Iteration 29, loss = 0.06374564\n",
      "Iteration 30, loss = 0.06245531\n",
      "Iteration 31, loss = 0.06336703\n",
      "Iteration 32, loss = 0.06312400\n",
      "Iteration 33, loss = 0.06393823\n",
      "Iteration 34, loss = 0.06320072\n",
      "Iteration 35, loss = 0.06300200\n",
      "Iteration 36, loss = 0.06183328\n",
      "Iteration 37, loss = 0.05988698\n",
      "Iteration 38, loss = 0.06168426\n",
      "Iteration 39, loss = 0.06359030\n",
      "Iteration 40, loss = 0.06028854\n",
      "Iteration 41, loss = 0.05997685\n",
      "Iteration 42, loss = 0.06139111\n",
      "Iteration 43, loss = 0.06000996\n",
      "Iteration 44, loss = 0.06112640\n",
      "Iteration 45, loss = 0.05993899\n",
      "Iteration 46, loss = 0.06011587\n",
      "Iteration 47, loss = 0.06055123\n",
      "Iteration 48, loss = 0.06087057\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.01, batch_size=50, hidden_layer_sizes=(100,), solver=adam; total time= 1.4min\n",
      "Iteration 1, loss = 0.39045238\n",
      "Iteration 2, loss = 0.20378699\n",
      "Iteration 3, loss = 0.16112178\n",
      "Iteration 4, loss = 0.13602363\n",
      "Iteration 5, loss = 0.12229738\n",
      "Iteration 6, loss = 0.10952400\n",
      "Iteration 7, loss = 0.10053418\n",
      "Iteration 8, loss = 0.09467102\n",
      "Iteration 9, loss = 0.08875898\n",
      "Iteration 10, loss = 0.08351889\n",
      "Iteration 11, loss = 0.08177149\n",
      "Iteration 12, loss = 0.07911444\n",
      "Iteration 13, loss = 0.07535278\n",
      "Iteration 14, loss = 0.07532952\n",
      "Iteration 15, loss = 0.07307800\n",
      "Iteration 16, loss = 0.07086956\n",
      "Iteration 17, loss = 0.07084869\n",
      "Iteration 18, loss = 0.06838119\n",
      "Iteration 19, loss = 0.06872439\n",
      "Iteration 20, loss = 0.06740739\n",
      "Iteration 21, loss = 0.06704452\n",
      "Iteration 22, loss = 0.06561922\n",
      "Iteration 23, loss = 0.06554919\n",
      "Iteration 24, loss = 0.06646351\n",
      "Iteration 25, loss = 0.06523435\n",
      "Iteration 26, loss = 0.06451999\n",
      "Iteration 27, loss = 0.06413759\n",
      "Iteration 28, loss = 0.06322481\n",
      "Iteration 29, loss = 0.06439263\n",
      "Iteration 30, loss = 0.06411277\n",
      "Iteration 31, loss = 0.06363662\n",
      "Iteration 32, loss = 0.06366347\n",
      "Iteration 33, loss = 0.06276338\n",
      "Iteration 34, loss = 0.06325741\n",
      "Iteration 35, loss = 0.06286539\n",
      "Iteration 36, loss = 0.06195229\n",
      "Iteration 37, loss = 0.06268056\n",
      "Iteration 38, loss = 0.06192156\n",
      "Iteration 39, loss = 0.06197442\n",
      "Iteration 40, loss = 0.06120497\n",
      "Iteration 41, loss = 0.06182478\n",
      "Iteration 42, loss = 0.06159547\n",
      "Iteration 43, loss = 0.06193281\n",
      "Iteration 44, loss = 0.05930331\n",
      "Iteration 45, loss = 0.06191508\n",
      "Iteration 46, loss = 0.06099158\n",
      "Iteration 47, loss = 0.06132837\n",
      "Iteration 48, loss = 0.06015380\n",
      "Iteration 49, loss = 0.06020255\n",
      "Iteration 50, loss = 0.06037496\n",
      "Iteration 51, loss = 0.05998358\n",
      "Iteration 52, loss = 0.05971085\n",
      "Iteration 53, loss = 0.06016598\n",
      "Iteration 54, loss = 0.06168180\n",
      "Iteration 55, loss = 0.05925663\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.01, batch_size=50, hidden_layer_sizes=(100,), solver=adam; total time= 1.6min\n",
      "Iteration 1, loss = 0.39405589\n",
      "Iteration 2, loss = 0.20488238\n",
      "Iteration 3, loss = 0.15963968\n",
      "Iteration 4, loss = 0.13507793\n",
      "Iteration 5, loss = 0.11909611\n",
      "Iteration 6, loss = 0.10908867\n",
      "Iteration 7, loss = 0.09956825\n",
      "Iteration 8, loss = 0.09333306\n",
      "Iteration 9, loss = 0.08855492\n",
      "Iteration 10, loss = 0.08623807\n",
      "Iteration 11, loss = 0.08259344\n",
      "Iteration 12, loss = 0.07967759\n",
      "Iteration 13, loss = 0.07674417\n",
      "Iteration 14, loss = 0.07665527\n",
      "Iteration 15, loss = 0.07489425\n",
      "Iteration 16, loss = 0.07252644\n",
      "Iteration 17, loss = 0.07235685\n",
      "Iteration 18, loss = 0.06962386\n",
      "Iteration 19, loss = 0.06940142\n",
      "Iteration 20, loss = 0.06904492\n",
      "Iteration 21, loss = 0.06865383\n",
      "Iteration 22, loss = 0.06825644\n",
      "Iteration 23, loss = 0.06899436\n",
      "Iteration 24, loss = 0.06656983\n",
      "Iteration 25, loss = 0.06742940\n",
      "Iteration 26, loss = 0.06451862\n",
      "Iteration 27, loss = 0.06304823\n",
      "Iteration 28, loss = 0.06624719\n",
      "Iteration 29, loss = 0.06521921\n",
      "Iteration 30, loss = 0.06482006\n",
      "Iteration 31, loss = 0.06462681\n",
      "Iteration 32, loss = 0.06475774\n",
      "Iteration 33, loss = 0.06619154\n",
      "Iteration 34, loss = 0.06293289\n",
      "Iteration 35, loss = 0.06288829\n",
      "Iteration 36, loss = 0.06154447\n",
      "Iteration 37, loss = 0.06471444\n",
      "Iteration 38, loss = 0.06470193\n",
      "Iteration 39, loss = 0.06166757\n",
      "Iteration 40, loss = 0.06123407\n",
      "Iteration 41, loss = 0.06180441\n",
      "Iteration 42, loss = 0.06176002\n",
      "Iteration 43, loss = 0.06199853\n",
      "Iteration 44, loss = 0.06161427\n",
      "Iteration 45, loss = 0.06349558\n",
      "Iteration 46, loss = 0.06273604\n",
      "Iteration 47, loss = 0.05993678\n",
      "Iteration 48, loss = 0.05990783\n",
      "Iteration 49, loss = 0.06115935\n",
      "Iteration 50, loss = 0.06224336\n",
      "Iteration 51, loss = 0.06154441\n",
      "Iteration 52, loss = 0.06121715\n",
      "Iteration 53, loss = 0.06287897\n",
      "Iteration 54, loss = 0.05978091\n",
      "Iteration 55, loss = 0.05942618\n",
      "Iteration 56, loss = 0.06166659\n",
      "Iteration 57, loss = 0.06056137\n",
      "Iteration 58, loss = 0.05995558\n",
      "Iteration 59, loss = 0.06251639\n",
      "Iteration 60, loss = 0.05878970\n",
      "Iteration 61, loss = 0.06171831\n",
      "Iteration 62, loss = 0.05936993\n",
      "Iteration 63, loss = 0.05911642\n",
      "Iteration 64, loss = 0.06048916\n",
      "Iteration 65, loss = 0.05919667\n",
      "Iteration 66, loss = 0.06227796\n",
      "Iteration 67, loss = 0.06138708\n",
      "Iteration 68, loss = 0.05801308\n",
      "Iteration 69, loss = 0.05934921\n",
      "Iteration 70, loss = 0.06007701\n",
      "Iteration 71, loss = 0.05989952\n",
      "Iteration 72, loss = 0.05868090\n",
      "Iteration 73, loss = 0.05961576\n",
      "Iteration 74, loss = 0.06000601\n",
      "Iteration 75, loss = 0.05873532\n",
      "Iteration 76, loss = 0.05879604\n",
      "Iteration 77, loss = 0.05769068\n",
      "Iteration 78, loss = 0.06050409\n",
      "Iteration 79, loss = 0.05954307\n",
      "Iteration 80, loss = 0.06021262\n",
      "Iteration 81, loss = 0.05848136\n",
      "Iteration 82, loss = 0.05925453\n",
      "Iteration 83, loss = 0.05737044\n",
      "Iteration 84, loss = 0.05996537\n",
      "Iteration 85, loss = 0.05883598\n",
      "Iteration 86, loss = 0.05781652\n",
      "Iteration 87, loss = 0.05805504\n",
      "Iteration 88, loss = 0.05951758\n",
      "Iteration 89, loss = 0.05753138\n",
      "Iteration 90, loss = 0.05825522\n",
      "Iteration 91, loss = 0.05935263\n",
      "Iteration 92, loss = 0.05812684\n",
      "Iteration 93, loss = 0.05972398\n",
      "Iteration 94, loss = 0.05893392\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.01, batch_size=50, hidden_layer_sizes=(100,), solver=adam; total time= 4.5min\n",
      "Iteration 1, loss = 1.06813228\n",
      "Iteration 2, loss = 0.50268067\n",
      "Iteration 3, loss = 0.41487418\n",
      "Iteration 4, loss = 0.37431663\n",
      "Iteration 5, loss = 0.34864251\n",
      "Iteration 6, loss = 0.32983534\n",
      "Iteration 7, loss = 0.31459746\n",
      "Iteration 8, loss = 0.30136423\n",
      "Iteration 9, loss = 0.29006231\n",
      "Iteration 10, loss = 0.27965915\n",
      "Iteration 11, loss = 0.26999298\n",
      "Iteration 12, loss = 0.26111459\n",
      "Iteration 13, loss = 0.25296984\n",
      "Iteration 14, loss = 0.24577948\n",
      "Iteration 15, loss = 0.23855797\n",
      "Iteration 16, loss = 0.23239289\n",
      "Iteration 17, loss = 0.22614727\n",
      "Iteration 18, loss = 0.22069389\n",
      "Iteration 19, loss = 0.21524477\n",
      "Iteration 20, loss = 0.21056199\n",
      "Iteration 21, loss = 0.20580682\n",
      "Iteration 22, loss = 0.20133628\n",
      "Iteration 23, loss = 0.19747035\n",
      "Iteration 24, loss = 0.19343236\n",
      "Iteration 25, loss = 0.18964589\n",
      "Iteration 26, loss = 0.18621739\n",
      "Iteration 27, loss = 0.18271685\n",
      "Iteration 28, loss = 0.17972223\n",
      "Iteration 29, loss = 0.17655214\n",
      "Iteration 30, loss = 0.17359577\n",
      "Iteration 31, loss = 0.17119504\n",
      "Iteration 32, loss = 0.16818436\n",
      "Iteration 33, loss = 0.16588639\n",
      "Iteration 34, loss = 0.16346814\n",
      "Iteration 35, loss = 0.16092487\n",
      "Iteration 36, loss = 0.15884024\n",
      "Iteration 37, loss = 0.15657441\n",
      "Iteration 38, loss = 0.15466577\n",
      "Iteration 39, loss = 0.15265036\n",
      "Iteration 40, loss = 0.15057653\n",
      "Iteration 41, loss = 0.14858736\n",
      "Iteration 42, loss = 0.14681867\n",
      "Iteration 43, loss = 0.14503459\n",
      "Iteration 44, loss = 0.14340380\n",
      "Iteration 45, loss = 0.14182581\n",
      "Iteration 46, loss = 0.14010132\n",
      "Iteration 47, loss = 0.13855381\n",
      "Iteration 48, loss = 0.13691969\n",
      "Iteration 49, loss = 0.13555677\n",
      "Iteration 50, loss = 0.13401177\n",
      "Iteration 51, loss = 0.13264993\n",
      "Iteration 52, loss = 0.13145782\n",
      "Iteration 53, loss = 0.13006139\n",
      "Iteration 54, loss = 0.12893797\n",
      "Iteration 55, loss = 0.12755933\n",
      "Iteration 56, loss = 0.12625727\n",
      "Iteration 57, loss = 0.12517550\n",
      "Iteration 58, loss = 0.12384570\n",
      "Iteration 59, loss = 0.12293735\n",
      "Iteration 60, loss = 0.12166424\n",
      "Iteration 61, loss = 0.12058083\n",
      "Iteration 62, loss = 0.11960612\n",
      "Iteration 63, loss = 0.11851527\n",
      "Iteration 64, loss = 0.11763495\n",
      "Iteration 65, loss = 0.11649139\n",
      "Iteration 66, loss = 0.11571846\n",
      "Iteration 67, loss = 0.11458822\n",
      "Iteration 68, loss = 0.11386038\n",
      "Iteration 69, loss = 0.11292435\n",
      "Iteration 70, loss = 0.11211781\n",
      "Iteration 71, loss = 0.11117747\n",
      "Iteration 72, loss = 0.11030647\n",
      "Iteration 73, loss = 0.10947154\n",
      "Iteration 74, loss = 0.10867174\n",
      "Iteration 75, loss = 0.10804628\n",
      "Iteration 76, loss = 0.10730728\n",
      "Iteration 77, loss = 0.10647407\n",
      "Iteration 78, loss = 0.10554618\n",
      "Iteration 79, loss = 0.10501346\n",
      "Iteration 80, loss = 0.10431950\n",
      "Iteration 81, loss = 0.10359618\n",
      "Iteration 82, loss = 0.10298722\n",
      "Iteration 83, loss = 0.10228237\n",
      "Iteration 84, loss = 0.10165236\n",
      "Iteration 85, loss = 0.10104557\n",
      "Iteration 86, loss = 0.10028761\n",
      "Iteration 87, loss = 0.09977203\n",
      "Iteration 88, loss = 0.09908006\n",
      "Iteration 89, loss = 0.09866398\n",
      "Iteration 90, loss = 0.09799337\n",
      "Iteration 91, loss = 0.09727173\n",
      "Iteration 92, loss = 0.09683202\n",
      "Iteration 93, loss = 0.09626541\n",
      "Iteration 94, loss = 0.09576834\n",
      "Iteration 95, loss = 0.09526356\n",
      "Iteration 96, loss = 0.09480012\n",
      "Iteration 97, loss = 0.09411014\n",
      "Iteration 98, loss = 0.09366386\n",
      "Iteration 99, loss = 0.09316651\n",
      "Iteration 100, loss = 0.09263614\n",
      "Iteration 101, loss = 0.09217624\n",
      "Iteration 102, loss = 0.09157921\n",
      "Iteration 103, loss = 0.09129214\n",
      "Iteration 104, loss = 0.09066282\n",
      "Iteration 105, loss = 0.09034761\n",
      "Iteration 106, loss = 0.08987991\n",
      "Iteration 107, loss = 0.08939950\n",
      "Iteration 108, loss = 0.08900882\n",
      "Iteration 109, loss = 0.08855948\n",
      "Iteration 110, loss = 0.08815537\n",
      "Iteration 111, loss = 0.08776231\n",
      "Iteration 112, loss = 0.08734916\n",
      "Iteration 113, loss = 0.08690203\n",
      "Iteration 114, loss = 0.08657161\n",
      "Iteration 115, loss = 0.08611842\n",
      "Iteration 116, loss = 0.08575675\n",
      "Iteration 117, loss = 0.08545851\n",
      "Iteration 118, loss = 0.08506933\n",
      "Iteration 119, loss = 0.08475646\n",
      "Iteration 120, loss = 0.08441963\n",
      "Iteration 121, loss = 0.08407293\n",
      "Iteration 122, loss = 0.08368588\n",
      "Iteration 123, loss = 0.08327053\n",
      "Iteration 124, loss = 0.08297679\n",
      "Iteration 125, loss = 0.08266207\n",
      "Iteration 126, loss = 0.08233292\n",
      "Iteration 127, loss = 0.08213632\n",
      "Iteration 128, loss = 0.08174596\n",
      "Iteration 129, loss = 0.08144419\n",
      "Iteration 130, loss = 0.08109776\n",
      "Iteration 131, loss = 0.08077457\n",
      "Iteration 132, loss = 0.08049882\n",
      "Iteration 133, loss = 0.08021352\n",
      "Iteration 134, loss = 0.07986141\n",
      "Iteration 135, loss = 0.07965458\n",
      "Iteration 136, loss = 0.07929850\n",
      "Iteration 137, loss = 0.07907758\n",
      "Iteration 138, loss = 0.07879419\n",
      "Iteration 139, loss = 0.07851560\n",
      "Iteration 140, loss = 0.07829496\n",
      "Iteration 141, loss = 0.07802908\n",
      "Iteration 142, loss = 0.07780872\n",
      "Iteration 143, loss = 0.07760934\n",
      "Iteration 144, loss = 0.07727690\n",
      "Iteration 145, loss = 0.07705115\n",
      "Iteration 146, loss = 0.07668334\n",
      "Iteration 147, loss = 0.07648623\n",
      "Iteration 148, loss = 0.07626158\n",
      "Iteration 149, loss = 0.07604830\n",
      "Iteration 150, loss = 0.07579934\n",
      "Iteration 151, loss = 0.07555491\n",
      "Iteration 152, loss = 0.07537429\n",
      "Iteration 153, loss = 0.07510713\n",
      "Iteration 154, loss = 0.07496005\n",
      "Iteration 155, loss = 0.07464900\n",
      "Iteration 156, loss = 0.07444925\n",
      "Iteration 157, loss = 0.07427706\n",
      "Iteration 158, loss = 0.07408006\n",
      "Iteration 159, loss = 0.07383819\n",
      "Iteration 160, loss = 0.07353834\n",
      "Iteration 161, loss = 0.07338842\n",
      "Iteration 162, loss = 0.07320218\n",
      "Iteration 163, loss = 0.07300911\n",
      "Iteration 164, loss = 0.07281713\n",
      "Iteration 165, loss = 0.07265904\n",
      "Iteration 166, loss = 0.07245412\n",
      "Iteration 167, loss = 0.07223468\n",
      "Iteration 168, loss = 0.07211499\n",
      "Iteration 169, loss = 0.07190399\n",
      "Iteration 170, loss = 0.07169205\n",
      "Iteration 171, loss = 0.07147530\n",
      "Iteration 172, loss = 0.07136588\n",
      "Iteration 173, loss = 0.07117150\n",
      "Iteration 174, loss = 0.07099736\n",
      "Iteration 175, loss = 0.07082741\n",
      "Iteration 176, loss = 0.07064128\n",
      "Iteration 177, loss = 0.07044786\n",
      "Iteration 178, loss = 0.07030408\n",
      "Iteration 179, loss = 0.07021643\n",
      "Iteration 180, loss = 0.06999768\n",
      "Iteration 181, loss = 0.06987698\n",
      "Iteration 182, loss = 0.06970898\n",
      "Iteration 183, loss = 0.06953989\n",
      "Iteration 184, loss = 0.06934511\n",
      "Iteration 185, loss = 0.06922688\n",
      "Iteration 186, loss = 0.06910900\n",
      "Iteration 187, loss = 0.06893515\n",
      "Iteration 188, loss = 0.06883111\n",
      "Iteration 189, loss = 0.06866491\n",
      "Iteration 190, loss = 0.06851741\n",
      "Iteration 191, loss = 0.06835248\n",
      "Iteration 192, loss = 0.06826679\n",
      "Iteration 193, loss = 0.06807051\n",
      "Iteration 194, loss = 0.06795076\n",
      "Iteration 195, loss = 0.06787505\n",
      "Iteration 196, loss = 0.06767613\n",
      "Iteration 197, loss = 0.06755618\n",
      "Iteration 198, loss = 0.06740628\n",
      "Iteration 199, loss = 0.06724067\n",
      "Iteration 200, loss = 0.06717576\n",
      "[CV] END activation=relu, alpha=0.01, batch_size=50, hidden_layer_sizes=(100,), solver=sgd; total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.11347666\n",
      "Iteration 2, loss = 0.50972015\n",
      "Iteration 3, loss = 0.41676191\n",
      "Iteration 4, loss = 0.37453552\n",
      "Iteration 5, loss = 0.34839178\n",
      "Iteration 6, loss = 0.33007214\n",
      "Iteration 7, loss = 0.31521337\n",
      "Iteration 8, loss = 0.30279344\n",
      "Iteration 9, loss = 0.29206497\n",
      "Iteration 10, loss = 0.28279654\n",
      "Iteration 11, loss = 0.27437708\n",
      "Iteration 12, loss = 0.26658244\n",
      "Iteration 13, loss = 0.25965838\n",
      "Iteration 14, loss = 0.25302164\n",
      "Iteration 15, loss = 0.24687699\n",
      "Iteration 16, loss = 0.24090460\n",
      "Iteration 17, loss = 0.23554355\n",
      "Iteration 18, loss = 0.23031720\n",
      "Iteration 19, loss = 0.22542408\n",
      "Iteration 20, loss = 0.22069043\n",
      "Iteration 21, loss = 0.21652122\n",
      "Iteration 22, loss = 0.21210971\n",
      "Iteration 23, loss = 0.20832907\n",
      "Iteration 24, loss = 0.20419282\n",
      "Iteration 25, loss = 0.20040967\n",
      "Iteration 26, loss = 0.19700531\n",
      "Iteration 27, loss = 0.19365634\n",
      "Iteration 28, loss = 0.19032432\n",
      "Iteration 29, loss = 0.18720841\n",
      "Iteration 30, loss = 0.18403176\n",
      "Iteration 31, loss = 0.18120171\n",
      "Iteration 32, loss = 0.17846349\n",
      "Iteration 33, loss = 0.17565222\n",
      "Iteration 34, loss = 0.17317138\n",
      "Iteration 35, loss = 0.17062445\n",
      "Iteration 36, loss = 0.16799907\n",
      "Iteration 37, loss = 0.16561860\n",
      "Iteration 38, loss = 0.16341793\n",
      "Iteration 39, loss = 0.16114833\n",
      "Iteration 40, loss = 0.15895779\n",
      "Iteration 41, loss = 0.15710882\n",
      "Iteration 42, loss = 0.15496168\n",
      "Iteration 43, loss = 0.15301995\n",
      "Iteration 44, loss = 0.15114369\n",
      "Iteration 45, loss = 0.14935524\n",
      "Iteration 46, loss = 0.14746640\n",
      "Iteration 47, loss = 0.14569817\n",
      "Iteration 48, loss = 0.14400503\n",
      "Iteration 49, loss = 0.14238264\n",
      "Iteration 50, loss = 0.14064809\n",
      "Iteration 51, loss = 0.13909888\n",
      "Iteration 52, loss = 0.13761131\n",
      "Iteration 53, loss = 0.13590983\n",
      "Iteration 54, loss = 0.13464733\n",
      "Iteration 55, loss = 0.13309360\n",
      "Iteration 56, loss = 0.13192368\n",
      "Iteration 57, loss = 0.13068419\n",
      "Iteration 58, loss = 0.12918401\n",
      "Iteration 59, loss = 0.12802716\n",
      "Iteration 60, loss = 0.12669340\n",
      "Iteration 61, loss = 0.12557493\n",
      "Iteration 62, loss = 0.12439612\n",
      "Iteration 63, loss = 0.12317106\n",
      "Iteration 64, loss = 0.12205561\n",
      "Iteration 65, loss = 0.12090315\n",
      "Iteration 66, loss = 0.11982560\n",
      "Iteration 67, loss = 0.11878620\n",
      "Iteration 68, loss = 0.11779459\n",
      "Iteration 69, loss = 0.11676051\n",
      "Iteration 70, loss = 0.11583112\n",
      "Iteration 71, loss = 0.11491409\n",
      "Iteration 72, loss = 0.11396877\n",
      "Iteration 73, loss = 0.11305368\n",
      "Iteration 74, loss = 0.11205156\n",
      "Iteration 75, loss = 0.11125980\n",
      "Iteration 76, loss = 0.11033656\n",
      "Iteration 77, loss = 0.10951743\n",
      "Iteration 78, loss = 0.10867619\n",
      "Iteration 79, loss = 0.10784194\n",
      "Iteration 80, loss = 0.10722350\n",
      "Iteration 81, loss = 0.10642153\n",
      "Iteration 82, loss = 0.10565775\n",
      "Iteration 83, loss = 0.10489250\n",
      "Iteration 84, loss = 0.10424677\n",
      "Iteration 85, loss = 0.10352761\n",
      "Iteration 86, loss = 0.10280018\n",
      "Iteration 87, loss = 0.10217962\n",
      "Iteration 88, loss = 0.10148580\n",
      "Iteration 89, loss = 0.10087425\n",
      "Iteration 90, loss = 0.10024818\n",
      "Iteration 91, loss = 0.09949846\n",
      "Iteration 92, loss = 0.09900432\n",
      "Iteration 93, loss = 0.09836275\n",
      "Iteration 94, loss = 0.09780270\n",
      "Iteration 95, loss = 0.09723348\n",
      "Iteration 96, loss = 0.09666031\n",
      "Iteration 97, loss = 0.09606525\n",
      "Iteration 98, loss = 0.09550102\n",
      "Iteration 99, loss = 0.09495918\n",
      "Iteration 100, loss = 0.09451370\n",
      "Iteration 101, loss = 0.09399389\n",
      "Iteration 102, loss = 0.09348602\n",
      "Iteration 103, loss = 0.09287378\n",
      "Iteration 104, loss = 0.09258219\n",
      "Iteration 105, loss = 0.09190825\n",
      "Iteration 106, loss = 0.09151739\n",
      "Iteration 107, loss = 0.09108626\n",
      "Iteration 108, loss = 0.09044621\n",
      "Iteration 109, loss = 0.09015733\n",
      "Iteration 110, loss = 0.08965620\n",
      "Iteration 111, loss = 0.08929184\n",
      "Iteration 112, loss = 0.08881963\n",
      "Iteration 113, loss = 0.08835363\n",
      "Iteration 114, loss = 0.08800259\n",
      "Iteration 115, loss = 0.08764898\n",
      "Iteration 116, loss = 0.08714094\n",
      "Iteration 117, loss = 0.08683147\n",
      "Iteration 118, loss = 0.08638581\n",
      "Iteration 119, loss = 0.08605251\n",
      "Iteration 120, loss = 0.08569228\n",
      "Iteration 121, loss = 0.08524974\n",
      "Iteration 122, loss = 0.08486704\n",
      "Iteration 123, loss = 0.08463314\n",
      "Iteration 124, loss = 0.08420673\n",
      "Iteration 125, loss = 0.08385191\n",
      "Iteration 126, loss = 0.08353601\n",
      "Iteration 127, loss = 0.08314157\n",
      "Iteration 128, loss = 0.08285335\n",
      "Iteration 129, loss = 0.08250330\n",
      "Iteration 130, loss = 0.08226743\n",
      "Iteration 131, loss = 0.08183657\n",
      "Iteration 132, loss = 0.08145130\n",
      "Iteration 133, loss = 0.08119456\n",
      "Iteration 134, loss = 0.08083730\n",
      "Iteration 135, loss = 0.08059429\n",
      "Iteration 136, loss = 0.08020365\n",
      "Iteration 137, loss = 0.08004802\n",
      "Iteration 138, loss = 0.07970264\n",
      "Iteration 139, loss = 0.07942297\n",
      "Iteration 140, loss = 0.07914006\n",
      "Iteration 141, loss = 0.07883299\n",
      "Iteration 142, loss = 0.07860755\n",
      "Iteration 143, loss = 0.07831689\n",
      "Iteration 144, loss = 0.07807468\n",
      "Iteration 145, loss = 0.07774702\n",
      "Iteration 146, loss = 0.07751933\n",
      "Iteration 147, loss = 0.07730408\n",
      "Iteration 148, loss = 0.07700341\n",
      "Iteration 149, loss = 0.07682941\n",
      "Iteration 150, loss = 0.07653667\n",
      "Iteration 151, loss = 0.07632033\n",
      "Iteration 152, loss = 0.07609520\n",
      "Iteration 153, loss = 0.07581469\n",
      "Iteration 154, loss = 0.07550864\n",
      "Iteration 155, loss = 0.07534864\n",
      "Iteration 156, loss = 0.07513932\n",
      "Iteration 157, loss = 0.07499498\n",
      "Iteration 158, loss = 0.07453111\n",
      "Iteration 159, loss = 0.07449626\n",
      "Iteration 160, loss = 0.07428464\n",
      "Iteration 161, loss = 0.07400037\n",
      "Iteration 162, loss = 0.07378799\n",
      "Iteration 163, loss = 0.07361179\n",
      "Iteration 164, loss = 0.07338259\n",
      "Iteration 165, loss = 0.07323616\n",
      "Iteration 166, loss = 0.07299123\n",
      "Iteration 167, loss = 0.07281136\n",
      "Iteration 168, loss = 0.07263082\n",
      "Iteration 169, loss = 0.07241615\n",
      "Iteration 170, loss = 0.07225699\n",
      "Iteration 171, loss = 0.07200737\n",
      "Iteration 172, loss = 0.07186698\n",
      "Iteration 173, loss = 0.07172555\n",
      "Iteration 174, loss = 0.07142872\n",
      "Iteration 175, loss = 0.07127188\n",
      "Iteration 176, loss = 0.07115214\n",
      "Iteration 177, loss = 0.07095212\n",
      "Iteration 178, loss = 0.07078689\n",
      "Iteration 179, loss = 0.07058975\n",
      "Iteration 180, loss = 0.07044170\n",
      "Iteration 181, loss = 0.07034702\n",
      "Iteration 182, loss = 0.07008364\n",
      "Iteration 183, loss = 0.06998848\n",
      "Iteration 184, loss = 0.06977778\n",
      "Iteration 185, loss = 0.06962222\n",
      "Iteration 186, loss = 0.06946909\n",
      "Iteration 187, loss = 0.06934323\n",
      "Iteration 188, loss = 0.06917429\n",
      "Iteration 189, loss = 0.06900016\n",
      "Iteration 190, loss = 0.06882615\n",
      "Iteration 191, loss = 0.06873650\n",
      "Iteration 192, loss = 0.06853967\n",
      "Iteration 193, loss = 0.06834195\n",
      "Iteration 194, loss = 0.06828209\n",
      "Iteration 195, loss = 0.06816385\n",
      "Iteration 196, loss = 0.06793124\n",
      "Iteration 197, loss = 0.06788886\n",
      "Iteration 198, loss = 0.06768231\n",
      "Iteration 199, loss = 0.06761741\n",
      "Iteration 200, loss = 0.06740707\n",
      "[CV] END activation=relu, alpha=0.01, batch_size=50, hidden_layer_sizes=(100,), solver=sgd; total time= 3.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.08141295\n",
      "Iteration 2, loss = 0.50361309\n",
      "Iteration 3, loss = 0.41296952\n",
      "Iteration 4, loss = 0.37111219\n",
      "Iteration 5, loss = 0.34504395\n",
      "Iteration 6, loss = 0.32586984\n",
      "Iteration 7, loss = 0.31067230\n",
      "Iteration 8, loss = 0.29790091\n",
      "Iteration 9, loss = 0.28650809\n",
      "Iteration 10, loss = 0.27697755\n",
      "Iteration 11, loss = 0.26812430\n",
      "Iteration 12, loss = 0.25967780\n",
      "Iteration 13, loss = 0.25242292\n",
      "Iteration 14, loss = 0.24505459\n",
      "Iteration 15, loss = 0.23854029\n",
      "Iteration 16, loss = 0.23248133\n",
      "Iteration 17, loss = 0.22657424\n",
      "Iteration 18, loss = 0.22099042\n",
      "Iteration 19, loss = 0.21604426\n",
      "Iteration 20, loss = 0.21101069\n",
      "Iteration 21, loss = 0.20628067\n",
      "Iteration 22, loss = 0.20209138\n",
      "Iteration 23, loss = 0.19783813\n",
      "Iteration 24, loss = 0.19371117\n",
      "Iteration 25, loss = 0.19028519\n",
      "Iteration 26, loss = 0.18679392\n",
      "Iteration 27, loss = 0.18315467\n",
      "Iteration 28, loss = 0.18003008\n",
      "Iteration 29, loss = 0.17684111\n",
      "Iteration 30, loss = 0.17392158\n",
      "Iteration 31, loss = 0.17100354\n",
      "Iteration 32, loss = 0.16821961\n",
      "Iteration 33, loss = 0.16564703\n",
      "Iteration 34, loss = 0.16295112\n",
      "Iteration 35, loss = 0.16068786\n",
      "Iteration 36, loss = 0.15830893\n",
      "Iteration 37, loss = 0.15587284\n",
      "Iteration 38, loss = 0.15384678\n",
      "Iteration 39, loss = 0.15182672\n",
      "Iteration 40, loss = 0.14966943\n",
      "Iteration 41, loss = 0.14772536\n",
      "Iteration 42, loss = 0.14580239\n",
      "Iteration 43, loss = 0.14410998\n",
      "Iteration 44, loss = 0.14220553\n",
      "Iteration 45, loss = 0.14049857\n",
      "Iteration 46, loss = 0.13883451\n",
      "Iteration 47, loss = 0.13713041\n",
      "Iteration 48, loss = 0.13573081\n",
      "Iteration 49, loss = 0.13416222\n",
      "Iteration 50, loss = 0.13252431\n",
      "Iteration 51, loss = 0.13124808\n",
      "Iteration 52, loss = 0.12982736\n",
      "Iteration 53, loss = 0.12848930\n",
      "Iteration 54, loss = 0.12712166\n",
      "Iteration 55, loss = 0.12578061\n",
      "Iteration 56, loss = 0.12477432\n",
      "Iteration 57, loss = 0.12350173\n",
      "Iteration 58, loss = 0.12239606\n",
      "Iteration 59, loss = 0.12120926\n",
      "Iteration 60, loss = 0.12003584\n",
      "Iteration 61, loss = 0.11895001\n",
      "Iteration 62, loss = 0.11779154\n",
      "Iteration 63, loss = 0.11692943\n",
      "Iteration 64, loss = 0.11579617\n",
      "Iteration 65, loss = 0.11484092\n",
      "Iteration 66, loss = 0.11377804\n",
      "Iteration 67, loss = 0.11298307\n",
      "Iteration 68, loss = 0.11192431\n",
      "Iteration 69, loss = 0.11108036\n",
      "Iteration 70, loss = 0.11025239\n",
      "Iteration 71, loss = 0.10938034\n",
      "Iteration 72, loss = 0.10859156\n",
      "Iteration 73, loss = 0.10777754\n",
      "Iteration 74, loss = 0.10693597\n",
      "Iteration 75, loss = 0.10605093\n",
      "Iteration 76, loss = 0.10541483\n",
      "Iteration 77, loss = 0.10452083\n",
      "Iteration 78, loss = 0.10386323\n",
      "Iteration 79, loss = 0.10305147\n",
      "Iteration 80, loss = 0.10232618\n",
      "Iteration 81, loss = 0.10180379\n",
      "Iteration 82, loss = 0.10107630\n",
      "Iteration 83, loss = 0.10031331\n",
      "Iteration 84, loss = 0.09979795\n",
      "Iteration 85, loss = 0.09898857\n",
      "Iteration 86, loss = 0.09846942\n",
      "Iteration 87, loss = 0.09781381\n",
      "Iteration 88, loss = 0.09729773\n",
      "Iteration 89, loss = 0.09672143\n",
      "Iteration 90, loss = 0.09604844\n",
      "Iteration 91, loss = 0.09550987\n",
      "Iteration 92, loss = 0.09494396\n",
      "Iteration 93, loss = 0.09444131\n",
      "Iteration 94, loss = 0.09385531\n",
      "Iteration 95, loss = 0.09336409\n",
      "Iteration 96, loss = 0.09282235\n",
      "Iteration 97, loss = 0.09228778\n",
      "Iteration 98, loss = 0.09174608\n",
      "Iteration 99, loss = 0.09127494\n",
      "Iteration 100, loss = 0.09080931\n",
      "Iteration 101, loss = 0.09020710\n",
      "Iteration 102, loss = 0.08986908\n",
      "Iteration 103, loss = 0.08935394\n",
      "Iteration 104, loss = 0.08896487\n",
      "Iteration 105, loss = 0.08843423\n",
      "Iteration 106, loss = 0.08809305\n",
      "Iteration 107, loss = 0.08756964\n",
      "Iteration 108, loss = 0.08716069\n",
      "Iteration 109, loss = 0.08681651\n",
      "Iteration 110, loss = 0.08640341\n",
      "Iteration 111, loss = 0.08601869\n",
      "Iteration 112, loss = 0.08557372\n",
      "Iteration 113, loss = 0.08517575\n",
      "Iteration 114, loss = 0.08479361\n",
      "Iteration 115, loss = 0.08448644\n",
      "Iteration 116, loss = 0.08401711\n",
      "Iteration 117, loss = 0.08365477\n",
      "Iteration 118, loss = 0.08335498\n",
      "Iteration 119, loss = 0.08285657\n",
      "Iteration 120, loss = 0.08252786\n",
      "Iteration 121, loss = 0.08224965\n",
      "Iteration 122, loss = 0.08195224\n",
      "Iteration 123, loss = 0.08158487\n",
      "Iteration 124, loss = 0.08127542\n",
      "Iteration 125, loss = 0.08095666\n",
      "Iteration 126, loss = 0.08049566\n",
      "Iteration 127, loss = 0.08027420\n",
      "Iteration 128, loss = 0.08000676\n",
      "Iteration 129, loss = 0.07965509\n",
      "Iteration 130, loss = 0.07937944\n",
      "Iteration 131, loss = 0.07905724\n",
      "Iteration 132, loss = 0.07885459\n",
      "Iteration 133, loss = 0.07844634\n",
      "Iteration 134, loss = 0.07819550\n",
      "Iteration 135, loss = 0.07785968\n",
      "Iteration 136, loss = 0.07762412\n",
      "Iteration 137, loss = 0.07744500\n",
      "Iteration 138, loss = 0.07713501\n",
      "Iteration 139, loss = 0.07692893\n",
      "Iteration 140, loss = 0.07658063\n",
      "Iteration 141, loss = 0.07633620\n",
      "Iteration 142, loss = 0.07606579\n",
      "Iteration 143, loss = 0.07587633\n",
      "Iteration 144, loss = 0.07552038\n",
      "Iteration 145, loss = 0.07537019\n",
      "Iteration 146, loss = 0.07509344\n",
      "Iteration 147, loss = 0.07492465\n",
      "Iteration 148, loss = 0.07464260\n",
      "Iteration 149, loss = 0.07434201\n",
      "Iteration 150, loss = 0.07420772\n",
      "Iteration 151, loss = 0.07392737\n",
      "Iteration 152, loss = 0.07373112\n",
      "Iteration 153, loss = 0.07344998\n",
      "Iteration 154, loss = 0.07323694\n",
      "Iteration 155, loss = 0.07306034\n",
      "Iteration 156, loss = 0.07282778\n",
      "Iteration 157, loss = 0.07263737\n",
      "Iteration 158, loss = 0.07242891\n",
      "Iteration 159, loss = 0.07219912\n",
      "Iteration 160, loss = 0.07206930\n",
      "Iteration 161, loss = 0.07190121\n",
      "Iteration 162, loss = 0.07167353\n",
      "Iteration 163, loss = 0.07136797\n",
      "Iteration 164, loss = 0.07130501\n",
      "Iteration 165, loss = 0.07109558\n",
      "Iteration 166, loss = 0.07084272\n",
      "Iteration 167, loss = 0.07071393\n",
      "Iteration 168, loss = 0.07056468\n",
      "Iteration 169, loss = 0.07030254\n",
      "Iteration 170, loss = 0.07022056\n",
      "Iteration 171, loss = 0.07000460\n",
      "Iteration 172, loss = 0.06980266\n",
      "Iteration 173, loss = 0.06965718\n",
      "Iteration 174, loss = 0.06949084\n",
      "Iteration 175, loss = 0.06928257\n",
      "Iteration 176, loss = 0.06921690\n",
      "Iteration 177, loss = 0.06902573\n",
      "Iteration 178, loss = 0.06888033\n",
      "Iteration 179, loss = 0.06870523\n",
      "Iteration 180, loss = 0.06850370\n",
      "Iteration 181, loss = 0.06839313\n",
      "Iteration 182, loss = 0.06817260\n",
      "Iteration 183, loss = 0.06803904\n",
      "Iteration 184, loss = 0.06795950\n",
      "Iteration 185, loss = 0.06778484\n",
      "Iteration 186, loss = 0.06761340\n",
      "Iteration 187, loss = 0.06750671\n",
      "Iteration 188, loss = 0.06737974\n",
      "Iteration 189, loss = 0.06718432\n",
      "Iteration 190, loss = 0.06703734\n",
      "Iteration 191, loss = 0.06694531\n",
      "Iteration 192, loss = 0.06678282\n",
      "Iteration 193, loss = 0.06667669\n",
      "Iteration 194, loss = 0.06654393\n",
      "Iteration 195, loss = 0.06642140\n",
      "Iteration 196, loss = 0.06628989\n",
      "Iteration 197, loss = 0.06613530\n",
      "Iteration 198, loss = 0.06600647\n",
      "Iteration 199, loss = 0.06588799\n",
      "Iteration 200, loss = 0.06580667\n",
      "[CV] END activation=relu, alpha=0.01, batch_size=50, hidden_layer_sizes=(100,), solver=sgd; total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.06585113\n",
      "Iteration 2, loss = 0.48881926\n",
      "Iteration 3, loss = 0.40726746\n",
      "Iteration 4, loss = 0.36963397\n",
      "Iteration 5, loss = 0.34538090\n",
      "Iteration 6, loss = 0.32778988\n",
      "Iteration 7, loss = 0.31346956\n",
      "Iteration 8, loss = 0.30154846\n",
      "Iteration 9, loss = 0.29123371\n",
      "Iteration 10, loss = 0.28175237\n",
      "Iteration 11, loss = 0.27304068\n",
      "Iteration 12, loss = 0.26499871\n",
      "Iteration 13, loss = 0.25768578\n",
      "Iteration 14, loss = 0.25065991\n",
      "Iteration 15, loss = 0.24420064\n",
      "Iteration 16, loss = 0.23807357\n",
      "Iteration 17, loss = 0.23249182\n",
      "Iteration 18, loss = 0.22696832\n",
      "Iteration 19, loss = 0.22157713\n",
      "Iteration 20, loss = 0.21656448\n",
      "Iteration 21, loss = 0.21196341\n",
      "Iteration 22, loss = 0.20757963\n",
      "Iteration 23, loss = 0.20343254\n",
      "Iteration 24, loss = 0.19935307\n",
      "Iteration 25, loss = 0.19538194\n",
      "Iteration 26, loss = 0.19184853\n",
      "Iteration 27, loss = 0.18832486\n",
      "Iteration 28, loss = 0.18508893\n",
      "Iteration 29, loss = 0.18189913\n",
      "Iteration 30, loss = 0.17887047\n",
      "Iteration 31, loss = 0.17597846\n",
      "Iteration 32, loss = 0.17306601\n",
      "Iteration 33, loss = 0.17052939\n",
      "Iteration 34, loss = 0.16772703\n",
      "Iteration 35, loss = 0.16540257\n",
      "Iteration 36, loss = 0.16288945\n",
      "Iteration 37, loss = 0.16075327\n",
      "Iteration 38, loss = 0.15845369\n",
      "Iteration 39, loss = 0.15633375\n",
      "Iteration 40, loss = 0.15412443\n",
      "Iteration 41, loss = 0.15209870\n",
      "Iteration 42, loss = 0.15033806\n",
      "Iteration 43, loss = 0.14835488\n",
      "Iteration 44, loss = 0.14682412\n",
      "Iteration 45, loss = 0.14491304\n",
      "Iteration 46, loss = 0.14318373\n",
      "Iteration 47, loss = 0.14164123\n",
      "Iteration 48, loss = 0.14005771\n",
      "Iteration 49, loss = 0.13841348\n",
      "Iteration 50, loss = 0.13679254\n",
      "Iteration 51, loss = 0.13556171\n",
      "Iteration 52, loss = 0.13406915\n",
      "Iteration 53, loss = 0.13279812\n",
      "Iteration 54, loss = 0.13133239\n",
      "Iteration 55, loss = 0.13017201\n",
      "Iteration 56, loss = 0.12872904\n",
      "Iteration 57, loss = 0.12756635\n",
      "Iteration 58, loss = 0.12636063\n",
      "Iteration 59, loss = 0.12536346\n",
      "Iteration 60, loss = 0.12395986\n",
      "Iteration 61, loss = 0.12291157\n",
      "Iteration 62, loss = 0.12167176\n",
      "Iteration 63, loss = 0.12089504\n",
      "Iteration 64, loss = 0.11969723\n",
      "Iteration 65, loss = 0.11884653\n",
      "Iteration 66, loss = 0.11761498\n",
      "Iteration 67, loss = 0.11673848\n",
      "Iteration 68, loss = 0.11573954\n",
      "Iteration 69, loss = 0.11488291\n",
      "Iteration 70, loss = 0.11396615\n",
      "Iteration 71, loss = 0.11302287\n",
      "Iteration 72, loss = 0.11226332\n",
      "Iteration 73, loss = 0.11130608\n",
      "Iteration 74, loss = 0.11055859\n",
      "Iteration 75, loss = 0.10970746\n",
      "Iteration 76, loss = 0.10890357\n",
      "Iteration 77, loss = 0.10813052\n",
      "Iteration 78, loss = 0.10741108\n",
      "Iteration 79, loss = 0.10673899\n",
      "Iteration 80, loss = 0.10594987\n",
      "Iteration 81, loss = 0.10511449\n",
      "Iteration 82, loss = 0.10442878\n",
      "Iteration 83, loss = 0.10389906\n",
      "Iteration 84, loss = 0.10305624\n",
      "Iteration 85, loss = 0.10251820\n",
      "Iteration 86, loss = 0.10188886\n",
      "Iteration 87, loss = 0.10122135\n",
      "Iteration 88, loss = 0.10054025\n",
      "Iteration 89, loss = 0.09993894\n",
      "Iteration 90, loss = 0.09935529\n",
      "Iteration 91, loss = 0.09870722\n",
      "Iteration 92, loss = 0.09816209\n",
      "Iteration 93, loss = 0.09760981\n",
      "Iteration 94, loss = 0.09702098\n",
      "Iteration 95, loss = 0.09650136\n",
      "Iteration 96, loss = 0.09598215\n",
      "Iteration 97, loss = 0.09552641\n",
      "Iteration 98, loss = 0.09478465\n",
      "Iteration 99, loss = 0.09437527\n",
      "Iteration 100, loss = 0.09392926\n",
      "Iteration 101, loss = 0.09339761\n",
      "Iteration 102, loss = 0.09274834\n",
      "Iteration 103, loss = 0.09238716\n",
      "Iteration 104, loss = 0.09182232\n",
      "Iteration 105, loss = 0.09151011\n",
      "Iteration 106, loss = 0.09101575\n",
      "Iteration 107, loss = 0.09057329\n",
      "Iteration 108, loss = 0.09017456\n",
      "Iteration 109, loss = 0.08956412\n",
      "Iteration 110, loss = 0.08924308\n",
      "Iteration 111, loss = 0.08878191\n",
      "Iteration 112, loss = 0.08843474\n",
      "Iteration 113, loss = 0.08804625\n",
      "Iteration 114, loss = 0.08764204\n",
      "Iteration 115, loss = 0.08726933\n",
      "Iteration 116, loss = 0.08680704\n",
      "Iteration 117, loss = 0.08638220\n",
      "Iteration 118, loss = 0.08596568\n",
      "Iteration 119, loss = 0.08562574\n",
      "Iteration 120, loss = 0.08526706\n",
      "Iteration 121, loss = 0.08492880\n",
      "Iteration 122, loss = 0.08457822\n",
      "Iteration 123, loss = 0.08420170\n",
      "Iteration 124, loss = 0.08379087\n",
      "Iteration 125, loss = 0.08349054\n",
      "Iteration 126, loss = 0.08320699\n",
      "Iteration 127, loss = 0.08291844\n",
      "Iteration 128, loss = 0.08252681\n",
      "Iteration 129, loss = 0.08228951\n",
      "Iteration 130, loss = 0.08190245\n",
      "Iteration 131, loss = 0.08161075\n",
      "Iteration 132, loss = 0.08127483\n",
      "Iteration 133, loss = 0.08097216\n",
      "Iteration 134, loss = 0.08061907\n",
      "Iteration 135, loss = 0.08039482\n",
      "Iteration 136, loss = 0.08002289\n",
      "Iteration 137, loss = 0.07981418\n",
      "Iteration 138, loss = 0.07946305\n",
      "Iteration 139, loss = 0.07925286\n",
      "Iteration 140, loss = 0.07901919\n",
      "Iteration 141, loss = 0.07864957\n",
      "Iteration 142, loss = 0.07840918\n",
      "Iteration 143, loss = 0.07804037\n",
      "Iteration 144, loss = 0.07784968\n",
      "Iteration 145, loss = 0.07756485\n",
      "Iteration 146, loss = 0.07731695\n",
      "Iteration 147, loss = 0.07715603\n",
      "Iteration 148, loss = 0.07685272\n",
      "Iteration 149, loss = 0.07654924\n",
      "Iteration 150, loss = 0.07637806\n",
      "Iteration 151, loss = 0.07609473\n",
      "Iteration 152, loss = 0.07585962\n",
      "Iteration 153, loss = 0.07564637\n",
      "Iteration 154, loss = 0.07545683\n",
      "Iteration 155, loss = 0.07515751\n",
      "Iteration 156, loss = 0.07492215\n",
      "Iteration 157, loss = 0.07474748\n",
      "Iteration 158, loss = 0.07447566\n",
      "Iteration 159, loss = 0.07426632\n",
      "Iteration 160, loss = 0.07410269\n",
      "Iteration 161, loss = 0.07390257\n",
      "Iteration 162, loss = 0.07357123\n",
      "Iteration 163, loss = 0.07338555\n",
      "Iteration 164, loss = 0.07323471\n",
      "Iteration 165, loss = 0.07298256\n",
      "Iteration 166, loss = 0.07281392\n",
      "Iteration 167, loss = 0.07265213\n",
      "Iteration 168, loss = 0.07241355\n",
      "Iteration 169, loss = 0.07226925\n",
      "Iteration 170, loss = 0.07206655\n",
      "Iteration 171, loss = 0.07187910\n",
      "Iteration 172, loss = 0.07171742\n",
      "Iteration 173, loss = 0.07153329\n",
      "Iteration 174, loss = 0.07133375\n",
      "Iteration 175, loss = 0.07109988\n",
      "Iteration 176, loss = 0.07099996\n",
      "Iteration 177, loss = 0.07079057\n",
      "Iteration 178, loss = 0.07064173\n",
      "Iteration 179, loss = 0.07048208\n",
      "Iteration 180, loss = 0.07028097\n",
      "Iteration 181, loss = 0.07009371\n",
      "Iteration 182, loss = 0.07001727\n",
      "Iteration 183, loss = 0.06980015\n",
      "Iteration 184, loss = 0.06963013\n",
      "Iteration 185, loss = 0.06947093\n",
      "Iteration 186, loss = 0.06932209\n",
      "Iteration 187, loss = 0.06917531\n",
      "Iteration 188, loss = 0.06902673\n",
      "Iteration 189, loss = 0.06886508\n",
      "Iteration 190, loss = 0.06871379\n",
      "Iteration 191, loss = 0.06854817\n",
      "Iteration 192, loss = 0.06848221\n",
      "Iteration 193, loss = 0.06826379\n",
      "Iteration 194, loss = 0.06813550\n",
      "Iteration 195, loss = 0.06805144\n",
      "Iteration 196, loss = 0.06783553\n",
      "Iteration 197, loss = 0.06770402\n",
      "Iteration 198, loss = 0.06751914\n",
      "Iteration 199, loss = 0.06748430\n",
      "Iteration 200, loss = 0.06730343\n",
      "[CV] END activation=relu, alpha=0.01, batch_size=50, hidden_layer_sizes=(100,), solver=sgd; total time= 3.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.04750799\n",
      "Iteration 2, loss = 0.50186012\n",
      "Iteration 3, loss = 0.41481392\n",
      "Iteration 4, loss = 0.37540833\n",
      "Iteration 5, loss = 0.35079162\n",
      "Iteration 6, loss = 0.33295027\n",
      "Iteration 7, loss = 0.31868020\n",
      "Iteration 8, loss = 0.30691046\n",
      "Iteration 9, loss = 0.29646611\n",
      "Iteration 10, loss = 0.28717889\n",
      "Iteration 11, loss = 0.27869138\n",
      "Iteration 12, loss = 0.27098001\n",
      "Iteration 13, loss = 0.26366518\n",
      "Iteration 14, loss = 0.25678303\n",
      "Iteration 15, loss = 0.25021732\n",
      "Iteration 16, loss = 0.24429381\n",
      "Iteration 17, loss = 0.23823860\n",
      "Iteration 18, loss = 0.23267917\n",
      "Iteration 19, loss = 0.22739294\n",
      "Iteration 20, loss = 0.22256629\n",
      "Iteration 21, loss = 0.21750327\n",
      "Iteration 22, loss = 0.21280317\n",
      "Iteration 23, loss = 0.20856721\n",
      "Iteration 24, loss = 0.20425350\n",
      "Iteration 25, loss = 0.20026089\n",
      "Iteration 26, loss = 0.19635174\n",
      "Iteration 27, loss = 0.19269797\n",
      "Iteration 28, loss = 0.18920325\n",
      "Iteration 29, loss = 0.18589752\n",
      "Iteration 30, loss = 0.18246708\n",
      "Iteration 31, loss = 0.17939974\n",
      "Iteration 32, loss = 0.17643195\n",
      "Iteration 33, loss = 0.17356837\n",
      "Iteration 34, loss = 0.17082175\n",
      "Iteration 35, loss = 0.16803076\n",
      "Iteration 36, loss = 0.16545577\n",
      "Iteration 37, loss = 0.16284435\n",
      "Iteration 38, loss = 0.16065615\n",
      "Iteration 39, loss = 0.15830018\n",
      "Iteration 40, loss = 0.15596698\n",
      "Iteration 41, loss = 0.15392495\n",
      "Iteration 42, loss = 0.15179963\n",
      "Iteration 43, loss = 0.14990763\n",
      "Iteration 44, loss = 0.14783939\n",
      "Iteration 45, loss = 0.14604826\n",
      "Iteration 46, loss = 0.14425520\n",
      "Iteration 47, loss = 0.14257636\n",
      "Iteration 48, loss = 0.14087461\n",
      "Iteration 49, loss = 0.13923547\n",
      "Iteration 50, loss = 0.13785253\n",
      "Iteration 51, loss = 0.13610606\n",
      "Iteration 52, loss = 0.13449438\n",
      "Iteration 53, loss = 0.13314825\n",
      "Iteration 54, loss = 0.13176147\n",
      "Iteration 55, loss = 0.13043989\n",
      "Iteration 56, loss = 0.12906245\n",
      "Iteration 57, loss = 0.12778072\n",
      "Iteration 58, loss = 0.12643065\n",
      "Iteration 59, loss = 0.12537864\n",
      "Iteration 60, loss = 0.12413494\n",
      "Iteration 61, loss = 0.12280584\n",
      "Iteration 62, loss = 0.12161186\n",
      "Iteration 63, loss = 0.12062852\n",
      "Iteration 64, loss = 0.11958645\n",
      "Iteration 65, loss = 0.11864381\n",
      "Iteration 66, loss = 0.11762156\n",
      "Iteration 67, loss = 0.11651677\n",
      "Iteration 68, loss = 0.11555444\n",
      "Iteration 69, loss = 0.11467147\n",
      "Iteration 70, loss = 0.11382259\n",
      "Iteration 71, loss = 0.11276180\n",
      "Iteration 72, loss = 0.11204962\n",
      "Iteration 73, loss = 0.11115837\n",
      "Iteration 74, loss = 0.11024115\n",
      "Iteration 75, loss = 0.10944544\n",
      "Iteration 76, loss = 0.10861350\n",
      "Iteration 77, loss = 0.10776581\n",
      "Iteration 78, loss = 0.10711798\n",
      "Iteration 79, loss = 0.10623406\n",
      "Iteration 80, loss = 0.10558382\n",
      "Iteration 81, loss = 0.10488490\n",
      "Iteration 82, loss = 0.10418279\n",
      "Iteration 83, loss = 0.10356480\n",
      "Iteration 84, loss = 0.10271832\n",
      "Iteration 85, loss = 0.10216095\n",
      "Iteration 86, loss = 0.10146963\n",
      "Iteration 87, loss = 0.10101672\n",
      "Iteration 88, loss = 0.10030783\n",
      "Iteration 89, loss = 0.09967355\n",
      "Iteration 90, loss = 0.09912904\n",
      "Iteration 91, loss = 0.09850162\n",
      "Iteration 92, loss = 0.09801927\n",
      "Iteration 93, loss = 0.09739040\n",
      "Iteration 94, loss = 0.09682616\n",
      "Iteration 95, loss = 0.09627750\n",
      "Iteration 96, loss = 0.09569261\n",
      "Iteration 97, loss = 0.09532186\n",
      "Iteration 98, loss = 0.09465944\n",
      "Iteration 99, loss = 0.09425512\n",
      "Iteration 100, loss = 0.09373835\n",
      "Iteration 101, loss = 0.09328784\n",
      "Iteration 102, loss = 0.09268238\n",
      "Iteration 103, loss = 0.09237333\n",
      "Iteration 104, loss = 0.09187814\n",
      "Iteration 105, loss = 0.09142606\n",
      "Iteration 106, loss = 0.09091014\n",
      "Iteration 107, loss = 0.09039520\n",
      "Iteration 108, loss = 0.09005121\n",
      "Iteration 109, loss = 0.08963901\n",
      "Iteration 110, loss = 0.08926529\n",
      "Iteration 111, loss = 0.08875659\n",
      "Iteration 112, loss = 0.08842205\n",
      "Iteration 113, loss = 0.08804105\n",
      "Iteration 114, loss = 0.08765143\n",
      "Iteration 115, loss = 0.08717313\n",
      "Iteration 116, loss = 0.08684819\n",
      "Iteration 117, loss = 0.08648101\n",
      "Iteration 118, loss = 0.08618028\n",
      "Iteration 119, loss = 0.08572831\n",
      "Iteration 120, loss = 0.08541609\n",
      "Iteration 121, loss = 0.08509226\n",
      "Iteration 122, loss = 0.08470372\n",
      "Iteration 123, loss = 0.08431943\n",
      "Iteration 124, loss = 0.08399147\n",
      "Iteration 125, loss = 0.08370076\n",
      "Iteration 126, loss = 0.08340141\n",
      "Iteration 127, loss = 0.08304197\n",
      "Iteration 128, loss = 0.08277001\n",
      "Iteration 129, loss = 0.08244660\n",
      "Iteration 130, loss = 0.08210501\n",
      "Iteration 131, loss = 0.08183921\n",
      "Iteration 132, loss = 0.08148288\n",
      "Iteration 133, loss = 0.08112773\n",
      "Iteration 134, loss = 0.08102024\n",
      "Iteration 135, loss = 0.08059184\n",
      "Iteration 136, loss = 0.08041111\n",
      "Iteration 137, loss = 0.07997713\n",
      "Iteration 138, loss = 0.07984098\n",
      "Iteration 139, loss = 0.07957040\n",
      "Iteration 140, loss = 0.07927094\n",
      "Iteration 141, loss = 0.07901567\n",
      "Iteration 142, loss = 0.07875616\n",
      "Iteration 143, loss = 0.07856446\n",
      "Iteration 144, loss = 0.07820770\n",
      "Iteration 145, loss = 0.07801626\n",
      "Iteration 146, loss = 0.07772821\n",
      "Iteration 147, loss = 0.07749593\n",
      "Iteration 148, loss = 0.07729145\n",
      "Iteration 149, loss = 0.07701483\n",
      "Iteration 150, loss = 0.07679941\n",
      "Iteration 151, loss = 0.07651372\n",
      "Iteration 152, loss = 0.07628635\n",
      "Iteration 153, loss = 0.07611498\n",
      "Iteration 154, loss = 0.07590095\n",
      "Iteration 155, loss = 0.07570421\n",
      "Iteration 156, loss = 0.07543532\n",
      "Iteration 157, loss = 0.07523597\n",
      "Iteration 158, loss = 0.07506621\n",
      "Iteration 159, loss = 0.07484171\n",
      "Iteration 160, loss = 0.07464310\n",
      "Iteration 161, loss = 0.07443214\n",
      "Iteration 162, loss = 0.07423973\n",
      "Iteration 163, loss = 0.07401177\n",
      "Iteration 164, loss = 0.07380445\n",
      "Iteration 165, loss = 0.07363815\n",
      "Iteration 166, loss = 0.07349936\n",
      "Iteration 167, loss = 0.07328759\n",
      "Iteration 168, loss = 0.07306272\n",
      "Iteration 169, loss = 0.07289275\n",
      "Iteration 170, loss = 0.07274277\n",
      "Iteration 171, loss = 0.07250887\n",
      "Iteration 172, loss = 0.07237580\n",
      "Iteration 173, loss = 0.07214681\n",
      "Iteration 174, loss = 0.07203196\n",
      "Iteration 175, loss = 0.07181681\n",
      "Iteration 176, loss = 0.07169149\n",
      "Iteration 177, loss = 0.07151140\n",
      "Iteration 178, loss = 0.07132484\n",
      "Iteration 179, loss = 0.07118329\n",
      "Iteration 180, loss = 0.07098083\n",
      "Iteration 181, loss = 0.07085749\n",
      "Iteration 182, loss = 0.07066378\n",
      "Iteration 183, loss = 0.07054334\n",
      "Iteration 184, loss = 0.07035170\n",
      "Iteration 185, loss = 0.07028195\n",
      "Iteration 186, loss = 0.07013398\n",
      "Iteration 187, loss = 0.06990259\n",
      "Iteration 188, loss = 0.06972100\n",
      "Iteration 189, loss = 0.06960847\n",
      "Iteration 190, loss = 0.06949197\n",
      "Iteration 191, loss = 0.06933142\n",
      "Iteration 192, loss = 0.06916489\n",
      "Iteration 193, loss = 0.06905000\n",
      "Iteration 194, loss = 0.06888959\n",
      "Iteration 195, loss = 0.06874521\n",
      "Iteration 196, loss = 0.06867479\n",
      "Iteration 197, loss = 0.06849747\n",
      "Iteration 198, loss = 0.06833154\n",
      "Iteration 199, loss = 0.06823543\n",
      "Iteration 200, loss = 0.06809700\n",
      "[CV] END activation=relu, alpha=0.01, batch_size=50, hidden_layer_sizes=(100,), solver=sgd; total time= 3.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.32117535\n",
      "Iteration 2, loss = 0.16488929\n",
      "Iteration 3, loss = 0.13252483\n",
      "Iteration 4, loss = 0.11653060\n",
      "Iteration 5, loss = 0.10318342\n",
      "Iteration 6, loss = 0.09926710\n",
      "Iteration 7, loss = 0.09658295\n",
      "Iteration 8, loss = 0.09059838\n",
      "Iteration 9, loss = 0.08810849\n",
      "Iteration 10, loss = 0.08512154\n",
      "Iteration 11, loss = 0.07778860\n",
      "Iteration 12, loss = 0.08043284\n",
      "Iteration 13, loss = 0.07529581\n",
      "Iteration 14, loss = 0.07175671\n",
      "Iteration 15, loss = 0.07235392\n",
      "Iteration 16, loss = 0.06868499\n",
      "Iteration 17, loss = 0.06977319\n",
      "Iteration 18, loss = 0.06653952\n",
      "Iteration 19, loss = 0.06082720\n",
      "Iteration 20, loss = 0.06610909\n",
      "Iteration 21, loss = 0.06635831\n",
      "Iteration 22, loss = 0.06154690\n",
      "Iteration 23, loss = 0.06417409\n",
      "Iteration 24, loss = 0.06065774\n",
      "Iteration 25, loss = 0.06037931\n",
      "Iteration 26, loss = 0.05883825\n",
      "Iteration 27, loss = 0.06622132\n",
      "Iteration 28, loss = 0.05813533\n",
      "Iteration 29, loss = 0.05481337\n",
      "Iteration 30, loss = 0.05781036\n",
      "Iteration 31, loss = 0.05579800\n",
      "Iteration 32, loss = 0.05942456\n",
      "Iteration 33, loss = 0.05375983\n",
      "Iteration 34, loss = 0.05718319\n",
      "Iteration 35, loss = 0.05456342\n",
      "Iteration 36, loss = 0.05465631\n",
      "Iteration 37, loss = 0.05338811\n",
      "Iteration 38, loss = 0.05380840\n",
      "Iteration 39, loss = 0.05815951\n",
      "Iteration 40, loss = 0.05709652\n",
      "Iteration 41, loss = 0.04846572\n",
      "Iteration 42, loss = 0.05624403\n",
      "Iteration 43, loss = 0.05230119\n",
      "Iteration 44, loss = 0.05116881\n",
      "Iteration 45, loss = 0.05466980\n",
      "Iteration 46, loss = 0.05118255\n",
      "Iteration 47, loss = 0.05215829\n",
      "Iteration 48, loss = 0.05034066\n",
      "Iteration 49, loss = 0.05676975\n",
      "Iteration 50, loss = 0.05184148\n",
      "Iteration 51, loss = 0.04755294\n",
      "Iteration 52, loss = 0.05453773\n",
      "Iteration 53, loss = 0.04972462\n",
      "Iteration 54, loss = 0.05219284\n",
      "Iteration 55, loss = 0.05050329\n",
      "Iteration 56, loss = 0.04991364\n",
      "Iteration 57, loss = 0.05370975\n",
      "Iteration 58, loss = 0.04835999\n",
      "Iteration 59, loss = 0.04873682\n",
      "Iteration 60, loss = 0.04888817\n",
      "Iteration 61, loss = 0.05344214\n",
      "Iteration 62, loss = 0.04718873\n",
      "Iteration 63, loss = 0.05064474\n",
      "Iteration 64, loss = 0.05127051\n",
      "Iteration 65, loss = 0.04880678\n",
      "Iteration 66, loss = 0.04927356\n",
      "Iteration 67, loss = 0.04789113\n",
      "Iteration 68, loss = 0.05126469\n",
      "Iteration 69, loss = 0.04814749\n",
      "Iteration 70, loss = 0.04833180\n",
      "Iteration 71, loss = 0.05284644\n",
      "Iteration 72, loss = 0.04793139\n",
      "Iteration 73, loss = 0.04709677\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.01, batch_size=50, hidden_layer_sizes=(400, 100), solver=adam; total time=22.0min\n",
      "Iteration 1, loss = 0.32674727\n",
      "Iteration 2, loss = 0.16623279\n",
      "Iteration 3, loss = 0.13559189\n",
      "Iteration 4, loss = 0.11555724\n",
      "Iteration 5, loss = 0.10727183\n",
      "Iteration 6, loss = 0.10078997\n",
      "Iteration 7, loss = 0.09584781\n",
      "Iteration 8, loss = 0.09352658\n",
      "Iteration 9, loss = 0.08906064\n",
      "Iteration 10, loss = 0.08339880\n",
      "Iteration 11, loss = 0.08149353\n",
      "Iteration 12, loss = 0.07992482\n",
      "Iteration 13, loss = 0.07680649\n",
      "Iteration 14, loss = 0.07437289\n",
      "Iteration 15, loss = 0.07253424\n",
      "Iteration 16, loss = 0.06958130\n",
      "Iteration 17, loss = 0.06945768\n",
      "Iteration 18, loss = 0.06716483\n",
      "Iteration 19, loss = 0.06670409\n",
      "Iteration 20, loss = 0.06453709\n",
      "Iteration 21, loss = 0.06557749\n",
      "Iteration 22, loss = 0.06423277\n",
      "Iteration 23, loss = 0.05995289\n",
      "Iteration 24, loss = 0.06360749\n",
      "Iteration 25, loss = 0.06107699\n",
      "Iteration 26, loss = 0.05671022\n",
      "Iteration 27, loss = 0.05992264\n",
      "Iteration 28, loss = 0.06223988\n",
      "Iteration 29, loss = 0.06017383\n",
      "Iteration 30, loss = 0.05410608\n",
      "Iteration 31, loss = 0.05286424\n",
      "Iteration 32, loss = 0.06420158\n",
      "Iteration 33, loss = 0.05771326\n",
      "Iteration 34, loss = 0.05541469\n",
      "Iteration 35, loss = 0.05452075\n",
      "Iteration 36, loss = 0.05591798\n",
      "Iteration 37, loss = 0.05500350\n",
      "Iteration 38, loss = 0.05236043\n",
      "Iteration 39, loss = 0.06021424\n",
      "Iteration 40, loss = 0.05303813\n",
      "Iteration 41, loss = 0.05615415\n",
      "Iteration 42, loss = 0.05476307\n",
      "Iteration 43, loss = 0.05340016\n",
      "Iteration 44, loss = 0.05291055\n",
      "Iteration 45, loss = 0.04967117\n",
      "Iteration 46, loss = 0.05372157\n",
      "Iteration 47, loss = 0.05693390\n",
      "Iteration 48, loss = 0.05124433\n",
      "Iteration 49, loss = 0.05123332\n",
      "Iteration 50, loss = 0.05295196\n",
      "Iteration 51, loss = 0.04926614\n",
      "Iteration 52, loss = 0.05222282\n",
      "Iteration 53, loss = 0.05035946\n",
      "Iteration 54, loss = 0.05344309\n",
      "Iteration 55, loss = 0.05376641\n",
      "Iteration 56, loss = 0.05108103\n",
      "Iteration 57, loss = 0.04907029\n",
      "Iteration 58, loss = 0.05262989\n",
      "Iteration 59, loss = 0.05330165\n",
      "Iteration 60, loss = 0.05112319\n",
      "Iteration 61, loss = 0.04538693\n",
      "Iteration 62, loss = 0.04951188\n",
      "Iteration 63, loss = 0.05493742\n",
      "Iteration 64, loss = 0.05221210\n",
      "Iteration 65, loss = 0.04721599\n",
      "Iteration 66, loss = 0.04579920\n",
      "Iteration 67, loss = 0.05197377\n",
      "Iteration 68, loss = 0.05107222\n",
      "Iteration 69, loss = 0.05000732\n",
      "Iteration 70, loss = 0.05092011\n",
      "Iteration 71, loss = 0.04821903\n",
      "Iteration 72, loss = 0.04872601\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.01, batch_size=50, hidden_layer_sizes=(400, 100), solver=adam; total time=22.0min\n",
      "Iteration 1, loss = 0.32087061\n",
      "Iteration 2, loss = 0.16430847\n",
      "Iteration 3, loss = 0.13299871\n",
      "Iteration 4, loss = 0.11604395\n",
      "Iteration 5, loss = 0.10386898\n",
      "Iteration 6, loss = 0.09864125\n",
      "Iteration 7, loss = 0.09365506\n",
      "Iteration 8, loss = 0.08885973\n",
      "Iteration 9, loss = 0.08961364\n",
      "Iteration 10, loss = 0.08155421\n",
      "Iteration 11, loss = 0.08134424\n",
      "Iteration 12, loss = 0.07473877\n",
      "Iteration 13, loss = 0.07741785\n",
      "Iteration 14, loss = 0.07553687\n",
      "Iteration 15, loss = 0.06948500\n",
      "Iteration 16, loss = 0.06483079\n",
      "Iteration 17, loss = 0.07441505\n",
      "Iteration 18, loss = 0.06735457\n",
      "Iteration 19, loss = 0.06440046\n",
      "Iteration 20, loss = 0.06429637\n",
      "Iteration 21, loss = 0.06499034\n",
      "Iteration 22, loss = 0.06261083\n",
      "Iteration 23, loss = 0.06469811\n",
      "Iteration 24, loss = 0.06145791\n",
      "Iteration 25, loss = 0.06105251\n",
      "Iteration 26, loss = 0.05852121\n",
      "Iteration 27, loss = 0.06401587\n",
      "Iteration 28, loss = 0.05956624\n",
      "Iteration 29, loss = 0.05733316\n",
      "Iteration 30, loss = 0.05731730\n",
      "Iteration 31, loss = 0.05884273\n",
      "Iteration 32, loss = 0.06024299\n",
      "Iteration 33, loss = 0.05474663\n",
      "Iteration 34, loss = 0.05529008\n",
      "Iteration 35, loss = 0.05744111\n",
      "Iteration 36, loss = 0.05599628\n",
      "Iteration 37, loss = 0.05761038\n",
      "Iteration 38, loss = 0.05779328\n",
      "Iteration 39, loss = 0.05440210\n",
      "Iteration 40, loss = 0.05281909\n",
      "Iteration 41, loss = 0.05219420\n",
      "Iteration 42, loss = 0.05408390\n",
      "Iteration 43, loss = 0.06042776\n",
      "Iteration 44, loss = 0.05234105\n",
      "Iteration 45, loss = 0.05588622\n",
      "Iteration 46, loss = 0.04907262\n",
      "Iteration 47, loss = 0.05405001\n",
      "Iteration 48, loss = 0.05683970\n",
      "Iteration 49, loss = 0.05020534\n",
      "Iteration 50, loss = 0.05274984\n",
      "Iteration 51, loss = 0.05039881\n",
      "Iteration 52, loss = 0.05397896\n",
      "Iteration 53, loss = 0.05183250\n",
      "Iteration 54, loss = 0.05202503\n",
      "Iteration 55, loss = 0.05256447\n",
      "Iteration 56, loss = 0.05150940\n",
      "Iteration 57, loss = 0.05065259\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.01, batch_size=50, hidden_layer_sizes=(400, 100), solver=adam; total time=16.5min\n",
      "Iteration 1, loss = 0.32763641\n",
      "Iteration 2, loss = 0.16789544\n",
      "Iteration 3, loss = 0.13497972\n",
      "Iteration 4, loss = 0.11547782\n",
      "Iteration 5, loss = 0.10756546\n",
      "Iteration 6, loss = 0.09991309\n",
      "Iteration 7, loss = 0.09496631\n",
      "Iteration 8, loss = 0.09235747\n",
      "Iteration 9, loss = 0.08469695\n",
      "Iteration 10, loss = 0.08672942\n",
      "Iteration 11, loss = 0.08295199\n",
      "Iteration 12, loss = 0.07756245\n",
      "Iteration 13, loss = 0.07293115\n",
      "Iteration 14, loss = 0.07598856\n",
      "Iteration 15, loss = 0.07127093\n",
      "Iteration 16, loss = 0.07024472\n",
      "Iteration 17, loss = 0.06834997\n",
      "Iteration 18, loss = 0.06417520\n",
      "Iteration 19, loss = 0.06982134\n",
      "Iteration 20, loss = 0.06621665\n",
      "Iteration 21, loss = 0.06549931\n",
      "Iteration 22, loss = 0.05908816\n",
      "Iteration 23, loss = 0.06510978\n",
      "Iteration 24, loss = 0.06449474\n",
      "Iteration 25, loss = 0.05836045\n",
      "Iteration 26, loss = 0.05541726\n",
      "Iteration 27, loss = 0.06472849\n",
      "Iteration 28, loss = 0.05983559\n",
      "Iteration 29, loss = 0.05491445\n",
      "Iteration 30, loss = 0.05787963\n",
      "Iteration 31, loss = 0.05953339\n",
      "Iteration 32, loss = 0.05695169\n",
      "Iteration 33, loss = 0.05685922\n",
      "Iteration 34, loss = 0.05823191\n",
      "Iteration 35, loss = 0.05454854\n",
      "Iteration 36, loss = 0.05584011\n",
      "Iteration 37, loss = 0.05692836\n",
      "Iteration 38, loss = 0.05314850\n",
      "Iteration 39, loss = 0.05232049\n",
      "Iteration 40, loss = 0.05807714\n",
      "Iteration 41, loss = 0.05331236\n",
      "Iteration 42, loss = 0.05259072\n",
      "Iteration 43, loss = 0.05184807\n",
      "Iteration 44, loss = 0.05140689\n",
      "Iteration 45, loss = 0.05724155\n",
      "Iteration 46, loss = 0.05691913\n",
      "Iteration 47, loss = 0.05306962\n",
      "Iteration 48, loss = 0.05115561\n",
      "Iteration 49, loss = 0.05000998\n",
      "Iteration 50, loss = 0.05319692\n",
      "Iteration 51, loss = 0.05418818\n",
      "Iteration 52, loss = 0.05422675\n",
      "Iteration 53, loss = 0.05000820\n",
      "Iteration 54, loss = 0.05374604\n",
      "Iteration 55, loss = 0.05146170\n",
      "Iteration 56, loss = 0.04943389\n",
      "Iteration 57, loss = 0.05085986\n",
      "Iteration 58, loss = 0.05256302\n",
      "Iteration 59, loss = 0.05146239\n",
      "Iteration 60, loss = 0.05069947\n",
      "Iteration 61, loss = 0.05197512\n",
      "Iteration 62, loss = 0.05143155\n",
      "Iteration 63, loss = 0.05027449\n",
      "Iteration 64, loss = 0.05038386\n",
      "Iteration 65, loss = 0.04802479\n",
      "Iteration 66, loss = 0.05225007\n",
      "Iteration 67, loss = 0.05238068\n",
      "Iteration 68, loss = 0.04666661\n",
      "Iteration 69, loss = 0.04831479\n",
      "Iteration 70, loss = 0.05248641\n",
      "Iteration 71, loss = 0.04634847\n",
      "Iteration 72, loss = 0.05485172\n",
      "Iteration 73, loss = 0.04766923\n",
      "Iteration 74, loss = 0.04679271\n",
      "Iteration 75, loss = 0.05368143\n",
      "Iteration 76, loss = 0.05074550\n",
      "Iteration 77, loss = 0.04997425\n",
      "Iteration 78, loss = 0.04480485\n",
      "Iteration 79, loss = 0.05155997\n",
      "Iteration 80, loss = 0.04866144\n",
      "Iteration 81, loss = 0.04902312\n",
      "Iteration 82, loss = 0.05154010\n",
      "Iteration 83, loss = 0.04919051\n",
      "Iteration 84, loss = 0.04473904\n",
      "Iteration 85, loss = 0.04750996\n",
      "Iteration 86, loss = 0.05200150\n",
      "Iteration 87, loss = 0.04790401\n",
      "Iteration 88, loss = 0.04833915\n",
      "Iteration 89, loss = 0.04705232\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.01, batch_size=50, hidden_layer_sizes=(400, 100), solver=adam; total time=27.8min\n",
      "Iteration 1, loss = 0.32625844\n",
      "Iteration 2, loss = 0.16898818\n",
      "Iteration 3, loss = 0.13301307\n",
      "Iteration 4, loss = 0.12097015\n",
      "Iteration 5, loss = 0.10566939\n",
      "Iteration 6, loss = 0.10208283\n",
      "Iteration 7, loss = 0.09434209\n",
      "Iteration 8, loss = 0.09166773\n",
      "Iteration 9, loss = 0.08935873\n",
      "Iteration 10, loss = 0.08502276\n",
      "Iteration 11, loss = 0.08451495\n",
      "Iteration 12, loss = 0.08104793\n",
      "Iteration 13, loss = 0.07696844\n",
      "Iteration 14, loss = 0.07523221\n",
      "Iteration 15, loss = 0.07121774\n",
      "Iteration 16, loss = 0.07288301\n",
      "Iteration 17, loss = 0.07008099\n",
      "Iteration 18, loss = 0.06666316\n",
      "Iteration 19, loss = 0.06471856\n",
      "Iteration 20, loss = 0.06749695\n",
      "Iteration 21, loss = 0.06449738\n",
      "Iteration 22, loss = 0.06475019\n",
      "Iteration 23, loss = 0.06436646\n",
      "Iteration 24, loss = 0.06160612\n",
      "Iteration 25, loss = 0.06020191\n",
      "Iteration 26, loss = 0.06273202\n",
      "Iteration 27, loss = 0.05874961\n",
      "Iteration 28, loss = 0.06409437\n",
      "Iteration 29, loss = 0.05902606\n",
      "Iteration 30, loss = 0.05473686\n",
      "Iteration 31, loss = 0.05899358\n",
      "Iteration 32, loss = 0.06215176\n",
      "Iteration 33, loss = 0.05729416\n",
      "Iteration 34, loss = 0.05590462\n",
      "Iteration 35, loss = 0.05635432\n",
      "Iteration 36, loss = 0.05757127\n",
      "Iteration 37, loss = 0.05587039\n",
      "Iteration 38, loss = 0.05821745\n",
      "Iteration 39, loss = 0.04950138\n",
      "Iteration 40, loss = 0.05389881\n",
      "Iteration 41, loss = 0.06156964\n",
      "Iteration 42, loss = 0.05290872\n",
      "Iteration 43, loss = 0.05183087\n",
      "Iteration 44, loss = 0.05676927\n",
      "Iteration 45, loss = 0.05297551\n",
      "Iteration 46, loss = 0.05425766\n",
      "Iteration 47, loss = 0.05127605\n",
      "Iteration 48, loss = 0.05557234\n",
      "Iteration 49, loss = 0.05413234\n",
      "Iteration 50, loss = 0.05236147\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.01, batch_size=50, hidden_layer_sizes=(400, 100), solver=adam; total time=13.7min\n",
      "Iteration 1, loss = 0.98450575\n",
      "Iteration 2, loss = 0.44807190\n",
      "Iteration 3, loss = 0.38191527\n",
      "Iteration 4, loss = 0.34696652\n",
      "Iteration 5, loss = 0.32199736\n",
      "Iteration 6, loss = 0.30245785\n",
      "Iteration 7, loss = 0.28617648\n",
      "Iteration 8, loss = 0.27176273\n",
      "Iteration 9, loss = 0.25956224\n",
      "Iteration 10, loss = 0.24853030\n",
      "Iteration 11, loss = 0.23897866\n",
      "Iteration 12, loss = 0.22989866\n",
      "Iteration 13, loss = 0.22202161\n",
      "Iteration 14, loss = 0.21469664\n",
      "Iteration 15, loss = 0.20773374\n",
      "Iteration 16, loss = 0.20136757\n",
      "Iteration 17, loss = 0.19570087\n",
      "Iteration 18, loss = 0.19022740\n",
      "Iteration 19, loss = 0.18495208\n",
      "Iteration 20, loss = 0.18020456\n",
      "Iteration 21, loss = 0.17568800\n",
      "Iteration 22, loss = 0.17150254\n",
      "Iteration 23, loss = 0.16738698\n",
      "Iteration 24, loss = 0.16393633\n",
      "Iteration 25, loss = 0.16026379\n",
      "Iteration 26, loss = 0.15690170\n",
      "Iteration 27, loss = 0.15383901\n",
      "Iteration 28, loss = 0.15060176\n",
      "Iteration 29, loss = 0.14789939\n",
      "Iteration 30, loss = 0.14529601\n",
      "Iteration 31, loss = 0.14250686\n",
      "Iteration 32, loss = 0.14015833\n",
      "Iteration 33, loss = 0.13780547\n",
      "Iteration 34, loss = 0.13559792\n",
      "Iteration 35, loss = 0.13345345\n",
      "Iteration 36, loss = 0.13132263\n",
      "Iteration 37, loss = 0.12962851\n",
      "Iteration 38, loss = 0.12759237\n",
      "Iteration 39, loss = 0.12596456\n",
      "Iteration 40, loss = 0.12429077\n",
      "Iteration 41, loss = 0.12248050\n",
      "Iteration 42, loss = 0.12107239\n",
      "Iteration 43, loss = 0.11970980\n",
      "Iteration 44, loss = 0.11799251\n",
      "Iteration 45, loss = 0.11667685\n",
      "Iteration 46, loss = 0.11545865\n",
      "Iteration 47, loss = 0.11440415\n",
      "Iteration 48, loss = 0.11312217\n",
      "Iteration 49, loss = 0.11198022\n",
      "Iteration 50, loss = 0.11069239\n",
      "Iteration 51, loss = 0.10972157\n",
      "Iteration 52, loss = 0.10867608\n",
      "Iteration 53, loss = 0.10770185\n",
      "Iteration 54, loss = 0.10666827\n",
      "Iteration 55, loss = 0.10581594\n",
      "Iteration 56, loss = 0.10472360\n",
      "Iteration 57, loss = 0.10408710\n",
      "Iteration 58, loss = 0.10322976\n",
      "Iteration 59, loss = 0.10246629\n",
      "Iteration 60, loss = 0.10162348\n",
      "Iteration 61, loss = 0.10077374\n",
      "Iteration 62, loss = 0.10012536\n",
      "Iteration 63, loss = 0.09944243\n",
      "Iteration 64, loss = 0.09878396\n",
      "Iteration 65, loss = 0.09814307\n",
      "Iteration 66, loss = 0.09734022\n",
      "Iteration 67, loss = 0.09680437\n",
      "Iteration 68, loss = 0.09620540\n",
      "Iteration 69, loss = 0.09570654\n",
      "Iteration 70, loss = 0.09512178\n",
      "Iteration 71, loss = 0.09438566\n",
      "Iteration 72, loss = 0.09398169\n",
      "Iteration 73, loss = 0.09339992\n",
      "Iteration 74, loss = 0.09304134\n",
      "Iteration 75, loss = 0.09246782\n",
      "Iteration 76, loss = 0.09201545\n",
      "Iteration 77, loss = 0.09158476\n",
      "Iteration 78, loss = 0.09115907\n",
      "Iteration 79, loss = 0.09059281\n",
      "Iteration 80, loss = 0.09027716\n",
      "Iteration 81, loss = 0.08983724\n",
      "Iteration 82, loss = 0.08937140\n",
      "Iteration 83, loss = 0.08903283\n",
      "Iteration 84, loss = 0.08863322\n",
      "Iteration 85, loss = 0.08825622\n",
      "Iteration 86, loss = 0.08791754\n",
      "Iteration 87, loss = 0.08751836\n",
      "Iteration 88, loss = 0.08718294\n",
      "Iteration 89, loss = 0.08691506\n",
      "Iteration 90, loss = 0.08660285\n",
      "Iteration 91, loss = 0.08623048\n",
      "Iteration 92, loss = 0.08594350\n",
      "Iteration 93, loss = 0.08559678\n",
      "Iteration 94, loss = 0.08527013\n",
      "Iteration 95, loss = 0.08499483\n",
      "Iteration 96, loss = 0.08462815\n",
      "Iteration 97, loss = 0.08443919\n",
      "Iteration 98, loss = 0.08414502\n",
      "Iteration 99, loss = 0.08388749\n",
      "Iteration 100, loss = 0.08361393\n",
      "Iteration 101, loss = 0.08331741\n",
      "Iteration 102, loss = 0.08307762\n",
      "Iteration 103, loss = 0.08282426\n",
      "Iteration 104, loss = 0.08258869\n",
      "Iteration 105, loss = 0.08231850\n",
      "Iteration 106, loss = 0.08208074\n",
      "Iteration 107, loss = 0.08185518\n",
      "Iteration 108, loss = 0.08163333\n",
      "Iteration 109, loss = 0.08140563\n",
      "Iteration 110, loss = 0.08119527\n",
      "Iteration 111, loss = 0.08096653\n",
      "Iteration 112, loss = 0.08070112\n",
      "Iteration 113, loss = 0.08052671\n",
      "Iteration 114, loss = 0.08034472\n",
      "Iteration 115, loss = 0.08011557\n",
      "Iteration 116, loss = 0.07989315\n",
      "Iteration 117, loss = 0.07968231\n",
      "Iteration 118, loss = 0.07950786\n",
      "Iteration 119, loss = 0.07927547\n",
      "Iteration 120, loss = 0.07912453\n",
      "Iteration 121, loss = 0.07894068\n",
      "Iteration 122, loss = 0.07872571\n",
      "Iteration 123, loss = 0.07852482\n",
      "Iteration 124, loss = 0.07837925\n",
      "Iteration 125, loss = 0.07818893\n",
      "Iteration 126, loss = 0.07802165\n",
      "Iteration 127, loss = 0.07782434\n",
      "Iteration 128, loss = 0.07763559\n",
      "Iteration 129, loss = 0.07746430\n",
      "Iteration 130, loss = 0.07731738\n",
      "Iteration 131, loss = 0.07716195\n",
      "Iteration 132, loss = 0.07696503\n",
      "Iteration 133, loss = 0.07678805\n",
      "Iteration 134, loss = 0.07665982\n",
      "Iteration 135, loss = 0.07648418\n",
      "Iteration 136, loss = 0.07628344\n",
      "Iteration 137, loss = 0.07612769\n",
      "Iteration 138, loss = 0.07599018\n",
      "Iteration 139, loss = 0.07585101\n",
      "Iteration 140, loss = 0.07568312\n",
      "Iteration 141, loss = 0.07553511\n",
      "Iteration 142, loss = 0.07537411\n",
      "Iteration 143, loss = 0.07521730\n",
      "Iteration 144, loss = 0.07506318\n",
      "Iteration 145, loss = 0.07490801\n",
      "Iteration 146, loss = 0.07476577\n",
      "Iteration 147, loss = 0.07462745\n",
      "Iteration 148, loss = 0.07446563\n",
      "Iteration 149, loss = 0.07431553\n",
      "Iteration 150, loss = 0.07418018\n",
      "Iteration 151, loss = 0.07403530\n",
      "Iteration 152, loss = 0.07389878\n",
      "Iteration 153, loss = 0.07376758\n",
      "Iteration 154, loss = 0.07361891\n",
      "Iteration 155, loss = 0.07347793\n",
      "Iteration 156, loss = 0.07334513\n",
      "Iteration 157, loss = 0.07320584\n",
      "Iteration 158, loss = 0.07307146\n",
      "Iteration 159, loss = 0.07293446\n",
      "Iteration 160, loss = 0.07278644\n",
      "Iteration 161, loss = 0.07265111\n",
      "Iteration 162, loss = 0.07252328\n",
      "Iteration 163, loss = 0.07240365\n",
      "Iteration 164, loss = 0.07226454\n",
      "Iteration 165, loss = 0.07212184\n",
      "Iteration 166, loss = 0.07198572\n",
      "Iteration 167, loss = 0.07187161\n",
      "Iteration 168, loss = 0.07172545\n",
      "Iteration 169, loss = 0.07160970\n",
      "Iteration 170, loss = 0.07147850\n",
      "Iteration 171, loss = 0.07134984\n",
      "Iteration 172, loss = 0.07122824\n",
      "Iteration 173, loss = 0.07109975\n",
      "Iteration 174, loss = 0.07095673\n",
      "Iteration 175, loss = 0.07084969\n",
      "Iteration 176, loss = 0.07070685\n",
      "Iteration 177, loss = 0.07059062\n",
      "Iteration 178, loss = 0.07047011\n",
      "Iteration 179, loss = 0.07035051\n",
      "Iteration 180, loss = 0.07020654\n",
      "Iteration 181, loss = 0.07012470\n",
      "Iteration 182, loss = 0.06997990\n",
      "Iteration 183, loss = 0.06985817\n",
      "Iteration 184, loss = 0.06974192\n",
      "Iteration 185, loss = 0.06963368\n",
      "Iteration 186, loss = 0.06950281\n",
      "Iteration 187, loss = 0.06936698\n",
      "Iteration 188, loss = 0.06927054\n",
      "Iteration 189, loss = 0.06913918\n",
      "Iteration 190, loss = 0.06901987\n",
      "Iteration 191, loss = 0.06891826\n",
      "Iteration 192, loss = 0.06879194\n",
      "Iteration 193, loss = 0.06866178\n",
      "Iteration 194, loss = 0.06857017\n",
      "Iteration 195, loss = 0.06843959\n",
      "Iteration 196, loss = 0.06832888\n",
      "Iteration 197, loss = 0.06822894\n",
      "Iteration 198, loss = 0.06809536\n",
      "Iteration 199, loss = 0.06798384\n",
      "Iteration 200, loss = 0.06786617\n",
      "[CV] END activation=relu, alpha=0.01, batch_size=50, hidden_layer_sizes=(400, 100), solver=sgd; total time=10.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.00576109\n",
      "Iteration 2, loss = 0.45301314\n",
      "Iteration 3, loss = 0.38241888\n",
      "Iteration 4, loss = 0.34696675\n",
      "Iteration 5, loss = 0.32240313\n",
      "Iteration 6, loss = 0.30272794\n",
      "Iteration 7, loss = 0.28687064\n",
      "Iteration 8, loss = 0.27272796\n",
      "Iteration 9, loss = 0.26027547\n",
      "Iteration 10, loss = 0.24961417\n",
      "Iteration 11, loss = 0.23999971\n",
      "Iteration 12, loss = 0.23091810\n",
      "Iteration 13, loss = 0.22274213\n",
      "Iteration 14, loss = 0.21514701\n",
      "Iteration 15, loss = 0.20887871\n",
      "Iteration 16, loss = 0.20223150\n",
      "Iteration 17, loss = 0.19659638\n",
      "Iteration 18, loss = 0.19101930\n",
      "Iteration 19, loss = 0.18575565\n",
      "Iteration 20, loss = 0.18110986\n",
      "Iteration 21, loss = 0.17676190\n",
      "Iteration 22, loss = 0.17234626\n",
      "Iteration 23, loss = 0.16865785\n",
      "Iteration 24, loss = 0.16474922\n",
      "Iteration 25, loss = 0.16108903\n",
      "Iteration 26, loss = 0.15794929\n",
      "Iteration 27, loss = 0.15446264\n",
      "Iteration 28, loss = 0.15161009\n",
      "Iteration 29, loss = 0.14900018\n",
      "Iteration 30, loss = 0.14609117\n",
      "Iteration 31, loss = 0.14369069\n",
      "Iteration 32, loss = 0.14122227\n",
      "Iteration 33, loss = 0.13896250\n",
      "Iteration 34, loss = 0.13659667\n",
      "Iteration 35, loss = 0.13436711\n",
      "Iteration 36, loss = 0.13250769\n",
      "Iteration 37, loss = 0.13031352\n",
      "Iteration 38, loss = 0.12843994\n",
      "Iteration 39, loss = 0.12696024\n",
      "Iteration 40, loss = 0.12507869\n",
      "Iteration 41, loss = 0.12359642\n",
      "Iteration 42, loss = 0.12197031\n",
      "Iteration 43, loss = 0.12026491\n",
      "Iteration 44, loss = 0.11880021\n",
      "Iteration 45, loss = 0.11758193\n",
      "Iteration 46, loss = 0.11630146\n",
      "Iteration 47, loss = 0.11498906\n",
      "Iteration 48, loss = 0.11369220\n",
      "Iteration 49, loss = 0.11261781\n",
      "Iteration 50, loss = 0.11159789\n",
      "Iteration 51, loss = 0.11007901\n",
      "Iteration 52, loss = 0.10915785\n",
      "Iteration 53, loss = 0.10822575\n",
      "Iteration 54, loss = 0.10721859\n",
      "Iteration 55, loss = 0.10619058\n",
      "Iteration 56, loss = 0.10541988\n",
      "Iteration 57, loss = 0.10451691\n",
      "Iteration 58, loss = 0.10375974\n",
      "Iteration 59, loss = 0.10287904\n",
      "Iteration 60, loss = 0.10213981\n",
      "Iteration 61, loss = 0.10138774\n",
      "Iteration 62, loss = 0.10046054\n",
      "Iteration 63, loss = 0.09993022\n",
      "Iteration 64, loss = 0.09921897\n",
      "Iteration 65, loss = 0.09872529\n",
      "Iteration 66, loss = 0.09780875\n",
      "Iteration 67, loss = 0.09718604\n",
      "Iteration 68, loss = 0.09672739\n",
      "Iteration 69, loss = 0.09597189\n",
      "Iteration 70, loss = 0.09539279\n",
      "Iteration 71, loss = 0.09498680\n",
      "Iteration 72, loss = 0.09431263\n",
      "Iteration 73, loss = 0.09380969\n",
      "Iteration 74, loss = 0.09332386\n",
      "Iteration 75, loss = 0.09282779\n",
      "Iteration 76, loss = 0.09233518\n",
      "Iteration 77, loss = 0.09181586\n",
      "Iteration 78, loss = 0.09148051\n",
      "Iteration 79, loss = 0.09094616\n",
      "Iteration 80, loss = 0.09057541\n",
      "Iteration 81, loss = 0.09025202\n",
      "Iteration 82, loss = 0.08976541\n",
      "Iteration 83, loss = 0.08938280\n",
      "Iteration 84, loss = 0.08893974\n",
      "Iteration 85, loss = 0.08862906\n",
      "Iteration 86, loss = 0.08818727\n",
      "Iteration 87, loss = 0.08787116\n",
      "Iteration 88, loss = 0.08750003\n",
      "Iteration 89, loss = 0.08717517\n",
      "Iteration 90, loss = 0.08683087\n",
      "Iteration 91, loss = 0.08657442\n",
      "Iteration 92, loss = 0.08622651\n",
      "Iteration 93, loss = 0.08591595\n",
      "Iteration 94, loss = 0.08563411\n",
      "Iteration 95, loss = 0.08532412\n",
      "Iteration 96, loss = 0.08496658\n",
      "Iteration 97, loss = 0.08470524\n",
      "Iteration 98, loss = 0.08445017\n",
      "Iteration 99, loss = 0.08420039\n",
      "Iteration 100, loss = 0.08388985\n",
      "Iteration 101, loss = 0.08366319\n",
      "Iteration 102, loss = 0.08341536\n",
      "Iteration 103, loss = 0.08317166\n",
      "Iteration 104, loss = 0.08286785\n",
      "Iteration 105, loss = 0.08266759\n",
      "Iteration 106, loss = 0.08241170\n",
      "Iteration 107, loss = 0.08219502\n",
      "Iteration 108, loss = 0.08195635\n",
      "Iteration 109, loss = 0.08168023\n",
      "Iteration 110, loss = 0.08151747\n",
      "Iteration 111, loss = 0.08125872\n",
      "Iteration 112, loss = 0.08104099\n",
      "Iteration 113, loss = 0.08089728\n",
      "Iteration 114, loss = 0.08065082\n",
      "Iteration 115, loss = 0.08039325\n",
      "Iteration 116, loss = 0.08023384\n",
      "Iteration 117, loss = 0.08001020\n",
      "Iteration 118, loss = 0.07981351\n",
      "Iteration 119, loss = 0.07963591\n",
      "Iteration 120, loss = 0.07944620\n",
      "Iteration 121, loss = 0.07919990\n",
      "Iteration 122, loss = 0.07904486\n",
      "Iteration 123, loss = 0.07885611\n",
      "Iteration 124, loss = 0.07871581\n",
      "Iteration 125, loss = 0.07851495\n",
      "Iteration 126, loss = 0.07831407\n",
      "Iteration 127, loss = 0.07807968\n",
      "Iteration 128, loss = 0.07798174\n",
      "Iteration 129, loss = 0.07780213\n",
      "Iteration 130, loss = 0.07760938\n",
      "Iteration 131, loss = 0.07741296\n",
      "Iteration 132, loss = 0.07730197\n",
      "Iteration 133, loss = 0.07708864\n",
      "Iteration 134, loss = 0.07691024\n",
      "Iteration 135, loss = 0.07682817\n",
      "Iteration 136, loss = 0.07661082\n",
      "Iteration 137, loss = 0.07646176\n",
      "Iteration 138, loss = 0.07630873\n",
      "Iteration 139, loss = 0.07613224\n",
      "Iteration 140, loss = 0.07596519\n",
      "Iteration 141, loss = 0.07579263\n",
      "Iteration 142, loss = 0.07564860\n",
      "Iteration 143, loss = 0.07553925\n",
      "Iteration 144, loss = 0.07532928\n",
      "Iteration 145, loss = 0.07521013\n",
      "Iteration 146, loss = 0.07504681\n",
      "Iteration 147, loss = 0.07489735\n",
      "Iteration 148, loss = 0.07474842\n",
      "Iteration 149, loss = 0.07461509\n",
      "Iteration 150, loss = 0.07445277\n",
      "Iteration 151, loss = 0.07430475\n",
      "Iteration 152, loss = 0.07416516\n",
      "Iteration 153, loss = 0.07399960\n",
      "Iteration 154, loss = 0.07388330\n",
      "Iteration 155, loss = 0.07374271\n",
      "Iteration 156, loss = 0.07360712\n",
      "Iteration 157, loss = 0.07347877\n",
      "Iteration 158, loss = 0.07333908\n",
      "Iteration 159, loss = 0.07319044\n",
      "Iteration 160, loss = 0.07305441\n",
      "Iteration 161, loss = 0.07292552\n",
      "Iteration 162, loss = 0.07277960\n",
      "Iteration 163, loss = 0.07268007\n",
      "Iteration 164, loss = 0.07251020\n",
      "Iteration 165, loss = 0.07238624\n",
      "Iteration 166, loss = 0.07226267\n",
      "Iteration 167, loss = 0.07214033\n",
      "Iteration 168, loss = 0.07198196\n",
      "Iteration 169, loss = 0.07185786\n",
      "Iteration 170, loss = 0.07172370\n",
      "Iteration 171, loss = 0.07160422\n",
      "Iteration 172, loss = 0.07147833\n",
      "Iteration 173, loss = 0.07133554\n",
      "Iteration 174, loss = 0.07123018\n",
      "Iteration 175, loss = 0.07108986\n",
      "Iteration 176, loss = 0.07095531\n",
      "Iteration 177, loss = 0.07083456\n",
      "Iteration 178, loss = 0.07072327\n",
      "Iteration 179, loss = 0.07060785\n",
      "Iteration 180, loss = 0.07047370\n",
      "Iteration 181, loss = 0.07032561\n",
      "Iteration 182, loss = 0.07022842\n",
      "Iteration 183, loss = 0.07011317\n",
      "Iteration 184, loss = 0.06997507\n",
      "Iteration 185, loss = 0.06987802\n",
      "Iteration 186, loss = 0.06974632\n",
      "Iteration 187, loss = 0.06961298\n",
      "Iteration 188, loss = 0.06950222\n",
      "Iteration 189, loss = 0.06938724\n",
      "Iteration 190, loss = 0.06927654\n",
      "Iteration 191, loss = 0.06915123\n",
      "Iteration 192, loss = 0.06901322\n",
      "Iteration 193, loss = 0.06891276\n",
      "Iteration 194, loss = 0.06880100\n",
      "Iteration 195, loss = 0.06869551\n",
      "Iteration 196, loss = 0.06856650\n",
      "Iteration 197, loss = 0.06845005\n",
      "Iteration 198, loss = 0.06832030\n",
      "Iteration 199, loss = 0.06821215\n",
      "Iteration 200, loss = 0.06810887\n",
      "[CV] END activation=relu, alpha=0.01, batch_size=50, hidden_layer_sizes=(400, 100), solver=sgd; total time=10.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.03399723\n",
      "Iteration 2, loss = 0.46203494\n",
      "Iteration 3, loss = 0.38611813\n",
      "Iteration 4, loss = 0.34911576\n",
      "Iteration 5, loss = 0.32396754\n",
      "Iteration 6, loss = 0.30394639\n",
      "Iteration 7, loss = 0.28740541\n",
      "Iteration 8, loss = 0.27318727\n",
      "Iteration 9, loss = 0.26074825\n",
      "Iteration 10, loss = 0.24989496\n",
      "Iteration 11, loss = 0.24046403\n",
      "Iteration 12, loss = 0.23110415\n",
      "Iteration 13, loss = 0.22286595\n",
      "Iteration 14, loss = 0.21563900\n",
      "Iteration 15, loss = 0.20883624\n",
      "Iteration 16, loss = 0.20258878\n",
      "Iteration 17, loss = 0.19692071\n",
      "Iteration 18, loss = 0.19157557\n",
      "Iteration 19, loss = 0.18620899\n",
      "Iteration 20, loss = 0.18196229\n",
      "Iteration 21, loss = 0.17742488\n",
      "Iteration 22, loss = 0.17336729\n",
      "Iteration 23, loss = 0.16941788\n",
      "Iteration 24, loss = 0.16545124\n",
      "Iteration 25, loss = 0.16222802\n",
      "Iteration 26, loss = 0.15867699\n",
      "Iteration 27, loss = 0.15591043\n",
      "Iteration 28, loss = 0.15254547\n",
      "Iteration 29, loss = 0.15019117\n",
      "Iteration 30, loss = 0.14737774\n",
      "Iteration 31, loss = 0.14492878\n",
      "Iteration 32, loss = 0.14218649\n",
      "Iteration 33, loss = 0.13987348\n",
      "Iteration 34, loss = 0.13764123\n",
      "Iteration 35, loss = 0.13558201\n",
      "Iteration 36, loss = 0.13357851\n",
      "Iteration 37, loss = 0.13176561\n",
      "Iteration 38, loss = 0.12994154\n",
      "Iteration 39, loss = 0.12787705\n",
      "Iteration 40, loss = 0.12633454\n",
      "Iteration 41, loss = 0.12464247\n",
      "Iteration 42, loss = 0.12299209\n",
      "Iteration 43, loss = 0.12158276\n",
      "Iteration 44, loss = 0.11996328\n",
      "Iteration 45, loss = 0.11877225\n",
      "Iteration 46, loss = 0.11735953\n",
      "Iteration 47, loss = 0.11612366\n",
      "Iteration 48, loss = 0.11470458\n",
      "Iteration 49, loss = 0.11358530\n",
      "Iteration 50, loss = 0.11236440\n",
      "Iteration 51, loss = 0.11131745\n",
      "Iteration 52, loss = 0.11043002\n",
      "Iteration 53, loss = 0.10923413\n",
      "Iteration 54, loss = 0.10809258\n",
      "Iteration 55, loss = 0.10721884\n",
      "Iteration 56, loss = 0.10621770\n",
      "Iteration 57, loss = 0.10525633\n",
      "Iteration 58, loss = 0.10430186\n",
      "Iteration 59, loss = 0.10346102\n",
      "Iteration 60, loss = 0.10270184\n",
      "Iteration 61, loss = 0.10198806\n",
      "Iteration 62, loss = 0.10126108\n",
      "Iteration 63, loss = 0.10054656\n",
      "Iteration 64, loss = 0.09985363\n",
      "Iteration 65, loss = 0.09892511\n",
      "Iteration 66, loss = 0.09839684\n",
      "Iteration 67, loss = 0.09777627\n",
      "Iteration 68, loss = 0.09713848\n",
      "Iteration 69, loss = 0.09651829\n",
      "Iteration 70, loss = 0.09604261\n",
      "Iteration 71, loss = 0.09541188\n",
      "Iteration 72, loss = 0.09481315\n",
      "Iteration 73, loss = 0.09421343\n",
      "Iteration 74, loss = 0.09384451\n",
      "Iteration 75, loss = 0.09322855\n",
      "Iteration 76, loss = 0.09270512\n",
      "Iteration 77, loss = 0.09228228\n",
      "Iteration 78, loss = 0.09181903\n",
      "Iteration 79, loss = 0.09139471\n",
      "Iteration 80, loss = 0.09096933\n",
      "Iteration 81, loss = 0.09058717\n",
      "Iteration 82, loss = 0.09018589\n",
      "Iteration 83, loss = 0.08971708\n",
      "Iteration 84, loss = 0.08931518\n",
      "Iteration 85, loss = 0.08893979\n",
      "Iteration 86, loss = 0.08855678\n",
      "Iteration 87, loss = 0.08815717\n",
      "Iteration 88, loss = 0.08786412\n",
      "Iteration 89, loss = 0.08748898\n",
      "Iteration 90, loss = 0.08715746\n",
      "Iteration 91, loss = 0.08681684\n",
      "Iteration 92, loss = 0.08653349\n",
      "Iteration 93, loss = 0.08623254\n",
      "Iteration 94, loss = 0.08582129\n",
      "Iteration 95, loss = 0.08552062\n",
      "Iteration 96, loss = 0.08529436\n",
      "Iteration 97, loss = 0.08499313\n",
      "Iteration 98, loss = 0.08473507\n",
      "Iteration 99, loss = 0.08441433\n",
      "Iteration 100, loss = 0.08415618\n",
      "Iteration 101, loss = 0.08384339\n",
      "Iteration 102, loss = 0.08364033\n",
      "Iteration 103, loss = 0.08332039\n",
      "Iteration 104, loss = 0.08315338\n",
      "Iteration 105, loss = 0.08289472\n",
      "Iteration 106, loss = 0.08264921\n",
      "Iteration 107, loss = 0.08241458\n",
      "Iteration 108, loss = 0.08220350\n",
      "Iteration 109, loss = 0.08190221\n",
      "Iteration 110, loss = 0.08168289\n",
      "Iteration 111, loss = 0.08142792\n",
      "Iteration 112, loss = 0.08124094\n",
      "Iteration 113, loss = 0.08099456\n",
      "Iteration 114, loss = 0.08079555\n",
      "Iteration 115, loss = 0.08060012\n",
      "Iteration 116, loss = 0.08037342\n",
      "Iteration 117, loss = 0.08019019\n",
      "Iteration 118, loss = 0.07994138\n",
      "Iteration 119, loss = 0.07974321\n",
      "Iteration 120, loss = 0.07957226\n",
      "Iteration 121, loss = 0.07938665\n",
      "Iteration 122, loss = 0.07916939\n",
      "Iteration 123, loss = 0.07901644\n",
      "Iteration 124, loss = 0.07880675\n",
      "Iteration 125, loss = 0.07861247\n",
      "Iteration 126, loss = 0.07845967\n",
      "Iteration 127, loss = 0.07825395\n",
      "Iteration 128, loss = 0.07807381\n",
      "Iteration 129, loss = 0.07790952\n",
      "Iteration 130, loss = 0.07771098\n",
      "Iteration 131, loss = 0.07756376\n",
      "Iteration 132, loss = 0.07737121\n",
      "Iteration 133, loss = 0.07722804\n",
      "Iteration 134, loss = 0.07705174\n",
      "Iteration 135, loss = 0.07687440\n",
      "Iteration 136, loss = 0.07668104\n",
      "Iteration 137, loss = 0.07653914\n",
      "Iteration 138, loss = 0.07638075\n",
      "Iteration 139, loss = 0.07622226\n",
      "Iteration 140, loss = 0.07605202\n",
      "Iteration 141, loss = 0.07592247\n",
      "Iteration 142, loss = 0.07573965\n",
      "Iteration 143, loss = 0.07559472\n",
      "Iteration 144, loss = 0.07543554\n",
      "Iteration 145, loss = 0.07525635\n",
      "Iteration 146, loss = 0.07510075\n",
      "Iteration 147, loss = 0.07498113\n",
      "Iteration 148, loss = 0.07482182\n",
      "Iteration 149, loss = 0.07467229\n",
      "Iteration 150, loss = 0.07450905\n",
      "Iteration 151, loss = 0.07437035\n",
      "Iteration 152, loss = 0.07422671\n",
      "Iteration 153, loss = 0.07408205\n",
      "Iteration 154, loss = 0.07392336\n",
      "Iteration 155, loss = 0.07381237\n",
      "Iteration 156, loss = 0.07366202\n",
      "Iteration 157, loss = 0.07352098\n",
      "Iteration 158, loss = 0.07339113\n",
      "Iteration 159, loss = 0.07324289\n",
      "Iteration 160, loss = 0.07309599\n",
      "Iteration 161, loss = 0.07296495\n",
      "Iteration 162, loss = 0.07281525\n",
      "Iteration 163, loss = 0.07266741\n",
      "Iteration 164, loss = 0.07255103\n",
      "Iteration 165, loss = 0.07242694\n",
      "Iteration 166, loss = 0.07226733\n",
      "Iteration 167, loss = 0.07215012\n",
      "Iteration 168, loss = 0.07201999\n",
      "Iteration 169, loss = 0.07187605\n",
      "Iteration 170, loss = 0.07176415\n",
      "Iteration 171, loss = 0.07161707\n",
      "Iteration 172, loss = 0.07150633\n",
      "Iteration 173, loss = 0.07136299\n",
      "Iteration 174, loss = 0.07123751\n",
      "Iteration 175, loss = 0.07111170\n",
      "Iteration 176, loss = 0.07098658\n",
      "Iteration 177, loss = 0.07086031\n",
      "Iteration 178, loss = 0.07073070\n",
      "Iteration 179, loss = 0.07061119\n",
      "Iteration 180, loss = 0.07048865\n",
      "Iteration 181, loss = 0.07035828\n",
      "Iteration 182, loss = 0.07023986\n",
      "Iteration 183, loss = 0.07010908\n",
      "Iteration 184, loss = 0.06997602\n",
      "Iteration 185, loss = 0.06987574\n",
      "Iteration 186, loss = 0.06974616\n",
      "Iteration 187, loss = 0.06963323\n",
      "Iteration 188, loss = 0.06952119\n",
      "Iteration 189, loss = 0.06939122\n",
      "Iteration 190, loss = 0.06925435\n",
      "Iteration 191, loss = 0.06915099\n",
      "Iteration 192, loss = 0.06903283\n",
      "Iteration 193, loss = 0.06892120\n",
      "Iteration 194, loss = 0.06880102\n",
      "Iteration 195, loss = 0.06868702\n",
      "Iteration 196, loss = 0.06856357\n",
      "Iteration 197, loss = 0.06844835\n",
      "Iteration 198, loss = 0.06832604\n",
      "Iteration 199, loss = 0.06820567\n",
      "Iteration 200, loss = 0.06811142\n",
      "[CV] END activation=relu, alpha=0.01, batch_size=50, hidden_layer_sizes=(400, 100), solver=sgd; total time=10.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.02004018\n",
      "Iteration 2, loss = 0.45992449\n",
      "Iteration 3, loss = 0.38826755\n",
      "Iteration 4, loss = 0.35163173\n",
      "Iteration 5, loss = 0.32559270\n",
      "Iteration 6, loss = 0.30587765\n",
      "Iteration 7, loss = 0.28961687\n",
      "Iteration 8, loss = 0.27482772\n",
      "Iteration 9, loss = 0.26253344\n",
      "Iteration 10, loss = 0.25140826\n",
      "Iteration 11, loss = 0.24112916\n",
      "Iteration 12, loss = 0.23198511\n",
      "Iteration 13, loss = 0.22355286\n",
      "Iteration 14, loss = 0.21628154\n",
      "Iteration 15, loss = 0.20940989\n",
      "Iteration 16, loss = 0.20290062\n",
      "Iteration 17, loss = 0.19666846\n",
      "Iteration 18, loss = 0.19143507\n",
      "Iteration 19, loss = 0.18627513\n",
      "Iteration 20, loss = 0.18141105\n",
      "Iteration 21, loss = 0.17682624\n",
      "Iteration 22, loss = 0.17240317\n",
      "Iteration 23, loss = 0.16854521\n",
      "Iteration 24, loss = 0.16493214\n",
      "Iteration 25, loss = 0.16099572\n",
      "Iteration 26, loss = 0.15803669\n",
      "Iteration 27, loss = 0.15479125\n",
      "Iteration 28, loss = 0.15189955\n",
      "Iteration 29, loss = 0.14856480\n",
      "Iteration 30, loss = 0.14611762\n",
      "Iteration 31, loss = 0.14343522\n",
      "Iteration 32, loss = 0.14101404\n",
      "Iteration 33, loss = 0.13878539\n",
      "Iteration 34, loss = 0.13645648\n",
      "Iteration 35, loss = 0.13438667\n",
      "Iteration 36, loss = 0.13248918\n",
      "Iteration 37, loss = 0.13032183\n",
      "Iteration 38, loss = 0.12853757\n",
      "Iteration 39, loss = 0.12691711\n",
      "Iteration 40, loss = 0.12500601\n",
      "Iteration 41, loss = 0.12357299\n",
      "Iteration 42, loss = 0.12180856\n",
      "Iteration 43, loss = 0.12030687\n",
      "Iteration 44, loss = 0.11894463\n",
      "Iteration 45, loss = 0.11758257\n",
      "Iteration 46, loss = 0.11616479\n",
      "Iteration 47, loss = 0.11472966\n",
      "Iteration 48, loss = 0.11367163\n",
      "Iteration 49, loss = 0.11236205\n",
      "Iteration 50, loss = 0.11135551\n",
      "Iteration 51, loss = 0.11024787\n",
      "Iteration 52, loss = 0.10912945\n",
      "Iteration 53, loss = 0.10811398\n",
      "Iteration 54, loss = 0.10724278\n",
      "Iteration 55, loss = 0.10633419\n",
      "Iteration 56, loss = 0.10533572\n",
      "Iteration 57, loss = 0.10445843\n",
      "Iteration 58, loss = 0.10349400\n",
      "Iteration 59, loss = 0.10273532\n",
      "Iteration 60, loss = 0.10190236\n",
      "Iteration 61, loss = 0.10104102\n",
      "Iteration 62, loss = 0.10049428\n",
      "Iteration 63, loss = 0.09963632\n",
      "Iteration 64, loss = 0.09891954\n",
      "Iteration 65, loss = 0.09842403\n",
      "Iteration 66, loss = 0.09752624\n",
      "Iteration 67, loss = 0.09696361\n",
      "Iteration 68, loss = 0.09642001\n",
      "Iteration 69, loss = 0.09589381\n",
      "Iteration 70, loss = 0.09517749\n",
      "Iteration 71, loss = 0.09463518\n",
      "Iteration 72, loss = 0.09403729\n",
      "Iteration 73, loss = 0.09348072\n",
      "Iteration 74, loss = 0.09303987\n",
      "Iteration 75, loss = 0.09253102\n",
      "Iteration 76, loss = 0.09209801\n",
      "Iteration 77, loss = 0.09167771\n",
      "Iteration 78, loss = 0.09115961\n",
      "Iteration 79, loss = 0.09079649\n",
      "Iteration 80, loss = 0.09032645\n",
      "Iteration 81, loss = 0.08985675\n",
      "Iteration 82, loss = 0.08944614\n",
      "Iteration 83, loss = 0.08907918\n",
      "Iteration 84, loss = 0.08869507\n",
      "Iteration 85, loss = 0.08834862\n",
      "Iteration 86, loss = 0.08799859\n",
      "Iteration 87, loss = 0.08764866\n",
      "Iteration 88, loss = 0.08727663\n",
      "Iteration 89, loss = 0.08689315\n",
      "Iteration 90, loss = 0.08663204\n",
      "Iteration 91, loss = 0.08625646\n",
      "Iteration 92, loss = 0.08596077\n",
      "Iteration 93, loss = 0.08566042\n",
      "Iteration 94, loss = 0.08530550\n",
      "Iteration 95, loss = 0.08504061\n",
      "Iteration 96, loss = 0.08475242\n",
      "Iteration 97, loss = 0.08449863\n",
      "Iteration 98, loss = 0.08420961\n",
      "Iteration 99, loss = 0.08389932\n",
      "Iteration 100, loss = 0.08368749\n",
      "Iteration 101, loss = 0.08338680\n",
      "Iteration 102, loss = 0.08314285\n",
      "Iteration 103, loss = 0.08287988\n",
      "Iteration 104, loss = 0.08266734\n",
      "Iteration 105, loss = 0.08235196\n",
      "Iteration 106, loss = 0.08216594\n",
      "Iteration 107, loss = 0.08188915\n",
      "Iteration 108, loss = 0.08170033\n",
      "Iteration 109, loss = 0.08149442\n",
      "Iteration 110, loss = 0.08125387\n",
      "Iteration 111, loss = 0.08106176\n",
      "Iteration 112, loss = 0.08084005\n",
      "Iteration 113, loss = 0.08060230\n",
      "Iteration 114, loss = 0.08040920\n",
      "Iteration 115, loss = 0.08015405\n",
      "Iteration 116, loss = 0.07999472\n",
      "Iteration 117, loss = 0.07981255\n",
      "Iteration 118, loss = 0.07957252\n",
      "Iteration 119, loss = 0.07939466\n",
      "Iteration 120, loss = 0.07920828\n",
      "Iteration 121, loss = 0.07898994\n",
      "Iteration 122, loss = 0.07881945\n",
      "Iteration 123, loss = 0.07862553\n",
      "Iteration 124, loss = 0.07843259\n",
      "Iteration 125, loss = 0.07825535\n",
      "Iteration 126, loss = 0.07808759\n",
      "Iteration 127, loss = 0.07789288\n",
      "Iteration 128, loss = 0.07773823\n",
      "Iteration 129, loss = 0.07753660\n",
      "Iteration 130, loss = 0.07737686\n",
      "Iteration 131, loss = 0.07721140\n",
      "Iteration 132, loss = 0.07705087\n",
      "Iteration 133, loss = 0.07683559\n",
      "Iteration 134, loss = 0.07668893\n",
      "Iteration 135, loss = 0.07654392\n",
      "Iteration 136, loss = 0.07635497\n",
      "Iteration 137, loss = 0.07620012\n",
      "Iteration 138, loss = 0.07609129\n",
      "Iteration 139, loss = 0.07591985\n",
      "Iteration 140, loss = 0.07573942\n",
      "Iteration 141, loss = 0.07559145\n",
      "Iteration 142, loss = 0.07544306\n",
      "Iteration 143, loss = 0.07529286\n",
      "Iteration 144, loss = 0.07512457\n",
      "Iteration 145, loss = 0.07496516\n",
      "Iteration 146, loss = 0.07485146\n",
      "Iteration 147, loss = 0.07470186\n",
      "Iteration 148, loss = 0.07452424\n",
      "Iteration 149, loss = 0.07438691\n",
      "Iteration 150, loss = 0.07422729\n",
      "Iteration 151, loss = 0.07409541\n",
      "Iteration 152, loss = 0.07395626\n",
      "Iteration 153, loss = 0.07382129\n",
      "Iteration 154, loss = 0.07365969\n",
      "Iteration 155, loss = 0.07353267\n",
      "Iteration 156, loss = 0.07339177\n",
      "Iteration 157, loss = 0.07325800\n",
      "Iteration 158, loss = 0.07310624\n",
      "Iteration 159, loss = 0.07296805\n",
      "Iteration 160, loss = 0.07283206\n",
      "Iteration 161, loss = 0.07270121\n",
      "Iteration 162, loss = 0.07256878\n",
      "Iteration 163, loss = 0.07244618\n",
      "Iteration 164, loss = 0.07229802\n",
      "Iteration 165, loss = 0.07216530\n",
      "Iteration 166, loss = 0.07204200\n",
      "Iteration 167, loss = 0.07191541\n",
      "Iteration 168, loss = 0.07175833\n",
      "Iteration 169, loss = 0.07165947\n",
      "Iteration 170, loss = 0.07151709\n",
      "Iteration 171, loss = 0.07138247\n",
      "Iteration 172, loss = 0.07124438\n",
      "Iteration 173, loss = 0.07114249\n",
      "Iteration 174, loss = 0.07100283\n",
      "Iteration 175, loss = 0.07088002\n",
      "Iteration 176, loss = 0.07075116\n",
      "Iteration 177, loss = 0.07063865\n",
      "Iteration 178, loss = 0.07051284\n",
      "Iteration 179, loss = 0.07038302\n",
      "Iteration 180, loss = 0.07026400\n",
      "Iteration 181, loss = 0.07014582\n",
      "Iteration 182, loss = 0.07002013\n",
      "Iteration 183, loss = 0.06990758\n",
      "Iteration 184, loss = 0.06977610\n",
      "Iteration 185, loss = 0.06965077\n",
      "Iteration 186, loss = 0.06953168\n",
      "Iteration 187, loss = 0.06941241\n",
      "Iteration 188, loss = 0.06929744\n",
      "Iteration 189, loss = 0.06918645\n",
      "Iteration 190, loss = 0.06905949\n",
      "Iteration 191, loss = 0.06894092\n",
      "Iteration 192, loss = 0.06880585\n",
      "Iteration 193, loss = 0.06869583\n",
      "Iteration 194, loss = 0.06859641\n",
      "Iteration 195, loss = 0.06847301\n",
      "Iteration 196, loss = 0.06834303\n",
      "Iteration 197, loss = 0.06823463\n",
      "Iteration 198, loss = 0.06814183\n",
      "Iteration 199, loss = 0.06800958\n",
      "Iteration 200, loss = 0.06791357\n",
      "[CV] END activation=relu, alpha=0.01, batch_size=50, hidden_layer_sizes=(400, 100), solver=sgd; total time=10.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.04182765\n",
      "Iteration 2, loss = 0.46548761\n",
      "Iteration 3, loss = 0.39557013\n",
      "Iteration 4, loss = 0.35923941\n",
      "Iteration 5, loss = 0.33347540\n",
      "Iteration 6, loss = 0.31281702\n",
      "Iteration 7, loss = 0.29637334\n",
      "Iteration 8, loss = 0.28191928\n",
      "Iteration 9, loss = 0.26911323\n",
      "Iteration 10, loss = 0.25763708\n",
      "Iteration 11, loss = 0.24732897\n",
      "Iteration 12, loss = 0.23780932\n",
      "Iteration 13, loss = 0.22955586\n",
      "Iteration 14, loss = 0.22132039\n",
      "Iteration 15, loss = 0.21435613\n",
      "Iteration 16, loss = 0.20755723\n",
      "Iteration 17, loss = 0.20164930\n",
      "Iteration 18, loss = 0.19594846\n",
      "Iteration 19, loss = 0.19065700\n",
      "Iteration 20, loss = 0.18571444\n",
      "Iteration 21, loss = 0.18088594\n",
      "Iteration 22, loss = 0.17635916\n",
      "Iteration 23, loss = 0.17240912\n",
      "Iteration 24, loss = 0.16837806\n",
      "Iteration 25, loss = 0.16508661\n",
      "Iteration 26, loss = 0.16129794\n",
      "Iteration 27, loss = 0.15809276\n",
      "Iteration 28, loss = 0.15506279\n",
      "Iteration 29, loss = 0.15195538\n",
      "Iteration 30, loss = 0.14914647\n",
      "Iteration 31, loss = 0.14660593\n",
      "Iteration 32, loss = 0.14412028\n",
      "Iteration 33, loss = 0.14137291\n",
      "Iteration 34, loss = 0.13931557\n",
      "Iteration 35, loss = 0.13717361\n",
      "Iteration 36, loss = 0.13492390\n",
      "Iteration 37, loss = 0.13310036\n",
      "Iteration 38, loss = 0.13088642\n",
      "Iteration 39, loss = 0.12937664\n",
      "Iteration 40, loss = 0.12744718\n",
      "Iteration 41, loss = 0.12591791\n",
      "Iteration 42, loss = 0.12397025\n",
      "Iteration 43, loss = 0.12261440\n",
      "Iteration 44, loss = 0.12107766\n",
      "Iteration 45, loss = 0.11960620\n",
      "Iteration 46, loss = 0.11808262\n",
      "Iteration 47, loss = 0.11698804\n",
      "Iteration 48, loss = 0.11559659\n",
      "Iteration 49, loss = 0.11441445\n",
      "Iteration 50, loss = 0.11322771\n",
      "Iteration 51, loss = 0.11206094\n",
      "Iteration 52, loss = 0.11089867\n",
      "Iteration 53, loss = 0.10977992\n",
      "Iteration 54, loss = 0.10884742\n",
      "Iteration 55, loss = 0.10790899\n",
      "Iteration 56, loss = 0.10673601\n",
      "Iteration 57, loss = 0.10591146\n",
      "Iteration 58, loss = 0.10503383\n",
      "Iteration 59, loss = 0.10430420\n",
      "Iteration 60, loss = 0.10327579\n",
      "Iteration 61, loss = 0.10246284\n",
      "Iteration 62, loss = 0.10171531\n",
      "Iteration 63, loss = 0.10102723\n",
      "Iteration 64, loss = 0.10029651\n",
      "Iteration 65, loss = 0.09970624\n",
      "Iteration 66, loss = 0.09903282\n",
      "Iteration 67, loss = 0.09832080\n",
      "Iteration 68, loss = 0.09772571\n",
      "Iteration 69, loss = 0.09710901\n",
      "Iteration 70, loss = 0.09641363\n",
      "Iteration 71, loss = 0.09594128\n",
      "Iteration 72, loss = 0.09527332\n",
      "Iteration 73, loss = 0.09477917\n",
      "Iteration 74, loss = 0.09427151\n",
      "Iteration 75, loss = 0.09375127\n",
      "Iteration 76, loss = 0.09325502\n",
      "Iteration 77, loss = 0.09270933\n",
      "Iteration 78, loss = 0.09230527\n",
      "Iteration 79, loss = 0.09190334\n",
      "Iteration 80, loss = 0.09146024\n",
      "Iteration 81, loss = 0.09094930\n",
      "Iteration 82, loss = 0.09056749\n",
      "Iteration 83, loss = 0.09024813\n",
      "Iteration 84, loss = 0.08966954\n",
      "Iteration 85, loss = 0.08933439\n",
      "Iteration 86, loss = 0.08897182\n",
      "Iteration 87, loss = 0.08859913\n",
      "Iteration 88, loss = 0.08821799\n",
      "Iteration 89, loss = 0.08789625\n",
      "Iteration 90, loss = 0.08755082\n",
      "Iteration 91, loss = 0.08719682\n",
      "Iteration 92, loss = 0.08689706\n",
      "Iteration 93, loss = 0.08655232\n",
      "Iteration 94, loss = 0.08626297\n",
      "Iteration 95, loss = 0.08596158\n",
      "Iteration 96, loss = 0.08570797\n",
      "Iteration 97, loss = 0.08532015\n",
      "Iteration 98, loss = 0.08508845\n",
      "Iteration 99, loss = 0.08476033\n",
      "Iteration 100, loss = 0.08450425\n",
      "Iteration 101, loss = 0.08424475\n",
      "Iteration 102, loss = 0.08398247\n",
      "Iteration 103, loss = 0.08366703\n",
      "Iteration 104, loss = 0.08340547\n",
      "Iteration 105, loss = 0.08317233\n",
      "Iteration 106, loss = 0.08296279\n",
      "Iteration 107, loss = 0.08270942\n",
      "Iteration 108, loss = 0.08244342\n",
      "Iteration 109, loss = 0.08221325\n",
      "Iteration 110, loss = 0.08199759\n",
      "Iteration 111, loss = 0.08180023\n",
      "Iteration 112, loss = 0.08157700\n",
      "Iteration 113, loss = 0.08132281\n",
      "Iteration 114, loss = 0.08111413\n",
      "Iteration 115, loss = 0.08088378\n",
      "Iteration 116, loss = 0.08069289\n",
      "Iteration 117, loss = 0.08045516\n",
      "Iteration 118, loss = 0.08026273\n",
      "Iteration 119, loss = 0.08005873\n",
      "Iteration 120, loss = 0.07984698\n",
      "Iteration 121, loss = 0.07964203\n",
      "Iteration 122, loss = 0.07947196\n",
      "Iteration 123, loss = 0.07927785\n",
      "Iteration 124, loss = 0.07910981\n",
      "Iteration 125, loss = 0.07888161\n",
      "Iteration 126, loss = 0.07871270\n",
      "Iteration 127, loss = 0.07849863\n",
      "Iteration 128, loss = 0.07833297\n",
      "Iteration 129, loss = 0.07817450\n",
      "Iteration 130, loss = 0.07796563\n",
      "Iteration 131, loss = 0.07780963\n",
      "Iteration 132, loss = 0.07759156\n",
      "Iteration 133, loss = 0.07748428\n",
      "Iteration 134, loss = 0.07724152\n",
      "Iteration 135, loss = 0.07712642\n",
      "Iteration 136, loss = 0.07695116\n",
      "Iteration 137, loss = 0.07675849\n",
      "Iteration 138, loss = 0.07663732\n",
      "Iteration 139, loss = 0.07647632\n",
      "Iteration 140, loss = 0.07630124\n",
      "Iteration 141, loss = 0.07611737\n",
      "Iteration 142, loss = 0.07596193\n",
      "Iteration 143, loss = 0.07579765\n",
      "Iteration 144, loss = 0.07567884\n",
      "Iteration 145, loss = 0.07549354\n",
      "Iteration 146, loss = 0.07534310\n",
      "Iteration 147, loss = 0.07520216\n",
      "Iteration 148, loss = 0.07505981\n",
      "Iteration 149, loss = 0.07491179\n",
      "Iteration 150, loss = 0.07474277\n",
      "Iteration 151, loss = 0.07460278\n",
      "Iteration 152, loss = 0.07449160\n",
      "Iteration 153, loss = 0.07430847\n",
      "Iteration 154, loss = 0.07417853\n",
      "Iteration 155, loss = 0.07401350\n",
      "Iteration 156, loss = 0.07391169\n",
      "Iteration 157, loss = 0.07375313\n",
      "Iteration 158, loss = 0.07359852\n",
      "Iteration 159, loss = 0.07348674\n",
      "Iteration 160, loss = 0.07333909\n",
      "Iteration 161, loss = 0.07319824\n",
      "Iteration 162, loss = 0.07306294\n",
      "Iteration 163, loss = 0.07290791\n",
      "Iteration 164, loss = 0.07278401\n",
      "Iteration 165, loss = 0.07265362\n",
      "Iteration 166, loss = 0.07251606\n",
      "Iteration 167, loss = 0.07239867\n",
      "Iteration 168, loss = 0.07225317\n",
      "Iteration 169, loss = 0.07211181\n",
      "Iteration 170, loss = 0.07198851\n",
      "Iteration 171, loss = 0.07184467\n",
      "Iteration 172, loss = 0.07172414\n",
      "Iteration 173, loss = 0.07159819\n",
      "Iteration 174, loss = 0.07147950\n",
      "Iteration 175, loss = 0.07133979\n",
      "Iteration 176, loss = 0.07120266\n",
      "Iteration 177, loss = 0.07109965\n",
      "Iteration 178, loss = 0.07096540\n",
      "Iteration 179, loss = 0.07083380\n",
      "Iteration 180, loss = 0.07070511\n",
      "Iteration 181, loss = 0.07059615\n",
      "Iteration 182, loss = 0.07045582\n",
      "Iteration 183, loss = 0.07033711\n",
      "Iteration 184, loss = 0.07022781\n",
      "Iteration 185, loss = 0.07008264\n",
      "Iteration 186, loss = 0.06996984\n",
      "Iteration 187, loss = 0.06984261\n",
      "Iteration 188, loss = 0.06973746\n",
      "Iteration 189, loss = 0.06961466\n",
      "Iteration 190, loss = 0.06949040\n",
      "Iteration 191, loss = 0.06937524\n",
      "Iteration 192, loss = 0.06923929\n",
      "Iteration 193, loss = 0.06914289\n",
      "Iteration 194, loss = 0.06901989\n",
      "Iteration 195, loss = 0.06891414\n",
      "Iteration 196, loss = 0.06879912\n",
      "Iteration 197, loss = 0.06866957\n",
      "Iteration 198, loss = 0.06856441\n",
      "Iteration 199, loss = 0.06844294\n",
      "Iteration 200, loss = 0.06833955\n",
      "[CV] END activation=relu, alpha=0.01, batch_size=50, hidden_layer_sizes=(400, 100), solver=sgd; total time=10.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.37820025\n",
      "Iteration 2, loss = 0.17853579\n",
      "Iteration 3, loss = 0.14101007\n",
      "Iteration 4, loss = 0.11767850\n",
      "Iteration 5, loss = 0.10494705\n",
      "Iteration 6, loss = 0.09677408\n",
      "Iteration 7, loss = 0.09113272\n",
      "Iteration 8, loss = 0.08542667\n",
      "Iteration 9, loss = 0.08208659\n",
      "Iteration 10, loss = 0.08153679\n",
      "Iteration 11, loss = 0.07614729\n",
      "Iteration 12, loss = 0.07391927\n",
      "Iteration 13, loss = 0.07460960\n",
      "Iteration 14, loss = 0.07252565\n",
      "Iteration 15, loss = 0.07107188\n",
      "Iteration 16, loss = 0.07097634\n",
      "Iteration 17, loss = 0.07067724\n",
      "Iteration 18, loss = 0.07300263\n",
      "Iteration 19, loss = 0.06446331\n",
      "Iteration 20, loss = 0.06419100\n",
      "Iteration 21, loss = 0.06733474\n",
      "Iteration 22, loss = 0.06554085\n",
      "Iteration 23, loss = 0.06245838\n",
      "Iteration 24, loss = 0.06401629\n",
      "Iteration 25, loss = 0.06408995\n",
      "Iteration 26, loss = 0.06160824\n",
      "Iteration 27, loss = 0.06282034\n",
      "Iteration 28, loss = 0.06205759\n",
      "Iteration 29, loss = 0.06026496\n",
      "Iteration 30, loss = 0.06214567\n",
      "Iteration 31, loss = 0.06203435\n",
      "Iteration 32, loss = 0.05726003\n",
      "Iteration 33, loss = 0.05841377\n",
      "Iteration 34, loss = 0.05964608\n",
      "Iteration 35, loss = 0.05928109\n",
      "Iteration 36, loss = 0.05779660\n",
      "Iteration 37, loss = 0.05852827\n",
      "Iteration 38, loss = 0.05393445\n",
      "Iteration 39, loss = 0.05904282\n",
      "Iteration 40, loss = 0.05415256\n",
      "Iteration 41, loss = 0.06087329\n",
      "Iteration 42, loss = 0.05476144\n",
      "Iteration 43, loss = 0.05632370\n",
      "Iteration 44, loss = 0.05487017\n",
      "Iteration 45, loss = 0.06113330\n",
      "Iteration 46, loss = 0.05234973\n",
      "Iteration 47, loss = 0.05776201\n",
      "Iteration 48, loss = 0.05440845\n",
      "Iteration 49, loss = 0.05478834\n",
      "Iteration 50, loss = 0.05453015\n",
      "Iteration 51, loss = 0.05451001\n",
      "Iteration 52, loss = 0.05671288\n",
      "Iteration 53, loss = 0.05131054\n",
      "Iteration 54, loss = 0.05719192\n",
      "Iteration 55, loss = 0.05120783\n",
      "Iteration 56, loss = 0.05013067\n",
      "Iteration 57, loss = 0.05672085\n",
      "Iteration 58, loss = 0.05106438\n",
      "Iteration 59, loss = 0.05413477\n",
      "Iteration 60, loss = 0.05302150\n",
      "Iteration 61, loss = 0.05213346\n",
      "Iteration 62, loss = 0.05107784\n",
      "Iteration 63, loss = 0.05309329\n",
      "Iteration 64, loss = 0.04957035\n",
      "Iteration 65, loss = 0.05424917\n",
      "Iteration 66, loss = 0.04942844\n",
      "Iteration 67, loss = 0.05521971\n",
      "Iteration 68, loss = 0.05497740\n",
      "Iteration 69, loss = 0.04444416\n",
      "Iteration 70, loss = 0.05614099\n",
      "Iteration 71, loss = 0.04987525\n",
      "Iteration 72, loss = 0.04504745\n",
      "Iteration 73, loss = 0.05392921\n",
      "Iteration 74, loss = 0.05063659\n",
      "Iteration 75, loss = 0.04652693\n",
      "Iteration 76, loss = 0.04445062\n",
      "Iteration 77, loss = 0.05948719\n",
      "Iteration 78, loss = 0.04879530\n",
      "Iteration 79, loss = 0.04906739\n",
      "Iteration 80, loss = 0.05053087\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.01, batch_size=50, hidden_layer_sizes=(100, 100, 100), solver=adam; total time= 6.7min\n",
      "Iteration 1, loss = 0.37008657\n",
      "Iteration 2, loss = 0.17231878\n",
      "Iteration 3, loss = 0.13714414\n",
      "Iteration 4, loss = 0.11698835\n",
      "Iteration 5, loss = 0.10482967\n",
      "Iteration 6, loss = 0.09502296\n",
      "Iteration 7, loss = 0.09050471\n",
      "Iteration 8, loss = 0.08821271\n",
      "Iteration 9, loss = 0.08388393\n",
      "Iteration 10, loss = 0.07948057\n",
      "Iteration 11, loss = 0.07511939\n",
      "Iteration 12, loss = 0.07676585\n",
      "Iteration 13, loss = 0.06994374\n",
      "Iteration 14, loss = 0.07049373\n",
      "Iteration 15, loss = 0.07218756\n",
      "Iteration 16, loss = 0.06964846\n",
      "Iteration 17, loss = 0.07044660\n",
      "Iteration 18, loss = 0.06625570\n",
      "Iteration 19, loss = 0.06681975\n",
      "Iteration 20, loss = 0.07018363\n",
      "Iteration 21, loss = 0.06489152\n",
      "Iteration 22, loss = 0.06568976\n",
      "Iteration 23, loss = 0.06320307\n",
      "Iteration 24, loss = 0.06408806\n",
      "Iteration 25, loss = 0.06442817\n",
      "Iteration 26, loss = 0.06129049\n",
      "Iteration 27, loss = 0.05934578\n",
      "Iteration 28, loss = 0.06144023\n",
      "Iteration 29, loss = 0.06144964\n",
      "Iteration 30, loss = 0.06128121\n",
      "Iteration 31, loss = 0.05743674\n",
      "Iteration 32, loss = 0.06392672\n",
      "Iteration 33, loss = 0.05993554\n",
      "Iteration 34, loss = 0.05382200\n",
      "Iteration 35, loss = 0.06207966\n",
      "Iteration 36, loss = 0.05868037\n",
      "Iteration 37, loss = 0.05477801\n",
      "Iteration 38, loss = 0.05826419\n",
      "Iteration 39, loss = 0.05588337\n",
      "Iteration 40, loss = 0.05501086\n",
      "Iteration 41, loss = 0.06157106\n",
      "Iteration 42, loss = 0.05488767\n",
      "Iteration 43, loss = 0.05343500\n",
      "Iteration 44, loss = 0.05283796\n",
      "Iteration 45, loss = 0.05837835\n",
      "Iteration 46, loss = 0.05754378\n",
      "Iteration 47, loss = 0.05220609\n",
      "Iteration 48, loss = 0.05857760\n",
      "Iteration 49, loss = 0.05299215\n",
      "Iteration 50, loss = 0.05746818\n",
      "Iteration 51, loss = 0.05410053\n",
      "Iteration 52, loss = 0.05571024\n",
      "Iteration 53, loss = 0.05377787\n",
      "Iteration 54, loss = 0.05469097\n",
      "Iteration 55, loss = 0.05386292\n",
      "Iteration 56, loss = 0.05751837\n",
      "Iteration 57, loss = 0.04849737\n",
      "Iteration 58, loss = 0.05531117\n",
      "Iteration 59, loss = 0.04683555\n",
      "Iteration 60, loss = 0.05499115\n",
      "Iteration 61, loss = 0.05503107\n",
      "Iteration 62, loss = 0.05380554\n",
      "Iteration 63, loss = 0.05420271\n",
      "Iteration 64, loss = 0.04869491\n",
      "Iteration 65, loss = 0.04885192\n",
      "Iteration 66, loss = 0.05780526\n",
      "Iteration 67, loss = 0.05050045\n",
      "Iteration 68, loss = 0.05260527\n",
      "Iteration 69, loss = 0.04646089\n",
      "Iteration 70, loss = 0.05721957\n",
      "Iteration 71, loss = 0.04770409\n",
      "Iteration 72, loss = 0.05269421\n",
      "Iteration 73, loss = 0.05042718\n",
      "Iteration 74, loss = 0.04907548\n",
      "Iteration 75, loss = 0.05180600\n",
      "Iteration 76, loss = 0.05376088\n",
      "Iteration 77, loss = 0.04869073\n",
      "Iteration 78, loss = 0.05197047\n",
      "Iteration 79, loss = 0.04882125\n",
      "Iteration 80, loss = 0.05085970\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.01, batch_size=50, hidden_layer_sizes=(100, 100, 100), solver=adam; total time= 6.7min\n",
      "Iteration 1, loss = 0.37818523\n",
      "Iteration 2, loss = 0.17444215\n",
      "Iteration 3, loss = 0.13403263\n",
      "Iteration 4, loss = 0.11773281\n",
      "Iteration 5, loss = 0.10391178\n",
      "Iteration 6, loss = 0.09746065\n",
      "Iteration 7, loss = 0.09066431\n",
      "Iteration 8, loss = 0.08410002\n",
      "Iteration 9, loss = 0.08113279\n",
      "Iteration 10, loss = 0.08297528\n",
      "Iteration 11, loss = 0.07457552\n",
      "Iteration 12, loss = 0.07519239\n",
      "Iteration 13, loss = 0.07235709\n",
      "Iteration 14, loss = 0.07138492\n",
      "Iteration 15, loss = 0.07437706\n",
      "Iteration 16, loss = 0.07041267\n",
      "Iteration 17, loss = 0.06837816\n",
      "Iteration 18, loss = 0.06820050\n",
      "Iteration 19, loss = 0.06521764\n",
      "Iteration 20, loss = 0.06347511\n",
      "Iteration 21, loss = 0.06622643\n",
      "Iteration 22, loss = 0.06117590\n",
      "Iteration 23, loss = 0.06651939\n",
      "Iteration 24, loss = 0.06435142\n",
      "Iteration 25, loss = 0.06201443\n",
      "Iteration 26, loss = 0.06606634\n",
      "Iteration 27, loss = 0.06155697\n",
      "Iteration 28, loss = 0.06161132\n",
      "Iteration 29, loss = 0.05772537\n",
      "Iteration 30, loss = 0.06319541\n",
      "Iteration 31, loss = 0.05825626\n",
      "Iteration 32, loss = 0.06248411\n",
      "Iteration 33, loss = 0.05121986\n",
      "Iteration 34, loss = 0.06524008\n",
      "Iteration 35, loss = 0.06216230\n",
      "Iteration 36, loss = 0.05702812\n",
      "Iteration 37, loss = 0.05637299\n",
      "Iteration 38, loss = 0.05283800\n",
      "Iteration 39, loss = 0.05948252\n",
      "Iteration 40, loss = 0.06064268\n",
      "Iteration 41, loss = 0.05472267\n",
      "Iteration 42, loss = 0.05857759\n",
      "Iteration 43, loss = 0.05740282\n",
      "Iteration 44, loss = 0.05543684\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.01, batch_size=50, hidden_layer_sizes=(100, 100, 100), solver=adam; total time= 3.1min\n",
      "Iteration 1, loss = 0.36798684\n",
      "Iteration 2, loss = 0.17426150\n",
      "Iteration 3, loss = 0.13966779\n",
      "Iteration 4, loss = 0.11841182\n",
      "Iteration 5, loss = 0.10927167\n",
      "Iteration 6, loss = 0.09740761\n",
      "Iteration 7, loss = 0.08979837\n",
      "Iteration 8, loss = 0.08750847\n",
      "Iteration 9, loss = 0.08277890\n",
      "Iteration 10, loss = 0.08062632\n",
      "Iteration 11, loss = 0.07937839\n",
      "Iteration 12, loss = 0.07472645\n",
      "Iteration 13, loss = 0.07507157\n",
      "Iteration 14, loss = 0.07299623\n",
      "Iteration 15, loss = 0.07358636\n",
      "Iteration 16, loss = 0.07165052\n",
      "Iteration 17, loss = 0.06720365\n",
      "Iteration 18, loss = 0.07277365\n",
      "Iteration 19, loss = 0.06569988\n",
      "Iteration 20, loss = 0.06738585\n",
      "Iteration 21, loss = 0.06733420\n",
      "Iteration 22, loss = 0.06769755\n",
      "Iteration 23, loss = 0.06459601\n",
      "Iteration 24, loss = 0.06796358\n",
      "Iteration 25, loss = 0.06263141\n",
      "Iteration 26, loss = 0.06503372\n",
      "Iteration 27, loss = 0.06149828\n",
      "Iteration 28, loss = 0.05882961\n",
      "Iteration 29, loss = 0.06744890\n",
      "Iteration 30, loss = 0.05952950\n",
      "Iteration 31, loss = 0.06240624\n",
      "Iteration 32, loss = 0.06299506\n",
      "Iteration 33, loss = 0.05772240\n",
      "Iteration 34, loss = 0.06000274\n",
      "Iteration 35, loss = 0.06065555\n",
      "Iteration 36, loss = 0.06018014\n",
      "Iteration 37, loss = 0.06024796\n",
      "Iteration 38, loss = 0.05686118\n",
      "Iteration 39, loss = 0.05793665\n",
      "Iteration 40, loss = 0.06057070\n",
      "Iteration 41, loss = 0.05600713\n",
      "Iteration 42, loss = 0.05424158\n",
      "Iteration 43, loss = 0.06282889\n",
      "Iteration 44, loss = 0.05755039\n",
      "Iteration 45, loss = 0.05371230\n",
      "Iteration 46, loss = 0.05735006\n",
      "Iteration 47, loss = 0.05603516\n",
      "Iteration 48, loss = 0.05503046\n",
      "Iteration 49, loss = 0.05694085\n",
      "Iteration 50, loss = 0.05258080\n",
      "Iteration 51, loss = 0.05485190\n",
      "Iteration 52, loss = 0.05774690\n",
      "Iteration 53, loss = 0.05418594\n",
      "Iteration 54, loss = 0.05390178\n",
      "Iteration 55, loss = 0.05672376\n",
      "Iteration 56, loss = 0.04986422\n",
      "Iteration 57, loss = 0.05300366\n",
      "Iteration 58, loss = 0.05974189\n",
      "Iteration 59, loss = 0.04935358\n",
      "Iteration 60, loss = 0.05181624\n",
      "Iteration 61, loss = 0.05681115\n",
      "Iteration 62, loss = 0.05139540\n",
      "Iteration 63, loss = 0.04789224\n",
      "Iteration 64, loss = 0.05443892\n",
      "Iteration 65, loss = 0.05355228\n",
      "Iteration 66, loss = 0.05407351\n",
      "Iteration 67, loss = 0.05192203\n",
      "Iteration 68, loss = 0.05611836\n",
      "Iteration 69, loss = 0.04709937\n",
      "Iteration 70, loss = 0.05162761\n",
      "Iteration 71, loss = 0.05414739\n",
      "Iteration 72, loss = 0.05122641\n",
      "Iteration 73, loss = 0.05280155\n",
      "Iteration 74, loss = 0.04360269\n",
      "Iteration 75, loss = 0.05305777\n",
      "Iteration 76, loss = 0.05546225\n",
      "Iteration 77, loss = 0.04883550\n",
      "Iteration 78, loss = 0.04957555\n",
      "Iteration 79, loss = 0.05084854\n",
      "Iteration 80, loss = 0.05089308\n",
      "Iteration 81, loss = 0.05317867\n",
      "Iteration 82, loss = 0.04663425\n",
      "Iteration 83, loss = 0.05138407\n",
      "Iteration 84, loss = 0.05327711\n",
      "Iteration 85, loss = 0.04856814\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.01, batch_size=50, hidden_layer_sizes=(100, 100, 100), solver=adam; total time= 7.3min\n",
      "Iteration 1, loss = 0.37059225\n",
      "Iteration 2, loss = 0.17250221\n",
      "Iteration 3, loss = 0.13478508\n",
      "Iteration 4, loss = 0.11616013\n",
      "Iteration 5, loss = 0.10166756\n",
      "Iteration 6, loss = 0.09798020\n",
      "Iteration 7, loss = 0.09095305\n",
      "Iteration 8, loss = 0.08494531\n",
      "Iteration 9, loss = 0.08541549\n",
      "Iteration 10, loss = 0.07925609\n",
      "Iteration 11, loss = 0.07756331\n",
      "Iteration 12, loss = 0.07699612\n",
      "Iteration 13, loss = 0.07580152\n",
      "Iteration 14, loss = 0.07099232\n",
      "Iteration 15, loss = 0.07043607\n",
      "Iteration 16, loss = 0.06579501\n",
      "Iteration 17, loss = 0.07066475\n",
      "Iteration 18, loss = 0.07092573\n",
      "Iteration 19, loss = 0.06637745\n",
      "Iteration 20, loss = 0.06438186\n",
      "Iteration 21, loss = 0.06485429\n",
      "Iteration 22, loss = 0.06401106\n",
      "Iteration 23, loss = 0.06352012\n",
      "Iteration 24, loss = 0.06554903\n",
      "Iteration 25, loss = 0.06565392\n",
      "Iteration 26, loss = 0.05820625\n",
      "Iteration 27, loss = 0.06024088\n",
      "Iteration 28, loss = 0.06441131\n",
      "Iteration 29, loss = 0.06613312\n",
      "Iteration 30, loss = 0.05748132\n",
      "Iteration 31, loss = 0.05883121\n",
      "Iteration 32, loss = 0.06362403\n",
      "Iteration 33, loss = 0.05879884\n",
      "Iteration 34, loss = 0.06004489\n",
      "Iteration 35, loss = 0.05739954\n",
      "Iteration 36, loss = 0.05655301\n",
      "Iteration 37, loss = 0.06090378\n",
      "Iteration 38, loss = 0.05672696\n",
      "Iteration 39, loss = 0.05820164\n",
      "Iteration 40, loss = 0.06044333\n",
      "Iteration 41, loss = 0.05862364\n",
      "Iteration 42, loss = 0.05698746\n",
      "Iteration 43, loss = 0.05728463\n",
      "Iteration 44, loss = 0.05441404\n",
      "Iteration 45, loss = 0.05480889\n",
      "Iteration 46, loss = 0.05941293\n",
      "Iteration 47, loss = 0.05462199\n",
      "Iteration 48, loss = 0.05604322\n",
      "Iteration 49, loss = 0.05618014\n",
      "Iteration 50, loss = 0.05574611\n",
      "Iteration 51, loss = 0.05667778\n",
      "Iteration 52, loss = 0.05225914\n",
      "Iteration 53, loss = 0.05619751\n",
      "Iteration 54, loss = 0.05423226\n",
      "Iteration 55, loss = 0.05488482\n",
      "Iteration 56, loss = 0.05458790\n",
      "Iteration 57, loss = 0.05170611\n",
      "Iteration 58, loss = 0.05511341\n",
      "Iteration 59, loss = 0.05437991\n",
      "Iteration 60, loss = 0.05047430\n",
      "Iteration 61, loss = 0.05482851\n",
      "Iteration 62, loss = 0.05443775\n",
      "Iteration 63, loss = 0.05039102\n",
      "Iteration 64, loss = 0.05102573\n",
      "Iteration 65, loss = 0.05277119\n",
      "Iteration 66, loss = 0.05508397\n",
      "Iteration 67, loss = 0.05297839\n",
      "Iteration 68, loss = 0.05151225\n",
      "Iteration 69, loss = 0.05273226\n",
      "Iteration 70, loss = 0.04543710\n",
      "Iteration 71, loss = 0.05553244\n",
      "Iteration 72, loss = 0.05310219\n",
      "Iteration 73, loss = 0.05085723\n",
      "Iteration 74, loss = 0.05090501\n",
      "Iteration 75, loss = 0.05334665\n",
      "Iteration 76, loss = 0.05032298\n",
      "Iteration 77, loss = 0.05184071\n",
      "Iteration 78, loss = 0.04884054\n",
      "Iteration 79, loss = 0.05216938\n",
      "Iteration 80, loss = 0.04622455\n",
      "Iteration 81, loss = 0.05910002\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.01, batch_size=50, hidden_layer_sizes=(100, 100, 100), solver=adam; total time= 6.9min\n",
      "Iteration 1, loss = 1.18824967\n",
      "Iteration 2, loss = 0.43738185\n",
      "Iteration 3, loss = 0.35869771\n",
      "Iteration 4, loss = 0.31874284\n",
      "Iteration 5, loss = 0.28923973\n",
      "Iteration 6, loss = 0.26648554\n",
      "Iteration 7, loss = 0.24842453\n",
      "Iteration 8, loss = 0.23292200\n",
      "Iteration 9, loss = 0.22046772\n",
      "Iteration 10, loss = 0.20849518\n",
      "Iteration 11, loss = 0.19908999\n",
      "Iteration 12, loss = 0.18992338\n",
      "Iteration 13, loss = 0.18201266\n",
      "Iteration 14, loss = 0.17506719\n",
      "Iteration 15, loss = 0.16821169\n",
      "Iteration 16, loss = 0.16220815\n",
      "Iteration 17, loss = 0.15679170\n",
      "Iteration 18, loss = 0.15158716\n",
      "Iteration 19, loss = 0.14670712\n",
      "Iteration 20, loss = 0.14221034\n",
      "Iteration 21, loss = 0.13843253\n",
      "Iteration 22, loss = 0.13410548\n",
      "Iteration 23, loss = 0.12993102\n",
      "Iteration 24, loss = 0.12702433\n",
      "Iteration 25, loss = 0.12369855\n",
      "Iteration 26, loss = 0.12038743\n",
      "Iteration 27, loss = 0.11781476\n",
      "Iteration 28, loss = 0.11456203\n",
      "Iteration 29, loss = 0.11169419\n",
      "Iteration 30, loss = 0.10896652\n",
      "Iteration 31, loss = 0.10654091\n",
      "Iteration 32, loss = 0.10433793\n",
      "Iteration 33, loss = 0.10238173\n",
      "Iteration 34, loss = 0.10003461\n",
      "Iteration 35, loss = 0.09782345\n",
      "Iteration 36, loss = 0.09614195\n",
      "Iteration 37, loss = 0.09398532\n",
      "Iteration 38, loss = 0.09226065\n",
      "Iteration 39, loss = 0.09071125\n",
      "Iteration 40, loss = 0.08943575\n",
      "Iteration 41, loss = 0.08687981\n",
      "Iteration 42, loss = 0.08592933\n",
      "Iteration 43, loss = 0.08484835\n",
      "Iteration 44, loss = 0.08311805\n",
      "Iteration 45, loss = 0.08180986\n",
      "Iteration 46, loss = 0.08063040\n",
      "Iteration 47, loss = 0.07929629\n",
      "Iteration 48, loss = 0.07831868\n",
      "Iteration 49, loss = 0.07706381\n",
      "Iteration 50, loss = 0.07587075\n",
      "Iteration 51, loss = 0.07508839\n",
      "Iteration 52, loss = 0.07392371\n",
      "Iteration 53, loss = 0.07345175\n",
      "Iteration 54, loss = 0.07199930\n",
      "Iteration 55, loss = 0.07107065\n",
      "Iteration 56, loss = 0.07043894\n",
      "Iteration 57, loss = 0.06963603\n",
      "Iteration 58, loss = 0.06893759\n",
      "Iteration 59, loss = 0.06806655\n",
      "Iteration 60, loss = 0.06729295\n",
      "Iteration 61, loss = 0.06683058\n",
      "Iteration 62, loss = 0.06610278\n",
      "Iteration 63, loss = 0.06533433\n",
      "Iteration 64, loss = 0.06489016\n",
      "Iteration 65, loss = 0.06444943\n",
      "Iteration 66, loss = 0.06369553\n",
      "Iteration 67, loss = 0.06325919\n",
      "Iteration 68, loss = 0.06262687\n",
      "Iteration 69, loss = 0.06226586\n",
      "Iteration 70, loss = 0.06171192\n",
      "Iteration 71, loss = 0.06147243\n",
      "Iteration 72, loss = 0.06105930\n",
      "Iteration 73, loss = 0.06069433\n",
      "Iteration 74, loss = 0.06000495\n",
      "Iteration 75, loss = 0.05977603\n",
      "Iteration 76, loss = 0.05939517\n",
      "Iteration 77, loss = 0.05897413\n",
      "Iteration 78, loss = 0.05880846\n",
      "Iteration 79, loss = 0.05847049\n",
      "Iteration 80, loss = 0.05802431\n",
      "Iteration 81, loss = 0.05774480\n",
      "Iteration 82, loss = 0.05762308\n",
      "Iteration 83, loss = 0.05724921\n",
      "Iteration 84, loss = 0.05698874\n",
      "Iteration 85, loss = 0.05677630\n",
      "Iteration 86, loss = 0.05651931\n",
      "Iteration 87, loss = 0.05636485\n",
      "Iteration 88, loss = 0.05608924\n",
      "Iteration 89, loss = 0.05579085\n",
      "Iteration 90, loss = 0.05558091\n",
      "Iteration 91, loss = 0.05537735\n",
      "Iteration 92, loss = 0.05519904\n",
      "Iteration 93, loss = 0.05499664\n",
      "Iteration 94, loss = 0.05487079\n",
      "Iteration 95, loss = 0.05464700\n",
      "Iteration 96, loss = 0.05440717\n",
      "Iteration 97, loss = 0.05427729\n",
      "Iteration 98, loss = 0.05411911\n",
      "Iteration 99, loss = 0.05397991\n",
      "Iteration 100, loss = 0.05375180\n",
      "Iteration 101, loss = 0.05357237\n",
      "Iteration 102, loss = 0.05344765\n",
      "Iteration 103, loss = 0.05332644\n",
      "Iteration 104, loss = 0.05320008\n",
      "Iteration 105, loss = 0.05305414\n",
      "Iteration 106, loss = 0.05286876\n",
      "Iteration 107, loss = 0.05274442\n",
      "Iteration 108, loss = 0.05264608\n",
      "Iteration 109, loss = 0.05245330\n",
      "Iteration 110, loss = 0.05238702\n",
      "Iteration 111, loss = 0.05225649\n",
      "Iteration 112, loss = 0.05209028\n",
      "Iteration 113, loss = 0.05199308\n",
      "Iteration 114, loss = 0.05184643\n",
      "Iteration 115, loss = 0.05176749\n",
      "Iteration 116, loss = 0.05165792\n",
      "Iteration 117, loss = 0.05152308\n",
      "Iteration 118, loss = 0.05146215\n",
      "Iteration 119, loss = 0.05131963\n",
      "Iteration 120, loss = 0.05121489\n",
      "Iteration 121, loss = 0.05109458\n",
      "Iteration 122, loss = 0.05100278\n",
      "Iteration 123, loss = 0.05086056\n",
      "Iteration 124, loss = 0.05078091\n",
      "Iteration 125, loss = 0.05067647\n",
      "Iteration 126, loss = 0.05059832\n",
      "Iteration 127, loss = 0.05047752\n",
      "Iteration 128, loss = 0.05037612\n",
      "Iteration 129, loss = 0.05028378\n",
      "Iteration 130, loss = 0.05021305\n",
      "Iteration 131, loss = 0.05010355\n",
      "Iteration 132, loss = 0.05000198\n",
      "Iteration 133, loss = 0.04992814\n",
      "Iteration 134, loss = 0.04980831\n",
      "Iteration 135, loss = 0.04972898\n",
      "Iteration 136, loss = 0.04965626\n",
      "Iteration 137, loss = 0.04953887\n",
      "Iteration 138, loss = 0.04947094\n",
      "Iteration 139, loss = 0.04938441\n",
      "Iteration 140, loss = 0.04930319\n",
      "Iteration 141, loss = 0.04921194\n",
      "Iteration 142, loss = 0.04914008\n",
      "Iteration 143, loss = 0.04907844\n",
      "Iteration 144, loss = 0.04894513\n",
      "Iteration 145, loss = 0.04889229\n",
      "Iteration 146, loss = 0.04883689\n",
      "Iteration 147, loss = 0.04870495\n",
      "Iteration 148, loss = 0.04865050\n",
      "Iteration 149, loss = 0.04854249\n",
      "Iteration 150, loss = 0.04848850\n",
      "Iteration 151, loss = 0.04841934\n",
      "Iteration 152, loss = 0.04835571\n",
      "Iteration 153, loss = 0.04823450\n",
      "Iteration 154, loss = 0.04816852\n",
      "Iteration 155, loss = 0.04808215\n",
      "Iteration 156, loss = 0.04802854\n",
      "Iteration 157, loss = 0.04793197\n",
      "Iteration 158, loss = 0.04789006\n",
      "Iteration 159, loss = 0.04779339\n",
      "Iteration 160, loss = 0.04771577\n",
      "Iteration 161, loss = 0.04766331\n",
      "Iteration 162, loss = 0.04757171\n",
      "Iteration 163, loss = 0.04750783\n",
      "Iteration 164, loss = 0.04742315\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.01, batch_size=50, hidden_layer_sizes=(100, 100, 100), solver=sgd; total time= 4.1min\n",
      "Iteration 1, loss = 1.32725157\n",
      "Iteration 2, loss = 0.47738665\n",
      "Iteration 3, loss = 0.37498767\n",
      "Iteration 4, loss = 0.33307483\n",
      "Iteration 5, loss = 0.30369522\n",
      "Iteration 6, loss = 0.28117977\n",
      "Iteration 7, loss = 0.26185594\n",
      "Iteration 8, loss = 0.24493297\n",
      "Iteration 9, loss = 0.23108260\n",
      "Iteration 10, loss = 0.21887387\n",
      "Iteration 11, loss = 0.20791946\n",
      "Iteration 12, loss = 0.19713327\n",
      "Iteration 13, loss = 0.18835231\n",
      "Iteration 14, loss = 0.18085144\n",
      "Iteration 15, loss = 0.17294048\n",
      "Iteration 16, loss = 0.16645560\n",
      "Iteration 17, loss = 0.16083198\n",
      "Iteration 18, loss = 0.15446254\n",
      "Iteration 19, loss = 0.15000142\n",
      "Iteration 20, loss = 0.14448082\n",
      "Iteration 21, loss = 0.14043947\n",
      "Iteration 22, loss = 0.13602433\n",
      "Iteration 23, loss = 0.13171483\n",
      "Iteration 24, loss = 0.12761574\n",
      "Iteration 25, loss = 0.12460694\n",
      "Iteration 26, loss = 0.12066396\n",
      "Iteration 27, loss = 0.11798183\n",
      "Iteration 28, loss = 0.11447727\n",
      "Iteration 29, loss = 0.11134368\n",
      "Iteration 30, loss = 0.10894808\n",
      "Iteration 31, loss = 0.10606789\n",
      "Iteration 32, loss = 0.10415639\n",
      "Iteration 33, loss = 0.10175964\n",
      "Iteration 34, loss = 0.09908289\n",
      "Iteration 35, loss = 0.09700233\n",
      "Iteration 36, loss = 0.09484199\n",
      "Iteration 37, loss = 0.09285555\n",
      "Iteration 38, loss = 0.09124644\n",
      "Iteration 39, loss = 0.08935530\n",
      "Iteration 40, loss = 0.08713526\n",
      "Iteration 41, loss = 0.08606084\n",
      "Iteration 42, loss = 0.08401349\n",
      "Iteration 43, loss = 0.08271640\n",
      "Iteration 44, loss = 0.08131681\n",
      "Iteration 45, loss = 0.07966931\n",
      "Iteration 46, loss = 0.07867592\n",
      "Iteration 47, loss = 0.07739494\n",
      "Iteration 48, loss = 0.07596371\n",
      "Iteration 49, loss = 0.07505910\n",
      "Iteration 50, loss = 0.07412636\n",
      "Iteration 51, loss = 0.07299749\n",
      "Iteration 52, loss = 0.07176503\n",
      "Iteration 53, loss = 0.07081087\n",
      "Iteration 54, loss = 0.06996987\n",
      "Iteration 55, loss = 0.06908312\n",
      "Iteration 56, loss = 0.06820146\n",
      "Iteration 57, loss = 0.06729114\n",
      "Iteration 58, loss = 0.06664687\n",
      "Iteration 59, loss = 0.06578751\n",
      "Iteration 60, loss = 0.06541060\n",
      "Iteration 61, loss = 0.06458497\n",
      "Iteration 62, loss = 0.06412606\n",
      "Iteration 63, loss = 0.06354194\n",
      "Iteration 64, loss = 0.06296159\n",
      "Iteration 65, loss = 0.06228974\n",
      "Iteration 66, loss = 0.06174371\n",
      "Iteration 67, loss = 0.06118616\n",
      "Iteration 68, loss = 0.06086202\n",
      "Iteration 69, loss = 0.06035894\n",
      "Iteration 70, loss = 0.06005538\n",
      "Iteration 71, loss = 0.05940796\n",
      "Iteration 72, loss = 0.05915758\n",
      "Iteration 73, loss = 0.05876364\n",
      "Iteration 74, loss = 0.05838410\n",
      "Iteration 75, loss = 0.05801346\n",
      "Iteration 76, loss = 0.05759096\n",
      "Iteration 77, loss = 0.05729973\n",
      "Iteration 78, loss = 0.05709398\n",
      "Iteration 79, loss = 0.05683085\n",
      "Iteration 80, loss = 0.05646427\n",
      "Iteration 81, loss = 0.05619572\n",
      "Iteration 82, loss = 0.05596156\n",
      "Iteration 83, loss = 0.05564053\n",
      "Iteration 84, loss = 0.05555479\n",
      "Iteration 85, loss = 0.05526747\n",
      "Iteration 86, loss = 0.05502096\n",
      "Iteration 87, loss = 0.05490290\n",
      "Iteration 88, loss = 0.05467667\n",
      "Iteration 89, loss = 0.05448079\n",
      "Iteration 90, loss = 0.05421388\n",
      "Iteration 91, loss = 0.05410059\n",
      "Iteration 92, loss = 0.05385123\n",
      "Iteration 93, loss = 0.05373396\n",
      "Iteration 94, loss = 0.05355565\n",
      "Iteration 95, loss = 0.05336973\n",
      "Iteration 96, loss = 0.05316526\n",
      "Iteration 97, loss = 0.05306862\n",
      "Iteration 98, loss = 0.05286063\n",
      "Iteration 99, loss = 0.05275422\n",
      "Iteration 100, loss = 0.05258698\n",
      "Iteration 101, loss = 0.05249614\n",
      "Iteration 102, loss = 0.05240090\n",
      "Iteration 103, loss = 0.05218107\n",
      "Iteration 104, loss = 0.05208731\n",
      "Iteration 105, loss = 0.05190067\n",
      "Iteration 106, loss = 0.05181888\n",
      "Iteration 107, loss = 0.05173634\n",
      "Iteration 108, loss = 0.05156304\n",
      "Iteration 109, loss = 0.05147097\n",
      "Iteration 110, loss = 0.05139616\n",
      "Iteration 111, loss = 0.05128911\n",
      "Iteration 112, loss = 0.05118592\n",
      "Iteration 113, loss = 0.05106895\n",
      "Iteration 114, loss = 0.05094982\n",
      "Iteration 115, loss = 0.05087639\n",
      "Iteration 116, loss = 0.05077614\n",
      "Iteration 117, loss = 0.05065117\n",
      "Iteration 118, loss = 0.05052715\n",
      "Iteration 119, loss = 0.05044903\n",
      "Iteration 120, loss = 0.05034405\n",
      "Iteration 121, loss = 0.05025143\n",
      "Iteration 122, loss = 0.05016293\n",
      "Iteration 123, loss = 0.05009814\n",
      "Iteration 124, loss = 0.04995019\n",
      "Iteration 125, loss = 0.04988935\n",
      "Iteration 126, loss = 0.04976178\n",
      "Iteration 127, loss = 0.04967763\n",
      "Iteration 128, loss = 0.04960724\n",
      "Iteration 129, loss = 0.04955147\n",
      "Iteration 130, loss = 0.04941864\n",
      "Iteration 131, loss = 0.04933281\n",
      "Iteration 132, loss = 0.04929299\n",
      "Iteration 133, loss = 0.04917013\n",
      "Iteration 134, loss = 0.04909157\n",
      "Iteration 135, loss = 0.04901430\n",
      "Iteration 136, loss = 0.04890544\n",
      "Iteration 137, loss = 0.04884370\n",
      "Iteration 138, loss = 0.04874622\n",
      "Iteration 139, loss = 0.04870068\n",
      "Iteration 140, loss = 0.04861185\n",
      "Iteration 141, loss = 0.04852101\n",
      "Iteration 142, loss = 0.04842665\n",
      "Iteration 143, loss = 0.04835481\n",
      "Iteration 144, loss = 0.04827208\n",
      "Iteration 145, loss = 0.04821116\n",
      "Iteration 146, loss = 0.04814261\n",
      "Iteration 147, loss = 0.04803324\n",
      "Iteration 148, loss = 0.04799661\n",
      "Iteration 149, loss = 0.04792271\n",
      "Iteration 150, loss = 0.04781779\n",
      "Iteration 151, loss = 0.04775160\n",
      "Iteration 152, loss = 0.04768164\n",
      "Iteration 153, loss = 0.04761549\n",
      "Iteration 154, loss = 0.04751748\n",
      "Iteration 155, loss = 0.04747014\n",
      "Iteration 156, loss = 0.04738290\n",
      "Iteration 157, loss = 0.04730150\n",
      "Iteration 158, loss = 0.04724986\n",
      "Iteration 159, loss = 0.04718303\n",
      "Iteration 160, loss = 0.04709349\n",
      "Iteration 161, loss = 0.04703103\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.01, batch_size=50, hidden_layer_sizes=(100, 100, 100), solver=sgd; total time= 4.0min\n",
      "Iteration 1, loss = 1.31753824\n",
      "Iteration 2, loss = 0.46505689\n",
      "Iteration 3, loss = 0.36263770\n",
      "Iteration 4, loss = 0.31999840\n",
      "Iteration 5, loss = 0.29069625\n",
      "Iteration 6, loss = 0.26790461\n",
      "Iteration 7, loss = 0.25042413\n",
      "Iteration 8, loss = 0.23514087\n",
      "Iteration 9, loss = 0.22191784\n",
      "Iteration 10, loss = 0.21052723\n",
      "Iteration 11, loss = 0.20059325\n",
      "Iteration 12, loss = 0.19271551\n",
      "Iteration 13, loss = 0.18418704\n",
      "Iteration 14, loss = 0.17658318\n",
      "Iteration 15, loss = 0.16972575\n",
      "Iteration 16, loss = 0.16392406\n",
      "Iteration 17, loss = 0.15785199\n",
      "Iteration 18, loss = 0.15243233\n",
      "Iteration 19, loss = 0.14746178\n",
      "Iteration 20, loss = 0.14253904\n",
      "Iteration 21, loss = 0.13838066\n",
      "Iteration 22, loss = 0.13437459\n",
      "Iteration 23, loss = 0.13077760\n",
      "Iteration 24, loss = 0.12650066\n",
      "Iteration 25, loss = 0.12302160\n",
      "Iteration 26, loss = 0.11986712\n",
      "Iteration 27, loss = 0.11645490\n",
      "Iteration 28, loss = 0.11350364\n",
      "Iteration 29, loss = 0.11068960\n",
      "Iteration 30, loss = 0.10830797\n",
      "Iteration 31, loss = 0.10578174\n",
      "Iteration 32, loss = 0.10302245\n",
      "Iteration 33, loss = 0.10055761\n",
      "Iteration 34, loss = 0.09916440\n",
      "Iteration 35, loss = 0.09666415\n",
      "Iteration 36, loss = 0.09466111\n",
      "Iteration 37, loss = 0.09230515\n",
      "Iteration 38, loss = 0.09096891\n",
      "Iteration 39, loss = 0.08915362\n",
      "Iteration 40, loss = 0.08747012\n",
      "Iteration 41, loss = 0.08607836\n",
      "Iteration 42, loss = 0.08450962\n",
      "Iteration 43, loss = 0.08342288\n",
      "Iteration 44, loss = 0.08172810\n",
      "Iteration 45, loss = 0.08036765\n",
      "Iteration 46, loss = 0.07910574\n",
      "Iteration 47, loss = 0.07788013\n",
      "Iteration 48, loss = 0.07693539\n",
      "Iteration 49, loss = 0.07562789\n",
      "Iteration 50, loss = 0.07518311\n",
      "Iteration 51, loss = 0.07367703\n",
      "Iteration 52, loss = 0.07270178\n",
      "Iteration 53, loss = 0.07179303\n",
      "Iteration 54, loss = 0.07054574\n",
      "Iteration 55, loss = 0.07011445\n",
      "Iteration 56, loss = 0.06926115\n",
      "Iteration 57, loss = 0.06856443\n",
      "Iteration 58, loss = 0.06796084\n",
      "Iteration 59, loss = 0.06705237\n",
      "Iteration 60, loss = 0.06632097\n",
      "Iteration 61, loss = 0.06561472\n",
      "Iteration 62, loss = 0.06519666\n",
      "Iteration 63, loss = 0.06465538\n",
      "Iteration 64, loss = 0.06401067\n",
      "Iteration 65, loss = 0.06347881\n",
      "Iteration 66, loss = 0.06305834\n",
      "Iteration 67, loss = 0.06262194\n",
      "Iteration 68, loss = 0.06203745\n",
      "Iteration 69, loss = 0.06155216\n",
      "Iteration 70, loss = 0.06130053\n",
      "Iteration 71, loss = 0.06076637\n",
      "Iteration 72, loss = 0.06021264\n",
      "Iteration 73, loss = 0.06007067\n",
      "Iteration 74, loss = 0.05956894\n",
      "Iteration 75, loss = 0.05927353\n",
      "Iteration 76, loss = 0.05898302\n",
      "Iteration 77, loss = 0.05865437\n",
      "Iteration 78, loss = 0.05820316\n",
      "Iteration 79, loss = 0.05789277\n",
      "Iteration 80, loss = 0.05767700\n",
      "Iteration 81, loss = 0.05743491\n",
      "Iteration 82, loss = 0.05712976\n",
      "Iteration 83, loss = 0.05691191\n",
      "Iteration 84, loss = 0.05660796\n",
      "Iteration 85, loss = 0.05634197\n",
      "Iteration 86, loss = 0.05604647\n",
      "Iteration 87, loss = 0.05589323\n",
      "Iteration 88, loss = 0.05567747\n",
      "Iteration 89, loss = 0.05538175\n",
      "Iteration 90, loss = 0.05516053\n",
      "Iteration 91, loss = 0.05512144\n",
      "Iteration 92, loss = 0.05478508\n",
      "Iteration 93, loss = 0.05460918\n",
      "Iteration 94, loss = 0.05446420\n",
      "Iteration 95, loss = 0.05432715\n",
      "Iteration 96, loss = 0.05407848\n",
      "Iteration 97, loss = 0.05391388\n",
      "Iteration 98, loss = 0.05371411\n",
      "Iteration 99, loss = 0.05358057\n",
      "Iteration 100, loss = 0.05344157\n",
      "Iteration 101, loss = 0.05326529\n",
      "Iteration 102, loss = 0.05306665\n",
      "Iteration 103, loss = 0.05292211\n",
      "Iteration 104, loss = 0.05276511\n",
      "Iteration 105, loss = 0.05270190\n",
      "Iteration 106, loss = 0.05255987\n",
      "Iteration 107, loss = 0.05241281\n",
      "Iteration 108, loss = 0.05222974\n",
      "Iteration 109, loss = 0.05211768\n",
      "Iteration 110, loss = 0.05191884\n",
      "Iteration 111, loss = 0.05183229\n",
      "Iteration 112, loss = 0.05175959\n",
      "Iteration 113, loss = 0.05163160\n",
      "Iteration 114, loss = 0.05153904\n",
      "Iteration 115, loss = 0.05142031\n",
      "Iteration 116, loss = 0.05133287\n",
      "Iteration 117, loss = 0.05118872\n",
      "Iteration 118, loss = 0.05100297\n",
      "Iteration 119, loss = 0.05092967\n",
      "Iteration 120, loss = 0.05086394\n",
      "Iteration 121, loss = 0.05075737\n",
      "Iteration 122, loss = 0.05068938\n",
      "Iteration 123, loss = 0.05052693\n",
      "Iteration 124, loss = 0.05047284\n",
      "Iteration 125, loss = 0.05036762\n",
      "Iteration 126, loss = 0.05027660\n",
      "Iteration 127, loss = 0.05017541\n",
      "Iteration 128, loss = 0.05007991\n",
      "Iteration 129, loss = 0.04998772\n",
      "Iteration 130, loss = 0.04987143\n",
      "Iteration 131, loss = 0.04981502\n",
      "Iteration 132, loss = 0.04977651\n",
      "Iteration 133, loss = 0.04965887\n",
      "Iteration 134, loss = 0.04953619\n",
      "Iteration 135, loss = 0.04945753\n",
      "Iteration 136, loss = 0.04939855\n",
      "Iteration 137, loss = 0.04928434\n",
      "Iteration 138, loss = 0.04920317\n",
      "Iteration 139, loss = 0.04909837\n",
      "Iteration 140, loss = 0.04902208\n",
      "Iteration 141, loss = 0.04895641\n",
      "Iteration 142, loss = 0.04885014\n",
      "Iteration 143, loss = 0.04879227\n",
      "Iteration 144, loss = 0.04870935\n",
      "Iteration 145, loss = 0.04860847\n",
      "Iteration 146, loss = 0.04854925\n",
      "Iteration 147, loss = 0.04848169\n",
      "Iteration 148, loss = 0.04839439\n",
      "Iteration 149, loss = 0.04832820\n",
      "Iteration 150, loss = 0.04823566\n",
      "Iteration 151, loss = 0.04815691\n",
      "Iteration 152, loss = 0.04810009\n",
      "Iteration 153, loss = 0.04801233\n",
      "Iteration 154, loss = 0.04796536\n",
      "Iteration 155, loss = 0.04787625\n",
      "Iteration 156, loss = 0.04778901\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.01, batch_size=50, hidden_layer_sizes=(100, 100, 100), solver=sgd; total time= 3.9min\n",
      "Iteration 1, loss = 1.28013078\n",
      "Iteration 2, loss = 0.44745034\n",
      "Iteration 3, loss = 0.36019700\n",
      "Iteration 4, loss = 0.31749836\n",
      "Iteration 5, loss = 0.28973255\n",
      "Iteration 6, loss = 0.26594332\n",
      "Iteration 7, loss = 0.24753206\n",
      "Iteration 8, loss = 0.23129863\n",
      "Iteration 9, loss = 0.21875301\n",
      "Iteration 10, loss = 0.20676141\n",
      "Iteration 11, loss = 0.19599719\n",
      "Iteration 12, loss = 0.18670264\n",
      "Iteration 13, loss = 0.17932234\n",
      "Iteration 14, loss = 0.17198563\n",
      "Iteration 15, loss = 0.16547402\n",
      "Iteration 16, loss = 0.15923269\n",
      "Iteration 17, loss = 0.15343833\n",
      "Iteration 18, loss = 0.14905832\n",
      "Iteration 19, loss = 0.14424169\n",
      "Iteration 20, loss = 0.13949792\n",
      "Iteration 21, loss = 0.13578073\n",
      "Iteration 22, loss = 0.13120672\n",
      "Iteration 23, loss = 0.12858208\n",
      "Iteration 24, loss = 0.12471018\n",
      "Iteration 25, loss = 0.12171038\n",
      "Iteration 26, loss = 0.11856261\n",
      "Iteration 27, loss = 0.11557112\n",
      "Iteration 28, loss = 0.11299042\n",
      "Iteration 29, loss = 0.11039092\n",
      "Iteration 30, loss = 0.10785614\n",
      "Iteration 31, loss = 0.10560672\n",
      "Iteration 32, loss = 0.10351582\n",
      "Iteration 33, loss = 0.10118948\n",
      "Iteration 34, loss = 0.09887693\n",
      "Iteration 35, loss = 0.09701062\n",
      "Iteration 36, loss = 0.09506918\n",
      "Iteration 37, loss = 0.09323775\n",
      "Iteration 38, loss = 0.09138290\n",
      "Iteration 39, loss = 0.08999019\n",
      "Iteration 40, loss = 0.08865507\n",
      "Iteration 41, loss = 0.08702832\n",
      "Iteration 42, loss = 0.08540329\n",
      "Iteration 43, loss = 0.08363285\n",
      "Iteration 44, loss = 0.08268256\n",
      "Iteration 45, loss = 0.08120372\n",
      "Iteration 46, loss = 0.07982650\n",
      "Iteration 47, loss = 0.07874671\n",
      "Iteration 48, loss = 0.07802946\n",
      "Iteration 49, loss = 0.07680331\n",
      "Iteration 50, loss = 0.07519959\n",
      "Iteration 51, loss = 0.07465623\n",
      "Iteration 52, loss = 0.07363329\n",
      "Iteration 53, loss = 0.07246120\n",
      "Iteration 54, loss = 0.07138068\n",
      "Iteration 55, loss = 0.07079400\n",
      "Iteration 56, loss = 0.06980758\n",
      "Iteration 57, loss = 0.06902430\n",
      "Iteration 58, loss = 0.06825735\n",
      "Iteration 59, loss = 0.06754307\n",
      "Iteration 60, loss = 0.06700655\n",
      "Iteration 61, loss = 0.06650367\n",
      "Iteration 62, loss = 0.06576634\n",
      "Iteration 63, loss = 0.06498416\n",
      "Iteration 64, loss = 0.06458572\n",
      "Iteration 65, loss = 0.06394420\n",
      "Iteration 66, loss = 0.06332115\n",
      "Iteration 67, loss = 0.06267602\n",
      "Iteration 68, loss = 0.06235521\n",
      "Iteration 69, loss = 0.06204280\n",
      "Iteration 70, loss = 0.06139473\n",
      "Iteration 71, loss = 0.06083467\n",
      "Iteration 72, loss = 0.06043093\n",
      "Iteration 73, loss = 0.06004227\n",
      "Iteration 74, loss = 0.05978796\n",
      "Iteration 75, loss = 0.05945697\n",
      "Iteration 76, loss = 0.05916481\n",
      "Iteration 77, loss = 0.05858946\n",
      "Iteration 78, loss = 0.05842855\n",
      "Iteration 79, loss = 0.05818968\n",
      "Iteration 80, loss = 0.05778329\n",
      "Iteration 81, loss = 0.05754280\n",
      "Iteration 82, loss = 0.05726236\n",
      "Iteration 83, loss = 0.05706216\n",
      "Iteration 84, loss = 0.05668279\n",
      "Iteration 85, loss = 0.05647274\n",
      "Iteration 86, loss = 0.05612108\n",
      "Iteration 87, loss = 0.05601259\n",
      "Iteration 88, loss = 0.05572467\n",
      "Iteration 89, loss = 0.05545580\n",
      "Iteration 90, loss = 0.05532861\n",
      "Iteration 91, loss = 0.05518388\n",
      "Iteration 92, loss = 0.05487477\n",
      "Iteration 93, loss = 0.05462836\n",
      "Iteration 94, loss = 0.05444515\n",
      "Iteration 95, loss = 0.05438357\n",
      "Iteration 96, loss = 0.05416213\n",
      "Iteration 97, loss = 0.05401075\n",
      "Iteration 98, loss = 0.05384628\n",
      "Iteration 99, loss = 0.05369972\n",
      "Iteration 100, loss = 0.05352830\n",
      "Iteration 101, loss = 0.05333732\n",
      "Iteration 102, loss = 0.05320887\n",
      "Iteration 103, loss = 0.05303235\n",
      "Iteration 104, loss = 0.05293218\n",
      "Iteration 105, loss = 0.05277678\n",
      "Iteration 106, loss = 0.05270405\n",
      "Iteration 107, loss = 0.05251804\n",
      "Iteration 108, loss = 0.05244027\n",
      "Iteration 109, loss = 0.05231639\n",
      "Iteration 110, loss = 0.05218516\n",
      "Iteration 111, loss = 0.05205691\n",
      "Iteration 112, loss = 0.05195385\n",
      "Iteration 113, loss = 0.05178444\n",
      "Iteration 114, loss = 0.05175495\n",
      "Iteration 115, loss = 0.05160041\n",
      "Iteration 116, loss = 0.05150126\n",
      "Iteration 117, loss = 0.05144868\n",
      "Iteration 118, loss = 0.05130852\n",
      "Iteration 119, loss = 0.05122799\n",
      "Iteration 120, loss = 0.05108536\n",
      "Iteration 121, loss = 0.05099777\n",
      "Iteration 122, loss = 0.05091865\n",
      "Iteration 123, loss = 0.05082391\n",
      "Iteration 124, loss = 0.05073223\n",
      "Iteration 125, loss = 0.05061675\n",
      "Iteration 126, loss = 0.05051388\n",
      "Iteration 127, loss = 0.05040806\n",
      "Iteration 128, loss = 0.05031741\n",
      "Iteration 129, loss = 0.05023165\n",
      "Iteration 130, loss = 0.05014992\n",
      "Iteration 131, loss = 0.05006303\n",
      "Iteration 132, loss = 0.04996984\n",
      "Iteration 133, loss = 0.04993301\n",
      "Iteration 134, loss = 0.04981388\n",
      "Iteration 135, loss = 0.04976353\n",
      "Iteration 136, loss = 0.04964578\n",
      "Iteration 137, loss = 0.04953998\n",
      "Iteration 138, loss = 0.04948859\n",
      "Iteration 139, loss = 0.04938694\n",
      "Iteration 140, loss = 0.04932183\n",
      "Iteration 141, loss = 0.04924801\n",
      "Iteration 142, loss = 0.04915703\n",
      "Iteration 143, loss = 0.04907497\n",
      "Iteration 144, loss = 0.04899717\n",
      "Iteration 145, loss = 0.04893310\n",
      "Iteration 146, loss = 0.04884201\n",
      "Iteration 147, loss = 0.04877177\n",
      "Iteration 148, loss = 0.04866526\n",
      "Iteration 149, loss = 0.04861782\n",
      "Iteration 150, loss = 0.04853029\n",
      "Iteration 151, loss = 0.04847250\n",
      "Iteration 152, loss = 0.04838334\n",
      "Iteration 153, loss = 0.04829650\n",
      "Iteration 154, loss = 0.04825172\n",
      "Iteration 155, loss = 0.04816317\n",
      "Iteration 156, loss = 0.04809797\n",
      "Iteration 157, loss = 0.04802182\n",
      "Iteration 158, loss = 0.04793938\n",
      "Iteration 159, loss = 0.04786223\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.01, batch_size=50, hidden_layer_sizes=(100, 100, 100), solver=sgd; total time= 4.0min\n",
      "Iteration 1, loss = 1.26653149\n",
      "Iteration 2, loss = 0.46152729\n",
      "Iteration 3, loss = 0.37771435\n",
      "Iteration 4, loss = 0.33529322\n",
      "Iteration 5, loss = 0.30510412\n",
      "Iteration 6, loss = 0.28284093\n",
      "Iteration 7, loss = 0.26256977\n",
      "Iteration 8, loss = 0.24756120\n",
      "Iteration 9, loss = 0.23334531\n",
      "Iteration 10, loss = 0.22085864\n",
      "Iteration 11, loss = 0.21028656\n",
      "Iteration 12, loss = 0.20022964\n",
      "Iteration 13, loss = 0.19123651\n",
      "Iteration 14, loss = 0.18392314\n",
      "Iteration 15, loss = 0.17618615\n",
      "Iteration 16, loss = 0.16978300\n",
      "Iteration 17, loss = 0.16321160\n",
      "Iteration 18, loss = 0.15782184\n",
      "Iteration 19, loss = 0.15267919\n",
      "Iteration 20, loss = 0.14764238\n",
      "Iteration 21, loss = 0.14316902\n",
      "Iteration 22, loss = 0.13867037\n",
      "Iteration 23, loss = 0.13492514\n",
      "Iteration 24, loss = 0.13094526\n",
      "Iteration 25, loss = 0.12767160\n",
      "Iteration 26, loss = 0.12452224\n",
      "Iteration 27, loss = 0.12036605\n",
      "Iteration 28, loss = 0.11825511\n",
      "Iteration 29, loss = 0.11509291\n",
      "Iteration 30, loss = 0.11188625\n",
      "Iteration 31, loss = 0.11029854\n",
      "Iteration 32, loss = 0.10755678\n",
      "Iteration 33, loss = 0.10500682\n",
      "Iteration 34, loss = 0.10325091\n",
      "Iteration 35, loss = 0.10115577\n",
      "Iteration 36, loss = 0.09908668\n",
      "Iteration 37, loss = 0.09696995\n",
      "Iteration 38, loss = 0.09491916\n",
      "Iteration 39, loss = 0.09371797\n",
      "Iteration 40, loss = 0.09200692\n",
      "Iteration 41, loss = 0.09019791\n",
      "Iteration 42, loss = 0.08886625\n",
      "Iteration 43, loss = 0.08703417\n",
      "Iteration 44, loss = 0.08607647\n",
      "Iteration 45, loss = 0.08470836\n",
      "Iteration 46, loss = 0.08332561\n",
      "Iteration 47, loss = 0.08185393\n",
      "Iteration 48, loss = 0.08082310\n",
      "Iteration 49, loss = 0.07992988\n",
      "Iteration 50, loss = 0.07835759\n",
      "Iteration 51, loss = 0.07746447\n",
      "Iteration 52, loss = 0.07631701\n",
      "Iteration 53, loss = 0.07564357\n",
      "Iteration 54, loss = 0.07424912\n",
      "Iteration 55, loss = 0.07356538\n",
      "Iteration 56, loss = 0.07260824\n",
      "Iteration 57, loss = 0.07214101\n",
      "Iteration 58, loss = 0.07114503\n",
      "Iteration 59, loss = 0.07058029\n",
      "Iteration 60, loss = 0.06949266\n",
      "Iteration 61, loss = 0.06894741\n",
      "Iteration 62, loss = 0.06836478\n",
      "Iteration 63, loss = 0.06743512\n",
      "Iteration 64, loss = 0.06704439\n",
      "Iteration 65, loss = 0.06638573\n",
      "Iteration 66, loss = 0.06568638\n",
      "Iteration 67, loss = 0.06518785\n",
      "Iteration 68, loss = 0.06477397\n",
      "Iteration 69, loss = 0.06436301\n",
      "Iteration 70, loss = 0.06356954\n",
      "Iteration 71, loss = 0.06306867\n",
      "Iteration 72, loss = 0.06258816\n",
      "Iteration 73, loss = 0.06223995\n",
      "Iteration 74, loss = 0.06216417\n",
      "Iteration 75, loss = 0.06150703\n",
      "Iteration 76, loss = 0.06098472\n",
      "Iteration 77, loss = 0.06047007\n",
      "Iteration 78, loss = 0.06034950\n",
      "Iteration 79, loss = 0.05992376\n",
      "Iteration 80, loss = 0.05944519\n",
      "Iteration 81, loss = 0.05921008\n",
      "Iteration 82, loss = 0.05898145\n",
      "Iteration 83, loss = 0.05848335\n",
      "Iteration 84, loss = 0.05832549\n",
      "Iteration 85, loss = 0.05799659\n",
      "Iteration 86, loss = 0.05773467\n",
      "Iteration 87, loss = 0.05753361\n",
      "Iteration 88, loss = 0.05710660\n",
      "Iteration 89, loss = 0.05690620\n",
      "Iteration 90, loss = 0.05662308\n",
      "Iteration 91, loss = 0.05636085\n",
      "Iteration 92, loss = 0.05627514\n",
      "Iteration 93, loss = 0.05589729\n",
      "Iteration 94, loss = 0.05571843\n",
      "Iteration 95, loss = 0.05550488\n",
      "Iteration 96, loss = 0.05538758\n",
      "Iteration 97, loss = 0.05514410\n",
      "Iteration 98, loss = 0.05489827\n",
      "Iteration 99, loss = 0.05473389\n",
      "Iteration 100, loss = 0.05454781\n",
      "Iteration 101, loss = 0.05439363\n",
      "Iteration 102, loss = 0.05418756\n",
      "Iteration 103, loss = 0.05397263\n",
      "Iteration 104, loss = 0.05391024\n",
      "Iteration 105, loss = 0.05369814\n",
      "Iteration 106, loss = 0.05359739\n",
      "Iteration 107, loss = 0.05346627\n",
      "Iteration 108, loss = 0.05326272\n",
      "Iteration 109, loss = 0.05309674\n",
      "Iteration 110, loss = 0.05304515\n",
      "Iteration 111, loss = 0.05287583\n",
      "Iteration 112, loss = 0.05273880\n",
      "Iteration 113, loss = 0.05258820\n",
      "Iteration 114, loss = 0.05251874\n",
      "Iteration 115, loss = 0.05230399\n",
      "Iteration 116, loss = 0.05226114\n",
      "Iteration 117, loss = 0.05211491\n",
      "Iteration 118, loss = 0.05201780\n",
      "Iteration 119, loss = 0.05187896\n",
      "Iteration 120, loss = 0.05176584\n",
      "Iteration 121, loss = 0.05165880\n",
      "Iteration 122, loss = 0.05154786\n",
      "Iteration 123, loss = 0.05150160\n",
      "Iteration 124, loss = 0.05130831\n",
      "Iteration 125, loss = 0.05124997\n",
      "Iteration 126, loss = 0.05115372\n",
      "Iteration 127, loss = 0.05103619\n",
      "Iteration 128, loss = 0.05091983\n",
      "Iteration 129, loss = 0.05076198\n",
      "Iteration 130, loss = 0.05077447\n",
      "Iteration 131, loss = 0.05062412\n",
      "Iteration 132, loss = 0.05051854\n",
      "Iteration 133, loss = 0.05042815\n",
      "Iteration 134, loss = 0.05034232\n",
      "Iteration 135, loss = 0.05026622\n",
      "Iteration 136, loss = 0.05017079\n",
      "Iteration 137, loss = 0.05006284\n",
      "Iteration 138, loss = 0.04997454\n",
      "Iteration 139, loss = 0.04990185\n",
      "Iteration 140, loss = 0.04982706\n",
      "Iteration 141, loss = 0.04973416\n",
      "Iteration 142, loss = 0.04963611\n",
      "Iteration 143, loss = 0.04955713\n",
      "Iteration 144, loss = 0.04948426\n",
      "Iteration 145, loss = 0.04939602\n",
      "Iteration 146, loss = 0.04930826\n",
      "Iteration 147, loss = 0.04920982\n",
      "Iteration 148, loss = 0.04914457\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.01, batch_size=50, hidden_layer_sizes=(100, 100, 100), solver=sgd; total time= 3.7min\n",
      "Iteration 1, loss = 0.44990996\n",
      "Iteration 2, loss = 0.21808243\n",
      "Iteration 3, loss = 0.16743526\n",
      "Iteration 4, loss = 0.13890515\n",
      "Iteration 5, loss = 0.11790365\n",
      "Iteration 6, loss = 0.10385272\n",
      "Iteration 7, loss = 0.09393510\n",
      "Iteration 8, loss = 0.08342959\n",
      "Iteration 9, loss = 0.07700788\n",
      "Iteration 10, loss = 0.07172572\n",
      "Iteration 11, loss = 0.06628927\n",
      "Iteration 12, loss = 0.06262375\n",
      "Iteration 13, loss = 0.05941025\n",
      "Iteration 14, loss = 0.05610048\n",
      "Iteration 15, loss = 0.05321650\n",
      "Iteration 16, loss = 0.05079687\n",
      "Iteration 17, loss = 0.05033010\n",
      "Iteration 18, loss = 0.04731889\n",
      "Iteration 19, loss = 0.04560541\n",
      "Iteration 20, loss = 0.04467813\n",
      "Iteration 21, loss = 0.04408702\n",
      "Iteration 22, loss = 0.04243127\n",
      "Iteration 23, loss = 0.04181261\n",
      "Iteration 24, loss = 0.04151378\n",
      "Iteration 25, loss = 0.04260611\n",
      "Iteration 26, loss = 0.04155108\n",
      "Iteration 27, loss = 0.04038723\n",
      "Iteration 28, loss = 0.03956646\n",
      "Iteration 29, loss = 0.03847609\n",
      "Iteration 30, loss = 0.03732964\n",
      "Iteration 31, loss = 0.03860366\n",
      "Iteration 32, loss = 0.03772214\n",
      "Iteration 33, loss = 0.03808902\n",
      "Iteration 34, loss = 0.03778769\n",
      "Iteration 35, loss = 0.03643226\n",
      "Iteration 36, loss = 0.03802179\n",
      "Iteration 37, loss = 0.03853018\n",
      "Iteration 38, loss = 0.03708072\n",
      "Iteration 39, loss = 0.03759909\n",
      "Iteration 40, loss = 0.03874777\n",
      "Iteration 41, loss = 0.03462040\n",
      "Iteration 42, loss = 0.03384750\n",
      "Iteration 43, loss = 0.03931210\n",
      "Iteration 44, loss = 0.03613686\n",
      "Iteration 45, loss = 0.03374609\n",
      "Iteration 46, loss = 0.03599775\n",
      "Iteration 47, loss = 0.03784671\n",
      "Iteration 48, loss = 0.03564129\n",
      "Iteration 49, loss = 0.03418618\n",
      "Iteration 50, loss = 0.03545116\n",
      "Iteration 51, loss = 0.03491697\n",
      "Iteration 52, loss = 0.03500594\n",
      "Iteration 53, loss = 0.03683184\n",
      "Iteration 54, loss = 0.03388962\n",
      "Iteration 55, loss = 0.03399724\n",
      "Iteration 56, loss = 0.03406551\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.01, batch_size=100, hidden_layer_sizes=(100,), solver=adam; total time= 1.3min\n",
      "Iteration 1, loss = 0.45130303\n",
      "Iteration 2, loss = 0.22006191\n",
      "Iteration 3, loss = 0.16977243\n",
      "Iteration 4, loss = 0.14197530\n",
      "Iteration 5, loss = 0.12194746\n",
      "Iteration 6, loss = 0.10797433\n",
      "Iteration 7, loss = 0.09567974\n",
      "Iteration 8, loss = 0.08776504\n",
      "Iteration 9, loss = 0.07951502\n",
      "Iteration 10, loss = 0.07422922\n",
      "Iteration 11, loss = 0.06885481\n",
      "Iteration 12, loss = 0.06484765\n",
      "Iteration 13, loss = 0.06175355\n",
      "Iteration 14, loss = 0.05821266\n",
      "Iteration 15, loss = 0.05500042\n",
      "Iteration 16, loss = 0.05391477\n",
      "Iteration 17, loss = 0.05073627\n",
      "Iteration 18, loss = 0.04919880\n",
      "Iteration 19, loss = 0.04787596\n",
      "Iteration 20, loss = 0.04694038\n",
      "Iteration 21, loss = 0.04686806\n",
      "Iteration 22, loss = 0.04557807\n",
      "Iteration 23, loss = 0.04322516\n",
      "Iteration 24, loss = 0.04212625\n",
      "Iteration 25, loss = 0.04132710\n",
      "Iteration 26, loss = 0.04216155\n",
      "Iteration 27, loss = 0.04111963\n",
      "Iteration 28, loss = 0.03928874\n",
      "Iteration 29, loss = 0.04129746\n",
      "Iteration 30, loss = 0.04167481\n",
      "Iteration 31, loss = 0.03964903\n",
      "Iteration 32, loss = 0.03745994\n",
      "Iteration 33, loss = 0.03820162\n",
      "Iteration 34, loss = 0.03753971\n",
      "Iteration 35, loss = 0.03884779\n",
      "Iteration 36, loss = 0.03854352\n",
      "Iteration 37, loss = 0.03693491\n",
      "Iteration 38, loss = 0.03846143\n",
      "Iteration 39, loss = 0.03716013\n",
      "Iteration 40, loss = 0.03703241\n",
      "Iteration 41, loss = 0.03891301\n",
      "Iteration 42, loss = 0.03653819\n",
      "Iteration 43, loss = 0.03438931\n",
      "Iteration 44, loss = 0.03839487\n",
      "Iteration 45, loss = 0.03590213\n",
      "Iteration 46, loss = 0.03660912\n",
      "Iteration 47, loss = 0.03615665\n",
      "Iteration 48, loss = 0.03581124\n",
      "Iteration 49, loss = 0.03372760\n",
      "Iteration 50, loss = 0.03565410\n",
      "Iteration 51, loss = 0.04004217\n",
      "Iteration 52, loss = 0.03529714\n",
      "Iteration 53, loss = 0.03559341\n",
      "Iteration 54, loss = 0.03490835\n",
      "Iteration 55, loss = 0.03495140\n",
      "Iteration 56, loss = 0.03379826\n",
      "Iteration 57, loss = 0.03329265\n",
      "Iteration 58, loss = 0.03617769\n",
      "Iteration 59, loss = 0.03867124\n",
      "Iteration 60, loss = 0.03493583\n",
      "Iteration 61, loss = 0.03351207\n",
      "Iteration 62, loss = 0.03529608\n",
      "Iteration 63, loss = 0.03473520\n",
      "Iteration 64, loss = 0.03441199\n",
      "Iteration 65, loss = 0.03715788\n",
      "Iteration 66, loss = 0.03245329\n",
      "Iteration 67, loss = 0.03312833\n",
      "Iteration 68, loss = 0.03441286\n",
      "Iteration 69, loss = 0.03383798\n",
      "Iteration 70, loss = 0.03490549\n",
      "Iteration 71, loss = 0.03452007\n",
      "Iteration 72, loss = 0.03469458\n",
      "Iteration 73, loss = 0.03453106\n",
      "Iteration 74, loss = 0.03264553\n",
      "Iteration 75, loss = 0.03185230\n",
      "Iteration 76, loss = 0.03529523\n",
      "Iteration 77, loss = 0.03442814\n",
      "Iteration 78, loss = 0.03427926\n",
      "Iteration 79, loss = 0.03203547\n",
      "Iteration 80, loss = 0.03157157\n",
      "Iteration 81, loss = 0.03485755\n",
      "Iteration 82, loss = 0.03618780\n",
      "Iteration 83, loss = 0.03440219\n",
      "Iteration 84, loss = 0.03252019\n",
      "Iteration 85, loss = 0.03320090\n",
      "Iteration 86, loss = 0.03468129\n",
      "Iteration 87, loss = 0.03179043\n",
      "Iteration 88, loss = 0.03375673\n",
      "Iteration 89, loss = 0.03375235\n",
      "Iteration 90, loss = 0.03527025\n",
      "Iteration 91, loss = 0.03215837\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.01, batch_size=100, hidden_layer_sizes=(100,), solver=adam; total time= 1.4min\n",
      "Iteration 1, loss = 0.44177756\n",
      "Iteration 2, loss = 0.22864642\n",
      "Iteration 3, loss = 0.17967337\n",
      "Iteration 4, loss = 0.14898225\n",
      "Iteration 5, loss = 0.12717797\n",
      "Iteration 6, loss = 0.11170196\n",
      "Iteration 7, loss = 0.09891782\n",
      "Iteration 8, loss = 0.08909278\n",
      "Iteration 9, loss = 0.08259246\n",
      "Iteration 10, loss = 0.07567179\n",
      "Iteration 11, loss = 0.06927699\n",
      "Iteration 12, loss = 0.06517552\n",
      "Iteration 13, loss = 0.06165300\n",
      "Iteration 14, loss = 0.05773520\n",
      "Iteration 15, loss = 0.05463972\n",
      "Iteration 16, loss = 0.05216040\n",
      "Iteration 17, loss = 0.04982932\n",
      "Iteration 18, loss = 0.04796401\n",
      "Iteration 19, loss = 0.04742120\n",
      "Iteration 20, loss = 0.04512239\n",
      "Iteration 21, loss = 0.04392936\n",
      "Iteration 22, loss = 0.04348836\n",
      "Iteration 23, loss = 0.04383105\n",
      "Iteration 24, loss = 0.04206771\n",
      "Iteration 25, loss = 0.04051460\n",
      "Iteration 26, loss = 0.04095005\n",
      "Iteration 27, loss = 0.03921018\n",
      "Iteration 28, loss = 0.03978199\n",
      "Iteration 29, loss = 0.03964426\n",
      "Iteration 30, loss = 0.03817132\n",
      "Iteration 31, loss = 0.03769333\n",
      "Iteration 32, loss = 0.03845686\n",
      "Iteration 33, loss = 0.03951153\n",
      "Iteration 34, loss = 0.04038903\n",
      "Iteration 35, loss = 0.03780305\n",
      "Iteration 36, loss = 0.03526553\n",
      "Iteration 37, loss = 0.03617243\n",
      "Iteration 38, loss = 0.03633831\n",
      "Iteration 39, loss = 0.03708114\n",
      "Iteration 40, loss = 0.03627272\n",
      "Iteration 41, loss = 0.03668966\n",
      "Iteration 42, loss = 0.03745545\n",
      "Iteration 43, loss = 0.03631692\n",
      "Iteration 44, loss = 0.03689877\n",
      "Iteration 45, loss = 0.03462931\n",
      "Iteration 46, loss = 0.03349500\n",
      "Iteration 47, loss = 0.03588560\n",
      "Iteration 48, loss = 0.03574489\n",
      "Iteration 49, loss = 0.03664682\n",
      "Iteration 50, loss = 0.03519193\n",
      "Iteration 51, loss = 0.03454997\n",
      "Iteration 52, loss = 0.03394886\n",
      "Iteration 53, loss = 0.03340426\n",
      "Iteration 54, loss = 0.03584685\n",
      "Iteration 55, loss = 0.03550006\n",
      "Iteration 56, loss = 0.03564704\n",
      "Iteration 57, loss = 0.03313542\n",
      "Iteration 58, loss = 0.03321932\n",
      "Iteration 59, loss = 0.03596973\n",
      "Iteration 60, loss = 0.03736506\n",
      "Iteration 61, loss = 0.03366824\n",
      "Iteration 62, loss = 0.03281250\n",
      "Iteration 63, loss = 0.03197674\n",
      "Iteration 64, loss = 0.03460053\n",
      "Iteration 65, loss = 0.03805557\n",
      "Iteration 66, loss = 0.03362230\n",
      "Iteration 67, loss = 0.03198460\n",
      "Iteration 68, loss = 0.03140252\n",
      "Iteration 69, loss = 0.03372273\n",
      "Iteration 70, loss = 0.03541937\n",
      "Iteration 71, loss = 0.03688340\n",
      "Iteration 72, loss = 0.03391182\n",
      "Iteration 73, loss = 0.03222531\n",
      "Iteration 74, loss = 0.03175146\n",
      "Iteration 75, loss = 0.03177638\n",
      "Iteration 76, loss = 0.03150815\n",
      "Iteration 77, loss = 0.03620872\n",
      "Iteration 78, loss = 0.03743438\n",
      "Iteration 79, loss = 0.03362859\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.01, batch_size=100, hidden_layer_sizes=(100,), solver=adam; total time= 1.7min\n",
      "Iteration 1, loss = 0.45886808\n",
      "Iteration 2, loss = 0.22601883\n",
      "Iteration 3, loss = 0.17284016\n",
      "Iteration 4, loss = 0.14422408\n",
      "Iteration 5, loss = 0.12169283\n",
      "Iteration 6, loss = 0.10773653\n",
      "Iteration 7, loss = 0.09569752\n",
      "Iteration 8, loss = 0.08655494\n",
      "Iteration 9, loss = 0.07841836\n",
      "Iteration 10, loss = 0.07333995\n",
      "Iteration 11, loss = 0.06818888\n",
      "Iteration 12, loss = 0.06310390\n",
      "Iteration 13, loss = 0.06109021\n",
      "Iteration 14, loss = 0.05727405\n",
      "Iteration 15, loss = 0.05378013\n",
      "Iteration 16, loss = 0.05165572\n",
      "Iteration 17, loss = 0.04913246\n",
      "Iteration 18, loss = 0.04834778\n",
      "Iteration 19, loss = 0.04691287\n",
      "Iteration 20, loss = 0.04616178\n",
      "Iteration 21, loss = 0.04396104\n",
      "Iteration 22, loss = 0.04416584\n",
      "Iteration 23, loss = 0.04276896\n",
      "Iteration 24, loss = 0.04254396\n",
      "Iteration 25, loss = 0.04061924\n",
      "Iteration 26, loss = 0.04033701\n",
      "Iteration 27, loss = 0.04009743\n",
      "Iteration 28, loss = 0.04132624\n",
      "Iteration 29, loss = 0.03946993\n",
      "Iteration 30, loss = 0.03844613\n",
      "Iteration 31, loss = 0.03804584\n",
      "Iteration 32, loss = 0.03746270\n",
      "Iteration 33, loss = 0.03933236\n",
      "Iteration 34, loss = 0.03812002\n",
      "Iteration 35, loss = 0.03805278\n",
      "Iteration 36, loss = 0.03802987\n",
      "Iteration 37, loss = 0.03778051\n",
      "Iteration 38, loss = 0.03632963\n",
      "Iteration 39, loss = 0.03609967\n",
      "Iteration 40, loss = 0.03826291\n",
      "Iteration 41, loss = 0.03735040\n",
      "Iteration 42, loss = 0.03487120\n",
      "Iteration 43, loss = 0.03388235\n",
      "Iteration 44, loss = 0.03635328\n",
      "Iteration 45, loss = 0.04151044\n",
      "Iteration 46, loss = 0.03577876\n",
      "Iteration 47, loss = 0.03385586\n",
      "Iteration 48, loss = 0.03385401\n",
      "Iteration 49, loss = 0.03725535\n",
      "Iteration 50, loss = 0.03628310\n",
      "Iteration 51, loss = 0.03445180\n",
      "Iteration 52, loss = 0.03360504\n",
      "Iteration 53, loss = 0.03550598\n",
      "Iteration 54, loss = 0.03442581\n",
      "Iteration 55, loss = 0.03856857\n",
      "Iteration 56, loss = 0.03473565\n",
      "Iteration 57, loss = 0.03306010\n",
      "Iteration 58, loss = 0.03579949\n",
      "Iteration 59, loss = 0.03521687\n",
      "Iteration 60, loss = 0.03448907\n",
      "Iteration 61, loss = 0.03568421\n",
      "Iteration 62, loss = 0.03292017\n",
      "Iteration 63, loss = 0.03335284\n",
      "Iteration 64, loss = 0.03908030\n",
      "Iteration 65, loss = 0.03445335\n",
      "Iteration 66, loss = 0.03189621\n",
      "Iteration 67, loss = 0.03208962\n",
      "Iteration 68, loss = 0.04085917\n",
      "Iteration 69, loss = 0.03542604\n",
      "Iteration 70, loss = 0.03194853\n",
      "Iteration 71, loss = 0.03203056\n",
      "Iteration 72, loss = 0.03508210\n",
      "Iteration 73, loss = 0.03589994\n",
      "Iteration 74, loss = 0.03480943\n",
      "Iteration 75, loss = 0.03217632\n",
      "Iteration 76, loss = 0.03738301\n",
      "Iteration 77, loss = 0.03307424\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.01, batch_size=100, hidden_layer_sizes=(100,), solver=adam; total time= 1.6min\n",
      "Iteration 1, loss = 0.46984273\n",
      "Iteration 2, loss = 0.22268983\n",
      "Iteration 3, loss = 0.17085874\n",
      "Iteration 4, loss = 0.14362304\n",
      "Iteration 5, loss = 0.12277951\n",
      "Iteration 6, loss = 0.10828976\n",
      "Iteration 7, loss = 0.09654383\n",
      "Iteration 8, loss = 0.08951288\n",
      "Iteration 9, loss = 0.08160365\n",
      "Iteration 10, loss = 0.07520534\n",
      "Iteration 11, loss = 0.06901465\n",
      "Iteration 12, loss = 0.06578273\n",
      "Iteration 13, loss = 0.06140584\n",
      "Iteration 14, loss = 0.05811536\n",
      "Iteration 15, loss = 0.05466093\n",
      "Iteration 16, loss = 0.05348370\n",
      "Iteration 17, loss = 0.05064081\n",
      "Iteration 18, loss = 0.04871106\n",
      "Iteration 19, loss = 0.04810000\n",
      "Iteration 20, loss = 0.04595921\n",
      "Iteration 21, loss = 0.04437830\n",
      "Iteration 22, loss = 0.04438983\n",
      "Iteration 23, loss = 0.04300103\n",
      "Iteration 24, loss = 0.04300813\n",
      "Iteration 25, loss = 0.04168889\n",
      "Iteration 26, loss = 0.04145317\n",
      "Iteration 27, loss = 0.04049212\n",
      "Iteration 28, loss = 0.03999188\n",
      "Iteration 29, loss = 0.04059045\n",
      "Iteration 30, loss = 0.03937413\n",
      "Iteration 31, loss = 0.03820103\n",
      "Iteration 32, loss = 0.03938506\n",
      "Iteration 33, loss = 0.03942167\n",
      "Iteration 34, loss = 0.03736230\n",
      "Iteration 35, loss = 0.03863933\n",
      "Iteration 36, loss = 0.03695040\n",
      "Iteration 37, loss = 0.03785627\n",
      "Iteration 38, loss = 0.03727168\n",
      "Iteration 39, loss = 0.03678079\n",
      "Iteration 40, loss = 0.03659649\n",
      "Iteration 41, loss = 0.03813930\n",
      "Iteration 42, loss = 0.03610146\n",
      "Iteration 43, loss = 0.03619907\n",
      "Iteration 44, loss = 0.03596996\n",
      "Iteration 45, loss = 0.03640144\n",
      "Iteration 46, loss = 0.03669620\n",
      "Iteration 47, loss = 0.03484871\n",
      "Iteration 48, loss = 0.03517609\n",
      "Iteration 49, loss = 0.03505194\n",
      "Iteration 50, loss = 0.04028672\n",
      "Iteration 51, loss = 0.03572884\n",
      "Iteration 52, loss = 0.03381292\n",
      "Iteration 53, loss = 0.03287730\n",
      "Iteration 54, loss = 0.03422344\n",
      "Iteration 55, loss = 0.03504982\n",
      "Iteration 56, loss = 0.03605926\n",
      "Iteration 57, loss = 0.03580404\n",
      "Iteration 58, loss = 0.03404256\n",
      "Iteration 59, loss = 0.03348569\n",
      "Iteration 60, loss = 0.03512238\n",
      "Iteration 61, loss = 0.03493312\n",
      "Iteration 62, loss = 0.03552706\n",
      "Iteration 63, loss = 0.03870920\n",
      "Iteration 64, loss = 0.03342828\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.01, batch_size=100, hidden_layer_sizes=(100,), solver=adam; total time= 1.5min\n",
      "Iteration 1, loss = 1.44394732\n",
      "Iteration 2, loss = 0.68740655\n",
      "Iteration 3, loss = 0.52156611\n",
      "Iteration 4, loss = 0.45190193\n",
      "Iteration 5, loss = 0.41185112\n",
      "Iteration 6, loss = 0.38545858\n",
      "Iteration 7, loss = 0.36595767\n",
      "Iteration 8, loss = 0.35095850\n",
      "Iteration 9, loss = 0.33863727\n",
      "Iteration 10, loss = 0.32816089\n",
      "Iteration 11, loss = 0.31921915\n",
      "Iteration 12, loss = 0.31088627\n",
      "Iteration 13, loss = 0.30389038\n",
      "Iteration 14, loss = 0.29727268\n",
      "Iteration 15, loss = 0.29136275\n",
      "Iteration 16, loss = 0.28531219\n",
      "Iteration 17, loss = 0.28003293\n",
      "Iteration 18, loss = 0.27494499\n",
      "Iteration 19, loss = 0.27017739\n",
      "Iteration 20, loss = 0.26569448\n",
      "Iteration 21, loss = 0.26122169\n",
      "Iteration 22, loss = 0.25704147\n",
      "Iteration 23, loss = 0.25305574\n",
      "Iteration 24, loss = 0.24908426\n",
      "Iteration 25, loss = 0.24535873\n",
      "Iteration 26, loss = 0.24169725\n",
      "Iteration 27, loss = 0.23812435\n",
      "Iteration 28, loss = 0.23467255\n",
      "Iteration 29, loss = 0.23156765\n",
      "Iteration 30, loss = 0.22841308\n",
      "Iteration 31, loss = 0.22513191\n",
      "Iteration 32, loss = 0.22229843\n",
      "Iteration 33, loss = 0.21937053\n",
      "Iteration 34, loss = 0.21665498\n",
      "Iteration 35, loss = 0.21380253\n",
      "Iteration 36, loss = 0.21107352\n",
      "Iteration 37, loss = 0.20848303\n",
      "Iteration 38, loss = 0.20599391\n",
      "Iteration 39, loss = 0.20342469\n",
      "Iteration 40, loss = 0.20112841\n",
      "Iteration 41, loss = 0.19872688\n",
      "Iteration 42, loss = 0.19636264\n",
      "Iteration 43, loss = 0.19411624\n",
      "Iteration 44, loss = 0.19202769\n",
      "Iteration 45, loss = 0.18992839\n",
      "Iteration 46, loss = 0.18772293\n",
      "Iteration 47, loss = 0.18586748\n",
      "Iteration 48, loss = 0.18394178\n",
      "Iteration 49, loss = 0.18199893\n",
      "Iteration 50, loss = 0.17999496\n",
      "Iteration 51, loss = 0.17834938\n",
      "Iteration 52, loss = 0.17653962\n",
      "Iteration 53, loss = 0.17490862\n",
      "Iteration 54, loss = 0.17312175\n",
      "Iteration 55, loss = 0.17135002\n",
      "Iteration 56, loss = 0.16973458\n",
      "Iteration 57, loss = 0.16817737\n",
      "Iteration 58, loss = 0.16664685\n",
      "Iteration 59, loss = 0.16499118\n",
      "Iteration 60, loss = 0.16337890\n",
      "Iteration 61, loss = 0.16202812\n",
      "Iteration 62, loss = 0.16054480\n",
      "Iteration 63, loss = 0.15913274\n",
      "Iteration 64, loss = 0.15773469\n",
      "Iteration 65, loss = 0.15629672\n",
      "Iteration 66, loss = 0.15491408\n",
      "Iteration 67, loss = 0.15359152\n",
      "Iteration 68, loss = 0.15225674\n",
      "Iteration 69, loss = 0.15098522\n",
      "Iteration 70, loss = 0.14977253\n",
      "Iteration 71, loss = 0.14841312\n",
      "Iteration 72, loss = 0.14730557\n",
      "Iteration 73, loss = 0.14614413\n",
      "Iteration 74, loss = 0.14482923\n",
      "Iteration 75, loss = 0.14371814\n",
      "Iteration 76, loss = 0.14266277\n",
      "Iteration 77, loss = 0.14147096\n",
      "Iteration 78, loss = 0.14027993\n",
      "Iteration 79, loss = 0.13928512\n",
      "Iteration 80, loss = 0.13817452\n",
      "Iteration 81, loss = 0.13713546\n",
      "Iteration 82, loss = 0.13615477\n",
      "Iteration 83, loss = 0.13509090\n",
      "Iteration 84, loss = 0.13408957\n",
      "Iteration 85, loss = 0.13313153\n",
      "Iteration 86, loss = 0.13205894\n",
      "Iteration 87, loss = 0.13121654\n",
      "Iteration 88, loss = 0.13034895\n",
      "Iteration 89, loss = 0.12926881\n",
      "Iteration 90, loss = 0.12839499\n",
      "Iteration 91, loss = 0.12744391\n",
      "Iteration 92, loss = 0.12663364\n",
      "Iteration 93, loss = 0.12571745\n",
      "Iteration 94, loss = 0.12497863\n",
      "Iteration 95, loss = 0.12396960\n",
      "Iteration 96, loss = 0.12325181\n",
      "Iteration 97, loss = 0.12245425\n",
      "Iteration 98, loss = 0.12170791\n",
      "Iteration 99, loss = 0.12080614\n",
      "Iteration 100, loss = 0.11999967\n",
      "Iteration 101, loss = 0.11927509\n",
      "Iteration 102, loss = 0.11843174\n",
      "Iteration 103, loss = 0.11772718\n",
      "Iteration 104, loss = 0.11690741\n",
      "Iteration 105, loss = 0.11625346\n",
      "Iteration 106, loss = 0.11559483\n",
      "Iteration 107, loss = 0.11488889\n",
      "Iteration 108, loss = 0.11411583\n",
      "Iteration 109, loss = 0.11345441\n",
      "Iteration 110, loss = 0.11274910\n",
      "Iteration 111, loss = 0.11217590\n",
      "Iteration 112, loss = 0.11142348\n",
      "Iteration 113, loss = 0.11086536\n",
      "Iteration 114, loss = 0.11012747\n",
      "Iteration 115, loss = 0.10950285\n",
      "Iteration 116, loss = 0.10886950\n",
      "Iteration 117, loss = 0.10838521\n",
      "Iteration 118, loss = 0.10770724\n",
      "Iteration 119, loss = 0.10712951\n",
      "Iteration 120, loss = 0.10647884\n",
      "Iteration 121, loss = 0.10588891\n",
      "Iteration 122, loss = 0.10538958\n",
      "Iteration 123, loss = 0.10477826\n",
      "Iteration 124, loss = 0.10421644\n",
      "Iteration 125, loss = 0.10362597\n",
      "Iteration 126, loss = 0.10306970\n",
      "Iteration 127, loss = 0.10252029\n",
      "Iteration 128, loss = 0.10209018\n",
      "Iteration 129, loss = 0.10157303\n",
      "Iteration 130, loss = 0.10097348\n",
      "Iteration 131, loss = 0.10046961\n",
      "Iteration 132, loss = 0.09994830\n",
      "Iteration 133, loss = 0.09934738\n",
      "Iteration 134, loss = 0.09894483\n",
      "Iteration 135, loss = 0.09847507\n",
      "Iteration 136, loss = 0.09804676\n",
      "Iteration 137, loss = 0.09751511\n",
      "Iteration 138, loss = 0.09702264\n",
      "Iteration 139, loss = 0.09651671\n",
      "Iteration 140, loss = 0.09607059\n",
      "Iteration 141, loss = 0.09561053\n",
      "Iteration 142, loss = 0.09519180\n",
      "Iteration 143, loss = 0.09468279\n",
      "Iteration 144, loss = 0.09425855\n",
      "Iteration 145, loss = 0.09380335\n",
      "Iteration 146, loss = 0.09336433\n",
      "Iteration 147, loss = 0.09293803\n",
      "Iteration 148, loss = 0.09255257\n",
      "Iteration 149, loss = 0.09211818\n",
      "Iteration 150, loss = 0.09164226\n",
      "Iteration 151, loss = 0.09127642\n",
      "Iteration 152, loss = 0.09086004\n",
      "Iteration 153, loss = 0.09043992\n",
      "Iteration 154, loss = 0.08999099\n",
      "Iteration 155, loss = 0.08962403\n",
      "Iteration 156, loss = 0.08925235\n",
      "Iteration 157, loss = 0.08887700\n",
      "Iteration 158, loss = 0.08850947\n",
      "Iteration 159, loss = 0.08811651\n",
      "Iteration 160, loss = 0.08773329\n",
      "Iteration 161, loss = 0.08728911\n",
      "Iteration 162, loss = 0.08698209\n",
      "Iteration 163, loss = 0.08662588\n",
      "Iteration 164, loss = 0.08626709\n",
      "Iteration 165, loss = 0.08583813\n",
      "Iteration 166, loss = 0.08551375\n",
      "Iteration 167, loss = 0.08517658\n",
      "Iteration 168, loss = 0.08486132\n",
      "Iteration 169, loss = 0.08449339\n",
      "Iteration 170, loss = 0.08416286\n",
      "Iteration 171, loss = 0.08377063\n",
      "Iteration 172, loss = 0.08348569\n",
      "Iteration 173, loss = 0.08316017\n",
      "Iteration 174, loss = 0.08274950\n",
      "Iteration 175, loss = 0.08245710\n",
      "Iteration 176, loss = 0.08217116\n",
      "Iteration 177, loss = 0.08181923\n",
      "Iteration 178, loss = 0.08147401\n",
      "Iteration 179, loss = 0.08117647\n",
      "Iteration 180, loss = 0.08091938\n",
      "Iteration 181, loss = 0.08052324\n",
      "Iteration 182, loss = 0.08016753\n",
      "Iteration 183, loss = 0.07985371\n",
      "Iteration 184, loss = 0.07963134\n",
      "Iteration 185, loss = 0.07935598\n",
      "Iteration 186, loss = 0.07908406\n",
      "Iteration 187, loss = 0.07876757\n",
      "Iteration 188, loss = 0.07846229\n",
      "Iteration 189, loss = 0.07810991\n",
      "Iteration 190, loss = 0.07789173\n",
      "Iteration 191, loss = 0.07768337\n",
      "Iteration 192, loss = 0.07733364\n",
      "Iteration 193, loss = 0.07701221\n",
      "Iteration 194, loss = 0.07670425\n",
      "Iteration 195, loss = 0.07652868\n",
      "Iteration 196, loss = 0.07623090\n",
      "Iteration 197, loss = 0.07590786\n",
      "Iteration 198, loss = 0.07569599\n",
      "Iteration 199, loss = 0.07545351\n",
      "Iteration 200, loss = 0.07512390\n",
      "[CV] END activation=relu, alpha=0.01, batch_size=100, hidden_layer_sizes=(100,), solver=sgd; total time= 1.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.48047401\n",
      "Iteration 2, loss = 0.69641013\n",
      "Iteration 3, loss = 0.52685124\n",
      "Iteration 4, loss = 0.45665270\n",
      "Iteration 5, loss = 0.41654744\n",
      "Iteration 6, loss = 0.38962829\n",
      "Iteration 7, loss = 0.36991684\n",
      "Iteration 8, loss = 0.35456314\n",
      "Iteration 9, loss = 0.34194234\n",
      "Iteration 10, loss = 0.33141519\n",
      "Iteration 11, loss = 0.32204343\n",
      "Iteration 12, loss = 0.31395893\n",
      "Iteration 13, loss = 0.30633036\n",
      "Iteration 14, loss = 0.29966891\n",
      "Iteration 15, loss = 0.29364178\n",
      "Iteration 16, loss = 0.28773692\n",
      "Iteration 17, loss = 0.28227781\n",
      "Iteration 18, loss = 0.27744087\n",
      "Iteration 19, loss = 0.27262190\n",
      "Iteration 20, loss = 0.26794894\n",
      "Iteration 21, loss = 0.26367611\n",
      "Iteration 22, loss = 0.25962569\n",
      "Iteration 23, loss = 0.25557959\n",
      "Iteration 24, loss = 0.25186782\n",
      "Iteration 25, loss = 0.24820519\n",
      "Iteration 26, loss = 0.24471873\n",
      "Iteration 27, loss = 0.24138760\n",
      "Iteration 28, loss = 0.23828197\n",
      "Iteration 29, loss = 0.23510275\n",
      "Iteration 30, loss = 0.23213313\n",
      "Iteration 31, loss = 0.22928523\n",
      "Iteration 32, loss = 0.22638502\n",
      "Iteration 33, loss = 0.22354443\n",
      "Iteration 34, loss = 0.22087966\n",
      "Iteration 35, loss = 0.21826356\n",
      "Iteration 36, loss = 0.21579519\n",
      "Iteration 37, loss = 0.21323358\n",
      "Iteration 38, loss = 0.21085919\n",
      "Iteration 39, loss = 0.20841579\n",
      "Iteration 40, loss = 0.20616133\n",
      "Iteration 41, loss = 0.20374329\n",
      "Iteration 42, loss = 0.20172288\n",
      "Iteration 43, loss = 0.19957753\n",
      "Iteration 44, loss = 0.19744994\n",
      "Iteration 45, loss = 0.19537174\n",
      "Iteration 46, loss = 0.19335528\n",
      "Iteration 47, loss = 0.19155475\n",
      "Iteration 48, loss = 0.18962939\n",
      "Iteration 49, loss = 0.18774463\n",
      "Iteration 50, loss = 0.18588009\n",
      "Iteration 51, loss = 0.18414278\n",
      "Iteration 52, loss = 0.18226192\n",
      "Iteration 53, loss = 0.18066206\n",
      "Iteration 54, loss = 0.17890613\n",
      "Iteration 55, loss = 0.17723105\n",
      "Iteration 56, loss = 0.17555066\n",
      "Iteration 57, loss = 0.17403203\n",
      "Iteration 58, loss = 0.17249152\n",
      "Iteration 59, loss = 0.17097362\n",
      "Iteration 60, loss = 0.16940995\n",
      "Iteration 61, loss = 0.16782505\n",
      "Iteration 62, loss = 0.16649062\n",
      "Iteration 63, loss = 0.16503649\n",
      "Iteration 64, loss = 0.16363925\n",
      "Iteration 65, loss = 0.16233594\n",
      "Iteration 66, loss = 0.16088116\n",
      "Iteration 67, loss = 0.15956593\n",
      "Iteration 68, loss = 0.15819839\n",
      "Iteration 69, loss = 0.15697541\n",
      "Iteration 70, loss = 0.15571665\n",
      "Iteration 71, loss = 0.15446851\n",
      "Iteration 72, loss = 0.15317795\n",
      "Iteration 73, loss = 0.15200500\n",
      "Iteration 74, loss = 0.15074751\n",
      "Iteration 75, loss = 0.14953095\n",
      "Iteration 76, loss = 0.14829968\n",
      "Iteration 77, loss = 0.14718944\n",
      "Iteration 78, loss = 0.14619824\n",
      "Iteration 79, loss = 0.14493891\n",
      "Iteration 80, loss = 0.14388417\n",
      "Iteration 81, loss = 0.14280442\n",
      "Iteration 82, loss = 0.14174296\n",
      "Iteration 83, loss = 0.14073205\n",
      "Iteration 84, loss = 0.13970473\n",
      "Iteration 85, loss = 0.13863955\n",
      "Iteration 86, loss = 0.13771303\n",
      "Iteration 87, loss = 0.13661512\n",
      "Iteration 88, loss = 0.13576528\n",
      "Iteration 89, loss = 0.13473621\n",
      "Iteration 90, loss = 0.13369653\n",
      "Iteration 91, loss = 0.13289562\n",
      "Iteration 92, loss = 0.13196202\n",
      "Iteration 93, loss = 0.13093999\n",
      "Iteration 94, loss = 0.13013595\n",
      "Iteration 95, loss = 0.12933123\n",
      "Iteration 96, loss = 0.12833432\n",
      "Iteration 97, loss = 0.12746861\n",
      "Iteration 98, loss = 0.12668393\n",
      "Iteration 99, loss = 0.12582427\n",
      "Iteration 100, loss = 0.12497043\n",
      "Iteration 101, loss = 0.12417790\n",
      "Iteration 102, loss = 0.12329540\n",
      "Iteration 103, loss = 0.12260925\n",
      "Iteration 104, loss = 0.12179496\n",
      "Iteration 105, loss = 0.12102820\n",
      "Iteration 106, loss = 0.12029103\n",
      "Iteration 107, loss = 0.11948334\n",
      "Iteration 108, loss = 0.11875729\n",
      "Iteration 109, loss = 0.11808496\n",
      "Iteration 110, loss = 0.11730760\n",
      "Iteration 111, loss = 0.11661059\n",
      "Iteration 112, loss = 0.11590798\n",
      "Iteration 113, loss = 0.11521514\n",
      "Iteration 114, loss = 0.11450731\n",
      "Iteration 115, loss = 0.11392881\n",
      "Iteration 116, loss = 0.11323251\n",
      "Iteration 117, loss = 0.11248719\n",
      "Iteration 118, loss = 0.11184208\n",
      "Iteration 119, loss = 0.11117552\n",
      "Iteration 120, loss = 0.11063541\n",
      "Iteration 121, loss = 0.10994299\n",
      "Iteration 122, loss = 0.10928004\n",
      "Iteration 123, loss = 0.10869972\n",
      "Iteration 124, loss = 0.10805420\n",
      "Iteration 125, loss = 0.10735860\n",
      "Iteration 126, loss = 0.10687992\n",
      "Iteration 127, loss = 0.10621136\n",
      "Iteration 128, loss = 0.10552935\n",
      "Iteration 129, loss = 0.10510998\n",
      "Iteration 130, loss = 0.10437239\n",
      "Iteration 131, loss = 0.10389018\n",
      "Iteration 132, loss = 0.10339327\n",
      "Iteration 133, loss = 0.10280254\n",
      "Iteration 134, loss = 0.10228457\n",
      "Iteration 135, loss = 0.10171540\n",
      "Iteration 136, loss = 0.10118682\n",
      "Iteration 137, loss = 0.10071004\n",
      "Iteration 138, loss = 0.10016202\n",
      "Iteration 139, loss = 0.09966837\n",
      "Iteration 140, loss = 0.09915323\n",
      "Iteration 141, loss = 0.09860784\n",
      "Iteration 142, loss = 0.09811908\n",
      "Iteration 143, loss = 0.09756794\n",
      "Iteration 144, loss = 0.09717976\n",
      "Iteration 145, loss = 0.09663515\n",
      "Iteration 146, loss = 0.09619361\n",
      "Iteration 147, loss = 0.09566145\n",
      "Iteration 148, loss = 0.09520678\n",
      "Iteration 149, loss = 0.09477192\n",
      "Iteration 150, loss = 0.09436988\n",
      "Iteration 151, loss = 0.09375054\n",
      "Iteration 152, loss = 0.09348087\n",
      "Iteration 153, loss = 0.09296372\n",
      "Iteration 154, loss = 0.09254415\n",
      "Iteration 155, loss = 0.09204455\n",
      "Iteration 156, loss = 0.09173564\n",
      "Iteration 157, loss = 0.09124963\n",
      "Iteration 158, loss = 0.09086015\n",
      "Iteration 159, loss = 0.09046622\n",
      "Iteration 160, loss = 0.09000606\n",
      "Iteration 161, loss = 0.08961090\n",
      "Iteration 162, loss = 0.08916507\n",
      "Iteration 163, loss = 0.08871007\n",
      "Iteration 164, loss = 0.08833440\n",
      "Iteration 165, loss = 0.08802863\n",
      "Iteration 166, loss = 0.08762822\n",
      "Iteration 167, loss = 0.08720613\n",
      "Iteration 168, loss = 0.08680639\n",
      "Iteration 169, loss = 0.08645269\n",
      "Iteration 170, loss = 0.08610851\n",
      "Iteration 171, loss = 0.08562291\n",
      "Iteration 172, loss = 0.08539605\n",
      "Iteration 173, loss = 0.08503669\n",
      "Iteration 174, loss = 0.08459563\n",
      "Iteration 175, loss = 0.08426587\n",
      "Iteration 176, loss = 0.08396295\n",
      "Iteration 177, loss = 0.08354676\n",
      "Iteration 178, loss = 0.08330019\n",
      "Iteration 179, loss = 0.08281330\n",
      "Iteration 180, loss = 0.08256137\n",
      "Iteration 181, loss = 0.08222476\n",
      "Iteration 182, loss = 0.08180697\n",
      "Iteration 183, loss = 0.08147502\n",
      "Iteration 184, loss = 0.08120471\n",
      "Iteration 185, loss = 0.08087384\n",
      "Iteration 186, loss = 0.08049627\n",
      "Iteration 187, loss = 0.08020593\n",
      "Iteration 188, loss = 0.08002040\n",
      "Iteration 189, loss = 0.07965298\n",
      "Iteration 190, loss = 0.07934367\n",
      "Iteration 191, loss = 0.07895276\n",
      "Iteration 192, loss = 0.07874087\n",
      "Iteration 193, loss = 0.07839366\n",
      "Iteration 194, loss = 0.07804210\n",
      "Iteration 195, loss = 0.07786332\n",
      "Iteration 196, loss = 0.07757281\n",
      "Iteration 197, loss = 0.07725641\n",
      "Iteration 198, loss = 0.07698157\n",
      "Iteration 199, loss = 0.07666943\n",
      "Iteration 200, loss = 0.07635727\n",
      "[CV] END activation=relu, alpha=0.01, batch_size=100, hidden_layer_sizes=(100,), solver=sgd; total time= 1.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.39956638\n",
      "Iteration 2, loss = 0.66836476\n",
      "Iteration 3, loss = 0.51257528\n",
      "Iteration 4, loss = 0.44509403\n",
      "Iteration 5, loss = 0.40591998\n",
      "Iteration 6, loss = 0.37944453\n",
      "Iteration 7, loss = 0.35941936\n",
      "Iteration 8, loss = 0.34382972\n",
      "Iteration 9, loss = 0.33084034\n",
      "Iteration 10, loss = 0.32000672\n",
      "Iteration 11, loss = 0.31007682\n",
      "Iteration 12, loss = 0.30139479\n",
      "Iteration 13, loss = 0.29347006\n",
      "Iteration 14, loss = 0.28649857\n",
      "Iteration 15, loss = 0.27988780\n",
      "Iteration 16, loss = 0.27372590\n",
      "Iteration 17, loss = 0.26789981\n",
      "Iteration 18, loss = 0.26254313\n",
      "Iteration 19, loss = 0.25737625\n",
      "Iteration 20, loss = 0.25242064\n",
      "Iteration 21, loss = 0.24793655\n",
      "Iteration 22, loss = 0.24357812\n",
      "Iteration 23, loss = 0.23924335\n",
      "Iteration 24, loss = 0.23536617\n",
      "Iteration 25, loss = 0.23152884\n",
      "Iteration 26, loss = 0.22775233\n",
      "Iteration 27, loss = 0.22426154\n",
      "Iteration 28, loss = 0.22083511\n",
      "Iteration 29, loss = 0.21757393\n",
      "Iteration 30, loss = 0.21439925\n",
      "Iteration 31, loss = 0.21145865\n",
      "Iteration 32, loss = 0.20845882\n",
      "Iteration 33, loss = 0.20555109\n",
      "Iteration 34, loss = 0.20280608\n",
      "Iteration 35, loss = 0.20022062\n",
      "Iteration 36, loss = 0.19749879\n",
      "Iteration 37, loss = 0.19506122\n",
      "Iteration 38, loss = 0.19250176\n",
      "Iteration 39, loss = 0.19016613\n",
      "Iteration 40, loss = 0.18783228\n",
      "Iteration 41, loss = 0.18561093\n",
      "Iteration 42, loss = 0.18342011\n",
      "Iteration 43, loss = 0.18125275\n",
      "Iteration 44, loss = 0.17924697\n",
      "Iteration 45, loss = 0.17713760\n",
      "Iteration 46, loss = 0.17521365\n",
      "Iteration 47, loss = 0.17318029\n",
      "Iteration 48, loss = 0.17128534\n",
      "Iteration 49, loss = 0.16939450\n",
      "Iteration 50, loss = 0.16774550\n",
      "Iteration 51, loss = 0.16598262\n",
      "Iteration 52, loss = 0.16422220\n",
      "Iteration 53, loss = 0.16253427\n",
      "Iteration 54, loss = 0.16078776\n",
      "Iteration 55, loss = 0.15921380\n",
      "Iteration 56, loss = 0.15775966\n",
      "Iteration 57, loss = 0.15608930\n",
      "Iteration 58, loss = 0.15463137\n",
      "Iteration 59, loss = 0.15319311\n",
      "Iteration 60, loss = 0.15176917\n",
      "Iteration 61, loss = 0.15039759\n",
      "Iteration 62, loss = 0.14905268\n",
      "Iteration 63, loss = 0.14778151\n",
      "Iteration 64, loss = 0.14631028\n",
      "Iteration 65, loss = 0.14514216\n",
      "Iteration 66, loss = 0.14381890\n",
      "Iteration 67, loss = 0.14260774\n",
      "Iteration 68, loss = 0.14129025\n",
      "Iteration 69, loss = 0.14010477\n",
      "Iteration 70, loss = 0.13895135\n",
      "Iteration 71, loss = 0.13781203\n",
      "Iteration 72, loss = 0.13671252\n",
      "Iteration 73, loss = 0.13568525\n",
      "Iteration 74, loss = 0.13454352\n",
      "Iteration 75, loss = 0.13349157\n",
      "Iteration 76, loss = 0.13242182\n",
      "Iteration 77, loss = 0.13134448\n",
      "Iteration 78, loss = 0.13035548\n",
      "Iteration 79, loss = 0.12932244\n",
      "Iteration 80, loss = 0.12836919\n",
      "Iteration 81, loss = 0.12739764\n",
      "Iteration 82, loss = 0.12640538\n",
      "Iteration 83, loss = 0.12550529\n",
      "Iteration 84, loss = 0.12466076\n",
      "Iteration 85, loss = 0.12370092\n",
      "Iteration 86, loss = 0.12279887\n",
      "Iteration 87, loss = 0.12188097\n",
      "Iteration 88, loss = 0.12100621\n",
      "Iteration 89, loss = 0.12031233\n",
      "Iteration 90, loss = 0.11934699\n",
      "Iteration 91, loss = 0.11850522\n",
      "Iteration 92, loss = 0.11770342\n",
      "Iteration 93, loss = 0.11686916\n",
      "Iteration 94, loss = 0.11613550\n",
      "Iteration 95, loss = 0.11538740\n",
      "Iteration 96, loss = 0.11461526\n",
      "Iteration 97, loss = 0.11387534\n",
      "Iteration 98, loss = 0.11320762\n",
      "Iteration 99, loss = 0.11230038\n",
      "Iteration 100, loss = 0.11162860\n",
      "Iteration 101, loss = 0.11097346\n",
      "Iteration 102, loss = 0.11025566\n",
      "Iteration 103, loss = 0.10963945\n",
      "Iteration 104, loss = 0.10890037\n",
      "Iteration 105, loss = 0.10821830\n",
      "Iteration 106, loss = 0.10769693\n",
      "Iteration 107, loss = 0.10693295\n",
      "Iteration 108, loss = 0.10627467\n",
      "Iteration 109, loss = 0.10570734\n",
      "Iteration 110, loss = 0.10504699\n",
      "Iteration 111, loss = 0.10440196\n",
      "Iteration 112, loss = 0.10374557\n",
      "Iteration 113, loss = 0.10322440\n",
      "Iteration 114, loss = 0.10267284\n",
      "Iteration 115, loss = 0.10207833\n",
      "Iteration 116, loss = 0.10149984\n",
      "Iteration 117, loss = 0.10080597\n",
      "Iteration 118, loss = 0.10033005\n",
      "Iteration 119, loss = 0.09980324\n",
      "Iteration 120, loss = 0.09919873\n",
      "Iteration 121, loss = 0.09867059\n",
      "Iteration 122, loss = 0.09818871\n",
      "Iteration 123, loss = 0.09766210\n",
      "Iteration 124, loss = 0.09710093\n",
      "Iteration 125, loss = 0.09657467\n",
      "Iteration 126, loss = 0.09607685\n",
      "Iteration 127, loss = 0.09552305\n",
      "Iteration 128, loss = 0.09513306\n",
      "Iteration 129, loss = 0.09467614\n",
      "Iteration 130, loss = 0.09406244\n",
      "Iteration 131, loss = 0.09360469\n",
      "Iteration 132, loss = 0.09309260\n",
      "Iteration 133, loss = 0.09270136\n",
      "Iteration 134, loss = 0.09216777\n",
      "Iteration 135, loss = 0.09172852\n",
      "Iteration 136, loss = 0.09129619\n",
      "Iteration 137, loss = 0.09082824\n",
      "Iteration 138, loss = 0.09041624\n",
      "Iteration 139, loss = 0.08995242\n",
      "Iteration 140, loss = 0.08957260\n",
      "Iteration 141, loss = 0.08910169\n",
      "Iteration 142, loss = 0.08864919\n",
      "Iteration 143, loss = 0.08831462\n",
      "Iteration 144, loss = 0.08782144\n",
      "Iteration 145, loss = 0.08747347\n",
      "Iteration 146, loss = 0.08701064\n",
      "Iteration 147, loss = 0.08668133\n",
      "Iteration 148, loss = 0.08622123\n",
      "Iteration 149, loss = 0.08581247\n",
      "Iteration 150, loss = 0.08540448\n",
      "Iteration 151, loss = 0.08507920\n",
      "Iteration 152, loss = 0.08464445\n",
      "Iteration 153, loss = 0.08428454\n",
      "Iteration 154, loss = 0.08395414\n",
      "Iteration 155, loss = 0.08358174\n",
      "Iteration 156, loss = 0.08319918\n",
      "Iteration 157, loss = 0.08284944\n",
      "Iteration 158, loss = 0.08248676\n",
      "Iteration 159, loss = 0.08209323\n",
      "Iteration 160, loss = 0.08176809\n",
      "Iteration 161, loss = 0.08141953\n",
      "Iteration 162, loss = 0.08104121\n",
      "Iteration 163, loss = 0.08070966\n",
      "Iteration 164, loss = 0.08039988\n",
      "Iteration 165, loss = 0.07996730\n",
      "Iteration 166, loss = 0.07970027\n",
      "Iteration 167, loss = 0.07941792\n",
      "Iteration 168, loss = 0.07902915\n",
      "Iteration 169, loss = 0.07874087\n",
      "Iteration 170, loss = 0.07844995\n",
      "Iteration 171, loss = 0.07808187\n",
      "Iteration 172, loss = 0.07776788\n",
      "Iteration 173, loss = 0.07752095\n",
      "Iteration 174, loss = 0.07718993\n",
      "Iteration 175, loss = 0.07693579\n",
      "Iteration 176, loss = 0.07661772\n",
      "Iteration 177, loss = 0.07626175\n",
      "Iteration 178, loss = 0.07601881\n",
      "Iteration 179, loss = 0.07569481\n",
      "Iteration 180, loss = 0.07542163\n",
      "Iteration 181, loss = 0.07509849\n",
      "Iteration 182, loss = 0.07487330\n",
      "Iteration 183, loss = 0.07455276\n",
      "Iteration 184, loss = 0.07430855\n",
      "Iteration 185, loss = 0.07403749\n",
      "Iteration 186, loss = 0.07370089\n",
      "Iteration 187, loss = 0.07344926\n",
      "Iteration 188, loss = 0.07318676\n",
      "Iteration 189, loss = 0.07295436\n",
      "Iteration 190, loss = 0.07264099\n",
      "Iteration 191, loss = 0.07243194\n",
      "Iteration 192, loss = 0.07211222\n",
      "Iteration 193, loss = 0.07184625\n",
      "Iteration 194, loss = 0.07166728\n",
      "Iteration 195, loss = 0.07131678\n",
      "Iteration 196, loss = 0.07112474\n",
      "Iteration 197, loss = 0.07084283\n",
      "Iteration 198, loss = 0.07068398\n",
      "Iteration 199, loss = 0.07036454\n",
      "Iteration 200, loss = 0.07019911\n",
      "[CV] END activation=relu, alpha=0.01, batch_size=100, hidden_layer_sizes=(100,), solver=sgd; total time= 1.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.43863489\n",
      "Iteration 2, loss = 0.68170446\n",
      "Iteration 3, loss = 0.51576770\n",
      "Iteration 4, loss = 0.44711414\n",
      "Iteration 5, loss = 0.40820695\n",
      "Iteration 6, loss = 0.38235732\n",
      "Iteration 7, loss = 0.36301981\n",
      "Iteration 8, loss = 0.34809489\n",
      "Iteration 9, loss = 0.33569949\n",
      "Iteration 10, loss = 0.32529254\n",
      "Iteration 11, loss = 0.31606917\n",
      "Iteration 12, loss = 0.30785375\n",
      "Iteration 13, loss = 0.30042545\n",
      "Iteration 14, loss = 0.29368662\n",
      "Iteration 15, loss = 0.28701371\n",
      "Iteration 16, loss = 0.28143470\n",
      "Iteration 17, loss = 0.27567642\n",
      "Iteration 18, loss = 0.27056037\n",
      "Iteration 19, loss = 0.26546596\n",
      "Iteration 20, loss = 0.26092364\n",
      "Iteration 21, loss = 0.25646368\n",
      "Iteration 22, loss = 0.25209198\n",
      "Iteration 23, loss = 0.24808873\n",
      "Iteration 24, loss = 0.24401603\n",
      "Iteration 25, loss = 0.24032761\n",
      "Iteration 26, loss = 0.23678958\n",
      "Iteration 27, loss = 0.23304559\n",
      "Iteration 28, loss = 0.22983074\n",
      "Iteration 29, loss = 0.22660757\n",
      "Iteration 30, loss = 0.22327439\n",
      "Iteration 31, loss = 0.22035674\n",
      "Iteration 32, loss = 0.21727002\n",
      "Iteration 33, loss = 0.21442610\n",
      "Iteration 34, loss = 0.21167231\n",
      "Iteration 35, loss = 0.20891465\n",
      "Iteration 36, loss = 0.20615662\n",
      "Iteration 37, loss = 0.20378165\n",
      "Iteration 38, loss = 0.20116983\n",
      "Iteration 39, loss = 0.19876201\n",
      "Iteration 40, loss = 0.19631254\n",
      "Iteration 41, loss = 0.19413960\n",
      "Iteration 42, loss = 0.19177172\n",
      "Iteration 43, loss = 0.18953311\n",
      "Iteration 44, loss = 0.18728035\n",
      "Iteration 45, loss = 0.18525178\n",
      "Iteration 46, loss = 0.18320010\n",
      "Iteration 47, loss = 0.18119684\n",
      "Iteration 48, loss = 0.17925245\n",
      "Iteration 49, loss = 0.17736269\n",
      "Iteration 50, loss = 0.17551812\n",
      "Iteration 51, loss = 0.17354165\n",
      "Iteration 52, loss = 0.17191641\n",
      "Iteration 53, loss = 0.17009785\n",
      "Iteration 54, loss = 0.16850269\n",
      "Iteration 55, loss = 0.16681817\n",
      "Iteration 56, loss = 0.16516849\n",
      "Iteration 57, loss = 0.16359582\n",
      "Iteration 58, loss = 0.16202128\n",
      "Iteration 59, loss = 0.16056283\n",
      "Iteration 60, loss = 0.15889994\n",
      "Iteration 61, loss = 0.15765342\n",
      "Iteration 62, loss = 0.15625720\n",
      "Iteration 63, loss = 0.15484412\n",
      "Iteration 64, loss = 0.15339798\n",
      "Iteration 65, loss = 0.15208543\n",
      "Iteration 66, loss = 0.15071559\n",
      "Iteration 67, loss = 0.14949159\n",
      "Iteration 68, loss = 0.14826283\n",
      "Iteration 69, loss = 0.14705775\n",
      "Iteration 70, loss = 0.14572843\n",
      "Iteration 71, loss = 0.14465376\n",
      "Iteration 72, loss = 0.14335143\n",
      "Iteration 73, loss = 0.14229506\n",
      "Iteration 74, loss = 0.14116775\n",
      "Iteration 75, loss = 0.14009582\n",
      "Iteration 76, loss = 0.13885864\n",
      "Iteration 77, loss = 0.13790196\n",
      "Iteration 78, loss = 0.13675349\n",
      "Iteration 79, loss = 0.13580612\n",
      "Iteration 80, loss = 0.13477922\n",
      "Iteration 81, loss = 0.13380714\n",
      "Iteration 82, loss = 0.13268838\n",
      "Iteration 83, loss = 0.13174171\n",
      "Iteration 84, loss = 0.13086358\n",
      "Iteration 85, loss = 0.12991962\n",
      "Iteration 86, loss = 0.12892431\n",
      "Iteration 87, loss = 0.12813008\n",
      "Iteration 88, loss = 0.12724856\n",
      "Iteration 89, loss = 0.12634773\n",
      "Iteration 90, loss = 0.12547572\n",
      "Iteration 91, loss = 0.12454684\n",
      "Iteration 92, loss = 0.12369655\n",
      "Iteration 93, loss = 0.12293841\n",
      "Iteration 94, loss = 0.12207194\n",
      "Iteration 95, loss = 0.12135751\n",
      "Iteration 96, loss = 0.12041663\n",
      "Iteration 97, loss = 0.11969124\n",
      "Iteration 98, loss = 0.11895264\n",
      "Iteration 99, loss = 0.11811039\n",
      "Iteration 100, loss = 0.11731095\n",
      "Iteration 101, loss = 0.11668304\n",
      "Iteration 102, loss = 0.11591302\n",
      "Iteration 103, loss = 0.11515973\n",
      "Iteration 104, loss = 0.11445847\n",
      "Iteration 105, loss = 0.11371106\n",
      "Iteration 106, loss = 0.11307350\n",
      "Iteration 107, loss = 0.11227596\n",
      "Iteration 108, loss = 0.11166053\n",
      "Iteration 109, loss = 0.11096835\n",
      "Iteration 110, loss = 0.11030891\n",
      "Iteration 111, loss = 0.10960690\n",
      "Iteration 112, loss = 0.10904090\n",
      "Iteration 113, loss = 0.10831062\n",
      "Iteration 114, loss = 0.10775251\n",
      "Iteration 115, loss = 0.10711382\n",
      "Iteration 116, loss = 0.10643720\n",
      "Iteration 117, loss = 0.10583264\n",
      "Iteration 118, loss = 0.10526013\n",
      "Iteration 119, loss = 0.10457514\n",
      "Iteration 120, loss = 0.10411288\n",
      "Iteration 121, loss = 0.10343083\n",
      "Iteration 122, loss = 0.10289050\n",
      "Iteration 123, loss = 0.10239141\n",
      "Iteration 124, loss = 0.10179708\n",
      "Iteration 125, loss = 0.10113101\n",
      "Iteration 126, loss = 0.10071896\n",
      "Iteration 127, loss = 0.10011533\n",
      "Iteration 128, loss = 0.09958229\n",
      "Iteration 129, loss = 0.09910793\n",
      "Iteration 130, loss = 0.09860389\n",
      "Iteration 131, loss = 0.09797935\n",
      "Iteration 132, loss = 0.09748959\n",
      "Iteration 133, loss = 0.09699237\n",
      "Iteration 134, loss = 0.09665604\n",
      "Iteration 135, loss = 0.09606112\n",
      "Iteration 136, loss = 0.09543333\n",
      "Iteration 137, loss = 0.09507923\n",
      "Iteration 138, loss = 0.09461798\n",
      "Iteration 139, loss = 0.09412982\n",
      "Iteration 140, loss = 0.09364119\n",
      "Iteration 141, loss = 0.09322438\n",
      "Iteration 142, loss = 0.09276204\n",
      "Iteration 143, loss = 0.09235083\n",
      "Iteration 144, loss = 0.09186551\n",
      "Iteration 145, loss = 0.09146050\n",
      "Iteration 146, loss = 0.09096852\n",
      "Iteration 147, loss = 0.09054107\n",
      "Iteration 148, loss = 0.09016416\n",
      "Iteration 149, loss = 0.08971657\n",
      "Iteration 150, loss = 0.08928332\n",
      "Iteration 151, loss = 0.08883524\n",
      "Iteration 152, loss = 0.08844683\n",
      "Iteration 153, loss = 0.08807413\n",
      "Iteration 154, loss = 0.08762280\n",
      "Iteration 155, loss = 0.08727511\n",
      "Iteration 156, loss = 0.08688824\n",
      "Iteration 157, loss = 0.08647625\n",
      "Iteration 158, loss = 0.08609606\n",
      "Iteration 159, loss = 0.08569699\n",
      "Iteration 160, loss = 0.08532467\n",
      "Iteration 161, loss = 0.08486409\n",
      "Iteration 162, loss = 0.08465209\n",
      "Iteration 163, loss = 0.08419899\n",
      "Iteration 164, loss = 0.08388166\n",
      "Iteration 165, loss = 0.08346997\n",
      "Iteration 166, loss = 0.08319499\n",
      "Iteration 167, loss = 0.08277628\n",
      "Iteration 168, loss = 0.08241953\n",
      "Iteration 169, loss = 0.08203259\n",
      "Iteration 170, loss = 0.08175588\n",
      "Iteration 171, loss = 0.08137124\n",
      "Iteration 172, loss = 0.08107574\n",
      "Iteration 173, loss = 0.08075586\n",
      "Iteration 174, loss = 0.08033971\n",
      "Iteration 175, loss = 0.08009993\n",
      "Iteration 176, loss = 0.07975001\n",
      "Iteration 177, loss = 0.07931209\n",
      "Iteration 178, loss = 0.07918154\n",
      "Iteration 179, loss = 0.07877794\n",
      "Iteration 180, loss = 0.07847391\n",
      "Iteration 181, loss = 0.07815825\n",
      "Iteration 182, loss = 0.07785855\n",
      "Iteration 183, loss = 0.07754496\n",
      "Iteration 184, loss = 0.07721603\n",
      "Iteration 185, loss = 0.07687960\n",
      "Iteration 186, loss = 0.07658238\n",
      "Iteration 187, loss = 0.07631172\n",
      "Iteration 188, loss = 0.07607068\n",
      "Iteration 189, loss = 0.07577550\n",
      "Iteration 190, loss = 0.07541906\n",
      "Iteration 191, loss = 0.07521139\n",
      "Iteration 192, loss = 0.07493839\n",
      "Iteration 193, loss = 0.07467734\n",
      "Iteration 194, loss = 0.07439614\n",
      "Iteration 195, loss = 0.07410244\n",
      "Iteration 196, loss = 0.07382046\n",
      "Iteration 197, loss = 0.07356972\n",
      "Iteration 198, loss = 0.07328493\n",
      "Iteration 199, loss = 0.07302983\n",
      "Iteration 200, loss = 0.07271276\n",
      "[CV] END activation=relu, alpha=0.01, batch_size=100, hidden_layer_sizes=(100,), solver=sgd; total time= 1.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.42633305\n",
      "Iteration 2, loss = 0.67806261\n",
      "Iteration 3, loss = 0.51994692\n",
      "Iteration 4, loss = 0.45307801\n",
      "Iteration 5, loss = 0.41455527\n",
      "Iteration 6, loss = 0.38869558\n",
      "Iteration 7, loss = 0.36942540\n",
      "Iteration 8, loss = 0.35427504\n",
      "Iteration 9, loss = 0.34155600\n",
      "Iteration 10, loss = 0.33096336\n",
      "Iteration 11, loss = 0.32152231\n",
      "Iteration 12, loss = 0.31309295\n",
      "Iteration 13, loss = 0.30553077\n",
      "Iteration 14, loss = 0.29839610\n",
      "Iteration 15, loss = 0.29191793\n",
      "Iteration 16, loss = 0.28579961\n",
      "Iteration 17, loss = 0.28028584\n",
      "Iteration 18, loss = 0.27499128\n",
      "Iteration 19, loss = 0.27004439\n",
      "Iteration 20, loss = 0.26535304\n",
      "Iteration 21, loss = 0.26103949\n",
      "Iteration 22, loss = 0.25673511\n",
      "Iteration 23, loss = 0.25239468\n",
      "Iteration 24, loss = 0.24851707\n",
      "Iteration 25, loss = 0.24466151\n",
      "Iteration 26, loss = 0.24109066\n",
      "Iteration 27, loss = 0.23743121\n",
      "Iteration 28, loss = 0.23401907\n",
      "Iteration 29, loss = 0.23082594\n",
      "Iteration 30, loss = 0.22740653\n",
      "Iteration 31, loss = 0.22432298\n",
      "Iteration 32, loss = 0.22143018\n",
      "Iteration 33, loss = 0.21838549\n",
      "Iteration 34, loss = 0.21556529\n",
      "Iteration 35, loss = 0.21289053\n",
      "Iteration 36, loss = 0.21001561\n",
      "Iteration 37, loss = 0.20745961\n",
      "Iteration 38, loss = 0.20494352\n",
      "Iteration 39, loss = 0.20241429\n",
      "Iteration 40, loss = 0.19993578\n",
      "Iteration 41, loss = 0.19760938\n",
      "Iteration 42, loss = 0.19529875\n",
      "Iteration 43, loss = 0.19293402\n",
      "Iteration 44, loss = 0.19070373\n",
      "Iteration 45, loss = 0.18856760\n",
      "Iteration 46, loss = 0.18651700\n",
      "Iteration 47, loss = 0.18439595\n",
      "Iteration 48, loss = 0.18240175\n",
      "Iteration 49, loss = 0.18040124\n",
      "Iteration 50, loss = 0.17844587\n",
      "Iteration 51, loss = 0.17659665\n",
      "Iteration 52, loss = 0.17473487\n",
      "Iteration 53, loss = 0.17295249\n",
      "Iteration 54, loss = 0.17119419\n",
      "Iteration 55, loss = 0.16944347\n",
      "Iteration 56, loss = 0.16778881\n",
      "Iteration 57, loss = 0.16620975\n",
      "Iteration 58, loss = 0.16456135\n",
      "Iteration 59, loss = 0.16284569\n",
      "Iteration 60, loss = 0.16149548\n",
      "Iteration 61, loss = 0.15974964\n",
      "Iteration 62, loss = 0.15816224\n",
      "Iteration 63, loss = 0.15686244\n",
      "Iteration 64, loss = 0.15543576\n",
      "Iteration 65, loss = 0.15407572\n",
      "Iteration 66, loss = 0.15271508\n",
      "Iteration 67, loss = 0.15134485\n",
      "Iteration 68, loss = 0.14995859\n",
      "Iteration 69, loss = 0.14873333\n",
      "Iteration 70, loss = 0.14737563\n",
      "Iteration 71, loss = 0.14622474\n",
      "Iteration 72, loss = 0.14486423\n",
      "Iteration 73, loss = 0.14363930\n",
      "Iteration 74, loss = 0.14255738\n",
      "Iteration 75, loss = 0.14130346\n",
      "Iteration 76, loss = 0.14027122\n",
      "Iteration 77, loss = 0.13908410\n",
      "Iteration 78, loss = 0.13801869\n",
      "Iteration 79, loss = 0.13686304\n",
      "Iteration 80, loss = 0.13578092\n",
      "Iteration 81, loss = 0.13482932\n",
      "Iteration 82, loss = 0.13379109\n",
      "Iteration 83, loss = 0.13274354\n",
      "Iteration 84, loss = 0.13177977\n",
      "Iteration 85, loss = 0.13079149\n",
      "Iteration 86, loss = 0.12988755\n",
      "Iteration 87, loss = 0.12884394\n",
      "Iteration 88, loss = 0.12796171\n",
      "Iteration 89, loss = 0.12705438\n",
      "Iteration 90, loss = 0.12616076\n",
      "Iteration 91, loss = 0.12522598\n",
      "Iteration 92, loss = 0.12442665\n",
      "Iteration 93, loss = 0.12352711\n",
      "Iteration 94, loss = 0.12254891\n",
      "Iteration 95, loss = 0.12180095\n",
      "Iteration 96, loss = 0.12110637\n",
      "Iteration 97, loss = 0.12013336\n",
      "Iteration 98, loss = 0.11923674\n",
      "Iteration 99, loss = 0.11863359\n",
      "Iteration 100, loss = 0.11790834\n",
      "Iteration 101, loss = 0.11708366\n",
      "Iteration 102, loss = 0.11636124\n",
      "Iteration 103, loss = 0.11565173\n",
      "Iteration 104, loss = 0.11483219\n",
      "Iteration 105, loss = 0.11414959\n",
      "Iteration 106, loss = 0.11345000\n",
      "Iteration 107, loss = 0.11262456\n",
      "Iteration 108, loss = 0.11198105\n",
      "Iteration 109, loss = 0.11132045\n",
      "Iteration 110, loss = 0.11066832\n",
      "Iteration 111, loss = 0.10999563\n",
      "Iteration 112, loss = 0.10928320\n",
      "Iteration 113, loss = 0.10861959\n",
      "Iteration 114, loss = 0.10803427\n",
      "Iteration 115, loss = 0.10735758\n",
      "Iteration 116, loss = 0.10672150\n",
      "Iteration 117, loss = 0.10615382\n",
      "Iteration 118, loss = 0.10550113\n",
      "Iteration 119, loss = 0.10500963\n",
      "Iteration 120, loss = 0.10437824\n",
      "Iteration 121, loss = 0.10377394\n",
      "Iteration 122, loss = 0.10310775\n",
      "Iteration 123, loss = 0.10258545\n",
      "Iteration 124, loss = 0.10199005\n",
      "Iteration 125, loss = 0.10154107\n",
      "Iteration 126, loss = 0.10099755\n",
      "Iteration 127, loss = 0.10032126\n",
      "Iteration 128, loss = 0.09975897\n",
      "Iteration 129, loss = 0.09928888\n",
      "Iteration 130, loss = 0.09879573\n",
      "Iteration 131, loss = 0.09828250\n",
      "Iteration 132, loss = 0.09774811\n",
      "Iteration 133, loss = 0.09720447\n",
      "Iteration 134, loss = 0.09679047\n",
      "Iteration 135, loss = 0.09618212\n",
      "Iteration 136, loss = 0.09577618\n",
      "Iteration 137, loss = 0.09527067\n",
      "Iteration 138, loss = 0.09481979\n",
      "Iteration 139, loss = 0.09439621\n",
      "Iteration 140, loss = 0.09388895\n",
      "Iteration 141, loss = 0.09344539\n",
      "Iteration 142, loss = 0.09292600\n",
      "Iteration 143, loss = 0.09250511\n",
      "Iteration 144, loss = 0.09211685\n",
      "Iteration 145, loss = 0.09157066\n",
      "Iteration 146, loss = 0.09122638\n",
      "Iteration 147, loss = 0.09081981\n",
      "Iteration 148, loss = 0.09044750\n",
      "Iteration 149, loss = 0.08996224\n",
      "Iteration 150, loss = 0.08957014\n",
      "Iteration 151, loss = 0.08905413\n",
      "Iteration 152, loss = 0.08878755\n",
      "Iteration 153, loss = 0.08835157\n",
      "Iteration 154, loss = 0.08795922\n",
      "Iteration 155, loss = 0.08752559\n",
      "Iteration 156, loss = 0.08715427\n",
      "Iteration 157, loss = 0.08669035\n",
      "Iteration 158, loss = 0.08638826\n",
      "Iteration 159, loss = 0.08590834\n",
      "Iteration 160, loss = 0.08565892\n",
      "Iteration 161, loss = 0.08525992\n",
      "Iteration 162, loss = 0.08490168\n",
      "Iteration 163, loss = 0.08458975\n",
      "Iteration 164, loss = 0.08413323\n",
      "Iteration 165, loss = 0.08373892\n",
      "Iteration 166, loss = 0.08348996\n",
      "Iteration 167, loss = 0.08316466\n",
      "Iteration 168, loss = 0.08277528\n",
      "Iteration 169, loss = 0.08235180\n",
      "Iteration 170, loss = 0.08204516\n",
      "Iteration 171, loss = 0.08171195\n",
      "Iteration 172, loss = 0.08138717\n",
      "Iteration 173, loss = 0.08112941\n",
      "Iteration 174, loss = 0.08078188\n",
      "Iteration 175, loss = 0.08035236\n",
      "Iteration 176, loss = 0.08006210\n",
      "Iteration 177, loss = 0.07978247\n",
      "Iteration 178, loss = 0.07948349\n",
      "Iteration 179, loss = 0.07912851\n",
      "Iteration 180, loss = 0.07886177\n",
      "Iteration 181, loss = 0.07852940\n",
      "Iteration 182, loss = 0.07827684\n",
      "Iteration 183, loss = 0.07791414\n",
      "Iteration 184, loss = 0.07762587\n",
      "Iteration 185, loss = 0.07739755\n",
      "Iteration 186, loss = 0.07702325\n",
      "Iteration 187, loss = 0.07671138\n",
      "Iteration 188, loss = 0.07640211\n",
      "Iteration 189, loss = 0.07612923\n",
      "Iteration 190, loss = 0.07588123\n",
      "Iteration 191, loss = 0.07555306\n",
      "Iteration 192, loss = 0.07531266\n",
      "Iteration 193, loss = 0.07503594\n",
      "Iteration 194, loss = 0.07479681\n",
      "Iteration 195, loss = 0.07449454\n",
      "Iteration 196, loss = 0.07426094\n",
      "Iteration 197, loss = 0.07402343\n",
      "Iteration 198, loss = 0.07372318\n",
      "Iteration 199, loss = 0.07345884\n",
      "Iteration 200, loss = 0.07313150\n",
      "[CV] END activation=relu, alpha=0.01, batch_size=100, hidden_layer_sizes=(100,), solver=sgd; total time= 1.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.33873633\n",
      "Iteration 2, loss = 0.15154259\n",
      "Iteration 3, loss = 0.11187159\n",
      "Iteration 4, loss = 0.08839488\n",
      "Iteration 5, loss = 0.07489905\n",
      "Iteration 6, loss = 0.06409039\n",
      "Iteration 7, loss = 0.06142185\n",
      "Iteration 8, loss = 0.05837356\n",
      "Iteration 9, loss = 0.05330936\n",
      "Iteration 10, loss = 0.05505418\n",
      "Iteration 11, loss = 0.05295938\n",
      "Iteration 12, loss = 0.04743976\n",
      "Iteration 13, loss = 0.04707876\n",
      "Iteration 14, loss = 0.04943600\n",
      "Iteration 15, loss = 0.05115797\n",
      "Iteration 16, loss = 0.04417733\n",
      "Iteration 17, loss = 0.04401217\n",
      "Iteration 18, loss = 0.05292216\n",
      "Iteration 19, loss = 0.04643035\n",
      "Iteration 20, loss = 0.04467339\n",
      "Iteration 21, loss = 0.04272905\n",
      "Iteration 22, loss = 0.04268268\n",
      "Iteration 23, loss = 0.03947301\n",
      "Iteration 24, loss = 0.04365561\n",
      "Iteration 25, loss = 0.04283942\n",
      "Iteration 26, loss = 0.04015506\n",
      "Iteration 27, loss = 0.03796579\n",
      "Iteration 28, loss = 0.03624731\n",
      "Iteration 29, loss = 0.03925900\n",
      "Iteration 30, loss = 0.04092811\n",
      "Iteration 31, loss = 0.03239854\n",
      "Iteration 32, loss = 0.03660204\n",
      "Iteration 33, loss = 0.03972914\n",
      "Iteration 34, loss = 0.03744759\n",
      "Iteration 35, loss = 0.03245239\n",
      "Iteration 36, loss = 0.02928421\n",
      "Iteration 37, loss = 0.04265117\n",
      "Iteration 38, loss = 0.03760478\n",
      "Iteration 39, loss = 0.03505559\n",
      "Iteration 40, loss = 0.03236286\n",
      "Iteration 41, loss = 0.03005036\n",
      "Iteration 42, loss = 0.03861524\n",
      "Iteration 43, loss = 0.03786443\n",
      "Iteration 44, loss = 0.02998659\n",
      "Iteration 45, loss = 0.02629630\n",
      "Iteration 46, loss = 0.03045344\n",
      "Iteration 47, loss = 0.03953015\n",
      "Iteration 48, loss = 0.03299283\n",
      "Iteration 49, loss = 0.02748598\n",
      "Iteration 50, loss = 0.02461727\n",
      "Iteration 51, loss = 0.02311936\n",
      "Iteration 52, loss = 0.03159917\n",
      "Iteration 53, loss = 0.04914558\n",
      "Iteration 54, loss = 0.03272409\n",
      "Iteration 55, loss = 0.02686651\n",
      "Iteration 56, loss = 0.02414470\n",
      "Iteration 57, loss = 0.02278057\n",
      "Iteration 58, loss = 0.02189079\n",
      "Iteration 59, loss = 0.04449521\n",
      "Iteration 60, loss = 0.03897267\n",
      "Iteration 61, loss = 0.03004277\n",
      "Iteration 62, loss = 0.02543804\n",
      "Iteration 63, loss = 0.02305201\n",
      "Iteration 64, loss = 0.02155620\n",
      "Iteration 65, loss = 0.02097617\n",
      "Iteration 66, loss = 0.04409418\n",
      "Iteration 67, loss = 0.04127047\n",
      "Iteration 68, loss = 0.02756682\n",
      "Iteration 69, loss = 0.02418716\n",
      "Iteration 70, loss = 0.02227022\n",
      "Iteration 71, loss = 0.02108409\n",
      "Iteration 72, loss = 0.02032301\n",
      "Iteration 73, loss = 0.04515904\n",
      "Iteration 74, loss = 0.03577043\n",
      "Iteration 75, loss = 0.02873815\n",
      "Iteration 76, loss = 0.02442211\n",
      "Iteration 77, loss = 0.02448678\n",
      "Iteration 78, loss = 0.03248116\n",
      "Iteration 79, loss = 0.03333217\n",
      "Iteration 80, loss = 0.02624315\n",
      "Iteration 81, loss = 0.02366857\n",
      "Iteration 82, loss = 0.02132123\n",
      "Iteration 83, loss = 0.02016934\n",
      "Iteration 84, loss = 0.04099319\n",
      "Iteration 85, loss = 0.03851335\n",
      "Iteration 86, loss = 0.02665134\n",
      "Iteration 87, loss = 0.02299347\n",
      "Iteration 88, loss = 0.02146270\n",
      "Iteration 89, loss = 0.02053729\n",
      "Iteration 90, loss = 0.03521384\n",
      "Iteration 91, loss = 0.03428225\n",
      "Iteration 92, loss = 0.02885763\n",
      "Iteration 93, loss = 0.02508573\n",
      "Iteration 94, loss = 0.02309460\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.01, batch_size=100, hidden_layer_sizes=(400, 100), solver=adam; total time=14.9min\n",
      "Iteration 1, loss = 0.33850159\n",
      "Iteration 2, loss = 0.15062145\n",
      "Iteration 3, loss = 0.11044918\n",
      "Iteration 4, loss = 0.09257440\n",
      "Iteration 5, loss = 0.07579408\n",
      "Iteration 6, loss = 0.07012920\n",
      "Iteration 7, loss = 0.06146647\n",
      "Iteration 8, loss = 0.05848418\n",
      "Iteration 9, loss = 0.05663008\n",
      "Iteration 10, loss = 0.05639011\n",
      "Iteration 11, loss = 0.05466216\n",
      "Iteration 12, loss = 0.04634317\n",
      "Iteration 13, loss = 0.05018205\n",
      "Iteration 14, loss = 0.05514711\n",
      "Iteration 15, loss = 0.05240150\n",
      "Iteration 16, loss = 0.05186132\n",
      "Iteration 17, loss = 0.04623643\n",
      "Iteration 18, loss = 0.04645315\n",
      "Iteration 19, loss = 0.04259943\n",
      "Iteration 20, loss = 0.04820866\n",
      "Iteration 21, loss = 0.04319122\n",
      "Iteration 22, loss = 0.04370696\n",
      "Iteration 23, loss = 0.04808520\n",
      "Iteration 24, loss = 0.04030975\n",
      "Iteration 25, loss = 0.03866774\n",
      "Iteration 26, loss = 0.04296651\n",
      "Iteration 27, loss = 0.04295160\n",
      "Iteration 28, loss = 0.03836779\n",
      "Iteration 29, loss = 0.04021810\n",
      "Iteration 30, loss = 0.04110322\n",
      "Iteration 31, loss = 0.03849010\n",
      "Iteration 32, loss = 0.03725923\n",
      "Iteration 33, loss = 0.03523971\n",
      "Iteration 34, loss = 0.03818218\n",
      "Iteration 35, loss = 0.04023281\n",
      "Iteration 36, loss = 0.03595638\n",
      "Iteration 37, loss = 0.03168203\n",
      "Iteration 38, loss = 0.04082745\n",
      "Iteration 39, loss = 0.03980342\n",
      "Iteration 40, loss = 0.03270843\n",
      "Iteration 41, loss = 0.02879670\n",
      "Iteration 42, loss = 0.02672562\n",
      "Iteration 43, loss = 0.04571317\n",
      "Iteration 44, loss = 0.03862945\n",
      "Iteration 45, loss = 0.03356088\n",
      "Iteration 46, loss = 0.02983188\n",
      "Iteration 47, loss = 0.03163226\n",
      "Iteration 48, loss = 0.03851501\n",
      "Iteration 49, loss = 0.03328374\n",
      "Iteration 50, loss = 0.02839077\n",
      "Iteration 51, loss = 0.02500256\n",
      "Iteration 52, loss = 0.02327061\n",
      "Iteration 53, loss = 0.02604984\n",
      "Iteration 54, loss = 0.05565948\n",
      "Iteration 55, loss = 0.03328467\n",
      "Iteration 56, loss = 0.02790050\n",
      "Iteration 57, loss = 0.02751098\n",
      "Iteration 58, loss = 0.02730100\n",
      "Iteration 59, loss = 0.03551311\n",
      "Iteration 60, loss = 0.03405062\n",
      "Iteration 61, loss = 0.02630631\n",
      "Iteration 62, loss = 0.02415103\n",
      "Iteration 63, loss = 0.02226909\n",
      "Iteration 64, loss = 0.02150951\n",
      "Iteration 65, loss = 0.05145182\n",
      "Iteration 66, loss = 0.03696573\n",
      "Iteration 67, loss = 0.02865941\n",
      "Iteration 68, loss = 0.02496582\n",
      "Iteration 69, loss = 0.02269543\n",
      "Iteration 70, loss = 0.02164374\n",
      "Iteration 71, loss = 0.04076203\n",
      "Iteration 72, loss = 0.03769006\n",
      "Iteration 73, loss = 0.02930381\n",
      "Iteration 74, loss = 0.02550779\n",
      "Iteration 75, loss = 0.02291937\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.01, batch_size=100, hidden_layer_sizes=(400, 100), solver=adam; total time=10.5min\n",
      "Iteration 1, loss = 0.33063169\n",
      "Iteration 2, loss = 0.15003960\n",
      "Iteration 3, loss = 0.10932505\n",
      "Iteration 4, loss = 0.08876860\n",
      "Iteration 5, loss = 0.07368837\n",
      "Iteration 6, loss = 0.06733535\n",
      "Iteration 7, loss = 0.05936888\n",
      "Iteration 8, loss = 0.05850194\n",
      "Iteration 9, loss = 0.05249046\n",
      "Iteration 10, loss = 0.05542424\n",
      "Iteration 11, loss = 0.05389784\n",
      "Iteration 12, loss = 0.05268145\n",
      "Iteration 13, loss = 0.05098617\n",
      "Iteration 14, loss = 0.04718249\n",
      "Iteration 15, loss = 0.04667880\n",
      "Iteration 16, loss = 0.04766698\n",
      "Iteration 17, loss = 0.04908273\n",
      "Iteration 18, loss = 0.04668444\n",
      "Iteration 19, loss = 0.04886584\n",
      "Iteration 20, loss = 0.04596146\n",
      "Iteration 21, loss = 0.04354525\n",
      "Iteration 22, loss = 0.04069313\n",
      "Iteration 23, loss = 0.04145720\n",
      "Iteration 24, loss = 0.04250137\n",
      "Iteration 25, loss = 0.04118352\n",
      "Iteration 26, loss = 0.03899206\n",
      "Iteration 27, loss = 0.04183451\n",
      "Iteration 28, loss = 0.03802545\n",
      "Iteration 29, loss = 0.03449113\n",
      "Iteration 30, loss = 0.04256678\n",
      "Iteration 31, loss = 0.04243610\n",
      "Iteration 32, loss = 0.03704863\n",
      "Iteration 33, loss = 0.03297060\n",
      "Iteration 34, loss = 0.03141055\n",
      "Iteration 35, loss = 0.04116594\n",
      "Iteration 36, loss = 0.04111081\n",
      "Iteration 37, loss = 0.03506292\n",
      "Iteration 38, loss = 0.04040136\n",
      "Iteration 39, loss = 0.03316267\n",
      "Iteration 40, loss = 0.02748267\n",
      "Iteration 41, loss = 0.02511200\n",
      "Iteration 42, loss = 0.02359586\n",
      "Iteration 43, loss = 0.05508864\n",
      "Iteration 44, loss = 0.03809730\n",
      "Iteration 45, loss = 0.03273176\n",
      "Iteration 46, loss = 0.02802222\n",
      "Iteration 47, loss = 0.02677856\n",
      "Iteration 48, loss = 0.02941284\n",
      "Iteration 49, loss = 0.04290315\n",
      "Iteration 50, loss = 0.03446139\n",
      "Iteration 51, loss = 0.02826401\n",
      "Iteration 52, loss = 0.02532943\n",
      "Iteration 53, loss = 0.02767932\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.01, batch_size=100, hidden_layer_sizes=(400, 100), solver=adam; total time= 5.8min\n",
      "Iteration 1, loss = 0.33780119\n",
      "Iteration 2, loss = 0.15006988\n",
      "Iteration 3, loss = 0.11070688\n",
      "Iteration 4, loss = 0.08870932\n",
      "Iteration 5, loss = 0.07493868\n",
      "Iteration 6, loss = 0.06774141\n",
      "Iteration 7, loss = 0.06160421\n",
      "Iteration 8, loss = 0.05598190\n",
      "Iteration 9, loss = 0.05668233\n",
      "Iteration 10, loss = 0.05293455\n",
      "Iteration 11, loss = 0.05208026\n",
      "Iteration 12, loss = 0.05527603\n",
      "Iteration 13, loss = 0.04787296\n",
      "Iteration 14, loss = 0.04823498\n",
      "Iteration 15, loss = 0.05043099\n",
      "Iteration 16, loss = 0.04879606\n",
      "Iteration 17, loss = 0.04449813\n",
      "Iteration 18, loss = 0.05305675\n",
      "Iteration 19, loss = 0.04898475\n",
      "Iteration 20, loss = 0.04243138\n",
      "Iteration 21, loss = 0.04467590\n",
      "Iteration 22, loss = 0.03885031\n",
      "Iteration 23, loss = 0.04337115\n",
      "Iteration 24, loss = 0.05167841\n",
      "Iteration 25, loss = 0.03955500\n",
      "Iteration 26, loss = 0.03808770\n",
      "Iteration 27, loss = 0.03476259\n",
      "Iteration 28, loss = 0.04308360\n",
      "Iteration 29, loss = 0.04855010\n",
      "Iteration 30, loss = 0.03962426\n",
      "Iteration 31, loss = 0.03189674\n",
      "Iteration 32, loss = 0.03172281\n",
      "Iteration 33, loss = 0.03735607\n",
      "Iteration 34, loss = 0.04286086\n",
      "Iteration 35, loss = 0.04047679\n",
      "Iteration 36, loss = 0.03287645\n",
      "Iteration 37, loss = 0.02877679\n",
      "Iteration 38, loss = 0.03690433\n",
      "Iteration 39, loss = 0.03987693\n",
      "Iteration 40, loss = 0.03409886\n",
      "Iteration 41, loss = 0.03262495\n",
      "Iteration 42, loss = 0.02866355\n",
      "Iteration 43, loss = 0.02770146\n",
      "Iteration 44, loss = 0.04277350\n",
      "Iteration 45, loss = 0.04001742\n",
      "Iteration 46, loss = 0.03361836\n",
      "Iteration 47, loss = 0.02887133\n",
      "Iteration 48, loss = 0.02614501\n",
      "Iteration 49, loss = 0.02379805\n",
      "Iteration 50, loss = 0.04850315\n",
      "Iteration 51, loss = 0.03623076\n",
      "Iteration 52, loss = 0.02977118\n",
      "Iteration 53, loss = 0.02788497\n",
      "Iteration 54, loss = 0.03422893\n",
      "Iteration 55, loss = 0.03549803\n",
      "Iteration 56, loss = 0.02911098\n",
      "Iteration 57, loss = 0.02574774\n",
      "Iteration 58, loss = 0.02347773\n",
      "Iteration 59, loss = 0.02393609\n",
      "Iteration 60, loss = 0.05213204\n",
      "Iteration 61, loss = 0.03367627\n",
      "Iteration 62, loss = 0.02792974\n",
      "Iteration 63, loss = 0.02550718\n",
      "Iteration 64, loss = 0.02304453\n",
      "Iteration 65, loss = 0.02168919\n",
      "Iteration 66, loss = 0.03394313\n",
      "Iteration 67, loss = 0.04159891\n",
      "Iteration 68, loss = 0.03081142\n",
      "Iteration 69, loss = 0.02756927\n",
      "Iteration 70, loss = 0.02443552\n",
      "Iteration 71, loss = 0.02238981\n",
      "Iteration 72, loss = 0.02167117\n",
      "Iteration 73, loss = 0.04748330\n",
      "Iteration 74, loss = 0.03092805\n",
      "Iteration 75, loss = 0.02618540\n",
      "Iteration 76, loss = 0.02412157\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.01, batch_size=100, hidden_layer_sizes=(400, 100), solver=adam; total time=10.3min\n",
      "Iteration 1, loss = 0.34371749\n",
      "Iteration 2, loss = 0.15379846\n",
      "Iteration 3, loss = 0.11158108\n",
      "Iteration 4, loss = 0.08818264\n",
      "Iteration 5, loss = 0.07637120\n",
      "Iteration 6, loss = 0.06714404\n",
      "Iteration 7, loss = 0.06316139\n",
      "Iteration 8, loss = 0.05731904\n",
      "Iteration 9, loss = 0.05338148\n",
      "Iteration 10, loss = 0.05203427\n",
      "Iteration 11, loss = 0.05421993\n",
      "Iteration 12, loss = 0.05274100\n",
      "Iteration 13, loss = 0.04763364\n",
      "Iteration 14, loss = 0.05273723\n",
      "Iteration 15, loss = 0.05018461\n",
      "Iteration 16, loss = 0.04690169\n",
      "Iteration 17, loss = 0.04896775\n",
      "Iteration 18, loss = 0.05014254\n",
      "Iteration 19, loss = 0.04867166\n",
      "Iteration 20, loss = 0.04548263\n",
      "Iteration 21, loss = 0.04161188\n",
      "Iteration 22, loss = 0.03980506\n",
      "Iteration 23, loss = 0.04321800\n",
      "Iteration 24, loss = 0.04228340\n",
      "Iteration 25, loss = 0.04163267\n",
      "Iteration 26, loss = 0.03966708\n",
      "Iteration 27, loss = 0.05070099\n",
      "Iteration 28, loss = 0.04161136\n",
      "Iteration 29, loss = 0.03767773\n",
      "Iteration 30, loss = 0.03483747\n",
      "Iteration 31, loss = 0.03671557\n",
      "Iteration 32, loss = 0.03966215\n",
      "Iteration 33, loss = 0.03550735\n",
      "Iteration 34, loss = 0.03919481\n",
      "Iteration 35, loss = 0.03563492\n",
      "Iteration 36, loss = 0.03797085\n",
      "Iteration 37, loss = 0.03819912\n",
      "Iteration 38, loss = 0.03689746\n",
      "Iteration 39, loss = 0.03539862\n",
      "Iteration 40, loss = 0.03475959\n",
      "Iteration 41, loss = 0.03367950\n",
      "Iteration 42, loss = 0.03904935\n",
      "Iteration 43, loss = 0.03282662\n",
      "Iteration 44, loss = 0.02747820\n",
      "Iteration 45, loss = 0.02470125\n",
      "Iteration 46, loss = 0.02293069\n",
      "Iteration 47, loss = 0.02931142\n",
      "Iteration 48, loss = 0.05756724\n",
      "Iteration 49, loss = 0.03724694\n",
      "Iteration 50, loss = 0.03268137\n",
      "Iteration 51, loss = 0.03086163\n",
      "Iteration 52, loss = 0.02795788\n",
      "Iteration 53, loss = 0.02635548\n",
      "Iteration 54, loss = 0.02875341\n",
      "Iteration 55, loss = 0.04585622\n",
      "Iteration 56, loss = 0.03310108\n",
      "Iteration 57, loss = 0.02723888\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.01, batch_size=100, hidden_layer_sizes=(400, 100), solver=adam; total time= 7.0min\n",
      "Iteration 1, loss = 1.33801876\n",
      "Iteration 2, loss = 0.56956250\n",
      "Iteration 3, loss = 0.43923120\n",
      "Iteration 4, loss = 0.38603960\n",
      "Iteration 5, loss = 0.35378621\n",
      "Iteration 6, loss = 0.33165503\n",
      "Iteration 7, loss = 0.31424528\n",
      "Iteration 8, loss = 0.30023867\n",
      "Iteration 9, loss = 0.28797275\n",
      "Iteration 10, loss = 0.27684074\n",
      "Iteration 11, loss = 0.26701827\n",
      "Iteration 12, loss = 0.25846034\n",
      "Iteration 13, loss = 0.24987011\n",
      "Iteration 14, loss = 0.24234786\n",
      "Iteration 15, loss = 0.23571787\n",
      "Iteration 16, loss = 0.22905513\n",
      "Iteration 17, loss = 0.22302056\n",
      "Iteration 18, loss = 0.21717304\n",
      "Iteration 19, loss = 0.21193486\n",
      "Iteration 20, loss = 0.20689762\n",
      "Iteration 21, loss = 0.20218768\n",
      "Iteration 22, loss = 0.19758477\n",
      "Iteration 23, loss = 0.19346585\n",
      "Iteration 24, loss = 0.18881438\n",
      "Iteration 25, loss = 0.18510572\n",
      "Iteration 26, loss = 0.18136839\n",
      "Iteration 27, loss = 0.17766552\n",
      "Iteration 28, loss = 0.17405767\n",
      "Iteration 29, loss = 0.17093427\n",
      "Iteration 30, loss = 0.16778065\n",
      "Iteration 31, loss = 0.16460231\n",
      "Iteration 32, loss = 0.16170289\n",
      "Iteration 33, loss = 0.15870673\n",
      "Iteration 34, loss = 0.15595815\n",
      "Iteration 35, loss = 0.15330634\n",
      "Iteration 36, loss = 0.15045659\n",
      "Iteration 37, loss = 0.14828014\n",
      "Iteration 38, loss = 0.14595544\n",
      "Iteration 39, loss = 0.14356173\n",
      "Iteration 40, loss = 0.14114293\n",
      "Iteration 41, loss = 0.13893898\n",
      "Iteration 42, loss = 0.13693196\n",
      "Iteration 43, loss = 0.13480479\n",
      "Iteration 44, loss = 0.13271755\n",
      "Iteration 45, loss = 0.13074347\n",
      "Iteration 46, loss = 0.12910612\n",
      "Iteration 47, loss = 0.12706809\n",
      "Iteration 48, loss = 0.12557303\n",
      "Iteration 49, loss = 0.12341947\n",
      "Iteration 50, loss = 0.12200062\n",
      "Iteration 51, loss = 0.12034149\n",
      "Iteration 52, loss = 0.11875689\n",
      "Iteration 53, loss = 0.11719621\n",
      "Iteration 54, loss = 0.11567646\n",
      "Iteration 55, loss = 0.11421475\n",
      "Iteration 56, loss = 0.11259110\n",
      "Iteration 57, loss = 0.11134365\n",
      "Iteration 58, loss = 0.10998933\n",
      "Iteration 59, loss = 0.10866176\n",
      "Iteration 60, loss = 0.10750187\n",
      "Iteration 61, loss = 0.10618691\n",
      "Iteration 62, loss = 0.10487358\n",
      "Iteration 63, loss = 0.10380058\n",
      "Iteration 64, loss = 0.10274503\n",
      "Iteration 65, loss = 0.10144898\n",
      "Iteration 66, loss = 0.10028857\n",
      "Iteration 67, loss = 0.09921404\n",
      "Iteration 68, loss = 0.09809229\n",
      "Iteration 69, loss = 0.09712217\n",
      "Iteration 70, loss = 0.09606852\n",
      "Iteration 71, loss = 0.09500492\n",
      "Iteration 72, loss = 0.09408555\n",
      "Iteration 73, loss = 0.09308473\n",
      "Iteration 74, loss = 0.09224192\n",
      "Iteration 75, loss = 0.09126333\n",
      "Iteration 76, loss = 0.09043831\n",
      "Iteration 77, loss = 0.08966318\n",
      "Iteration 78, loss = 0.08877477\n",
      "Iteration 79, loss = 0.08789615\n",
      "Iteration 80, loss = 0.08703046\n",
      "Iteration 81, loss = 0.08617682\n",
      "Iteration 82, loss = 0.08548873\n",
      "Iteration 83, loss = 0.08462593\n",
      "Iteration 84, loss = 0.08393071\n",
      "Iteration 85, loss = 0.08315364\n",
      "Iteration 86, loss = 0.08250613\n",
      "Iteration 87, loss = 0.08191949\n",
      "Iteration 88, loss = 0.08116862\n",
      "Iteration 89, loss = 0.08048651\n",
      "Iteration 90, loss = 0.07987968\n",
      "Iteration 91, loss = 0.07921876\n",
      "Iteration 92, loss = 0.07864250\n",
      "Iteration 93, loss = 0.07797382\n",
      "Iteration 94, loss = 0.07745667\n",
      "Iteration 95, loss = 0.07675156\n",
      "Iteration 96, loss = 0.07615432\n",
      "Iteration 97, loss = 0.07559783\n",
      "Iteration 98, loss = 0.07503404\n",
      "Iteration 99, loss = 0.07460003\n",
      "Iteration 100, loss = 0.07411083\n",
      "Iteration 101, loss = 0.07356361\n",
      "Iteration 102, loss = 0.07297034\n",
      "Iteration 103, loss = 0.07245980\n",
      "Iteration 104, loss = 0.07198484\n",
      "Iteration 105, loss = 0.07156185\n",
      "Iteration 106, loss = 0.07101489\n",
      "Iteration 107, loss = 0.07049533\n",
      "Iteration 108, loss = 0.07014214\n",
      "Iteration 109, loss = 0.06973507\n",
      "Iteration 110, loss = 0.06925250\n",
      "Iteration 111, loss = 0.06886175\n",
      "Iteration 112, loss = 0.06849489\n",
      "Iteration 113, loss = 0.06804475\n",
      "Iteration 114, loss = 0.06770281\n",
      "Iteration 115, loss = 0.06718463\n",
      "Iteration 116, loss = 0.06684716\n",
      "Iteration 117, loss = 0.06639679\n",
      "Iteration 118, loss = 0.06617810\n",
      "Iteration 119, loss = 0.06568492\n",
      "Iteration 120, loss = 0.06534657\n",
      "Iteration 121, loss = 0.06501900\n",
      "Iteration 122, loss = 0.06458877\n",
      "Iteration 123, loss = 0.06426866\n",
      "Iteration 124, loss = 0.06407347\n",
      "Iteration 125, loss = 0.06377576\n",
      "Iteration 126, loss = 0.06340152\n",
      "Iteration 127, loss = 0.06304428\n",
      "Iteration 128, loss = 0.06276513\n",
      "Iteration 129, loss = 0.06244238\n",
      "Iteration 130, loss = 0.06213718\n",
      "Iteration 131, loss = 0.06189249\n",
      "Iteration 132, loss = 0.06164345\n",
      "Iteration 133, loss = 0.06135643\n",
      "Iteration 134, loss = 0.06108975\n",
      "Iteration 135, loss = 0.06086256\n",
      "Iteration 136, loss = 0.06058288\n",
      "Iteration 137, loss = 0.06022351\n",
      "Iteration 138, loss = 0.06010338\n",
      "Iteration 139, loss = 0.05976457\n",
      "Iteration 140, loss = 0.05954574\n",
      "Iteration 141, loss = 0.05928564\n",
      "Iteration 142, loss = 0.05903665\n",
      "Iteration 143, loss = 0.05886133\n",
      "Iteration 144, loss = 0.05862341\n",
      "Iteration 145, loss = 0.05838746\n",
      "Iteration 146, loss = 0.05816564\n",
      "Iteration 147, loss = 0.05795827\n",
      "Iteration 148, loss = 0.05770380\n",
      "Iteration 149, loss = 0.05752573\n",
      "Iteration 150, loss = 0.05732643\n",
      "Iteration 151, loss = 0.05712671\n",
      "Iteration 152, loss = 0.05692936\n",
      "Iteration 153, loss = 0.05670095\n",
      "Iteration 154, loss = 0.05651323\n",
      "Iteration 155, loss = 0.05635940\n",
      "Iteration 156, loss = 0.05615666\n",
      "Iteration 157, loss = 0.05597576\n",
      "Iteration 158, loss = 0.05585389\n",
      "Iteration 159, loss = 0.05559422\n",
      "Iteration 160, loss = 0.05540627\n",
      "Iteration 161, loss = 0.05531420\n",
      "Iteration 162, loss = 0.05510365\n",
      "Iteration 163, loss = 0.05491305\n",
      "Iteration 164, loss = 0.05479579\n",
      "Iteration 165, loss = 0.05462341\n",
      "Iteration 166, loss = 0.05445858\n",
      "Iteration 167, loss = 0.05430967\n",
      "Iteration 168, loss = 0.05417508\n",
      "Iteration 169, loss = 0.05401630\n",
      "Iteration 170, loss = 0.05385122\n",
      "Iteration 171, loss = 0.05368934\n",
      "Iteration 172, loss = 0.05355882\n",
      "Iteration 173, loss = 0.05345860\n",
      "Iteration 174, loss = 0.05328106\n",
      "Iteration 175, loss = 0.05315295\n",
      "Iteration 176, loss = 0.05306558\n",
      "Iteration 177, loss = 0.05288001\n",
      "Iteration 178, loss = 0.05277062\n",
      "Iteration 179, loss = 0.05265713\n",
      "Iteration 180, loss = 0.05251325\n",
      "Iteration 181, loss = 0.05237444\n",
      "Iteration 182, loss = 0.05230923\n",
      "Iteration 183, loss = 0.05212988\n",
      "Iteration 184, loss = 0.05206721\n",
      "Iteration 185, loss = 0.05192535\n",
      "Iteration 186, loss = 0.05179397\n",
      "Iteration 187, loss = 0.05172289\n",
      "Iteration 188, loss = 0.05154703\n",
      "Iteration 189, loss = 0.05146371\n",
      "Iteration 190, loss = 0.05139690\n",
      "Iteration 191, loss = 0.05124743\n",
      "Iteration 192, loss = 0.05114850\n",
      "Iteration 193, loss = 0.05105767\n",
      "Iteration 194, loss = 0.05094247\n",
      "Iteration 195, loss = 0.05084905\n",
      "Iteration 196, loss = 0.05075416\n",
      "Iteration 197, loss = 0.05063582\n",
      "Iteration 198, loss = 0.05056679\n",
      "Iteration 199, loss = 0.05047683\n",
      "Iteration 200, loss = 0.05035470\n",
      "[CV] END activation=relu, alpha=0.01, batch_size=100, hidden_layer_sizes=(400, 100), solver=sgd; total time= 6.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.36893160\n",
      "Iteration 2, loss = 0.57655146\n",
      "Iteration 3, loss = 0.44742112\n",
      "Iteration 4, loss = 0.39398056\n",
      "Iteration 5, loss = 0.36192461\n",
      "Iteration 6, loss = 0.33976029\n",
      "Iteration 7, loss = 0.32224863\n",
      "Iteration 8, loss = 0.30862460\n",
      "Iteration 9, loss = 0.29650439\n",
      "Iteration 10, loss = 0.28540968\n",
      "Iteration 11, loss = 0.27602966\n",
      "Iteration 12, loss = 0.26730835\n",
      "Iteration 13, loss = 0.25865775\n",
      "Iteration 14, loss = 0.25148153\n",
      "Iteration 15, loss = 0.24477479\n",
      "Iteration 16, loss = 0.23788489\n",
      "Iteration 17, loss = 0.23146890\n",
      "Iteration 18, loss = 0.22553276\n",
      "Iteration 19, loss = 0.22017999\n",
      "Iteration 20, loss = 0.21453854\n",
      "Iteration 21, loss = 0.20982492\n",
      "Iteration 22, loss = 0.20474560\n",
      "Iteration 23, loss = 0.20027535\n",
      "Iteration 24, loss = 0.19609561\n",
      "Iteration 25, loss = 0.19152977\n",
      "Iteration 26, loss = 0.18802079\n",
      "Iteration 27, loss = 0.18410981\n",
      "Iteration 28, loss = 0.18037149\n",
      "Iteration 29, loss = 0.17695633\n",
      "Iteration 30, loss = 0.17325882\n",
      "Iteration 31, loss = 0.16997989\n",
      "Iteration 32, loss = 0.16682382\n",
      "Iteration 33, loss = 0.16429198\n",
      "Iteration 34, loss = 0.16103395\n",
      "Iteration 35, loss = 0.15837566\n",
      "Iteration 36, loss = 0.15567387\n",
      "Iteration 37, loss = 0.15321416\n",
      "Iteration 38, loss = 0.15059172\n",
      "Iteration 39, loss = 0.14786543\n",
      "Iteration 40, loss = 0.14572179\n",
      "Iteration 41, loss = 0.14342633\n",
      "Iteration 42, loss = 0.14120121\n",
      "Iteration 43, loss = 0.13882179\n",
      "Iteration 44, loss = 0.13678689\n",
      "Iteration 45, loss = 0.13490052\n",
      "Iteration 46, loss = 0.13287668\n",
      "Iteration 47, loss = 0.13096544\n",
      "Iteration 48, loss = 0.12889585\n",
      "Iteration 49, loss = 0.12715936\n",
      "Iteration 50, loss = 0.12557699\n",
      "Iteration 51, loss = 0.12368914\n",
      "Iteration 52, loss = 0.12184897\n",
      "Iteration 53, loss = 0.12062522\n",
      "Iteration 54, loss = 0.11899635\n",
      "Iteration 55, loss = 0.11714546\n",
      "Iteration 56, loss = 0.11561016\n",
      "Iteration 57, loss = 0.11426358\n",
      "Iteration 58, loss = 0.11281358\n",
      "Iteration 59, loss = 0.11152018\n",
      "Iteration 60, loss = 0.11013896\n",
      "Iteration 61, loss = 0.10867494\n",
      "Iteration 62, loss = 0.10734635\n",
      "Iteration 63, loss = 0.10598376\n",
      "Iteration 64, loss = 0.10481748\n",
      "Iteration 65, loss = 0.10362260\n",
      "Iteration 66, loss = 0.10251802\n",
      "Iteration 67, loss = 0.10141697\n",
      "Iteration 68, loss = 0.10027790\n",
      "Iteration 69, loss = 0.09913900\n",
      "Iteration 70, loss = 0.09808729\n",
      "Iteration 71, loss = 0.09707210\n",
      "Iteration 72, loss = 0.09602890\n",
      "Iteration 73, loss = 0.09496824\n",
      "Iteration 74, loss = 0.09395685\n",
      "Iteration 75, loss = 0.09309226\n",
      "Iteration 76, loss = 0.09216277\n",
      "Iteration 77, loss = 0.09121284\n",
      "Iteration 78, loss = 0.09033347\n",
      "Iteration 79, loss = 0.08956543\n",
      "Iteration 80, loss = 0.08849607\n",
      "Iteration 81, loss = 0.08776882\n",
      "Iteration 82, loss = 0.08680094\n",
      "Iteration 83, loss = 0.08606338\n",
      "Iteration 84, loss = 0.08517600\n",
      "Iteration 85, loss = 0.08466748\n",
      "Iteration 86, loss = 0.08383961\n",
      "Iteration 87, loss = 0.08315246\n",
      "Iteration 88, loss = 0.08235942\n",
      "Iteration 89, loss = 0.08152273\n",
      "Iteration 90, loss = 0.08104517\n",
      "Iteration 91, loss = 0.08025381\n",
      "Iteration 92, loss = 0.07967947\n",
      "Iteration 93, loss = 0.07896403\n",
      "Iteration 94, loss = 0.07829644\n",
      "Iteration 95, loss = 0.07785197\n",
      "Iteration 96, loss = 0.07704688\n",
      "Iteration 97, loss = 0.07658046\n",
      "Iteration 98, loss = 0.07602473\n",
      "Iteration 99, loss = 0.07552245\n",
      "Iteration 100, loss = 0.07482052\n",
      "Iteration 101, loss = 0.07429680\n",
      "Iteration 102, loss = 0.07382695\n",
      "Iteration 103, loss = 0.07323582\n",
      "Iteration 104, loss = 0.07266619\n",
      "Iteration 105, loss = 0.07215650\n",
      "Iteration 106, loss = 0.07171721\n",
      "Iteration 107, loss = 0.07124234\n",
      "Iteration 108, loss = 0.07077371\n",
      "Iteration 109, loss = 0.07031406\n",
      "Iteration 110, loss = 0.06983080\n",
      "Iteration 111, loss = 0.06939207\n",
      "Iteration 112, loss = 0.06897385\n",
      "Iteration 113, loss = 0.06852490\n",
      "Iteration 114, loss = 0.06814042\n",
      "Iteration 115, loss = 0.06767039\n",
      "Iteration 116, loss = 0.06721297\n",
      "Iteration 117, loss = 0.06689995\n",
      "Iteration 118, loss = 0.06646781\n",
      "Iteration 119, loss = 0.06615990\n",
      "Iteration 120, loss = 0.06588634\n",
      "Iteration 121, loss = 0.06537249\n",
      "Iteration 122, loss = 0.06506721\n",
      "Iteration 123, loss = 0.06460177\n",
      "Iteration 124, loss = 0.06432881\n",
      "Iteration 125, loss = 0.06396690\n",
      "Iteration 126, loss = 0.06367663\n",
      "Iteration 127, loss = 0.06334598\n",
      "Iteration 128, loss = 0.06298569\n",
      "Iteration 129, loss = 0.06278581\n",
      "Iteration 130, loss = 0.06233424\n",
      "Iteration 131, loss = 0.06205604\n",
      "Iteration 132, loss = 0.06177617\n",
      "Iteration 133, loss = 0.06144049\n",
      "Iteration 134, loss = 0.06120410\n",
      "Iteration 135, loss = 0.06093505\n",
      "Iteration 136, loss = 0.06067006\n",
      "Iteration 137, loss = 0.06038390\n",
      "Iteration 138, loss = 0.06010163\n",
      "Iteration 139, loss = 0.05979609\n",
      "Iteration 140, loss = 0.05956281\n",
      "Iteration 141, loss = 0.05935872\n",
      "Iteration 142, loss = 0.05910448\n",
      "Iteration 143, loss = 0.05887664\n",
      "Iteration 144, loss = 0.05868160\n",
      "Iteration 145, loss = 0.05842010\n",
      "Iteration 146, loss = 0.05816193\n",
      "Iteration 147, loss = 0.05794014\n",
      "Iteration 148, loss = 0.05770985\n",
      "Iteration 149, loss = 0.05755465\n",
      "Iteration 150, loss = 0.05732621\n",
      "Iteration 151, loss = 0.05708420\n",
      "Iteration 152, loss = 0.05688360\n",
      "Iteration 153, loss = 0.05668785\n",
      "Iteration 154, loss = 0.05649875\n",
      "Iteration 155, loss = 0.05625569\n",
      "Iteration 156, loss = 0.05613015\n",
      "Iteration 157, loss = 0.05599051\n",
      "Iteration 158, loss = 0.05574296\n",
      "Iteration 159, loss = 0.05558781\n",
      "Iteration 160, loss = 0.05537795\n",
      "Iteration 161, loss = 0.05522245\n",
      "Iteration 162, loss = 0.05507917\n",
      "Iteration 163, loss = 0.05489099\n",
      "Iteration 164, loss = 0.05474894\n",
      "Iteration 165, loss = 0.05456154\n",
      "Iteration 166, loss = 0.05441706\n",
      "Iteration 167, loss = 0.05426931\n",
      "Iteration 168, loss = 0.05406056\n",
      "Iteration 169, loss = 0.05395659\n",
      "Iteration 170, loss = 0.05377972\n",
      "Iteration 171, loss = 0.05365694\n",
      "Iteration 172, loss = 0.05345996\n",
      "Iteration 173, loss = 0.05329472\n",
      "Iteration 174, loss = 0.05324781\n",
      "Iteration 175, loss = 0.05303200\n",
      "Iteration 176, loss = 0.05288372\n",
      "Iteration 177, loss = 0.05282472\n",
      "Iteration 178, loss = 0.05267593\n",
      "Iteration 179, loss = 0.05250736\n",
      "Iteration 180, loss = 0.05242515\n",
      "Iteration 181, loss = 0.05230371\n",
      "Iteration 182, loss = 0.05218352\n",
      "Iteration 183, loss = 0.05205101\n",
      "Iteration 184, loss = 0.05194295\n",
      "Iteration 185, loss = 0.05182374\n",
      "Iteration 186, loss = 0.05168660\n",
      "Iteration 187, loss = 0.05156971\n",
      "Iteration 188, loss = 0.05148610\n",
      "Iteration 189, loss = 0.05137759\n",
      "Iteration 190, loss = 0.05123553\n",
      "Iteration 191, loss = 0.05114362\n",
      "Iteration 192, loss = 0.05103166\n",
      "Iteration 193, loss = 0.05095008\n",
      "Iteration 194, loss = 0.05084057\n",
      "Iteration 195, loss = 0.05076599\n",
      "Iteration 196, loss = 0.05067669\n",
      "Iteration 197, loss = 0.05054796\n",
      "Iteration 198, loss = 0.05047071\n",
      "Iteration 199, loss = 0.05036720\n",
      "Iteration 200, loss = 0.05023904\n",
      "[CV] END activation=relu, alpha=0.01, batch_size=100, hidden_layer_sizes=(400, 100), solver=sgd; total time= 6.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.39618240\n",
      "Iteration 2, loss = 0.57728749\n",
      "Iteration 3, loss = 0.44007084\n",
      "Iteration 4, loss = 0.38393452\n",
      "Iteration 5, loss = 0.35204400\n",
      "Iteration 6, loss = 0.32856913\n",
      "Iteration 7, loss = 0.31041890\n",
      "Iteration 8, loss = 0.29494283\n",
      "Iteration 9, loss = 0.28233762\n",
      "Iteration 10, loss = 0.27064488\n",
      "Iteration 11, loss = 0.26029871\n",
      "Iteration 12, loss = 0.25115756\n",
      "Iteration 13, loss = 0.24272132\n",
      "Iteration 14, loss = 0.23547422\n",
      "Iteration 15, loss = 0.22809718\n",
      "Iteration 16, loss = 0.22128547\n",
      "Iteration 17, loss = 0.21510444\n",
      "Iteration 18, loss = 0.20955771\n",
      "Iteration 19, loss = 0.20387468\n",
      "Iteration 20, loss = 0.19902859\n",
      "Iteration 21, loss = 0.19425187\n",
      "Iteration 22, loss = 0.18999321\n",
      "Iteration 23, loss = 0.18548431\n",
      "Iteration 24, loss = 0.18141246\n",
      "Iteration 25, loss = 0.17784228\n",
      "Iteration 26, loss = 0.17425971\n",
      "Iteration 27, loss = 0.17065074\n",
      "Iteration 28, loss = 0.16730562\n",
      "Iteration 29, loss = 0.16422304\n",
      "Iteration 30, loss = 0.16120082\n",
      "Iteration 31, loss = 0.15833378\n",
      "Iteration 32, loss = 0.15526211\n",
      "Iteration 33, loss = 0.15260497\n",
      "Iteration 34, loss = 0.15015163\n",
      "Iteration 35, loss = 0.14734445\n",
      "Iteration 36, loss = 0.14523152\n",
      "Iteration 37, loss = 0.14298820\n",
      "Iteration 38, loss = 0.14055962\n",
      "Iteration 39, loss = 0.13808388\n",
      "Iteration 40, loss = 0.13621166\n",
      "Iteration 41, loss = 0.13407516\n",
      "Iteration 42, loss = 0.13212890\n",
      "Iteration 43, loss = 0.13027092\n",
      "Iteration 44, loss = 0.12839411\n",
      "Iteration 45, loss = 0.12671504\n",
      "Iteration 46, loss = 0.12473564\n",
      "Iteration 47, loss = 0.12318396\n",
      "Iteration 48, loss = 0.12145333\n",
      "Iteration 49, loss = 0.11985691\n",
      "Iteration 50, loss = 0.11823804\n",
      "Iteration 51, loss = 0.11663226\n",
      "Iteration 52, loss = 0.11522502\n",
      "Iteration 53, loss = 0.11370878\n",
      "Iteration 54, loss = 0.11234025\n",
      "Iteration 55, loss = 0.11078777\n",
      "Iteration 56, loss = 0.10961095\n",
      "Iteration 57, loss = 0.10831056\n",
      "Iteration 58, loss = 0.10704210\n",
      "Iteration 59, loss = 0.10576301\n",
      "Iteration 60, loss = 0.10433866\n",
      "Iteration 61, loss = 0.10348622\n",
      "Iteration 62, loss = 0.10221743\n",
      "Iteration 63, loss = 0.10116751\n",
      "Iteration 64, loss = 0.09996986\n",
      "Iteration 65, loss = 0.09889827\n",
      "Iteration 66, loss = 0.09789349\n",
      "Iteration 67, loss = 0.09684717\n",
      "Iteration 68, loss = 0.09593677\n",
      "Iteration 69, loss = 0.09494175\n",
      "Iteration 70, loss = 0.09376583\n",
      "Iteration 71, loss = 0.09285250\n",
      "Iteration 72, loss = 0.09208860\n",
      "Iteration 73, loss = 0.09109512\n",
      "Iteration 74, loss = 0.09027980\n",
      "Iteration 75, loss = 0.08955175\n",
      "Iteration 76, loss = 0.08862173\n",
      "Iteration 77, loss = 0.08776436\n",
      "Iteration 78, loss = 0.08702452\n",
      "Iteration 79, loss = 0.08633486\n",
      "Iteration 80, loss = 0.08545949\n",
      "Iteration 81, loss = 0.08467383\n",
      "Iteration 82, loss = 0.08394124\n",
      "Iteration 83, loss = 0.08315766\n",
      "Iteration 84, loss = 0.08237616\n",
      "Iteration 85, loss = 0.08193671\n",
      "Iteration 86, loss = 0.08113350\n",
      "Iteration 87, loss = 0.08043139\n",
      "Iteration 88, loss = 0.07992264\n",
      "Iteration 89, loss = 0.07922623\n",
      "Iteration 90, loss = 0.07857877\n",
      "Iteration 91, loss = 0.07796860\n",
      "Iteration 92, loss = 0.07729509\n",
      "Iteration 93, loss = 0.07675193\n",
      "Iteration 94, loss = 0.07614262\n",
      "Iteration 95, loss = 0.07548153\n",
      "Iteration 96, loss = 0.07495051\n",
      "Iteration 97, loss = 0.07444379\n",
      "Iteration 98, loss = 0.07394276\n",
      "Iteration 99, loss = 0.07336681\n",
      "Iteration 100, loss = 0.07301614\n",
      "Iteration 101, loss = 0.07233825\n",
      "Iteration 102, loss = 0.07188365\n",
      "Iteration 103, loss = 0.07142293\n",
      "Iteration 104, loss = 0.07089409\n",
      "Iteration 105, loss = 0.07051135\n",
      "Iteration 106, loss = 0.06997687\n",
      "Iteration 107, loss = 0.06954007\n",
      "Iteration 108, loss = 0.06911288\n",
      "Iteration 109, loss = 0.06867102\n",
      "Iteration 110, loss = 0.06814340\n",
      "Iteration 111, loss = 0.06788084\n",
      "Iteration 112, loss = 0.06735226\n",
      "Iteration 113, loss = 0.06705186\n",
      "Iteration 114, loss = 0.06663092\n",
      "Iteration 115, loss = 0.06622947\n",
      "Iteration 116, loss = 0.06583091\n",
      "Iteration 117, loss = 0.06549466\n",
      "Iteration 118, loss = 0.06513877\n",
      "Iteration 119, loss = 0.06479591\n",
      "Iteration 120, loss = 0.06435408\n",
      "Iteration 121, loss = 0.06418340\n",
      "Iteration 122, loss = 0.06372442\n",
      "Iteration 123, loss = 0.06345264\n",
      "Iteration 124, loss = 0.06309760\n",
      "Iteration 125, loss = 0.06275179\n",
      "Iteration 126, loss = 0.06241512\n",
      "Iteration 127, loss = 0.06221947\n",
      "Iteration 128, loss = 0.06184390\n",
      "Iteration 129, loss = 0.06158497\n",
      "Iteration 130, loss = 0.06126767\n",
      "Iteration 131, loss = 0.06102258\n",
      "Iteration 132, loss = 0.06065924\n",
      "Iteration 133, loss = 0.06046740\n",
      "Iteration 134, loss = 0.06012089\n",
      "Iteration 135, loss = 0.05981258\n",
      "Iteration 136, loss = 0.05959637\n",
      "Iteration 137, loss = 0.05936690\n",
      "Iteration 138, loss = 0.05908535\n",
      "Iteration 139, loss = 0.05889408\n",
      "Iteration 140, loss = 0.05863216\n",
      "Iteration 141, loss = 0.05833838\n",
      "Iteration 142, loss = 0.05815474\n",
      "Iteration 143, loss = 0.05790951\n",
      "Iteration 144, loss = 0.05771850\n",
      "Iteration 145, loss = 0.05756295\n",
      "Iteration 146, loss = 0.05727649\n",
      "Iteration 147, loss = 0.05701840\n",
      "Iteration 148, loss = 0.05684334\n",
      "Iteration 149, loss = 0.05669383\n",
      "Iteration 150, loss = 0.05647088\n",
      "Iteration 151, loss = 0.05626597\n",
      "Iteration 152, loss = 0.05605667\n",
      "Iteration 153, loss = 0.05587072\n",
      "Iteration 154, loss = 0.05567585\n",
      "Iteration 155, loss = 0.05549287\n",
      "Iteration 156, loss = 0.05526833\n",
      "Iteration 157, loss = 0.05512680\n",
      "Iteration 158, loss = 0.05498350\n",
      "Iteration 159, loss = 0.05482580\n",
      "Iteration 160, loss = 0.05457334\n",
      "Iteration 161, loss = 0.05443885\n",
      "Iteration 162, loss = 0.05427958\n",
      "Iteration 163, loss = 0.05415696\n",
      "Iteration 164, loss = 0.05394433\n",
      "Iteration 165, loss = 0.05378936\n",
      "Iteration 166, loss = 0.05368621\n",
      "Iteration 167, loss = 0.05351179\n",
      "Iteration 168, loss = 0.05335229\n",
      "Iteration 169, loss = 0.05322300\n",
      "Iteration 170, loss = 0.05309509\n",
      "Iteration 171, loss = 0.05292381\n",
      "Iteration 172, loss = 0.05280510\n",
      "Iteration 173, loss = 0.05265477\n",
      "Iteration 174, loss = 0.05253082\n",
      "Iteration 175, loss = 0.05242398\n",
      "Iteration 176, loss = 0.05226921\n",
      "Iteration 177, loss = 0.05214890\n",
      "Iteration 178, loss = 0.05202895\n",
      "Iteration 179, loss = 0.05187176\n",
      "Iteration 180, loss = 0.05173380\n",
      "Iteration 181, loss = 0.05162797\n",
      "Iteration 182, loss = 0.05149889\n",
      "Iteration 183, loss = 0.05139790\n",
      "Iteration 184, loss = 0.05127273\n",
      "Iteration 185, loss = 0.05116522\n",
      "Iteration 186, loss = 0.05107149\n",
      "Iteration 187, loss = 0.05095242\n",
      "Iteration 188, loss = 0.05085096\n",
      "Iteration 189, loss = 0.05073257\n",
      "Iteration 190, loss = 0.05065149\n",
      "Iteration 191, loss = 0.05053833\n",
      "Iteration 192, loss = 0.05044298\n",
      "Iteration 193, loss = 0.05033190\n",
      "Iteration 194, loss = 0.05020881\n",
      "Iteration 195, loss = 0.05015301\n",
      "Iteration 196, loss = 0.05005296\n",
      "Iteration 197, loss = 0.04991876\n",
      "Iteration 198, loss = 0.04984120\n",
      "Iteration 199, loss = 0.04975258\n",
      "Iteration 200, loss = 0.04966984\n",
      "[CV] END activation=relu, alpha=0.01, batch_size=100, hidden_layer_sizes=(400, 100), solver=sgd; total time= 6.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.31887077\n",
      "Iteration 2, loss = 0.56242580\n",
      "Iteration 3, loss = 0.44047529\n",
      "Iteration 4, loss = 0.39061298\n",
      "Iteration 5, loss = 0.36088404\n",
      "Iteration 6, loss = 0.33987938\n",
      "Iteration 7, loss = 0.32373261\n",
      "Iteration 8, loss = 0.30986376\n",
      "Iteration 9, loss = 0.29766809\n",
      "Iteration 10, loss = 0.28762175\n",
      "Iteration 11, loss = 0.27823414\n",
      "Iteration 12, loss = 0.26921587\n",
      "Iteration 13, loss = 0.26115122\n",
      "Iteration 14, loss = 0.25357059\n",
      "Iteration 15, loss = 0.24612724\n",
      "Iteration 16, loss = 0.23951963\n",
      "Iteration 17, loss = 0.23329347\n",
      "Iteration 18, loss = 0.22731015\n",
      "Iteration 19, loss = 0.22155570\n",
      "Iteration 20, loss = 0.21613325\n",
      "Iteration 21, loss = 0.21075464\n",
      "Iteration 22, loss = 0.20547174\n",
      "Iteration 23, loss = 0.20076540\n",
      "Iteration 24, loss = 0.19640047\n",
      "Iteration 25, loss = 0.19194206\n",
      "Iteration 26, loss = 0.18779092\n",
      "Iteration 27, loss = 0.18387380\n",
      "Iteration 28, loss = 0.17993283\n",
      "Iteration 29, loss = 0.17619205\n",
      "Iteration 30, loss = 0.17263733\n",
      "Iteration 31, loss = 0.16955455\n",
      "Iteration 32, loss = 0.16601231\n",
      "Iteration 33, loss = 0.16292002\n",
      "Iteration 34, loss = 0.15996356\n",
      "Iteration 35, loss = 0.15717497\n",
      "Iteration 36, loss = 0.15437776\n",
      "Iteration 37, loss = 0.15176623\n",
      "Iteration 38, loss = 0.14878370\n",
      "Iteration 39, loss = 0.14662886\n",
      "Iteration 40, loss = 0.14405393\n",
      "Iteration 41, loss = 0.14183129\n",
      "Iteration 42, loss = 0.13966554\n",
      "Iteration 43, loss = 0.13723004\n",
      "Iteration 44, loss = 0.13527452\n",
      "Iteration 45, loss = 0.13307127\n",
      "Iteration 46, loss = 0.13118631\n",
      "Iteration 47, loss = 0.12942410\n",
      "Iteration 48, loss = 0.12727645\n",
      "Iteration 49, loss = 0.12539530\n",
      "Iteration 50, loss = 0.12386454\n",
      "Iteration 51, loss = 0.12224140\n",
      "Iteration 52, loss = 0.12049799\n",
      "Iteration 53, loss = 0.11873897\n",
      "Iteration 54, loss = 0.11686679\n",
      "Iteration 55, loss = 0.11569727\n",
      "Iteration 56, loss = 0.11415090\n",
      "Iteration 57, loss = 0.11264195\n",
      "Iteration 58, loss = 0.11120030\n",
      "Iteration 59, loss = 0.10983916\n",
      "Iteration 60, loss = 0.10866378\n",
      "Iteration 61, loss = 0.10716062\n",
      "Iteration 62, loss = 0.10589161\n",
      "Iteration 63, loss = 0.10448988\n",
      "Iteration 64, loss = 0.10340981\n",
      "Iteration 65, loss = 0.10220394\n",
      "Iteration 66, loss = 0.10125939\n",
      "Iteration 67, loss = 0.10011945\n",
      "Iteration 68, loss = 0.09885676\n",
      "Iteration 69, loss = 0.09780724\n",
      "Iteration 70, loss = 0.09675163\n",
      "Iteration 71, loss = 0.09566799\n",
      "Iteration 72, loss = 0.09475686\n",
      "Iteration 73, loss = 0.09375060\n",
      "Iteration 74, loss = 0.09259477\n",
      "Iteration 75, loss = 0.09170150\n",
      "Iteration 76, loss = 0.09093685\n",
      "Iteration 77, loss = 0.08997522\n",
      "Iteration 78, loss = 0.08917695\n",
      "Iteration 79, loss = 0.08822074\n",
      "Iteration 80, loss = 0.08731058\n",
      "Iteration 81, loss = 0.08653962\n",
      "Iteration 82, loss = 0.08570007\n",
      "Iteration 83, loss = 0.08474387\n",
      "Iteration 84, loss = 0.08408102\n",
      "Iteration 85, loss = 0.08337158\n",
      "Iteration 86, loss = 0.08267923\n",
      "Iteration 87, loss = 0.08181228\n",
      "Iteration 88, loss = 0.08136846\n",
      "Iteration 89, loss = 0.08057576\n",
      "Iteration 90, loss = 0.07982886\n",
      "Iteration 91, loss = 0.07930199\n",
      "Iteration 92, loss = 0.07862123\n",
      "Iteration 93, loss = 0.07806858\n",
      "Iteration 94, loss = 0.07737250\n",
      "Iteration 95, loss = 0.07668385\n",
      "Iteration 96, loss = 0.07615993\n",
      "Iteration 97, loss = 0.07559768\n",
      "Iteration 98, loss = 0.07498519\n",
      "Iteration 99, loss = 0.07440421\n",
      "Iteration 100, loss = 0.07387486\n",
      "Iteration 101, loss = 0.07340525\n",
      "Iteration 102, loss = 0.07281986\n",
      "Iteration 103, loss = 0.07237335\n",
      "Iteration 104, loss = 0.07190935\n",
      "Iteration 105, loss = 0.07137197\n",
      "Iteration 106, loss = 0.07091634\n",
      "Iteration 107, loss = 0.07045095\n",
      "Iteration 108, loss = 0.07003605\n",
      "Iteration 109, loss = 0.06951959\n",
      "Iteration 110, loss = 0.06900981\n",
      "Iteration 111, loss = 0.06859801\n",
      "Iteration 112, loss = 0.06827222\n",
      "Iteration 113, loss = 0.06782387\n",
      "Iteration 114, loss = 0.06727219\n",
      "Iteration 115, loss = 0.06696358\n",
      "Iteration 116, loss = 0.06651709\n",
      "Iteration 117, loss = 0.06616730\n",
      "Iteration 118, loss = 0.06574823\n",
      "Iteration 119, loss = 0.06543446\n",
      "Iteration 120, loss = 0.06506544\n",
      "Iteration 121, loss = 0.06470891\n",
      "Iteration 122, loss = 0.06433827\n",
      "Iteration 123, loss = 0.06401479\n",
      "Iteration 124, loss = 0.06360385\n",
      "Iteration 125, loss = 0.06336274\n",
      "Iteration 126, loss = 0.06298178\n",
      "Iteration 127, loss = 0.06265762\n",
      "Iteration 128, loss = 0.06229576\n",
      "Iteration 129, loss = 0.06206766\n",
      "Iteration 130, loss = 0.06171917\n",
      "Iteration 131, loss = 0.06147892\n",
      "Iteration 132, loss = 0.06111038\n",
      "Iteration 133, loss = 0.06086713\n",
      "Iteration 134, loss = 0.06054286\n",
      "Iteration 135, loss = 0.06032224\n",
      "Iteration 136, loss = 0.06000682\n",
      "Iteration 137, loss = 0.05970806\n",
      "Iteration 138, loss = 0.05947662\n",
      "Iteration 139, loss = 0.05921439\n",
      "Iteration 140, loss = 0.05891705\n",
      "Iteration 141, loss = 0.05874180\n",
      "Iteration 142, loss = 0.05852034\n",
      "Iteration 143, loss = 0.05823116\n",
      "Iteration 144, loss = 0.05795761\n",
      "Iteration 145, loss = 0.05777777\n",
      "Iteration 146, loss = 0.05749390\n",
      "Iteration 147, loss = 0.05733925\n",
      "Iteration 148, loss = 0.05713509\n",
      "Iteration 149, loss = 0.05693413\n",
      "Iteration 150, loss = 0.05669934\n",
      "Iteration 151, loss = 0.05649839\n",
      "Iteration 152, loss = 0.05631941\n",
      "Iteration 153, loss = 0.05610454\n",
      "Iteration 154, loss = 0.05591513\n",
      "Iteration 155, loss = 0.05568082\n",
      "Iteration 156, loss = 0.05550159\n",
      "Iteration 157, loss = 0.05533547\n",
      "Iteration 158, loss = 0.05512635\n",
      "Iteration 159, loss = 0.05501018\n",
      "Iteration 160, loss = 0.05477475\n",
      "Iteration 161, loss = 0.05459902\n",
      "Iteration 162, loss = 0.05449122\n",
      "Iteration 163, loss = 0.05432039\n",
      "Iteration 164, loss = 0.05412718\n",
      "Iteration 165, loss = 0.05395981\n",
      "Iteration 166, loss = 0.05383634\n",
      "Iteration 167, loss = 0.05365949\n",
      "Iteration 168, loss = 0.05350297\n",
      "Iteration 169, loss = 0.05337330\n",
      "Iteration 170, loss = 0.05321443\n",
      "Iteration 171, loss = 0.05307448\n",
      "Iteration 172, loss = 0.05291800\n",
      "Iteration 173, loss = 0.05281275\n",
      "Iteration 174, loss = 0.05265944\n",
      "Iteration 175, loss = 0.05253233\n",
      "Iteration 176, loss = 0.05240453\n",
      "Iteration 177, loss = 0.05228555\n",
      "Iteration 178, loss = 0.05214970\n",
      "Iteration 179, loss = 0.05200649\n",
      "Iteration 180, loss = 0.05193115\n",
      "Iteration 181, loss = 0.05174400\n",
      "Iteration 182, loss = 0.05168214\n",
      "Iteration 183, loss = 0.05153988\n",
      "Iteration 184, loss = 0.05139970\n",
      "Iteration 185, loss = 0.05130523\n",
      "Iteration 186, loss = 0.05119820\n",
      "Iteration 187, loss = 0.05105386\n",
      "Iteration 188, loss = 0.05098967\n",
      "Iteration 189, loss = 0.05089626\n",
      "Iteration 190, loss = 0.05077751\n",
      "Iteration 191, loss = 0.05067938\n",
      "Iteration 192, loss = 0.05056396\n",
      "Iteration 193, loss = 0.05046429\n",
      "Iteration 194, loss = 0.05038333\n",
      "Iteration 195, loss = 0.05026612\n",
      "Iteration 196, loss = 0.05016715\n",
      "Iteration 197, loss = 0.05005609\n",
      "Iteration 198, loss = 0.04997116\n",
      "Iteration 199, loss = 0.04990102\n",
      "Iteration 200, loss = 0.04979993\n",
      "[CV] END activation=relu, alpha=0.01, batch_size=100, hidden_layer_sizes=(400, 100), solver=sgd; total time= 6.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.41597694\n",
      "Iteration 2, loss = 0.60103282\n",
      "Iteration 3, loss = 0.45615015\n",
      "Iteration 4, loss = 0.39857391\n",
      "Iteration 5, loss = 0.36477950\n",
      "Iteration 6, loss = 0.34246396\n",
      "Iteration 7, loss = 0.32447328\n",
      "Iteration 8, loss = 0.31016395\n",
      "Iteration 9, loss = 0.29748086\n",
      "Iteration 10, loss = 0.28657901\n",
      "Iteration 11, loss = 0.27658353\n",
      "Iteration 12, loss = 0.26767716\n",
      "Iteration 13, loss = 0.25936412\n",
      "Iteration 14, loss = 0.25205125\n",
      "Iteration 15, loss = 0.24473131\n",
      "Iteration 16, loss = 0.23772038\n",
      "Iteration 17, loss = 0.23147208\n",
      "Iteration 18, loss = 0.22538236\n",
      "Iteration 19, loss = 0.21973629\n",
      "Iteration 20, loss = 0.21391349\n",
      "Iteration 21, loss = 0.20894755\n",
      "Iteration 22, loss = 0.20417963\n",
      "Iteration 23, loss = 0.19927998\n",
      "Iteration 24, loss = 0.19516748\n",
      "Iteration 25, loss = 0.19085514\n",
      "Iteration 26, loss = 0.18650250\n",
      "Iteration 27, loss = 0.18263686\n",
      "Iteration 28, loss = 0.17893823\n",
      "Iteration 29, loss = 0.17546290\n",
      "Iteration 30, loss = 0.17173314\n",
      "Iteration 31, loss = 0.16860154\n",
      "Iteration 32, loss = 0.16571062\n",
      "Iteration 33, loss = 0.16234331\n",
      "Iteration 34, loss = 0.15941113\n",
      "Iteration 35, loss = 0.15662371\n",
      "Iteration 36, loss = 0.15390666\n",
      "Iteration 37, loss = 0.15111437\n",
      "Iteration 38, loss = 0.14877922\n",
      "Iteration 39, loss = 0.14621676\n",
      "Iteration 40, loss = 0.14366218\n",
      "Iteration 41, loss = 0.14147785\n",
      "Iteration 42, loss = 0.13923678\n",
      "Iteration 43, loss = 0.13714662\n",
      "Iteration 44, loss = 0.13488650\n",
      "Iteration 45, loss = 0.13299668\n",
      "Iteration 46, loss = 0.13121252\n",
      "Iteration 47, loss = 0.12911062\n",
      "Iteration 48, loss = 0.12711292\n",
      "Iteration 49, loss = 0.12559175\n",
      "Iteration 50, loss = 0.12361333\n",
      "Iteration 51, loss = 0.12176662\n",
      "Iteration 52, loss = 0.12033165\n",
      "Iteration 53, loss = 0.11872462\n",
      "Iteration 54, loss = 0.11698863\n",
      "Iteration 55, loss = 0.11560850\n",
      "Iteration 56, loss = 0.11410845\n",
      "Iteration 57, loss = 0.11280139\n",
      "Iteration 58, loss = 0.11148418\n",
      "Iteration 59, loss = 0.11014463\n",
      "Iteration 60, loss = 0.10851518\n",
      "Iteration 61, loss = 0.10737008\n",
      "Iteration 62, loss = 0.10607349\n",
      "Iteration 63, loss = 0.10485827\n",
      "Iteration 64, loss = 0.10370593\n",
      "Iteration 65, loss = 0.10258127\n",
      "Iteration 66, loss = 0.10128427\n",
      "Iteration 67, loss = 0.10030825\n",
      "Iteration 68, loss = 0.09911920\n",
      "Iteration 69, loss = 0.09816249\n",
      "Iteration 70, loss = 0.09722725\n",
      "Iteration 71, loss = 0.09601618\n",
      "Iteration 72, loss = 0.09480191\n",
      "Iteration 73, loss = 0.09435753\n",
      "Iteration 74, loss = 0.09313457\n",
      "Iteration 75, loss = 0.09212779\n",
      "Iteration 76, loss = 0.09121725\n",
      "Iteration 77, loss = 0.09045670\n",
      "Iteration 78, loss = 0.08953438\n",
      "Iteration 79, loss = 0.08868826\n",
      "Iteration 80, loss = 0.08781995\n",
      "Iteration 81, loss = 0.08705447\n",
      "Iteration 82, loss = 0.08626556\n",
      "Iteration 83, loss = 0.08547116\n",
      "Iteration 84, loss = 0.08486936\n",
      "Iteration 85, loss = 0.08385481\n",
      "Iteration 86, loss = 0.08323312\n",
      "Iteration 87, loss = 0.08249985\n",
      "Iteration 88, loss = 0.08181181\n",
      "Iteration 89, loss = 0.08113310\n",
      "Iteration 90, loss = 0.08044712\n",
      "Iteration 91, loss = 0.07976809\n",
      "Iteration 92, loss = 0.07913789\n",
      "Iteration 93, loss = 0.07853280\n",
      "Iteration 94, loss = 0.07788433\n",
      "Iteration 95, loss = 0.07726990\n",
      "Iteration 96, loss = 0.07657401\n",
      "Iteration 97, loss = 0.07607610\n",
      "Iteration 98, loss = 0.07547461\n",
      "Iteration 99, loss = 0.07500275\n",
      "Iteration 100, loss = 0.07436907\n",
      "Iteration 101, loss = 0.07391070\n",
      "Iteration 102, loss = 0.07332496\n",
      "Iteration 103, loss = 0.07286003\n",
      "Iteration 104, loss = 0.07248305\n",
      "Iteration 105, loss = 0.07175088\n",
      "Iteration 106, loss = 0.07142456\n",
      "Iteration 107, loss = 0.07088820\n",
      "Iteration 108, loss = 0.07041647\n",
      "Iteration 109, loss = 0.06992394\n",
      "Iteration 110, loss = 0.06945617\n",
      "Iteration 111, loss = 0.06906300\n",
      "Iteration 112, loss = 0.06860025\n",
      "Iteration 113, loss = 0.06817098\n",
      "Iteration 114, loss = 0.06771972\n",
      "Iteration 115, loss = 0.06736358\n",
      "Iteration 116, loss = 0.06697128\n",
      "Iteration 117, loss = 0.06657098\n",
      "Iteration 118, loss = 0.06619040\n",
      "Iteration 119, loss = 0.06583686\n",
      "Iteration 120, loss = 0.06552557\n",
      "Iteration 121, loss = 0.06509439\n",
      "Iteration 122, loss = 0.06479343\n",
      "Iteration 123, loss = 0.06430779\n",
      "Iteration 124, loss = 0.06403757\n",
      "Iteration 125, loss = 0.06369181\n",
      "Iteration 126, loss = 0.06336185\n",
      "Iteration 127, loss = 0.06309675\n",
      "Iteration 128, loss = 0.06274753\n",
      "Iteration 129, loss = 0.06243755\n",
      "Iteration 130, loss = 0.06215012\n",
      "Iteration 131, loss = 0.06180201\n",
      "Iteration 132, loss = 0.06153134\n",
      "Iteration 133, loss = 0.06126775\n",
      "Iteration 134, loss = 0.06092647\n",
      "Iteration 135, loss = 0.06064233\n",
      "Iteration 136, loss = 0.06042564\n",
      "Iteration 137, loss = 0.06013770\n",
      "Iteration 138, loss = 0.05982990\n",
      "Iteration 139, loss = 0.05959228\n",
      "Iteration 140, loss = 0.05938956\n",
      "Iteration 141, loss = 0.05912807\n",
      "Iteration 142, loss = 0.05888722\n",
      "Iteration 143, loss = 0.05861100\n",
      "Iteration 144, loss = 0.05840539\n",
      "Iteration 145, loss = 0.05816606\n",
      "Iteration 146, loss = 0.05798778\n",
      "Iteration 147, loss = 0.05770670\n",
      "Iteration 148, loss = 0.05745512\n",
      "Iteration 149, loss = 0.05727406\n",
      "Iteration 150, loss = 0.05707087\n",
      "Iteration 151, loss = 0.05685737\n",
      "Iteration 152, loss = 0.05664555\n",
      "Iteration 153, loss = 0.05641932\n",
      "Iteration 154, loss = 0.05623057\n",
      "Iteration 155, loss = 0.05607644\n",
      "Iteration 156, loss = 0.05582585\n",
      "Iteration 157, loss = 0.05566243\n",
      "Iteration 158, loss = 0.05552355\n",
      "Iteration 159, loss = 0.05529073\n",
      "Iteration 160, loss = 0.05512224\n",
      "Iteration 161, loss = 0.05493487\n",
      "Iteration 162, loss = 0.05476330\n",
      "Iteration 163, loss = 0.05459414\n",
      "Iteration 164, loss = 0.05442336\n",
      "Iteration 165, loss = 0.05428870\n",
      "Iteration 166, loss = 0.05409936\n",
      "Iteration 167, loss = 0.05396793\n",
      "Iteration 168, loss = 0.05380796\n",
      "Iteration 169, loss = 0.05364273\n",
      "Iteration 170, loss = 0.05346026\n",
      "Iteration 171, loss = 0.05331796\n",
      "Iteration 172, loss = 0.05316251\n",
      "Iteration 173, loss = 0.05304592\n",
      "Iteration 174, loss = 0.05290520\n",
      "Iteration 175, loss = 0.05279684\n",
      "Iteration 176, loss = 0.05262344\n",
      "Iteration 177, loss = 0.05250223\n",
      "Iteration 178, loss = 0.05238317\n",
      "Iteration 179, loss = 0.05222940\n",
      "Iteration 180, loss = 0.05211425\n",
      "Iteration 181, loss = 0.05195743\n",
      "Iteration 182, loss = 0.05188690\n",
      "Iteration 183, loss = 0.05172220\n",
      "Iteration 184, loss = 0.05161055\n",
      "Iteration 185, loss = 0.05149747\n",
      "Iteration 186, loss = 0.05134978\n",
      "Iteration 187, loss = 0.05122551\n",
      "Iteration 188, loss = 0.05118607\n",
      "Iteration 189, loss = 0.05099237\n",
      "Iteration 190, loss = 0.05093109\n",
      "Iteration 191, loss = 0.05076994\n",
      "Iteration 192, loss = 0.05069142\n",
      "Iteration 193, loss = 0.05058412\n",
      "Iteration 194, loss = 0.05050481\n",
      "Iteration 195, loss = 0.05038400\n",
      "Iteration 196, loss = 0.05025621\n",
      "Iteration 197, loss = 0.05017929\n",
      "Iteration 198, loss = 0.05009144\n",
      "Iteration 199, loss = 0.04995710\n",
      "Iteration 200, loss = 0.04989722\n",
      "[CV] END activation=relu, alpha=0.01, batch_size=100, hidden_layer_sizes=(400, 100), solver=sgd; total time= 6.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.44395011\n",
      "Iteration 2, loss = 0.17896453\n",
      "Iteration 3, loss = 0.13022564\n",
      "Iteration 4, loss = 0.10360944\n",
      "Iteration 5, loss = 0.09080024\n",
      "Iteration 6, loss = 0.07571987\n",
      "Iteration 7, loss = 0.06670698\n",
      "Iteration 8, loss = 0.05916604\n",
      "Iteration 9, loss = 0.05385017\n",
      "Iteration 10, loss = 0.05051123\n",
      "Iteration 11, loss = 0.05215259\n",
      "Iteration 12, loss = 0.05083755\n",
      "Iteration 13, loss = 0.04481878\n",
      "Iteration 14, loss = 0.04440435\n",
      "Iteration 15, loss = 0.04053207\n",
      "Iteration 16, loss = 0.04475029\n",
      "Iteration 17, loss = 0.03914036\n",
      "Iteration 18, loss = 0.04655792\n",
      "Iteration 19, loss = 0.04221387\n",
      "Iteration 20, loss = 0.03830059\n",
      "Iteration 21, loss = 0.03979991\n",
      "Iteration 22, loss = 0.04246218\n",
      "Iteration 23, loss = 0.04198147\n",
      "Iteration 24, loss = 0.03691085\n",
      "Iteration 25, loss = 0.03881172\n",
      "Iteration 26, loss = 0.03815876\n",
      "Iteration 27, loss = 0.03787407\n",
      "Iteration 28, loss = 0.03813330\n",
      "Iteration 29, loss = 0.03481912\n",
      "Iteration 30, loss = 0.03481877\n",
      "Iteration 31, loss = 0.04243876\n",
      "Iteration 32, loss = 0.03805593\n",
      "Iteration 33, loss = 0.02975419\n",
      "Iteration 34, loss = 0.03175792\n",
      "Iteration 35, loss = 0.03981707\n",
      "Iteration 36, loss = 0.04278660\n",
      "Iteration 37, loss = 0.03416364\n",
      "Iteration 38, loss = 0.03096546\n",
      "Iteration 39, loss = 0.03137410\n",
      "Iteration 40, loss = 0.03574547\n",
      "Iteration 41, loss = 0.03732794\n",
      "Iteration 42, loss = 0.03900734\n",
      "Iteration 43, loss = 0.03248401\n",
      "Iteration 44, loss = 0.03264902\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.01, batch_size=100, hidden_layer_sizes=(100, 100, 100), solver=adam; total time= 1.7min\n",
      "Iteration 1, loss = 0.42342974\n",
      "Iteration 2, loss = 0.18231628\n",
      "Iteration 3, loss = 0.13653523\n",
      "Iteration 4, loss = 0.11282928\n",
      "Iteration 5, loss = 0.09492178\n",
      "Iteration 6, loss = 0.08428589\n",
      "Iteration 7, loss = 0.07015528\n",
      "Iteration 8, loss = 0.06678936\n",
      "Iteration 9, loss = 0.06100658\n",
      "Iteration 10, loss = 0.05641643\n",
      "Iteration 11, loss = 0.05432599\n",
      "Iteration 12, loss = 0.05138583\n",
      "Iteration 13, loss = 0.04947206\n",
      "Iteration 14, loss = 0.04593220\n",
      "Iteration 15, loss = 0.04823256\n",
      "Iteration 16, loss = 0.04651941\n",
      "Iteration 17, loss = 0.04115207\n",
      "Iteration 18, loss = 0.04445367\n",
      "Iteration 19, loss = 0.03911515\n",
      "Iteration 20, loss = 0.04646817\n",
      "Iteration 21, loss = 0.04301408\n",
      "Iteration 22, loss = 0.03866745\n",
      "Iteration 23, loss = 0.04368080\n",
      "Iteration 24, loss = 0.03500757\n",
      "Iteration 25, loss = 0.04365331\n",
      "Iteration 26, loss = 0.04436450\n",
      "Iteration 27, loss = 0.03707076\n",
      "Iteration 28, loss = 0.03711749\n",
      "Iteration 29, loss = 0.04941904\n",
      "Iteration 30, loss = 0.04099061\n",
      "Iteration 31, loss = 0.03174316\n",
      "Iteration 32, loss = 0.02866186\n",
      "Iteration 33, loss = 0.02704792\n",
      "Iteration 34, loss = 0.02558614\n",
      "Iteration 35, loss = 0.02530885\n",
      "Iteration 36, loss = 0.07204450\n",
      "Iteration 37, loss = 0.04255971\n",
      "Iteration 38, loss = 0.03258176\n",
      "Iteration 39, loss = 0.03119147\n",
      "Iteration 40, loss = 0.03813368\n",
      "Iteration 41, loss = 0.03830109\n",
      "Iteration 42, loss = 0.03527090\n",
      "Iteration 43, loss = 0.03282792\n",
      "Iteration 44, loss = 0.04030571\n",
      "Iteration 45, loss = 0.03588475\n",
      "Iteration 46, loss = 0.03481626\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.01, batch_size=100, hidden_layer_sizes=(100, 100, 100), solver=adam; total time= 1.5min\n",
      "Iteration 1, loss = 0.41201340\n",
      "Iteration 2, loss = 0.17686405\n",
      "Iteration 3, loss = 0.13199412\n",
      "Iteration 4, loss = 0.10287886\n",
      "Iteration 5, loss = 0.08959594\n",
      "Iteration 6, loss = 0.07483669\n",
      "Iteration 7, loss = 0.06571444\n",
      "Iteration 8, loss = 0.05952169\n",
      "Iteration 9, loss = 0.05693683\n",
      "Iteration 10, loss = 0.04821475\n",
      "Iteration 11, loss = 0.05037290\n",
      "Iteration 12, loss = 0.04683353\n",
      "Iteration 13, loss = 0.04642029\n",
      "Iteration 14, loss = 0.04923265\n",
      "Iteration 15, loss = 0.04366126\n",
      "Iteration 16, loss = 0.04469092\n",
      "Iteration 17, loss = 0.04160536\n",
      "Iteration 18, loss = 0.04134679\n",
      "Iteration 19, loss = 0.04160110\n",
      "Iteration 20, loss = 0.04392472\n",
      "Iteration 21, loss = 0.04140190\n",
      "Iteration 22, loss = 0.03533098\n",
      "Iteration 23, loss = 0.04071490\n",
      "Iteration 24, loss = 0.04118197\n",
      "Iteration 25, loss = 0.03688872\n",
      "Iteration 26, loss = 0.03845799\n",
      "Iteration 27, loss = 0.03658953\n",
      "Iteration 28, loss = 0.04222939\n",
      "Iteration 29, loss = 0.03476049\n",
      "Iteration 30, loss = 0.03377073\n",
      "Iteration 31, loss = 0.04570606\n",
      "Iteration 32, loss = 0.03652166\n",
      "Iteration 33, loss = 0.03044348\n",
      "Iteration 34, loss = 0.02725657\n",
      "Iteration 35, loss = 0.02546779\n",
      "Iteration 36, loss = 0.02400921\n",
      "Iteration 37, loss = 0.02281396\n",
      "Iteration 38, loss = 0.06187215\n",
      "Iteration 39, loss = 0.03942143\n",
      "Iteration 40, loss = 0.03603117\n",
      "Iteration 41, loss = 0.03422304\n",
      "Iteration 42, loss = 0.03708264\n",
      "Iteration 43, loss = 0.03337489\n",
      "Iteration 44, loss = 0.03074321\n",
      "Iteration 45, loss = 0.03947873\n",
      "Iteration 46, loss = 0.03309338\n",
      "Iteration 47, loss = 0.02646677\n",
      "Iteration 48, loss = 0.02466573\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.01, batch_size=100, hidden_layer_sizes=(100, 100, 100), solver=adam; total time= 1.7min\n",
      "Iteration 1, loss = 0.40759582\n",
      "Iteration 2, loss = 0.17175290\n",
      "Iteration 3, loss = 0.12892199\n",
      "Iteration 4, loss = 0.10393619\n",
      "Iteration 5, loss = 0.09082809\n",
      "Iteration 6, loss = 0.07463895\n",
      "Iteration 7, loss = 0.06638523\n",
      "Iteration 8, loss = 0.06133514\n",
      "Iteration 9, loss = 0.05725691\n",
      "Iteration 10, loss = 0.05202744\n",
      "Iteration 11, loss = 0.05013106\n",
      "Iteration 12, loss = 0.04890872\n",
      "Iteration 13, loss = 0.04695370\n",
      "Iteration 14, loss = 0.04297280\n",
      "Iteration 15, loss = 0.04406555\n",
      "Iteration 16, loss = 0.04573378\n",
      "Iteration 17, loss = 0.04171087\n",
      "Iteration 18, loss = 0.04871744\n",
      "Iteration 19, loss = 0.03785839\n",
      "Iteration 20, loss = 0.03667975\n",
      "Iteration 21, loss = 0.04281408\n",
      "Iteration 22, loss = 0.04064929\n",
      "Iteration 23, loss = 0.04133440\n",
      "Iteration 24, loss = 0.04330039\n",
      "Iteration 25, loss = 0.03914390\n",
      "Iteration 26, loss = 0.03645716\n",
      "Iteration 27, loss = 0.03501821\n",
      "Iteration 28, loss = 0.03762378\n",
      "Iteration 29, loss = 0.04449345\n",
      "Iteration 30, loss = 0.04397972\n",
      "Iteration 31, loss = 0.03306282\n",
      "Iteration 32, loss = 0.03424455\n",
      "Iteration 33, loss = 0.03879146\n",
      "Iteration 34, loss = 0.04031063\n",
      "Iteration 35, loss = 0.03861146\n",
      "Iteration 36, loss = 0.03555887\n",
      "Iteration 37, loss = 0.03207208\n",
      "Iteration 38, loss = 0.03977535\n",
      "Iteration 39, loss = 0.04112818\n",
      "Iteration 40, loss = 0.03560722\n",
      "Iteration 41, loss = 0.03785158\n",
      "Iteration 42, loss = 0.03372245\n",
      "Iteration 43, loss = 0.03240490\n",
      "Iteration 44, loss = 0.03653690\n",
      "Iteration 45, loss = 0.03760031\n",
      "Iteration 46, loss = 0.03455350\n",
      "Iteration 47, loss = 0.03501345\n",
      "Iteration 48, loss = 0.02903863\n",
      "Iteration 49, loss = 0.02592406\n",
      "Iteration 50, loss = 0.02434209\n",
      "Iteration 51, loss = 0.02295536\n",
      "Iteration 52, loss = 0.04856686\n",
      "Iteration 53, loss = 0.04147618\n",
      "Iteration 54, loss = 0.03353748\n",
      "Iteration 55, loss = 0.02961134\n",
      "Iteration 56, loss = 0.03076015\n",
      "Iteration 57, loss = 0.03846603\n",
      "Iteration 58, loss = 0.03412323\n",
      "Iteration 59, loss = 0.03465887\n",
      "Iteration 60, loss = 0.03267529\n",
      "Iteration 61, loss = 0.03199035\n",
      "Iteration 62, loss = 0.02811739\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.01, batch_size=100, hidden_layer_sizes=(100, 100, 100), solver=adam; total time= 2.3min\n",
      "Iteration 1, loss = 0.43050394\n",
      "Iteration 2, loss = 0.17266539\n",
      "Iteration 3, loss = 0.13047388\n",
      "Iteration 4, loss = 0.10377564\n",
      "Iteration 5, loss = 0.08754699\n",
      "Iteration 6, loss = 0.07544770\n",
      "Iteration 7, loss = 0.06830189\n",
      "Iteration 8, loss = 0.06063346\n",
      "Iteration 9, loss = 0.05551431\n",
      "Iteration 10, loss = 0.05112299\n",
      "Iteration 11, loss = 0.05383057\n",
      "Iteration 12, loss = 0.04935074\n",
      "Iteration 13, loss = 0.04717909\n",
      "Iteration 14, loss = 0.04577140\n",
      "Iteration 15, loss = 0.04115929\n",
      "Iteration 16, loss = 0.04336146\n",
      "Iteration 17, loss = 0.04551350\n",
      "Iteration 18, loss = 0.04077241\n",
      "Iteration 19, loss = 0.04494332\n",
      "Iteration 20, loss = 0.04385917\n",
      "Iteration 21, loss = 0.04450312\n",
      "Iteration 22, loss = 0.03924425\n",
      "Iteration 23, loss = 0.03330614\n",
      "Iteration 24, loss = 0.04342284\n",
      "Iteration 25, loss = 0.03990664\n",
      "Iteration 26, loss = 0.03941800\n",
      "Iteration 27, loss = 0.04348634\n",
      "Iteration 28, loss = 0.03626176\n",
      "Iteration 29, loss = 0.03982960\n",
      "Iteration 30, loss = 0.03982238\n",
      "Iteration 31, loss = 0.03947986\n",
      "Iteration 32, loss = 0.04310096\n",
      "Iteration 33, loss = 0.03903464\n",
      "Iteration 34, loss = 0.03355290\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.01, batch_size=100, hidden_layer_sizes=(100, 100, 100), solver=adam; total time= 1.1min\n",
      "Iteration 1, loss = 1.72057539\n",
      "Iteration 2, loss = 0.66871819\n",
      "Iteration 3, loss = 0.45109527\n",
      "Iteration 4, loss = 0.37765162\n",
      "Iteration 5, loss = 0.33950351\n",
      "Iteration 6, loss = 0.31446028\n",
      "Iteration 7, loss = 0.29539223\n",
      "Iteration 8, loss = 0.27877402\n",
      "Iteration 9, loss = 0.26582934\n",
      "Iteration 10, loss = 0.25371005\n",
      "Iteration 11, loss = 0.24301765\n",
      "Iteration 12, loss = 0.23305609\n",
      "Iteration 13, loss = 0.22464091\n",
      "Iteration 14, loss = 0.21658786\n",
      "Iteration 15, loss = 0.20912029\n",
      "Iteration 16, loss = 0.20169218\n",
      "Iteration 17, loss = 0.19555586\n",
      "Iteration 18, loss = 0.18974674\n",
      "Iteration 19, loss = 0.18391041\n",
      "Iteration 20, loss = 0.17928508\n",
      "Iteration 21, loss = 0.17434641\n",
      "Iteration 22, loss = 0.16891340\n",
      "Iteration 23, loss = 0.16508122\n",
      "Iteration 24, loss = 0.16021326\n",
      "Iteration 25, loss = 0.15642647\n",
      "Iteration 26, loss = 0.15226028\n",
      "Iteration 27, loss = 0.14892531\n",
      "Iteration 28, loss = 0.14509774\n",
      "Iteration 29, loss = 0.14188267\n",
      "Iteration 30, loss = 0.13873250\n",
      "Iteration 31, loss = 0.13577805\n",
      "Iteration 32, loss = 0.13293831\n",
      "Iteration 33, loss = 0.12999630\n",
      "Iteration 34, loss = 0.12696013\n",
      "Iteration 35, loss = 0.12461578\n",
      "Iteration 36, loss = 0.12233664\n",
      "Iteration 37, loss = 0.11934122\n",
      "Iteration 38, loss = 0.11737995\n",
      "Iteration 39, loss = 0.11514335\n",
      "Iteration 40, loss = 0.11279722\n",
      "Iteration 41, loss = 0.11090692\n",
      "Iteration 42, loss = 0.10862234\n",
      "Iteration 43, loss = 0.10628285\n",
      "Iteration 44, loss = 0.10430312\n",
      "Iteration 45, loss = 0.10248417\n",
      "Iteration 46, loss = 0.10126424\n",
      "Iteration 47, loss = 0.09901426\n",
      "Iteration 48, loss = 0.09723875\n",
      "Iteration 49, loss = 0.09568149\n",
      "Iteration 50, loss = 0.09425007\n",
      "Iteration 51, loss = 0.09231193\n",
      "Iteration 52, loss = 0.09061549\n",
      "Iteration 53, loss = 0.08943478\n",
      "Iteration 54, loss = 0.08781029\n",
      "Iteration 55, loss = 0.08621745\n",
      "Iteration 56, loss = 0.08497943\n",
      "Iteration 57, loss = 0.08350748\n",
      "Iteration 58, loss = 0.08213741\n",
      "Iteration 59, loss = 0.08103195\n",
      "Iteration 60, loss = 0.07980567\n",
      "Iteration 61, loss = 0.07820575\n",
      "Iteration 62, loss = 0.07733815\n",
      "Iteration 63, loss = 0.07552610\n",
      "Iteration 64, loss = 0.07468619\n",
      "Iteration 65, loss = 0.07366788\n",
      "Iteration 66, loss = 0.07265169\n",
      "Iteration 67, loss = 0.07149390\n",
      "Iteration 68, loss = 0.07026041\n",
      "Iteration 69, loss = 0.06962396\n",
      "Iteration 70, loss = 0.06830138\n",
      "Iteration 71, loss = 0.06724998\n",
      "Iteration 72, loss = 0.06632828\n",
      "Iteration 73, loss = 0.06555651\n",
      "Iteration 74, loss = 0.06465923\n",
      "Iteration 75, loss = 0.06378838\n",
      "Iteration 76, loss = 0.06271190\n",
      "Iteration 77, loss = 0.06208457\n",
      "Iteration 78, loss = 0.06114864\n",
      "Iteration 79, loss = 0.06032475\n",
      "Iteration 80, loss = 0.05958185\n",
      "Iteration 81, loss = 0.05878670\n",
      "Iteration 82, loss = 0.05786649\n",
      "Iteration 83, loss = 0.05728686\n",
      "Iteration 84, loss = 0.05646279\n",
      "Iteration 85, loss = 0.05581798\n",
      "Iteration 86, loss = 0.05523558\n",
      "Iteration 87, loss = 0.05449073\n",
      "Iteration 88, loss = 0.05386698\n",
      "Iteration 89, loss = 0.05320737\n",
      "Iteration 90, loss = 0.05258876\n",
      "Iteration 91, loss = 0.05195330\n",
      "Iteration 92, loss = 0.05140920\n",
      "Iteration 93, loss = 0.05094743\n",
      "Iteration 94, loss = 0.05024696\n",
      "Iteration 95, loss = 0.04981254\n",
      "Iteration 96, loss = 0.04924187\n",
      "Iteration 97, loss = 0.04868875\n",
      "Iteration 98, loss = 0.04836499\n",
      "Iteration 99, loss = 0.04779413\n",
      "Iteration 100, loss = 0.04732076\n",
      "Iteration 101, loss = 0.04695931\n",
      "Iteration 102, loss = 0.04651194\n",
      "Iteration 103, loss = 0.04593656\n",
      "Iteration 104, loss = 0.04544699\n",
      "Iteration 105, loss = 0.04512423\n",
      "Iteration 106, loss = 0.04474122\n",
      "Iteration 107, loss = 0.04433200\n",
      "Iteration 108, loss = 0.04372578\n",
      "Iteration 109, loss = 0.04356162\n",
      "Iteration 110, loss = 0.04313361\n",
      "Iteration 111, loss = 0.04256932\n",
      "Iteration 112, loss = 0.04241161\n",
      "Iteration 113, loss = 0.04211535\n",
      "Iteration 114, loss = 0.04174060\n",
      "Iteration 115, loss = 0.04136800\n",
      "Iteration 116, loss = 0.04099829\n",
      "Iteration 117, loss = 0.04079233\n",
      "Iteration 118, loss = 0.04036670\n",
      "Iteration 119, loss = 0.04026003\n",
      "Iteration 120, loss = 0.03992990\n",
      "Iteration 121, loss = 0.03963217\n",
      "Iteration 122, loss = 0.03939878\n",
      "Iteration 123, loss = 0.03909921\n",
      "Iteration 124, loss = 0.03886624\n",
      "Iteration 125, loss = 0.03857083\n",
      "Iteration 126, loss = 0.03836795\n",
      "Iteration 127, loss = 0.03812952\n",
      "Iteration 128, loss = 0.03775980\n",
      "Iteration 129, loss = 0.03753355\n",
      "Iteration 130, loss = 0.03747218\n",
      "Iteration 131, loss = 0.03714752\n",
      "Iteration 132, loss = 0.03698631\n",
      "Iteration 133, loss = 0.03683116\n",
      "Iteration 134, loss = 0.03660799\n",
      "Iteration 135, loss = 0.03639540\n",
      "Iteration 136, loss = 0.03619778\n",
      "Iteration 137, loss = 0.03606922\n",
      "Iteration 138, loss = 0.03584382\n",
      "Iteration 139, loss = 0.03570089\n",
      "Iteration 140, loss = 0.03559943\n",
      "Iteration 141, loss = 0.03530805\n",
      "Iteration 142, loss = 0.03513661\n",
      "Iteration 143, loss = 0.03495247\n",
      "Iteration 144, loss = 0.03493974\n",
      "Iteration 145, loss = 0.03479318\n",
      "Iteration 146, loss = 0.03454886\n",
      "Iteration 147, loss = 0.03441614\n",
      "Iteration 148, loss = 0.03427003\n",
      "Iteration 149, loss = 0.03408778\n",
      "Iteration 150, loss = 0.03402198\n",
      "Iteration 151, loss = 0.03388169\n",
      "Iteration 152, loss = 0.03377953\n",
      "Iteration 153, loss = 0.03359599\n",
      "Iteration 154, loss = 0.03358325\n",
      "Iteration 155, loss = 0.03342751\n",
      "Iteration 156, loss = 0.03334044\n",
      "Iteration 157, loss = 0.03319231\n",
      "Iteration 158, loss = 0.03308557\n",
      "Iteration 159, loss = 0.03300798\n",
      "Iteration 160, loss = 0.03290476\n",
      "Iteration 161, loss = 0.03274502\n",
      "Iteration 162, loss = 0.03270910\n",
      "Iteration 163, loss = 0.03259310\n",
      "Iteration 164, loss = 0.03247976\n",
      "Iteration 165, loss = 0.03241047\n",
      "Iteration 166, loss = 0.03230554\n",
      "Iteration 167, loss = 0.03221699\n",
      "Iteration 168, loss = 0.03215713\n",
      "Iteration 169, loss = 0.03203986\n",
      "Iteration 170, loss = 0.03200073\n",
      "Iteration 171, loss = 0.03192643\n",
      "Iteration 172, loss = 0.03181072\n",
      "Iteration 173, loss = 0.03173610\n",
      "Iteration 174, loss = 0.03164572\n",
      "Iteration 175, loss = 0.03161611\n",
      "Iteration 176, loss = 0.03155708\n",
      "Iteration 177, loss = 0.03146447\n",
      "Iteration 178, loss = 0.03140756\n",
      "Iteration 179, loss = 0.03134784\n",
      "Iteration 180, loss = 0.03126290\n",
      "Iteration 181, loss = 0.03119366\n",
      "Iteration 182, loss = 0.03111921\n",
      "Iteration 183, loss = 0.03109368\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.01, batch_size=100, hidden_layer_sizes=(100, 100, 100), solver=sgd; total time= 2.6min\n",
      "Iteration 1, loss = 1.86505772\n",
      "Iteration 2, loss = 0.76353610\n",
      "Iteration 3, loss = 0.49110513\n",
      "Iteration 4, loss = 0.40640417\n",
      "Iteration 5, loss = 0.36404892\n",
      "Iteration 6, loss = 0.33572405\n",
      "Iteration 7, loss = 0.31522299\n",
      "Iteration 8, loss = 0.29879618\n",
      "Iteration 9, loss = 0.28495022\n",
      "Iteration 10, loss = 0.27204814\n",
      "Iteration 11, loss = 0.26070864\n",
      "Iteration 12, loss = 0.25076809\n",
      "Iteration 13, loss = 0.24120901\n",
      "Iteration 14, loss = 0.23262937\n",
      "Iteration 15, loss = 0.22484345\n",
      "Iteration 16, loss = 0.21732473\n",
      "Iteration 17, loss = 0.21044297\n",
      "Iteration 18, loss = 0.20395884\n",
      "Iteration 19, loss = 0.19722923\n",
      "Iteration 20, loss = 0.19169060\n",
      "Iteration 21, loss = 0.18613866\n",
      "Iteration 22, loss = 0.18080800\n",
      "Iteration 23, loss = 0.17658845\n",
      "Iteration 24, loss = 0.17185338\n",
      "Iteration 25, loss = 0.16724757\n",
      "Iteration 26, loss = 0.16359290\n",
      "Iteration 27, loss = 0.15921893\n",
      "Iteration 28, loss = 0.15568107\n",
      "Iteration 29, loss = 0.15222086\n",
      "Iteration 30, loss = 0.14852244\n",
      "Iteration 31, loss = 0.14521679\n",
      "Iteration 32, loss = 0.14247159\n",
      "Iteration 33, loss = 0.13923680\n",
      "Iteration 34, loss = 0.13623314\n",
      "Iteration 35, loss = 0.13412571\n",
      "Iteration 36, loss = 0.13096015\n",
      "Iteration 37, loss = 0.12796163\n",
      "Iteration 38, loss = 0.12624365\n",
      "Iteration 39, loss = 0.12309200\n",
      "Iteration 40, loss = 0.12113376\n",
      "Iteration 41, loss = 0.11881873\n",
      "Iteration 42, loss = 0.11644626\n",
      "Iteration 43, loss = 0.11421514\n",
      "Iteration 44, loss = 0.11264768\n",
      "Iteration 45, loss = 0.11038945\n",
      "Iteration 46, loss = 0.10833993\n",
      "Iteration 47, loss = 0.10647729\n",
      "Iteration 48, loss = 0.10450243\n",
      "Iteration 49, loss = 0.10297064\n",
      "Iteration 50, loss = 0.10079327\n",
      "Iteration 51, loss = 0.09924294\n",
      "Iteration 52, loss = 0.09741269\n",
      "Iteration 53, loss = 0.09584928\n",
      "Iteration 54, loss = 0.09415077\n",
      "Iteration 55, loss = 0.09281079\n",
      "Iteration 56, loss = 0.09112108\n",
      "Iteration 57, loss = 0.09019111\n",
      "Iteration 58, loss = 0.08861952\n",
      "Iteration 59, loss = 0.08691285\n",
      "Iteration 60, loss = 0.08569828\n",
      "Iteration 61, loss = 0.08436428\n",
      "Iteration 62, loss = 0.08277283\n",
      "Iteration 63, loss = 0.08178075\n",
      "Iteration 64, loss = 0.08023852\n",
      "Iteration 65, loss = 0.07913191\n",
      "Iteration 66, loss = 0.07804652\n",
      "Iteration 67, loss = 0.07685578\n",
      "Iteration 68, loss = 0.07567923\n",
      "Iteration 69, loss = 0.07474203\n",
      "Iteration 70, loss = 0.07356396\n",
      "Iteration 71, loss = 0.07256510\n",
      "Iteration 72, loss = 0.07148582\n",
      "Iteration 73, loss = 0.07041409\n",
      "Iteration 74, loss = 0.06939398\n",
      "Iteration 75, loss = 0.06871342\n",
      "Iteration 76, loss = 0.06764590\n",
      "Iteration 77, loss = 0.06676875\n",
      "Iteration 78, loss = 0.06589087\n",
      "Iteration 79, loss = 0.06489754\n",
      "Iteration 80, loss = 0.06397870\n",
      "Iteration 81, loss = 0.06332489\n",
      "Iteration 82, loss = 0.06258128\n",
      "Iteration 83, loss = 0.06180820\n",
      "Iteration 84, loss = 0.06084025\n",
      "Iteration 85, loss = 0.06007493\n",
      "Iteration 86, loss = 0.05973332\n",
      "Iteration 87, loss = 0.05866582\n",
      "Iteration 88, loss = 0.05812819\n",
      "Iteration 89, loss = 0.05752400\n",
      "Iteration 90, loss = 0.05663698\n",
      "Iteration 91, loss = 0.05580171\n",
      "Iteration 92, loss = 0.05534631\n",
      "Iteration 93, loss = 0.05478675\n",
      "Iteration 94, loss = 0.05409359\n",
      "Iteration 95, loss = 0.05359675\n",
      "Iteration 96, loss = 0.05288101\n",
      "Iteration 97, loss = 0.05236282\n",
      "Iteration 98, loss = 0.05177894\n",
      "Iteration 99, loss = 0.05117658\n",
      "Iteration 100, loss = 0.05095965\n",
      "Iteration 101, loss = 0.05019510\n",
      "Iteration 102, loss = 0.04962857\n",
      "Iteration 103, loss = 0.04919392\n",
      "Iteration 104, loss = 0.04867826\n",
      "Iteration 105, loss = 0.04808451\n",
      "Iteration 106, loss = 0.04784329\n",
      "Iteration 107, loss = 0.04732872\n",
      "Iteration 108, loss = 0.04692948\n",
      "Iteration 109, loss = 0.04645591\n",
      "Iteration 110, loss = 0.04608546\n",
      "Iteration 111, loss = 0.04559090\n",
      "Iteration 112, loss = 0.04515686\n",
      "Iteration 113, loss = 0.04479057\n",
      "Iteration 114, loss = 0.04419234\n",
      "Iteration 115, loss = 0.04404994\n",
      "Iteration 116, loss = 0.04370315\n",
      "Iteration 117, loss = 0.04340879\n",
      "Iteration 118, loss = 0.04292036\n",
      "Iteration 119, loss = 0.04255544\n",
      "Iteration 120, loss = 0.04235587\n",
      "Iteration 121, loss = 0.04201859\n",
      "Iteration 122, loss = 0.04166940\n",
      "Iteration 123, loss = 0.04140255\n",
      "Iteration 124, loss = 0.04108541\n",
      "Iteration 125, loss = 0.04084513\n",
      "Iteration 126, loss = 0.04038293\n",
      "Iteration 127, loss = 0.04014573\n",
      "Iteration 128, loss = 0.03999756\n",
      "Iteration 129, loss = 0.03976546\n",
      "Iteration 130, loss = 0.03942419\n",
      "Iteration 131, loss = 0.03928771\n",
      "Iteration 132, loss = 0.03881425\n",
      "Iteration 133, loss = 0.03865095\n",
      "Iteration 134, loss = 0.03846502\n",
      "Iteration 135, loss = 0.03828403\n",
      "Iteration 136, loss = 0.03801224\n",
      "Iteration 137, loss = 0.03777296\n",
      "Iteration 138, loss = 0.03756234\n",
      "Iteration 139, loss = 0.03741017\n",
      "Iteration 140, loss = 0.03712279\n",
      "Iteration 141, loss = 0.03695332\n",
      "Iteration 142, loss = 0.03684346\n",
      "Iteration 143, loss = 0.03654399\n",
      "Iteration 144, loss = 0.03643684\n",
      "Iteration 145, loss = 0.03630841\n",
      "Iteration 146, loss = 0.03604693\n",
      "Iteration 147, loss = 0.03591342\n",
      "Iteration 148, loss = 0.03571898\n",
      "Iteration 149, loss = 0.03560485\n",
      "Iteration 150, loss = 0.03541936\n",
      "Iteration 151, loss = 0.03536388\n",
      "Iteration 152, loss = 0.03508561\n",
      "Iteration 153, loss = 0.03500037\n",
      "Iteration 154, loss = 0.03475275\n",
      "Iteration 155, loss = 0.03474745\n",
      "Iteration 156, loss = 0.03456632\n",
      "Iteration 157, loss = 0.03442899\n",
      "Iteration 158, loss = 0.03427954\n",
      "Iteration 159, loss = 0.03413316\n",
      "Iteration 160, loss = 0.03404236\n",
      "Iteration 161, loss = 0.03395408\n",
      "Iteration 162, loss = 0.03379384\n",
      "Iteration 163, loss = 0.03367642\n",
      "Iteration 164, loss = 0.03361823\n",
      "Iteration 165, loss = 0.03345432\n",
      "Iteration 166, loss = 0.03340980\n",
      "Iteration 167, loss = 0.03322746\n",
      "Iteration 168, loss = 0.03318630\n",
      "Iteration 169, loss = 0.03311272\n",
      "Iteration 170, loss = 0.03297321\n",
      "Iteration 171, loss = 0.03290439\n",
      "Iteration 172, loss = 0.03272716\n",
      "Iteration 173, loss = 0.03268295\n",
      "Iteration 174, loss = 0.03265664\n",
      "Iteration 175, loss = 0.03251451\n",
      "Iteration 176, loss = 0.03243370\n",
      "Iteration 177, loss = 0.03235299\n",
      "Iteration 178, loss = 0.03228574\n",
      "Iteration 179, loss = 0.03216951\n",
      "Iteration 180, loss = 0.03209832\n",
      "Iteration 181, loss = 0.03203632\n",
      "Iteration 182, loss = 0.03197903\n",
      "Iteration 183, loss = 0.03186834\n",
      "Iteration 184, loss = 0.03177405\n",
      "Iteration 185, loss = 0.03178730\n",
      "Iteration 186, loss = 0.03167743\n",
      "Iteration 187, loss = 0.03160413\n",
      "Iteration 188, loss = 0.03158965\n",
      "Iteration 189, loss = 0.03152464\n",
      "Iteration 190, loss = 0.03141613\n",
      "Iteration 191, loss = 0.03137912\n",
      "Iteration 192, loss = 0.03132219\n",
      "Iteration 193, loss = 0.03127358\n",
      "Iteration 194, loss = 0.03119241\n",
      "Iteration 195, loss = 0.03115814\n",
      "Iteration 196, loss = 0.03111724\n",
      "Iteration 197, loss = 0.03101679\n",
      "Iteration 198, loss = 0.03100904\n",
      "Iteration 199, loss = 0.03094074\n",
      "Iteration 200, loss = 0.03088167\n",
      "[CV] END activation=relu, alpha=0.01, batch_size=100, hidden_layer_sizes=(100, 100, 100), solver=sgd; total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.76758282\n",
      "Iteration 2, loss = 0.65669164\n",
      "Iteration 3, loss = 0.45241338\n",
      "Iteration 4, loss = 0.38450835\n",
      "Iteration 5, loss = 0.34721397\n",
      "Iteration 6, loss = 0.32087355\n",
      "Iteration 7, loss = 0.30099068\n",
      "Iteration 8, loss = 0.28454300\n",
      "Iteration 9, loss = 0.27110965\n",
      "Iteration 10, loss = 0.25789838\n",
      "Iteration 11, loss = 0.24637989\n",
      "Iteration 12, loss = 0.23601140\n",
      "Iteration 13, loss = 0.22671334\n",
      "Iteration 14, loss = 0.21856531\n",
      "Iteration 15, loss = 0.21063114\n",
      "Iteration 16, loss = 0.20375270\n",
      "Iteration 17, loss = 0.19658855\n",
      "Iteration 18, loss = 0.19047223\n",
      "Iteration 19, loss = 0.18436472\n",
      "Iteration 20, loss = 0.17914148\n",
      "Iteration 21, loss = 0.17415216\n",
      "Iteration 22, loss = 0.16899893\n",
      "Iteration 23, loss = 0.16464173\n",
      "Iteration 24, loss = 0.15998661\n",
      "Iteration 25, loss = 0.15627911\n",
      "Iteration 26, loss = 0.15195641\n",
      "Iteration 27, loss = 0.14846646\n",
      "Iteration 28, loss = 0.14505482\n",
      "Iteration 29, loss = 0.14148223\n",
      "Iteration 30, loss = 0.13826979\n",
      "Iteration 31, loss = 0.13479750\n",
      "Iteration 32, loss = 0.13186313\n",
      "Iteration 33, loss = 0.12942535\n",
      "Iteration 34, loss = 0.12639430\n",
      "Iteration 35, loss = 0.12405460\n",
      "Iteration 36, loss = 0.12156595\n",
      "Iteration 37, loss = 0.11897279\n",
      "Iteration 38, loss = 0.11652424\n",
      "Iteration 39, loss = 0.11401040\n",
      "Iteration 40, loss = 0.11187908\n",
      "Iteration 41, loss = 0.10999674\n",
      "Iteration 42, loss = 0.10776474\n",
      "Iteration 43, loss = 0.10547415\n",
      "Iteration 44, loss = 0.10374998\n",
      "Iteration 45, loss = 0.10171741\n",
      "Iteration 46, loss = 0.10009932\n",
      "Iteration 47, loss = 0.09855624\n",
      "Iteration 48, loss = 0.09672474\n",
      "Iteration 49, loss = 0.09457658\n",
      "Iteration 50, loss = 0.09317732\n",
      "Iteration 51, loss = 0.09183645\n",
      "Iteration 52, loss = 0.09016806\n",
      "Iteration 53, loss = 0.08865262\n",
      "Iteration 54, loss = 0.08719407\n",
      "Iteration 55, loss = 0.08537437\n",
      "Iteration 56, loss = 0.08413746\n",
      "Iteration 57, loss = 0.08295373\n",
      "Iteration 58, loss = 0.08140570\n",
      "Iteration 59, loss = 0.08009040\n",
      "Iteration 60, loss = 0.07896938\n",
      "Iteration 61, loss = 0.07787791\n",
      "Iteration 62, loss = 0.07647714\n",
      "Iteration 63, loss = 0.07529409\n",
      "Iteration 64, loss = 0.07431045\n",
      "Iteration 65, loss = 0.07297927\n",
      "Iteration 66, loss = 0.07219319\n",
      "Iteration 67, loss = 0.07099900\n",
      "Iteration 68, loss = 0.07035533\n",
      "Iteration 69, loss = 0.06890145\n",
      "Iteration 70, loss = 0.06806464\n",
      "Iteration 71, loss = 0.06715538\n",
      "Iteration 72, loss = 0.06606996\n",
      "Iteration 73, loss = 0.06516802\n",
      "Iteration 74, loss = 0.06420067\n",
      "Iteration 75, loss = 0.06339374\n",
      "Iteration 76, loss = 0.06253670\n",
      "Iteration 77, loss = 0.06168404\n",
      "Iteration 78, loss = 0.06089995\n",
      "Iteration 79, loss = 0.05999929\n",
      "Iteration 80, loss = 0.05954581\n",
      "Iteration 81, loss = 0.05860489\n",
      "Iteration 82, loss = 0.05785782\n",
      "Iteration 83, loss = 0.05721679\n",
      "Iteration 84, loss = 0.05643725\n",
      "Iteration 85, loss = 0.05573344\n",
      "Iteration 86, loss = 0.05496255\n",
      "Iteration 87, loss = 0.05446358\n",
      "Iteration 88, loss = 0.05389898\n",
      "Iteration 89, loss = 0.05312031\n",
      "Iteration 90, loss = 0.05258968\n",
      "Iteration 91, loss = 0.05197200\n",
      "Iteration 92, loss = 0.05126649\n",
      "Iteration 93, loss = 0.05104569\n",
      "Iteration 94, loss = 0.05042347\n",
      "Iteration 95, loss = 0.04972758\n",
      "Iteration 96, loss = 0.04935569\n",
      "Iteration 97, loss = 0.04849824\n",
      "Iteration 98, loss = 0.04827115\n",
      "Iteration 99, loss = 0.04789567\n",
      "Iteration 100, loss = 0.04759625\n",
      "Iteration 101, loss = 0.04692012\n",
      "Iteration 102, loss = 0.04648802\n",
      "Iteration 103, loss = 0.04595461\n",
      "Iteration 104, loss = 0.04565125\n",
      "Iteration 105, loss = 0.04510807\n",
      "Iteration 106, loss = 0.04474441\n",
      "Iteration 107, loss = 0.04411196\n",
      "Iteration 108, loss = 0.04382903\n",
      "Iteration 109, loss = 0.04367789\n",
      "Iteration 110, loss = 0.04322786\n",
      "Iteration 111, loss = 0.04292421\n",
      "Iteration 112, loss = 0.04263438\n",
      "Iteration 113, loss = 0.04218363\n",
      "Iteration 114, loss = 0.04181394\n",
      "Iteration 115, loss = 0.04149777\n",
      "Iteration 116, loss = 0.04124438\n",
      "Iteration 117, loss = 0.04082187\n",
      "Iteration 118, loss = 0.04050922\n",
      "Iteration 119, loss = 0.04032598\n",
      "Iteration 120, loss = 0.04003092\n",
      "Iteration 121, loss = 0.03973943\n",
      "Iteration 122, loss = 0.03940213\n",
      "Iteration 123, loss = 0.03918732\n",
      "Iteration 124, loss = 0.03893410\n",
      "Iteration 125, loss = 0.03873704\n",
      "Iteration 126, loss = 0.03835039\n",
      "Iteration 127, loss = 0.03824496\n",
      "Iteration 128, loss = 0.03794316\n",
      "Iteration 129, loss = 0.03769419\n",
      "Iteration 130, loss = 0.03751673\n",
      "Iteration 131, loss = 0.03734952\n",
      "Iteration 132, loss = 0.03718558\n",
      "Iteration 133, loss = 0.03695775\n",
      "Iteration 134, loss = 0.03679142\n",
      "Iteration 135, loss = 0.03663188\n",
      "Iteration 136, loss = 0.03632362\n",
      "Iteration 137, loss = 0.03616855\n",
      "Iteration 138, loss = 0.03601042\n",
      "Iteration 139, loss = 0.03576777\n",
      "Iteration 140, loss = 0.03565634\n",
      "Iteration 141, loss = 0.03552756\n",
      "Iteration 142, loss = 0.03520606\n",
      "Iteration 143, loss = 0.03510522\n",
      "Iteration 144, loss = 0.03500022\n",
      "Iteration 145, loss = 0.03488132\n",
      "Iteration 146, loss = 0.03478193\n",
      "Iteration 147, loss = 0.03457601\n",
      "Iteration 148, loss = 0.03442421\n",
      "Iteration 149, loss = 0.03430397\n",
      "Iteration 150, loss = 0.03409255\n",
      "Iteration 151, loss = 0.03406641\n",
      "Iteration 152, loss = 0.03385627\n",
      "Iteration 153, loss = 0.03380717\n",
      "Iteration 154, loss = 0.03368237\n",
      "Iteration 155, loss = 0.03365291\n",
      "Iteration 156, loss = 0.03338054\n",
      "Iteration 157, loss = 0.03338269\n",
      "Iteration 158, loss = 0.03321709\n",
      "Iteration 159, loss = 0.03306862\n",
      "Iteration 160, loss = 0.03295786\n",
      "Iteration 161, loss = 0.03292574\n",
      "Iteration 162, loss = 0.03283397\n",
      "Iteration 163, loss = 0.03273741\n",
      "Iteration 164, loss = 0.03262017\n",
      "Iteration 165, loss = 0.03251927\n",
      "Iteration 166, loss = 0.03242267\n",
      "Iteration 167, loss = 0.03237334\n",
      "Iteration 168, loss = 0.03222926\n",
      "Iteration 169, loss = 0.03218748\n",
      "Iteration 170, loss = 0.03210424\n",
      "Iteration 171, loss = 0.03205672\n",
      "Iteration 172, loss = 0.03194023\n",
      "Iteration 173, loss = 0.03186915\n",
      "Iteration 174, loss = 0.03178732\n",
      "Iteration 175, loss = 0.03168892\n",
      "Iteration 176, loss = 0.03164536\n",
      "Iteration 177, loss = 0.03155410\n",
      "Iteration 178, loss = 0.03154107\n",
      "Iteration 179, loss = 0.03146758\n",
      "Iteration 180, loss = 0.03138190\n",
      "Iteration 181, loss = 0.03134026\n",
      "Iteration 182, loss = 0.03126123\n",
      "Iteration 183, loss = 0.03119212\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.01, batch_size=100, hidden_layer_sizes=(100, 100, 100), solver=sgd; total time= 2.7min\n",
      "Iteration 1, loss = 1.84933074\n",
      "Iteration 2, loss = 0.72676482\n",
      "Iteration 3, loss = 0.48034755\n",
      "Iteration 4, loss = 0.39746109\n",
      "Iteration 5, loss = 0.35478835\n",
      "Iteration 6, loss = 0.32637925\n",
      "Iteration 7, loss = 0.30503542\n",
      "Iteration 8, loss = 0.28720732\n",
      "Iteration 9, loss = 0.27203666\n",
      "Iteration 10, loss = 0.25957916\n",
      "Iteration 11, loss = 0.24752879\n",
      "Iteration 12, loss = 0.23777247\n",
      "Iteration 13, loss = 0.22840832\n",
      "Iteration 14, loss = 0.22024829\n",
      "Iteration 15, loss = 0.21259051\n",
      "Iteration 16, loss = 0.20550401\n",
      "Iteration 17, loss = 0.19894770\n",
      "Iteration 18, loss = 0.19306164\n",
      "Iteration 19, loss = 0.18698278\n",
      "Iteration 20, loss = 0.18180494\n",
      "Iteration 21, loss = 0.17713610\n",
      "Iteration 22, loss = 0.17270319\n",
      "Iteration 23, loss = 0.16803641\n",
      "Iteration 24, loss = 0.16401779\n",
      "Iteration 25, loss = 0.15994967\n",
      "Iteration 26, loss = 0.15583612\n",
      "Iteration 27, loss = 0.15201813\n",
      "Iteration 28, loss = 0.14909707\n",
      "Iteration 29, loss = 0.14521375\n",
      "Iteration 30, loss = 0.14275878\n",
      "Iteration 31, loss = 0.13944832\n",
      "Iteration 32, loss = 0.13625670\n",
      "Iteration 33, loss = 0.13380140\n",
      "Iteration 34, loss = 0.13085011\n",
      "Iteration 35, loss = 0.12799873\n",
      "Iteration 36, loss = 0.12553165\n",
      "Iteration 37, loss = 0.12337018\n",
      "Iteration 38, loss = 0.12087652\n",
      "Iteration 39, loss = 0.11844998\n",
      "Iteration 40, loss = 0.11644471\n",
      "Iteration 41, loss = 0.11466861\n",
      "Iteration 42, loss = 0.11218597\n",
      "Iteration 43, loss = 0.11001807\n",
      "Iteration 44, loss = 0.10812537\n",
      "Iteration 45, loss = 0.10647711\n",
      "Iteration 46, loss = 0.10444071\n",
      "Iteration 47, loss = 0.10235160\n",
      "Iteration 48, loss = 0.10080136\n",
      "Iteration 49, loss = 0.09916120\n",
      "Iteration 50, loss = 0.09723528\n",
      "Iteration 51, loss = 0.09582515\n",
      "Iteration 52, loss = 0.09409361\n",
      "Iteration 53, loss = 0.09283372\n",
      "Iteration 54, loss = 0.09129575\n",
      "Iteration 55, loss = 0.08986164\n",
      "Iteration 56, loss = 0.08804302\n",
      "Iteration 57, loss = 0.08701415\n",
      "Iteration 58, loss = 0.08555741\n",
      "Iteration 59, loss = 0.08447484\n",
      "Iteration 60, loss = 0.08269639\n",
      "Iteration 61, loss = 0.08147763\n",
      "Iteration 62, loss = 0.08008246\n",
      "Iteration 63, loss = 0.07907190\n",
      "Iteration 64, loss = 0.07796287\n",
      "Iteration 65, loss = 0.07686816\n",
      "Iteration 66, loss = 0.07556654\n",
      "Iteration 67, loss = 0.07469807\n",
      "Iteration 68, loss = 0.07342518\n",
      "Iteration 69, loss = 0.07248926\n",
      "Iteration 70, loss = 0.07137450\n",
      "Iteration 71, loss = 0.07040971\n",
      "Iteration 72, loss = 0.06928675\n",
      "Iteration 73, loss = 0.06835199\n",
      "Iteration 74, loss = 0.06775049\n",
      "Iteration 75, loss = 0.06656831\n",
      "Iteration 76, loss = 0.06579559\n",
      "Iteration 77, loss = 0.06478038\n",
      "Iteration 78, loss = 0.06398251\n",
      "Iteration 79, loss = 0.06327394\n",
      "Iteration 80, loss = 0.06218819\n",
      "Iteration 81, loss = 0.06175897\n",
      "Iteration 82, loss = 0.06074577\n",
      "Iteration 83, loss = 0.06006471\n",
      "Iteration 84, loss = 0.05942987\n",
      "Iteration 85, loss = 0.05862952\n",
      "Iteration 86, loss = 0.05794804\n",
      "Iteration 87, loss = 0.05741702\n",
      "Iteration 88, loss = 0.05654628\n",
      "Iteration 89, loss = 0.05593522\n",
      "Iteration 90, loss = 0.05517173\n",
      "Iteration 91, loss = 0.05467490\n",
      "Iteration 92, loss = 0.05405828\n",
      "Iteration 93, loss = 0.05334858\n",
      "Iteration 94, loss = 0.05294616\n",
      "Iteration 95, loss = 0.05240410\n",
      "Iteration 96, loss = 0.05167123\n",
      "Iteration 97, loss = 0.05140590\n",
      "Iteration 98, loss = 0.05064441\n",
      "Iteration 99, loss = 0.04974433\n",
      "Iteration 100, loss = 0.04982284\n",
      "Iteration 101, loss = 0.04896585\n",
      "Iteration 102, loss = 0.04866736\n",
      "Iteration 103, loss = 0.04831018\n",
      "Iteration 104, loss = 0.04780314\n",
      "Iteration 105, loss = 0.04733904\n",
      "Iteration 106, loss = 0.04683020\n",
      "Iteration 107, loss = 0.04641789\n",
      "Iteration 108, loss = 0.04608613\n",
      "Iteration 109, loss = 0.04565671\n",
      "Iteration 110, loss = 0.04515041\n",
      "Iteration 111, loss = 0.04480397\n",
      "Iteration 112, loss = 0.04441342\n",
      "Iteration 113, loss = 0.04407164\n",
      "Iteration 114, loss = 0.04376655\n",
      "Iteration 115, loss = 0.04341358\n",
      "Iteration 116, loss = 0.04297665\n",
      "Iteration 117, loss = 0.04259899\n",
      "Iteration 118, loss = 0.04241943\n",
      "Iteration 119, loss = 0.04209957\n",
      "Iteration 120, loss = 0.04161841\n",
      "Iteration 121, loss = 0.04133922\n",
      "Iteration 122, loss = 0.04124329\n",
      "Iteration 123, loss = 0.04082519\n",
      "Iteration 124, loss = 0.04038953\n",
      "Iteration 125, loss = 0.04021444\n",
      "Iteration 126, loss = 0.03995765\n",
      "Iteration 127, loss = 0.03958630\n",
      "Iteration 128, loss = 0.03945392\n",
      "Iteration 129, loss = 0.03925361\n",
      "Iteration 130, loss = 0.03887201\n",
      "Iteration 131, loss = 0.03867757\n",
      "Iteration 132, loss = 0.03842507\n",
      "Iteration 133, loss = 0.03821813\n",
      "Iteration 134, loss = 0.03792942\n",
      "Iteration 135, loss = 0.03779661\n",
      "Iteration 136, loss = 0.03755225\n",
      "Iteration 137, loss = 0.03739324\n",
      "Iteration 138, loss = 0.03712314\n",
      "Iteration 139, loss = 0.03692947\n",
      "Iteration 140, loss = 0.03672647\n",
      "Iteration 141, loss = 0.03656965\n",
      "Iteration 142, loss = 0.03643750\n",
      "Iteration 143, loss = 0.03619605\n",
      "Iteration 144, loss = 0.03606166\n",
      "Iteration 145, loss = 0.03589493\n",
      "Iteration 146, loss = 0.03578767\n",
      "Iteration 147, loss = 0.03554516\n",
      "Iteration 148, loss = 0.03541381\n",
      "Iteration 149, loss = 0.03529522\n",
      "Iteration 150, loss = 0.03507652\n",
      "Iteration 151, loss = 0.03492103\n",
      "Iteration 152, loss = 0.03484905\n",
      "Iteration 153, loss = 0.03465712\n",
      "Iteration 154, loss = 0.03451567\n",
      "Iteration 155, loss = 0.03438473\n",
      "Iteration 156, loss = 0.03424021\n",
      "Iteration 157, loss = 0.03418558\n",
      "Iteration 158, loss = 0.03406417\n",
      "Iteration 159, loss = 0.03392991\n",
      "Iteration 160, loss = 0.03378630\n",
      "Iteration 161, loss = 0.03370304\n",
      "Iteration 162, loss = 0.03351452\n",
      "Iteration 163, loss = 0.03339738\n",
      "Iteration 164, loss = 0.03336909\n",
      "Iteration 165, loss = 0.03319463\n",
      "Iteration 166, loss = 0.03320717\n",
      "Iteration 167, loss = 0.03301479\n",
      "Iteration 168, loss = 0.03293858\n",
      "Iteration 169, loss = 0.03289431\n",
      "Iteration 170, loss = 0.03266401\n",
      "Iteration 171, loss = 0.03274119\n",
      "Iteration 172, loss = 0.03252378\n",
      "Iteration 173, loss = 0.03250106\n",
      "Iteration 174, loss = 0.03240646\n",
      "Iteration 175, loss = 0.03233181\n",
      "Iteration 176, loss = 0.03222514\n",
      "Iteration 177, loss = 0.03217086\n",
      "Iteration 178, loss = 0.03206702\n",
      "Iteration 179, loss = 0.03204092\n",
      "Iteration 180, loss = 0.03190706\n",
      "Iteration 181, loss = 0.03180092\n",
      "Iteration 182, loss = 0.03175905\n",
      "Iteration 183, loss = 0.03171222\n",
      "Iteration 184, loss = 0.03163250\n",
      "Iteration 185, loss = 0.03157929\n",
      "Iteration 186, loss = 0.03150470\n",
      "Iteration 187, loss = 0.03146831\n",
      "Iteration 188, loss = 0.03135058\n",
      "Iteration 189, loss = 0.03134081\n",
      "Iteration 190, loss = 0.03121514\n",
      "Iteration 191, loss = 0.03120322\n",
      "Iteration 192, loss = 0.03112712\n",
      "Iteration 193, loss = 0.03109446\n",
      "Iteration 194, loss = 0.03102110\n",
      "Iteration 195, loss = 0.03098083\n",
      "Iteration 196, loss = 0.03091733\n",
      "Iteration 197, loss = 0.03087705\n",
      "Iteration 198, loss = 0.03081676\n",
      "Iteration 199, loss = 0.03077693\n",
      "Iteration 200, loss = 0.03071668\n",
      "[CV] END activation=relu, alpha=0.01, batch_size=100, hidden_layer_sizes=(100, 100, 100), solver=sgd; total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.77646130\n",
      "Iteration 2, loss = 0.68116713\n",
      "Iteration 3, loss = 0.45658359\n",
      "Iteration 4, loss = 0.38639271\n",
      "Iteration 5, loss = 0.34788845\n",
      "Iteration 6, loss = 0.32339786\n",
      "Iteration 7, loss = 0.30328349\n",
      "Iteration 8, loss = 0.28648235\n",
      "Iteration 9, loss = 0.27183180\n",
      "Iteration 10, loss = 0.25959619\n",
      "Iteration 11, loss = 0.24828207\n",
      "Iteration 12, loss = 0.23827540\n",
      "Iteration 13, loss = 0.22871199\n",
      "Iteration 14, loss = 0.21969463\n",
      "Iteration 15, loss = 0.21204156\n",
      "Iteration 16, loss = 0.20486980\n",
      "Iteration 17, loss = 0.19807569\n",
      "Iteration 18, loss = 0.19148880\n",
      "Iteration 19, loss = 0.18568159\n",
      "Iteration 20, loss = 0.17969931\n",
      "Iteration 21, loss = 0.17491206\n",
      "Iteration 22, loss = 0.17039206\n",
      "Iteration 23, loss = 0.16563967\n",
      "Iteration 24, loss = 0.16114147\n",
      "Iteration 25, loss = 0.15725636\n",
      "Iteration 26, loss = 0.15321762\n",
      "Iteration 27, loss = 0.14966559\n",
      "Iteration 28, loss = 0.14560190\n",
      "Iteration 29, loss = 0.14283114\n",
      "Iteration 30, loss = 0.13955435\n",
      "Iteration 31, loss = 0.13644771\n",
      "Iteration 32, loss = 0.13323965\n",
      "Iteration 33, loss = 0.13031871\n",
      "Iteration 34, loss = 0.12754019\n",
      "Iteration 35, loss = 0.12452436\n",
      "Iteration 36, loss = 0.12194345\n",
      "Iteration 37, loss = 0.11964582\n",
      "Iteration 38, loss = 0.11731228\n",
      "Iteration 39, loss = 0.11530981\n",
      "Iteration 40, loss = 0.11261048\n",
      "Iteration 41, loss = 0.11042231\n",
      "Iteration 42, loss = 0.10859508\n",
      "Iteration 43, loss = 0.10649096\n",
      "Iteration 44, loss = 0.10468359\n",
      "Iteration 45, loss = 0.10313779\n",
      "Iteration 46, loss = 0.10063950\n",
      "Iteration 47, loss = 0.09936566\n",
      "Iteration 48, loss = 0.09716697\n",
      "Iteration 49, loss = 0.09588900\n",
      "Iteration 50, loss = 0.09402924\n",
      "Iteration 51, loss = 0.09268752\n",
      "Iteration 52, loss = 0.09127586\n",
      "Iteration 53, loss = 0.08919237\n",
      "Iteration 54, loss = 0.08791142\n",
      "Iteration 55, loss = 0.08667012\n",
      "Iteration 56, loss = 0.08540627\n",
      "Iteration 57, loss = 0.08373369\n",
      "Iteration 58, loss = 0.08233438\n",
      "Iteration 59, loss = 0.08119556\n",
      "Iteration 60, loss = 0.08023241\n",
      "Iteration 61, loss = 0.07858788\n",
      "Iteration 62, loss = 0.07745004\n",
      "Iteration 63, loss = 0.07626468\n",
      "Iteration 64, loss = 0.07509363\n",
      "Iteration 65, loss = 0.07429755\n",
      "Iteration 66, loss = 0.07309989\n",
      "Iteration 67, loss = 0.07195741\n",
      "Iteration 68, loss = 0.07086374\n",
      "Iteration 69, loss = 0.06986905\n",
      "Iteration 70, loss = 0.06906552\n",
      "Iteration 71, loss = 0.06841777\n",
      "Iteration 72, loss = 0.06693426\n",
      "Iteration 73, loss = 0.06628631\n",
      "Iteration 74, loss = 0.06535902\n",
      "Iteration 75, loss = 0.06437321\n",
      "Iteration 76, loss = 0.06363695\n",
      "Iteration 77, loss = 0.06278530\n",
      "Iteration 78, loss = 0.06174053\n",
      "Iteration 79, loss = 0.06119122\n",
      "Iteration 80, loss = 0.06022759\n",
      "Iteration 81, loss = 0.05977698\n",
      "Iteration 82, loss = 0.05891741\n",
      "Iteration 83, loss = 0.05809719\n",
      "Iteration 84, loss = 0.05739514\n",
      "Iteration 85, loss = 0.05683852\n",
      "Iteration 86, loss = 0.05606570\n",
      "Iteration 87, loss = 0.05520662\n",
      "Iteration 88, loss = 0.05481325\n",
      "Iteration 89, loss = 0.05420356\n",
      "Iteration 90, loss = 0.05331526\n",
      "Iteration 91, loss = 0.05291146\n",
      "Iteration 92, loss = 0.05237960\n",
      "Iteration 93, loss = 0.05167781\n",
      "Iteration 94, loss = 0.05127355\n",
      "Iteration 95, loss = 0.05077863\n",
      "Iteration 96, loss = 0.05003961\n",
      "Iteration 97, loss = 0.04965933\n",
      "Iteration 98, loss = 0.04918228\n",
      "Iteration 99, loss = 0.04842713\n",
      "Iteration 100, loss = 0.04810621\n",
      "Iteration 101, loss = 0.04741491\n",
      "Iteration 102, loss = 0.04722555\n",
      "Iteration 103, loss = 0.04679152\n",
      "Iteration 104, loss = 0.04619052\n",
      "Iteration 105, loss = 0.04585244\n",
      "Iteration 106, loss = 0.04535965\n",
      "Iteration 107, loss = 0.04531671\n",
      "Iteration 108, loss = 0.04469964\n",
      "Iteration 109, loss = 0.04421759\n",
      "Iteration 110, loss = 0.04396064\n",
      "Iteration 111, loss = 0.04351361\n",
      "Iteration 112, loss = 0.04307371\n",
      "Iteration 113, loss = 0.04290224\n",
      "Iteration 114, loss = 0.04261006\n",
      "Iteration 115, loss = 0.04220706\n",
      "Iteration 116, loss = 0.04178601\n",
      "Iteration 117, loss = 0.04154325\n",
      "Iteration 118, loss = 0.04127919\n",
      "Iteration 119, loss = 0.04096182\n",
      "Iteration 120, loss = 0.04073965\n",
      "Iteration 121, loss = 0.04042115\n",
      "Iteration 122, loss = 0.04018996\n",
      "Iteration 123, loss = 0.03976328\n",
      "Iteration 124, loss = 0.03956111\n",
      "Iteration 125, loss = 0.03930053\n",
      "Iteration 126, loss = 0.03908496\n",
      "Iteration 127, loss = 0.03880598\n",
      "Iteration 128, loss = 0.03863882\n",
      "Iteration 129, loss = 0.03830629\n",
      "Iteration 130, loss = 0.03803589\n",
      "Iteration 131, loss = 0.03792099\n",
      "Iteration 132, loss = 0.03766848\n",
      "Iteration 133, loss = 0.03752385\n",
      "Iteration 134, loss = 0.03722002\n",
      "Iteration 135, loss = 0.03707728\n",
      "Iteration 136, loss = 0.03687447\n",
      "Iteration 137, loss = 0.03671773\n",
      "Iteration 138, loss = 0.03651294\n",
      "Iteration 139, loss = 0.03634285\n",
      "Iteration 140, loss = 0.03615750\n",
      "Iteration 141, loss = 0.03597392\n",
      "Iteration 142, loss = 0.03584755\n",
      "Iteration 143, loss = 0.03566123\n",
      "Iteration 144, loss = 0.03554106\n",
      "Iteration 145, loss = 0.03532564\n",
      "Iteration 146, loss = 0.03525245\n",
      "Iteration 147, loss = 0.03505310\n",
      "Iteration 148, loss = 0.03488361\n",
      "Iteration 149, loss = 0.03474184\n",
      "Iteration 150, loss = 0.03459911\n",
      "Iteration 151, loss = 0.03447619\n",
      "Iteration 152, loss = 0.03427319\n",
      "Iteration 153, loss = 0.03425990\n",
      "Iteration 154, loss = 0.03411001\n",
      "Iteration 155, loss = 0.03399727\n",
      "Iteration 156, loss = 0.03385691\n",
      "Iteration 157, loss = 0.03375821\n",
      "Iteration 158, loss = 0.03370945\n",
      "Iteration 159, loss = 0.03351441\n",
      "Iteration 160, loss = 0.03346481\n",
      "Iteration 161, loss = 0.03332059\n",
      "Iteration 162, loss = 0.03318446\n",
      "Iteration 163, loss = 0.03312088\n",
      "Iteration 164, loss = 0.03297705\n",
      "Iteration 165, loss = 0.03288928\n",
      "Iteration 166, loss = 0.03283020\n",
      "Iteration 167, loss = 0.03271102\n",
      "Iteration 168, loss = 0.03267461\n",
      "Iteration 169, loss = 0.03253902\n",
      "Iteration 170, loss = 0.03246292\n",
      "Iteration 171, loss = 0.03239942\n",
      "Iteration 172, loss = 0.03228645\n",
      "Iteration 173, loss = 0.03218954\n",
      "Iteration 174, loss = 0.03216036\n",
      "Iteration 175, loss = 0.03204788\n",
      "Iteration 176, loss = 0.03197159\n",
      "Iteration 177, loss = 0.03190222\n",
      "Iteration 178, loss = 0.03182601\n",
      "Iteration 179, loss = 0.03176032\n",
      "Iteration 180, loss = 0.03167544\n",
      "Iteration 181, loss = 0.03162184\n",
      "Iteration 182, loss = 0.03162374\n",
      "Iteration 183, loss = 0.03150267\n",
      "Iteration 184, loss = 0.03141517\n",
      "Iteration 185, loss = 0.03141169\n",
      "Iteration 186, loss = 0.03131235\n",
      "Iteration 187, loss = 0.03126935\n",
      "Iteration 188, loss = 0.03122329\n",
      "Iteration 189, loss = 0.03117279\n",
      "Iteration 190, loss = 0.03108010\n",
      "Iteration 191, loss = 0.03102392\n",
      "Iteration 192, loss = 0.03101894\n",
      "Iteration 193, loss = 0.03089079\n",
      "Iteration 194, loss = 0.03088135\n",
      "Iteration 195, loss = 0.03080393\n",
      "Iteration 196, loss = 0.03078753\n",
      "Iteration 197, loss = 0.03073381\n",
      "Iteration 198, loss = 0.03070466\n",
      "Iteration 199, loss = 0.03063029\n",
      "Iteration 200, loss = 0.03059128\n",
      "[CV] END activation=relu, alpha=0.01, batch_size=100, hidden_layer_sizes=(100, 100, 100), solver=sgd; total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.55194716\n",
      "Iteration 2, loss = 0.25233221\n",
      "Iteration 3, loss = 0.19995384\n",
      "Iteration 4, loss = 0.16803926\n",
      "Iteration 5, loss = 0.14364482\n",
      "Iteration 6, loss = 0.12664924\n",
      "Iteration 7, loss = 0.11168498\n",
      "Iteration 8, loss = 0.10102051\n",
      "Iteration 9, loss = 0.09263170\n",
      "Iteration 10, loss = 0.08262986\n",
      "Iteration 11, loss = 0.07569231\n",
      "Iteration 12, loss = 0.07057738\n",
      "Iteration 13, loss = 0.06453564\n",
      "Iteration 14, loss = 0.06020982\n",
      "Iteration 15, loss = 0.05631867\n",
      "Iteration 16, loss = 0.05256174\n",
      "Iteration 17, loss = 0.04902226\n",
      "Iteration 18, loss = 0.04690209\n",
      "Iteration 19, loss = 0.04379087\n",
      "Iteration 20, loss = 0.04265101\n",
      "Iteration 21, loss = 0.03986872\n",
      "Iteration 22, loss = 0.03789988\n",
      "Iteration 23, loss = 0.03594671\n",
      "Iteration 24, loss = 0.03437753\n",
      "Iteration 25, loss = 0.03319383\n",
      "Iteration 26, loss = 0.03219114\n",
      "Iteration 27, loss = 0.03077984\n",
      "Iteration 28, loss = 0.02959430\n",
      "Iteration 29, loss = 0.02920458\n",
      "Iteration 30, loss = 0.02842380\n",
      "Iteration 31, loss = 0.02766292\n",
      "Iteration 32, loss = 0.02667791\n",
      "Iteration 33, loss = 0.02633616\n",
      "Iteration 34, loss = 0.02538294\n",
      "Iteration 35, loss = 0.02563282\n",
      "Iteration 36, loss = 0.02472117\n",
      "Iteration 37, loss = 0.02373334\n",
      "Iteration 38, loss = 0.02378546\n",
      "Iteration 39, loss = 0.02328707\n",
      "Iteration 40, loss = 0.02313435\n",
      "Iteration 41, loss = 0.02297033\n",
      "Iteration 42, loss = 0.02296195\n",
      "Iteration 43, loss = 0.02251594\n",
      "Iteration 44, loss = 0.02206399\n",
      "Iteration 45, loss = 0.02492131\n",
      "Iteration 46, loss = 0.02465627\n",
      "Iteration 47, loss = 0.02200708\n",
      "Iteration 48, loss = 0.02125894\n",
      "Iteration 49, loss = 0.02097741\n",
      "Iteration 50, loss = 0.02044639\n",
      "Iteration 51, loss = 0.02054248\n",
      "Iteration 52, loss = 0.02275562\n",
      "Iteration 53, loss = 0.02262372\n",
      "Iteration 54, loss = 0.02110757\n",
      "Iteration 55, loss = 0.02026320\n",
      "Iteration 56, loss = 0.01992286\n",
      "Iteration 57, loss = 0.02035977\n",
      "Iteration 58, loss = 0.02346278\n",
      "Iteration 59, loss = 0.02551265\n",
      "Iteration 60, loss = 0.02172171\n",
      "Iteration 61, loss = 0.02016349\n",
      "Iteration 62, loss = 0.01972783\n",
      "Iteration 63, loss = 0.01906857\n",
      "Iteration 64, loss = 0.01876596\n",
      "Iteration 65, loss = 0.01875361\n",
      "Iteration 66, loss = 0.01902426\n",
      "Iteration 67, loss = 0.01961897\n",
      "Iteration 68, loss = 0.02307122\n",
      "Iteration 69, loss = 0.02153540\n",
      "Iteration 70, loss = 0.02066195\n",
      "Iteration 71, loss = 0.01915810\n",
      "Iteration 72, loss = 0.01859775\n",
      "Iteration 73, loss = 0.01828982\n",
      "Iteration 74, loss = 0.01820926\n",
      "Iteration 75, loss = 0.01831746\n",
      "Iteration 76, loss = 0.02039015\n",
      "Iteration 77, loss = 0.02083647\n",
      "Iteration 78, loss = 0.02179179\n",
      "Iteration 79, loss = 0.02278290\n",
      "Iteration 80, loss = 0.02097964\n",
      "Iteration 81, loss = 0.01861092\n",
      "Iteration 82, loss = 0.01804781\n",
      "Iteration 83, loss = 0.01775045\n",
      "Iteration 84, loss = 0.01755049\n",
      "Iteration 85, loss = 0.01829205\n",
      "Iteration 86, loss = 0.02095993\n",
      "Iteration 87, loss = 0.02170051\n",
      "Iteration 88, loss = 0.01954462\n",
      "Iteration 89, loss = 0.01815462\n",
      "Iteration 90, loss = 0.01800944\n",
      "Iteration 91, loss = 0.01772077\n",
      "Iteration 92, loss = 0.01780246\n",
      "Iteration 93, loss = 0.01811231\n",
      "Iteration 94, loss = 0.02018551\n",
      "Iteration 95, loss = 0.02330661\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.01, batch_size=200, hidden_layer_sizes=(100,), solver=adam; total time= 1.1min\n",
      "Iteration 1, loss = 0.56865035\n",
      "Iteration 2, loss = 0.25879078\n",
      "Iteration 3, loss = 0.20562768\n",
      "Iteration 4, loss = 0.17227235\n",
      "Iteration 5, loss = 0.14762231\n",
      "Iteration 6, loss = 0.12979260\n",
      "Iteration 7, loss = 0.11471836\n",
      "Iteration 8, loss = 0.10189634\n",
      "Iteration 9, loss = 0.09326397\n",
      "Iteration 10, loss = 0.08377968\n",
      "Iteration 11, loss = 0.07589715\n",
      "Iteration 12, loss = 0.06988109\n",
      "Iteration 13, loss = 0.06452180\n",
      "Iteration 14, loss = 0.05968802\n",
      "Iteration 15, loss = 0.05600198\n",
      "Iteration 16, loss = 0.05177891\n",
      "Iteration 17, loss = 0.04955674\n",
      "Iteration 18, loss = 0.04557893\n",
      "Iteration 19, loss = 0.04319137\n",
      "Iteration 20, loss = 0.04084080\n",
      "Iteration 21, loss = 0.03915864\n",
      "Iteration 22, loss = 0.03788648\n",
      "Iteration 23, loss = 0.03633553\n",
      "Iteration 24, loss = 0.03424332\n",
      "Iteration 25, loss = 0.03298214\n",
      "Iteration 26, loss = 0.03183665\n",
      "Iteration 27, loss = 0.03093304\n",
      "Iteration 28, loss = 0.03009752\n",
      "Iteration 29, loss = 0.02866769\n",
      "Iteration 30, loss = 0.02770758\n",
      "Iteration 31, loss = 0.02731780\n",
      "Iteration 32, loss = 0.02629887\n",
      "Iteration 33, loss = 0.02691356\n",
      "Iteration 34, loss = 0.02610293\n",
      "Iteration 35, loss = 0.02482914\n",
      "Iteration 36, loss = 0.02486376\n",
      "Iteration 37, loss = 0.02473928\n",
      "Iteration 38, loss = 0.02497590\n",
      "Iteration 39, loss = 0.02394030\n",
      "Iteration 40, loss = 0.02316919\n",
      "Iteration 41, loss = 0.02255865\n",
      "Iteration 42, loss = 0.02300558\n",
      "Iteration 43, loss = 0.02252249\n",
      "Iteration 44, loss = 0.02231137\n",
      "Iteration 45, loss = 0.02242412\n",
      "Iteration 46, loss = 0.02391237\n",
      "Iteration 47, loss = 0.02322658\n",
      "Iteration 48, loss = 0.02131338\n",
      "Iteration 49, loss = 0.02189291\n",
      "Iteration 50, loss = 0.02089947\n",
      "Iteration 51, loss = 0.02112857\n",
      "Iteration 52, loss = 0.02075855\n",
      "Iteration 53, loss = 0.02141164\n",
      "Iteration 54, loss = 0.02181410\n",
      "Iteration 55, loss = 0.02128448\n",
      "Iteration 56, loss = 0.02100756\n",
      "Iteration 57, loss = 0.02090180\n",
      "Iteration 58, loss = 0.02017785\n",
      "Iteration 59, loss = 0.02113780\n",
      "Iteration 60, loss = 0.02124152\n",
      "Iteration 61, loss = 0.02144892\n",
      "Iteration 62, loss = 0.02194079\n",
      "Iteration 63, loss = 0.02082852\n",
      "Iteration 64, loss = 0.02098253\n",
      "Iteration 65, loss = 0.02025516\n",
      "Iteration 66, loss = 0.01893986\n",
      "Iteration 67, loss = 0.01861234\n",
      "Iteration 68, loss = 0.01866688\n",
      "Iteration 69, loss = 0.01931933\n",
      "Iteration 70, loss = 0.02474480\n",
      "Iteration 71, loss = 0.02402825\n",
      "Iteration 72, loss = 0.02197147\n",
      "Iteration 73, loss = 0.01955900\n",
      "Iteration 74, loss = 0.01863216\n",
      "Iteration 75, loss = 0.01825950\n",
      "Iteration 76, loss = 0.01809042\n",
      "Iteration 77, loss = 0.01812256\n",
      "Iteration 78, loss = 0.02667166\n",
      "Iteration 79, loss = 0.02117800\n",
      "Iteration 80, loss = 0.01895080\n",
      "Iteration 81, loss = 0.01822099\n",
      "Iteration 82, loss = 0.01789787\n",
      "Iteration 83, loss = 0.01774868\n",
      "Iteration 84, loss = 0.01783043\n",
      "Iteration 85, loss = 0.01785951\n",
      "Iteration 86, loss = 0.01842680\n",
      "Iteration 87, loss = 0.02170272\n",
      "Iteration 88, loss = 0.02658535\n",
      "Iteration 89, loss = 0.02578576\n",
      "Iteration 90, loss = 0.01945441\n",
      "Iteration 91, loss = 0.01825592\n",
      "Iteration 92, loss = 0.01785359\n",
      "Iteration 93, loss = 0.01766642\n",
      "Iteration 94, loss = 0.01775038\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.01, batch_size=200, hidden_layer_sizes=(100,), solver=adam; total time=  51.4s\n",
      "Iteration 1, loss = 0.55569563\n",
      "Iteration 2, loss = 0.25186379\n",
      "Iteration 3, loss = 0.19883616\n",
      "Iteration 4, loss = 0.16615311\n",
      "Iteration 5, loss = 0.14338118\n",
      "Iteration 6, loss = 0.12659198\n",
      "Iteration 7, loss = 0.11232537\n",
      "Iteration 8, loss = 0.10129218\n",
      "Iteration 9, loss = 0.09163222\n",
      "Iteration 10, loss = 0.08367031\n",
      "Iteration 11, loss = 0.07736448\n",
      "Iteration 12, loss = 0.07053333\n",
      "Iteration 13, loss = 0.06515605\n",
      "Iteration 14, loss = 0.06074680\n",
      "Iteration 15, loss = 0.05664553\n",
      "Iteration 16, loss = 0.05273220\n",
      "Iteration 17, loss = 0.04942614\n",
      "Iteration 18, loss = 0.04600985\n",
      "Iteration 19, loss = 0.04336306\n",
      "Iteration 20, loss = 0.04076018\n",
      "Iteration 21, loss = 0.03926261\n",
      "Iteration 22, loss = 0.03773624\n",
      "Iteration 23, loss = 0.03579299\n",
      "Iteration 24, loss = 0.03454683\n",
      "Iteration 25, loss = 0.03272636\n",
      "Iteration 26, loss = 0.03199137\n",
      "Iteration 27, loss = 0.03031657\n",
      "Iteration 28, loss = 0.02969046\n",
      "Iteration 29, loss = 0.02931347\n",
      "Iteration 30, loss = 0.02762052\n",
      "Iteration 31, loss = 0.02690587\n",
      "Iteration 32, loss = 0.02599892\n",
      "Iteration 33, loss = 0.02543053\n",
      "Iteration 34, loss = 0.02472777\n",
      "Iteration 35, loss = 0.02496764\n",
      "Iteration 36, loss = 0.02477518\n",
      "Iteration 37, loss = 0.02327246\n",
      "Iteration 38, loss = 0.02385254\n",
      "Iteration 39, loss = 0.02323294\n",
      "Iteration 40, loss = 0.02509046\n",
      "Iteration 41, loss = 0.02332466\n",
      "Iteration 42, loss = 0.02214943\n",
      "Iteration 43, loss = 0.02174886\n",
      "Iteration 44, loss = 0.02145796\n",
      "Iteration 45, loss = 0.02130826\n",
      "Iteration 46, loss = 0.02171154\n",
      "Iteration 47, loss = 0.02139892\n",
      "Iteration 48, loss = 0.02152631\n",
      "Iteration 49, loss = 0.02220167\n",
      "Iteration 50, loss = 0.02141005\n",
      "Iteration 51, loss = 0.02323576\n",
      "Iteration 52, loss = 0.02419494\n",
      "Iteration 53, loss = 0.02102449\n",
      "Iteration 54, loss = 0.01990473\n",
      "Iteration 55, loss = 0.02033303\n",
      "Iteration 56, loss = 0.01977029\n",
      "Iteration 57, loss = 0.01979042\n",
      "Iteration 58, loss = 0.01922430\n",
      "Iteration 59, loss = 0.01924646\n",
      "Iteration 60, loss = 0.01986845\n",
      "Iteration 61, loss = 0.02188642\n",
      "Iteration 62, loss = 0.02226582\n",
      "Iteration 63, loss = 0.02273427\n",
      "Iteration 64, loss = 0.02344221\n",
      "Iteration 65, loss = 0.01938519\n",
      "Iteration 66, loss = 0.01873102\n",
      "Iteration 67, loss = 0.01838561\n",
      "Iteration 68, loss = 0.01826585\n",
      "Iteration 69, loss = 0.01822583\n",
      "Iteration 70, loss = 0.01831375\n",
      "Iteration 71, loss = 0.01850419\n",
      "Iteration 72, loss = 0.02715824\n",
      "Iteration 73, loss = 0.02421223\n",
      "Iteration 74, loss = 0.01969364\n",
      "Iteration 75, loss = 0.01855694\n",
      "Iteration 76, loss = 0.01804511\n",
      "Iteration 77, loss = 0.01810854\n",
      "Iteration 78, loss = 0.01775255\n",
      "Iteration 79, loss = 0.01762277\n",
      "Iteration 80, loss = 0.01794193\n",
      "Iteration 81, loss = 0.02306769\n",
      "Iteration 82, loss = 0.02598127\n",
      "Iteration 83, loss = 0.02140167\n",
      "Iteration 84, loss = 0.01838301\n",
      "Iteration 85, loss = 0.01761739\n",
      "Iteration 86, loss = 0.01744371\n",
      "Iteration 87, loss = 0.01724069\n",
      "Iteration 88, loss = 0.01722798\n",
      "Iteration 89, loss = 0.01733398\n",
      "Iteration 90, loss = 0.01753364\n",
      "Iteration 91, loss = 0.01765376\n",
      "Iteration 92, loss = 0.01815475\n",
      "Iteration 93, loss = 0.02746776\n",
      "Iteration 94, loss = 0.02591976\n",
      "Iteration 95, loss = 0.01935698\n",
      "Iteration 96, loss = 0.01805622\n",
      "Iteration 97, loss = 0.01746688\n",
      "Iteration 98, loss = 0.01716012\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.01, batch_size=200, hidden_layer_sizes=(100,), solver=adam; total time=  53.1s\n",
      "Iteration 1, loss = 0.56255652\n",
      "Iteration 2, loss = 0.26517469\n",
      "Iteration 3, loss = 0.20754195\n",
      "Iteration 4, loss = 0.17291743\n",
      "Iteration 5, loss = 0.14688932\n",
      "Iteration 6, loss = 0.12752750\n",
      "Iteration 7, loss = 0.11344967\n",
      "Iteration 8, loss = 0.10165061\n",
      "Iteration 9, loss = 0.09253558\n",
      "Iteration 10, loss = 0.08431987\n",
      "Iteration 11, loss = 0.07817635\n",
      "Iteration 12, loss = 0.07202731\n",
      "Iteration 13, loss = 0.06591853\n",
      "Iteration 14, loss = 0.06169206\n",
      "Iteration 15, loss = 0.05777717\n",
      "Iteration 16, loss = 0.05438720\n",
      "Iteration 17, loss = 0.05086432\n",
      "Iteration 18, loss = 0.04793744\n",
      "Iteration 19, loss = 0.04579302\n",
      "Iteration 20, loss = 0.04325789\n",
      "Iteration 21, loss = 0.04102681\n",
      "Iteration 22, loss = 0.03882454\n",
      "Iteration 23, loss = 0.03728052\n",
      "Iteration 24, loss = 0.03600709\n",
      "Iteration 25, loss = 0.03493209\n",
      "Iteration 26, loss = 0.03359192\n",
      "Iteration 27, loss = 0.03113333\n",
      "Iteration 28, loss = 0.03075209\n",
      "Iteration 29, loss = 0.02926617\n",
      "Iteration 30, loss = 0.02847852\n",
      "Iteration 31, loss = 0.02823551\n",
      "Iteration 32, loss = 0.02720329\n",
      "Iteration 33, loss = 0.02638784\n",
      "Iteration 34, loss = 0.02627712\n",
      "Iteration 35, loss = 0.02528442\n",
      "Iteration 36, loss = 0.02561133\n",
      "Iteration 37, loss = 0.02507089\n",
      "Iteration 38, loss = 0.02447271\n",
      "Iteration 39, loss = 0.02405340\n",
      "Iteration 40, loss = 0.02370566\n",
      "Iteration 41, loss = 0.02350606\n",
      "Iteration 42, loss = 0.02327458\n",
      "Iteration 43, loss = 0.02294843\n",
      "Iteration 44, loss = 0.02309134\n",
      "Iteration 45, loss = 0.02231032\n",
      "Iteration 46, loss = 0.02194684\n",
      "Iteration 47, loss = 0.02127008\n",
      "Iteration 48, loss = 0.02222985\n",
      "Iteration 49, loss = 0.02256434\n",
      "Iteration 50, loss = 0.02224305\n",
      "Iteration 51, loss = 0.02159278\n",
      "Iteration 52, loss = 0.02147962\n",
      "Iteration 53, loss = 0.02211810\n",
      "Iteration 54, loss = 0.02144247\n",
      "Iteration 55, loss = 0.02304612\n",
      "Iteration 56, loss = 0.02156066\n",
      "Iteration 57, loss = 0.02061123\n",
      "Iteration 58, loss = 0.01984794\n",
      "Iteration 59, loss = 0.01936827\n",
      "Iteration 60, loss = 0.01952005\n",
      "Iteration 61, loss = 0.01986046\n",
      "Iteration 62, loss = 0.02148615\n",
      "Iteration 63, loss = 0.02284122\n",
      "Iteration 64, loss = 0.02572352\n",
      "Iteration 65, loss = 0.02100506\n",
      "Iteration 66, loss = 0.01913035\n",
      "Iteration 67, loss = 0.01868682\n",
      "Iteration 68, loss = 0.01852542\n",
      "Iteration 69, loss = 0.01874776\n",
      "Iteration 70, loss = 0.01904958\n",
      "Iteration 71, loss = 0.02420443\n",
      "Iteration 72, loss = 0.02422911\n",
      "Iteration 73, loss = 0.02061396\n",
      "Iteration 74, loss = 0.01872479\n",
      "Iteration 75, loss = 0.01829430\n",
      "Iteration 76, loss = 0.01815641\n",
      "Iteration 77, loss = 0.01808820\n",
      "Iteration 78, loss = 0.01807212\n",
      "Iteration 79, loss = 0.02030954\n",
      "Iteration 80, loss = 0.02991093\n",
      "Iteration 81, loss = 0.02444818\n",
      "Iteration 82, loss = 0.01952028\n",
      "Iteration 83, loss = 0.01837092\n",
      "Iteration 84, loss = 0.01799597\n",
      "Iteration 85, loss = 0.01786581\n",
      "Iteration 86, loss = 0.01782635\n",
      "Iteration 87, loss = 0.01768287\n",
      "Iteration 88, loss = 0.01771347\n",
      "Iteration 89, loss = 0.01785888\n",
      "Iteration 90, loss = 0.01982265\n",
      "Iteration 91, loss = 0.02849368\n",
      "Iteration 92, loss = 0.02230098\n",
      "Iteration 93, loss = 0.01883616\n",
      "Iteration 94, loss = 0.01803005\n",
      "Iteration 95, loss = 0.01755870\n",
      "Iteration 96, loss = 0.01741244\n",
      "Iteration 97, loss = 0.01730184\n",
      "Iteration 98, loss = 0.01744554\n",
      "Iteration 99, loss = 0.01988485\n",
      "Iteration 100, loss = 0.01941392\n",
      "Iteration 101, loss = 0.01889096\n",
      "Iteration 102, loss = 0.01820338\n",
      "Iteration 103, loss = 0.01770137\n",
      "Iteration 104, loss = 0.01744664\n",
      "Iteration 105, loss = 0.01794466\n",
      "Iteration 106, loss = 0.02240632\n",
      "Iteration 107, loss = 0.02721152\n",
      "Iteration 108, loss = 0.01979406\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.01, batch_size=200, hidden_layer_sizes=(100,), solver=adam; total time= 1.3min\n",
      "Iteration 1, loss = 0.56023150\n",
      "Iteration 2, loss = 0.26279178\n",
      "Iteration 3, loss = 0.20738170\n",
      "Iteration 4, loss = 0.17048366\n",
      "Iteration 5, loss = 0.14505141\n",
      "Iteration 6, loss = 0.12620549\n",
      "Iteration 7, loss = 0.11246694\n",
      "Iteration 8, loss = 0.10043747\n",
      "Iteration 9, loss = 0.09033892\n",
      "Iteration 10, loss = 0.08264476\n",
      "Iteration 11, loss = 0.07506331\n",
      "Iteration 12, loss = 0.06892631\n",
      "Iteration 13, loss = 0.06467261\n",
      "Iteration 14, loss = 0.06024104\n",
      "Iteration 15, loss = 0.05483512\n",
      "Iteration 16, loss = 0.05137569\n",
      "Iteration 17, loss = 0.04882425\n",
      "Iteration 18, loss = 0.04526710\n",
      "Iteration 19, loss = 0.04367348\n",
      "Iteration 20, loss = 0.04075593\n",
      "Iteration 21, loss = 0.03853341\n",
      "Iteration 22, loss = 0.03773032\n",
      "Iteration 23, loss = 0.03533435\n",
      "Iteration 24, loss = 0.03451946\n",
      "Iteration 25, loss = 0.03303867\n",
      "Iteration 26, loss = 0.03188537\n",
      "Iteration 27, loss = 0.03096344\n",
      "Iteration 28, loss = 0.02957401\n",
      "Iteration 29, loss = 0.02888619\n",
      "Iteration 30, loss = 0.02843737\n",
      "Iteration 31, loss = 0.02776623\n",
      "Iteration 32, loss = 0.02631594\n",
      "Iteration 33, loss = 0.02616939\n",
      "Iteration 34, loss = 0.02626391\n",
      "Iteration 35, loss = 0.02458754\n",
      "Iteration 36, loss = 0.02465473\n",
      "Iteration 37, loss = 0.02437257\n",
      "Iteration 38, loss = 0.02426089\n",
      "Iteration 39, loss = 0.02385753\n",
      "Iteration 40, loss = 0.02323294\n",
      "Iteration 41, loss = 0.02313716\n",
      "Iteration 42, loss = 0.02293775\n",
      "Iteration 43, loss = 0.02316299\n",
      "Iteration 44, loss = 0.02243794\n",
      "Iteration 45, loss = 0.02283911\n",
      "Iteration 46, loss = 0.02207745\n",
      "Iteration 47, loss = 0.02253732\n",
      "Iteration 48, loss = 0.02253764\n",
      "Iteration 49, loss = 0.02195340\n",
      "Iteration 50, loss = 0.02205594\n",
      "Iteration 51, loss = 0.02356136\n",
      "Iteration 52, loss = 0.02220769\n",
      "Iteration 53, loss = 0.02138321\n",
      "Iteration 54, loss = 0.02166960\n",
      "Iteration 55, loss = 0.02031773\n",
      "Iteration 56, loss = 0.02142690\n",
      "Iteration 57, loss = 0.02066180\n",
      "Iteration 58, loss = 0.02068625\n",
      "Iteration 59, loss = 0.02025495\n",
      "Iteration 60, loss = 0.02309766\n",
      "Iteration 61, loss = 0.03158909\n",
      "Iteration 62, loss = 0.02332365\n",
      "Iteration 63, loss = 0.02014555\n",
      "Iteration 64, loss = 0.01952894\n",
      "Iteration 65, loss = 0.01908524\n",
      "Iteration 66, loss = 0.01885938\n",
      "Iteration 67, loss = 0.01878229\n",
      "Iteration 68, loss = 0.01883695\n",
      "Iteration 69, loss = 0.02046895\n",
      "Iteration 70, loss = 0.02285204\n",
      "Iteration 71, loss = 0.02399675\n",
      "Iteration 72, loss = 0.02056416\n",
      "Iteration 73, loss = 0.01906227\n",
      "Iteration 74, loss = 0.01857817\n",
      "Iteration 75, loss = 0.01841714\n",
      "Iteration 76, loss = 0.01860906\n",
      "Iteration 77, loss = 0.02334959\n",
      "Iteration 78, loss = 0.02373181\n",
      "Iteration 79, loss = 0.02053045\n",
      "Iteration 80, loss = 0.01976587\n",
      "Iteration 81, loss = 0.01936542\n",
      "Iteration 82, loss = 0.01912087\n",
      "Iteration 83, loss = 0.01829900\n",
      "Iteration 84, loss = 0.01806047\n",
      "Iteration 85, loss = 0.01793964\n",
      "Iteration 86, loss = 0.01818979\n",
      "Iteration 87, loss = 0.01869427\n",
      "Iteration 88, loss = 0.03054686\n",
      "Iteration 89, loss = 0.02276434\n",
      "Iteration 90, loss = 0.01946948\n",
      "Iteration 91, loss = 0.01826884\n",
      "Iteration 92, loss = 0.01786650\n",
      "Iteration 93, loss = 0.01799512\n",
      "Iteration 94, loss = 0.01763658\n",
      "Iteration 95, loss = 0.01764672\n",
      "Iteration 96, loss = 0.01785269\n",
      "Iteration 97, loss = 0.01892474\n",
      "Iteration 98, loss = 0.01973392\n",
      "Iteration 99, loss = 0.02166633\n",
      "Iteration 100, loss = 0.02334853\n",
      "Iteration 101, loss = 0.02031546\n",
      "Iteration 102, loss = 0.01836624\n",
      "Iteration 103, loss = 0.01810426\n",
      "Iteration 104, loss = 0.01772983\n",
      "Iteration 105, loss = 0.01743847\n",
      "Iteration 106, loss = 0.01764027\n",
      "Iteration 107, loss = 0.01919089\n",
      "Iteration 108, loss = 0.02429385\n",
      "Iteration 109, loss = 0.02256783\n",
      "Iteration 110, loss = 0.01867993\n",
      "Iteration 111, loss = 0.01785427\n",
      "Iteration 112, loss = 0.01741092\n",
      "Iteration 113, loss = 0.01729895\n",
      "Iteration 114, loss = 0.01731661\n",
      "Iteration 115, loss = 0.01731291\n",
      "Iteration 116, loss = 0.01785106\n",
      "Iteration 117, loss = 0.02262275\n",
      "Iteration 118, loss = 0.02763854\n",
      "Iteration 119, loss = 0.02177652\n",
      "Iteration 120, loss = 0.01826572\n",
      "Iteration 121, loss = 0.01753845\n",
      "Iteration 122, loss = 0.01730296\n",
      "Iteration 123, loss = 0.01711084\n",
      "Iteration 124, loss = 0.01696796\n",
      "Iteration 125, loss = 0.01693951\n",
      "Iteration 126, loss = 0.01765693\n",
      "Iteration 127, loss = 0.01759190\n",
      "Iteration 128, loss = 0.02553620\n",
      "Iteration 129, loss = 0.02713036\n",
      "Iteration 130, loss = 0.01949835\n",
      "Iteration 131, loss = 0.01803462\n",
      "Iteration 132, loss = 0.01741581\n",
      "Iteration 133, loss = 0.01712262\n",
      "Iteration 134, loss = 0.01707938\n",
      "Iteration 135, loss = 0.01702966\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.01, batch_size=200, hidden_layer_sizes=(100,), solver=adam; total time= 1.2min\n",
      "Iteration 1, loss = 1.79611544\n",
      "Iteration 2, loss = 1.01445836\n",
      "Iteration 3, loss = 0.72818580\n",
      "Iteration 4, loss = 0.60368672\n",
      "Iteration 5, loss = 0.53395881\n",
      "Iteration 6, loss = 0.48896724\n",
      "Iteration 7, loss = 0.45691665\n",
      "Iteration 8, loss = 0.43296812\n",
      "Iteration 9, loss = 0.41409023\n",
      "Iteration 10, loss = 0.39876304\n",
      "Iteration 11, loss = 0.38588475\n",
      "Iteration 12, loss = 0.37495008\n",
      "Iteration 13, loss = 0.36537129\n",
      "Iteration 14, loss = 0.35705608\n",
      "Iteration 15, loss = 0.34953693\n",
      "Iteration 16, loss = 0.34277710\n",
      "Iteration 17, loss = 0.33674303\n",
      "Iteration 18, loss = 0.33111576\n",
      "Iteration 19, loss = 0.32582447\n",
      "Iteration 20, loss = 0.32088277\n",
      "Iteration 21, loss = 0.31622255\n",
      "Iteration 22, loss = 0.31196207\n",
      "Iteration 23, loss = 0.30779377\n",
      "Iteration 24, loss = 0.30386971\n",
      "Iteration 25, loss = 0.30012977\n",
      "Iteration 26, loss = 0.29650052\n",
      "Iteration 27, loss = 0.29319826\n",
      "Iteration 28, loss = 0.28974321\n",
      "Iteration 29, loss = 0.28658733\n",
      "Iteration 30, loss = 0.28356708\n",
      "Iteration 31, loss = 0.28049034\n",
      "Iteration 32, loss = 0.27765471\n",
      "Iteration 33, loss = 0.27475802\n",
      "Iteration 34, loss = 0.27200006\n",
      "Iteration 35, loss = 0.26924564\n",
      "Iteration 36, loss = 0.26662431\n",
      "Iteration 37, loss = 0.26421617\n",
      "Iteration 38, loss = 0.26177563\n",
      "Iteration 39, loss = 0.25922814\n",
      "Iteration 40, loss = 0.25693786\n",
      "Iteration 41, loss = 0.25455662\n",
      "Iteration 42, loss = 0.25240248\n",
      "Iteration 43, loss = 0.25026714\n",
      "Iteration 44, loss = 0.24802972\n",
      "Iteration 45, loss = 0.24591453\n",
      "Iteration 46, loss = 0.24391371\n",
      "Iteration 47, loss = 0.24176487\n",
      "Iteration 48, loss = 0.23974253\n",
      "Iteration 49, loss = 0.23781063\n",
      "Iteration 50, loss = 0.23585861\n",
      "Iteration 51, loss = 0.23402760\n",
      "Iteration 52, loss = 0.23209335\n",
      "Iteration 53, loss = 0.23033322\n",
      "Iteration 54, loss = 0.22853911\n",
      "Iteration 55, loss = 0.22674645\n",
      "Iteration 56, loss = 0.22502836\n",
      "Iteration 57, loss = 0.22328056\n",
      "Iteration 58, loss = 0.22165485\n",
      "Iteration 59, loss = 0.21997502\n",
      "Iteration 60, loss = 0.21843553\n",
      "Iteration 61, loss = 0.21685363\n",
      "Iteration 62, loss = 0.21527309\n",
      "Iteration 63, loss = 0.21375194\n",
      "Iteration 64, loss = 0.21215422\n",
      "Iteration 65, loss = 0.21068946\n",
      "Iteration 66, loss = 0.20920265\n",
      "Iteration 67, loss = 0.20779075\n",
      "Iteration 68, loss = 0.20631300\n",
      "Iteration 69, loss = 0.20491744\n",
      "Iteration 70, loss = 0.20350977\n",
      "Iteration 71, loss = 0.20218676\n",
      "Iteration 72, loss = 0.20077919\n",
      "Iteration 73, loss = 0.19947783\n",
      "Iteration 74, loss = 0.19820468\n",
      "Iteration 75, loss = 0.19685719\n",
      "Iteration 76, loss = 0.19556013\n",
      "Iteration 77, loss = 0.19441900\n",
      "Iteration 78, loss = 0.19311096\n",
      "Iteration 79, loss = 0.19186392\n",
      "Iteration 80, loss = 0.19067838\n",
      "Iteration 81, loss = 0.18949807\n",
      "Iteration 82, loss = 0.18828017\n",
      "Iteration 83, loss = 0.18702885\n",
      "Iteration 84, loss = 0.18597070\n",
      "Iteration 85, loss = 0.18481501\n",
      "Iteration 86, loss = 0.18370339\n",
      "Iteration 87, loss = 0.18255267\n",
      "Iteration 88, loss = 0.18149339\n",
      "Iteration 89, loss = 0.18034510\n",
      "Iteration 90, loss = 0.17928263\n",
      "Iteration 91, loss = 0.17824209\n",
      "Iteration 92, loss = 0.17715505\n",
      "Iteration 93, loss = 0.17615257\n",
      "Iteration 94, loss = 0.17518401\n",
      "Iteration 95, loss = 0.17415810\n",
      "Iteration 96, loss = 0.17319297\n",
      "Iteration 97, loss = 0.17221617\n",
      "Iteration 98, loss = 0.17120540\n",
      "Iteration 99, loss = 0.17023672\n",
      "Iteration 100, loss = 0.16931817\n",
      "Iteration 101, loss = 0.16832564\n",
      "Iteration 102, loss = 0.16738890\n",
      "Iteration 103, loss = 0.16645902\n",
      "Iteration 104, loss = 0.16557686\n",
      "Iteration 105, loss = 0.16471584\n",
      "Iteration 106, loss = 0.16376036\n",
      "Iteration 107, loss = 0.16291810\n",
      "Iteration 108, loss = 0.16197326\n",
      "Iteration 109, loss = 0.16111180\n",
      "Iteration 110, loss = 0.16027118\n",
      "Iteration 111, loss = 0.15949386\n",
      "Iteration 112, loss = 0.15865836\n",
      "Iteration 113, loss = 0.15783872\n",
      "Iteration 114, loss = 0.15699893\n",
      "Iteration 115, loss = 0.15615392\n",
      "Iteration 116, loss = 0.15531926\n",
      "Iteration 117, loss = 0.15460575\n",
      "Iteration 118, loss = 0.15373872\n",
      "Iteration 119, loss = 0.15304076\n",
      "Iteration 120, loss = 0.15226995\n",
      "Iteration 121, loss = 0.15150186\n",
      "Iteration 122, loss = 0.15069930\n",
      "Iteration 123, loss = 0.14998906\n",
      "Iteration 124, loss = 0.14926573\n",
      "Iteration 125, loss = 0.14851026\n",
      "Iteration 126, loss = 0.14778875\n",
      "Iteration 127, loss = 0.14707837\n",
      "Iteration 128, loss = 0.14637679\n",
      "Iteration 129, loss = 0.14570946\n",
      "Iteration 130, loss = 0.14498157\n",
      "Iteration 131, loss = 0.14433376\n",
      "Iteration 132, loss = 0.14364536\n",
      "Iteration 133, loss = 0.14298723\n",
      "Iteration 134, loss = 0.14228454\n",
      "Iteration 135, loss = 0.14162774\n",
      "Iteration 136, loss = 0.14103084\n",
      "Iteration 137, loss = 0.14029617\n",
      "Iteration 138, loss = 0.13971677\n",
      "Iteration 139, loss = 0.13907115\n",
      "Iteration 140, loss = 0.13845025\n",
      "Iteration 141, loss = 0.13784980\n",
      "Iteration 142, loss = 0.13723223\n",
      "Iteration 143, loss = 0.13665551\n",
      "Iteration 144, loss = 0.13600090\n",
      "Iteration 145, loss = 0.13540094\n",
      "Iteration 146, loss = 0.13483393\n",
      "Iteration 147, loss = 0.13414908\n",
      "Iteration 148, loss = 0.13371135\n",
      "Iteration 149, loss = 0.13304561\n",
      "Iteration 150, loss = 0.13250151\n",
      "Iteration 151, loss = 0.13186367\n",
      "Iteration 152, loss = 0.13148151\n",
      "Iteration 153, loss = 0.13083464\n",
      "Iteration 154, loss = 0.13025535\n",
      "Iteration 155, loss = 0.12970403\n",
      "Iteration 156, loss = 0.12921836\n",
      "Iteration 157, loss = 0.12864666\n",
      "Iteration 158, loss = 0.12808715\n",
      "Iteration 159, loss = 0.12752326\n",
      "Iteration 160, loss = 0.12702322\n",
      "Iteration 161, loss = 0.12651465\n",
      "Iteration 162, loss = 0.12603430\n",
      "Iteration 163, loss = 0.12553492\n",
      "Iteration 164, loss = 0.12503220\n",
      "Iteration 165, loss = 0.12449997\n",
      "Iteration 166, loss = 0.12397407\n",
      "Iteration 167, loss = 0.12350586\n",
      "Iteration 168, loss = 0.12309598\n",
      "Iteration 169, loss = 0.12252447\n",
      "Iteration 170, loss = 0.12200728\n",
      "Iteration 171, loss = 0.12161792\n",
      "Iteration 172, loss = 0.12110017\n",
      "Iteration 173, loss = 0.12056666\n",
      "Iteration 174, loss = 0.12013777\n",
      "Iteration 175, loss = 0.11969604\n",
      "Iteration 176, loss = 0.11920225\n",
      "Iteration 177, loss = 0.11876840\n",
      "Iteration 178, loss = 0.11829909\n",
      "Iteration 179, loss = 0.11787246\n",
      "Iteration 180, loss = 0.11737248\n",
      "Iteration 181, loss = 0.11702016\n",
      "Iteration 182, loss = 0.11652957\n",
      "Iteration 183, loss = 0.11614292\n",
      "Iteration 184, loss = 0.11572556\n",
      "Iteration 185, loss = 0.11519570\n",
      "Iteration 186, loss = 0.11475018\n",
      "Iteration 187, loss = 0.11443268\n",
      "Iteration 188, loss = 0.11392907\n",
      "Iteration 189, loss = 0.11357242\n",
      "Iteration 190, loss = 0.11314432\n",
      "Iteration 191, loss = 0.11272849\n",
      "Iteration 192, loss = 0.11229735\n",
      "Iteration 193, loss = 0.11189034\n",
      "Iteration 194, loss = 0.11148335\n",
      "Iteration 195, loss = 0.11114341\n",
      "Iteration 196, loss = 0.11069847\n",
      "Iteration 197, loss = 0.11035249\n",
      "Iteration 198, loss = 0.10991678\n",
      "Iteration 199, loss = 0.10949896\n",
      "Iteration 200, loss = 0.10918128\n",
      "[CV] END activation=relu, alpha=0.01, batch_size=200, hidden_layer_sizes=(100,), solver=sgd; total time= 1.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.75473150\n",
      "Iteration 2, loss = 1.03817113\n",
      "Iteration 3, loss = 0.75029147\n",
      "Iteration 4, loss = 0.61743222\n",
      "Iteration 5, loss = 0.54232892\n",
      "Iteration 6, loss = 0.49390966\n",
      "Iteration 7, loss = 0.45971774\n",
      "Iteration 8, loss = 0.43428081\n",
      "Iteration 9, loss = 0.41426789\n",
      "Iteration 10, loss = 0.39828067\n",
      "Iteration 11, loss = 0.38485003\n",
      "Iteration 12, loss = 0.37355101\n",
      "Iteration 13, loss = 0.36348181\n",
      "Iteration 14, loss = 0.35484052\n",
      "Iteration 15, loss = 0.34696855\n",
      "Iteration 16, loss = 0.34002996\n",
      "Iteration 17, loss = 0.33351930\n",
      "Iteration 18, loss = 0.32764271\n",
      "Iteration 19, loss = 0.32226667\n",
      "Iteration 20, loss = 0.31711833\n",
      "Iteration 21, loss = 0.31236897\n",
      "Iteration 22, loss = 0.30794332\n",
      "Iteration 23, loss = 0.30371058\n",
      "Iteration 24, loss = 0.29961989\n",
      "Iteration 25, loss = 0.29586187\n",
      "Iteration 26, loss = 0.29215865\n",
      "Iteration 27, loss = 0.28871110\n",
      "Iteration 28, loss = 0.28525210\n",
      "Iteration 29, loss = 0.28218883\n",
      "Iteration 30, loss = 0.27909545\n",
      "Iteration 31, loss = 0.27612407\n",
      "Iteration 32, loss = 0.27317031\n",
      "Iteration 33, loss = 0.27051408\n",
      "Iteration 34, loss = 0.26772988\n",
      "Iteration 35, loss = 0.26502960\n",
      "Iteration 36, loss = 0.26270884\n",
      "Iteration 37, loss = 0.26021021\n",
      "Iteration 38, loss = 0.25780771\n",
      "Iteration 39, loss = 0.25550430\n",
      "Iteration 40, loss = 0.25316484\n",
      "Iteration 41, loss = 0.25114843\n",
      "Iteration 42, loss = 0.24900697\n",
      "Iteration 43, loss = 0.24681524\n",
      "Iteration 44, loss = 0.24471082\n",
      "Iteration 45, loss = 0.24276272\n",
      "Iteration 46, loss = 0.24081084\n",
      "Iteration 47, loss = 0.23896651\n",
      "Iteration 48, loss = 0.23709933\n",
      "Iteration 49, loss = 0.23529519\n",
      "Iteration 50, loss = 0.23344749\n",
      "Iteration 51, loss = 0.23181610\n",
      "Iteration 52, loss = 0.23007178\n",
      "Iteration 53, loss = 0.22831573\n",
      "Iteration 54, loss = 0.22669147\n",
      "Iteration 55, loss = 0.22507102\n",
      "Iteration 56, loss = 0.22353316\n",
      "Iteration 57, loss = 0.22190870\n",
      "Iteration 58, loss = 0.22034111\n",
      "Iteration 59, loss = 0.21897554\n",
      "Iteration 60, loss = 0.21741796\n",
      "Iteration 61, loss = 0.21609194\n",
      "Iteration 62, loss = 0.21452782\n",
      "Iteration 63, loss = 0.21315609\n",
      "Iteration 64, loss = 0.21180754\n",
      "Iteration 65, loss = 0.21045424\n",
      "Iteration 66, loss = 0.20911770\n",
      "Iteration 67, loss = 0.20777897\n",
      "Iteration 68, loss = 0.20645890\n",
      "Iteration 69, loss = 0.20519955\n",
      "Iteration 70, loss = 0.20396409\n",
      "Iteration 71, loss = 0.20274967\n",
      "Iteration 72, loss = 0.20153585\n",
      "Iteration 73, loss = 0.20025143\n",
      "Iteration 74, loss = 0.19916719\n",
      "Iteration 75, loss = 0.19790057\n",
      "Iteration 76, loss = 0.19672982\n",
      "Iteration 77, loss = 0.19560213\n",
      "Iteration 78, loss = 0.19449890\n",
      "Iteration 79, loss = 0.19339134\n",
      "Iteration 80, loss = 0.19231982\n",
      "Iteration 81, loss = 0.19127758\n",
      "Iteration 82, loss = 0.19017421\n",
      "Iteration 83, loss = 0.18916726\n",
      "Iteration 84, loss = 0.18813086\n",
      "Iteration 85, loss = 0.18703902\n",
      "Iteration 86, loss = 0.18611630\n",
      "Iteration 87, loss = 0.18511500\n",
      "Iteration 88, loss = 0.18405930\n",
      "Iteration 89, loss = 0.18314327\n",
      "Iteration 90, loss = 0.18218766\n",
      "Iteration 91, loss = 0.18123663\n",
      "Iteration 92, loss = 0.18030701\n",
      "Iteration 93, loss = 0.17938264\n",
      "Iteration 94, loss = 0.17846676\n",
      "Iteration 95, loss = 0.17758149\n",
      "Iteration 96, loss = 0.17663424\n",
      "Iteration 97, loss = 0.17586204\n",
      "Iteration 98, loss = 0.17490773\n",
      "Iteration 99, loss = 0.17409352\n",
      "Iteration 100, loss = 0.17319795\n",
      "Iteration 101, loss = 0.17232684\n",
      "Iteration 102, loss = 0.17154406\n",
      "Iteration 103, loss = 0.17067124\n",
      "Iteration 104, loss = 0.16986090\n",
      "Iteration 105, loss = 0.16900004\n",
      "Iteration 106, loss = 0.16822698\n",
      "Iteration 107, loss = 0.16745492\n",
      "Iteration 108, loss = 0.16670488\n",
      "Iteration 109, loss = 0.16595177\n",
      "Iteration 110, loss = 0.16511176\n",
      "Iteration 111, loss = 0.16434294\n",
      "Iteration 112, loss = 0.16355630\n",
      "Iteration 113, loss = 0.16285151\n",
      "Iteration 114, loss = 0.16210971\n",
      "Iteration 115, loss = 0.16140378\n",
      "Iteration 116, loss = 0.16066940\n",
      "Iteration 117, loss = 0.15996130\n",
      "Iteration 118, loss = 0.15922058\n",
      "Iteration 119, loss = 0.15845266\n",
      "Iteration 120, loss = 0.15779527\n",
      "Iteration 121, loss = 0.15709445\n",
      "Iteration 122, loss = 0.15641609\n",
      "Iteration 123, loss = 0.15571316\n",
      "Iteration 124, loss = 0.15502998\n",
      "Iteration 125, loss = 0.15436102\n",
      "Iteration 126, loss = 0.15372155\n",
      "Iteration 127, loss = 0.15304493\n",
      "Iteration 128, loss = 0.15239438\n",
      "Iteration 129, loss = 0.15168369\n",
      "Iteration 130, loss = 0.15110125\n",
      "Iteration 131, loss = 0.15040955\n",
      "Iteration 132, loss = 0.14977471\n",
      "Iteration 133, loss = 0.14913678\n",
      "Iteration 134, loss = 0.14854838\n",
      "Iteration 135, loss = 0.14785018\n",
      "Iteration 136, loss = 0.14731613\n",
      "Iteration 137, loss = 0.14667055\n",
      "Iteration 138, loss = 0.14602112\n",
      "Iteration 139, loss = 0.14537650\n",
      "Iteration 140, loss = 0.14485570\n",
      "Iteration 141, loss = 0.14423177\n",
      "Iteration 142, loss = 0.14359435\n",
      "Iteration 143, loss = 0.14303247\n",
      "Iteration 144, loss = 0.14246431\n",
      "Iteration 145, loss = 0.14195296\n",
      "Iteration 146, loss = 0.14130126\n",
      "Iteration 147, loss = 0.14073691\n",
      "Iteration 148, loss = 0.14016412\n",
      "Iteration 149, loss = 0.13965254\n",
      "Iteration 150, loss = 0.13898908\n",
      "Iteration 151, loss = 0.13852948\n",
      "Iteration 152, loss = 0.13802648\n",
      "Iteration 153, loss = 0.13745369\n",
      "Iteration 154, loss = 0.13684614\n",
      "Iteration 155, loss = 0.13627261\n",
      "Iteration 156, loss = 0.13575163\n",
      "Iteration 157, loss = 0.13518181\n",
      "Iteration 158, loss = 0.13467042\n",
      "Iteration 159, loss = 0.13414761\n",
      "Iteration 160, loss = 0.13363622\n",
      "Iteration 161, loss = 0.13315177\n",
      "Iteration 162, loss = 0.13262323\n",
      "Iteration 163, loss = 0.13212175\n",
      "Iteration 164, loss = 0.13159410\n",
      "Iteration 165, loss = 0.13114101\n",
      "Iteration 166, loss = 0.13059201\n",
      "Iteration 167, loss = 0.13002263\n",
      "Iteration 168, loss = 0.12966202\n",
      "Iteration 169, loss = 0.12907822\n",
      "Iteration 170, loss = 0.12858438\n",
      "Iteration 171, loss = 0.12816146\n",
      "Iteration 172, loss = 0.12774081\n",
      "Iteration 173, loss = 0.12717170\n",
      "Iteration 174, loss = 0.12674398\n",
      "Iteration 175, loss = 0.12625510\n",
      "Iteration 176, loss = 0.12571226\n",
      "Iteration 177, loss = 0.12528355\n",
      "Iteration 178, loss = 0.12486807\n",
      "Iteration 179, loss = 0.12436204\n",
      "Iteration 180, loss = 0.12390195\n",
      "Iteration 181, loss = 0.12341244\n",
      "Iteration 182, loss = 0.12302407\n",
      "Iteration 183, loss = 0.12258007\n",
      "Iteration 184, loss = 0.12206360\n",
      "Iteration 185, loss = 0.12161579\n",
      "Iteration 186, loss = 0.12123775\n",
      "Iteration 187, loss = 0.12078629\n",
      "Iteration 188, loss = 0.12037031\n",
      "Iteration 189, loss = 0.11992768\n",
      "Iteration 190, loss = 0.11952282\n",
      "Iteration 191, loss = 0.11908940\n",
      "Iteration 192, loss = 0.11870851\n",
      "Iteration 193, loss = 0.11824442\n",
      "Iteration 194, loss = 0.11789233\n",
      "Iteration 195, loss = 0.11746461\n",
      "Iteration 196, loss = 0.11704102\n",
      "Iteration 197, loss = 0.11661593\n",
      "Iteration 198, loss = 0.11627786\n",
      "Iteration 199, loss = 0.11586682\n",
      "Iteration 200, loss = 0.11547249\n",
      "[CV] END activation=relu, alpha=0.01, batch_size=200, hidden_layer_sizes=(100,), solver=sgd; total time= 1.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.84876040\n",
      "Iteration 2, loss = 1.06129865\n",
      "Iteration 3, loss = 0.75191502\n",
      "Iteration 4, loss = 0.61583522\n",
      "Iteration 5, loss = 0.54049247\n",
      "Iteration 6, loss = 0.49221911\n",
      "Iteration 7, loss = 0.45806677\n",
      "Iteration 8, loss = 0.43272286\n",
      "Iteration 9, loss = 0.41268537\n",
      "Iteration 10, loss = 0.39671925\n",
      "Iteration 11, loss = 0.38318849\n",
      "Iteration 12, loss = 0.37185875\n",
      "Iteration 13, loss = 0.36200058\n",
      "Iteration 14, loss = 0.35322494\n",
      "Iteration 15, loss = 0.34559268\n",
      "Iteration 16, loss = 0.33842467\n",
      "Iteration 17, loss = 0.33208713\n",
      "Iteration 18, loss = 0.32611549\n",
      "Iteration 19, loss = 0.32073501\n",
      "Iteration 20, loss = 0.31571126\n",
      "Iteration 21, loss = 0.31104175\n",
      "Iteration 22, loss = 0.30648778\n",
      "Iteration 23, loss = 0.30228351\n",
      "Iteration 24, loss = 0.29841417\n",
      "Iteration 25, loss = 0.29462354\n",
      "Iteration 26, loss = 0.29092974\n",
      "Iteration 27, loss = 0.28750709\n",
      "Iteration 28, loss = 0.28422194\n",
      "Iteration 29, loss = 0.28086938\n",
      "Iteration 30, loss = 0.27792815\n",
      "Iteration 31, loss = 0.27487321\n",
      "Iteration 32, loss = 0.27201307\n",
      "Iteration 33, loss = 0.26928293\n",
      "Iteration 34, loss = 0.26648221\n",
      "Iteration 35, loss = 0.26384426\n",
      "Iteration 36, loss = 0.26129883\n",
      "Iteration 37, loss = 0.25891161\n",
      "Iteration 38, loss = 0.25653021\n",
      "Iteration 39, loss = 0.25414551\n",
      "Iteration 40, loss = 0.25180609\n",
      "Iteration 41, loss = 0.24946080\n",
      "Iteration 42, loss = 0.24738874\n",
      "Iteration 43, loss = 0.24515504\n",
      "Iteration 44, loss = 0.24317874\n",
      "Iteration 45, loss = 0.24102269\n",
      "Iteration 46, loss = 0.23899963\n",
      "Iteration 47, loss = 0.23705118\n",
      "Iteration 48, loss = 0.23518843\n",
      "Iteration 49, loss = 0.23328311\n",
      "Iteration 50, loss = 0.23144418\n",
      "Iteration 51, loss = 0.22954374\n",
      "Iteration 52, loss = 0.22776774\n",
      "Iteration 53, loss = 0.22608176\n",
      "Iteration 54, loss = 0.22428942\n",
      "Iteration 55, loss = 0.22267138\n",
      "Iteration 56, loss = 0.22087423\n",
      "Iteration 57, loss = 0.21942918\n",
      "Iteration 58, loss = 0.21768266\n",
      "Iteration 59, loss = 0.21627495\n",
      "Iteration 60, loss = 0.21464535\n",
      "Iteration 61, loss = 0.21308041\n",
      "Iteration 62, loss = 0.21158503\n",
      "Iteration 63, loss = 0.21012574\n",
      "Iteration 64, loss = 0.20865104\n",
      "Iteration 65, loss = 0.20723526\n",
      "Iteration 66, loss = 0.20581479\n",
      "Iteration 67, loss = 0.20439519\n",
      "Iteration 68, loss = 0.20301934\n",
      "Iteration 69, loss = 0.20171584\n",
      "Iteration 70, loss = 0.20025441\n",
      "Iteration 71, loss = 0.19895176\n",
      "Iteration 72, loss = 0.19774698\n",
      "Iteration 73, loss = 0.19640239\n",
      "Iteration 74, loss = 0.19504178\n",
      "Iteration 75, loss = 0.19369127\n",
      "Iteration 76, loss = 0.19262690\n",
      "Iteration 77, loss = 0.19138649\n",
      "Iteration 78, loss = 0.19010653\n",
      "Iteration 79, loss = 0.18892663\n",
      "Iteration 80, loss = 0.18777419\n",
      "Iteration 81, loss = 0.18656173\n",
      "Iteration 82, loss = 0.18535058\n",
      "Iteration 83, loss = 0.18424020\n",
      "Iteration 84, loss = 0.18317707\n",
      "Iteration 85, loss = 0.18202607\n",
      "Iteration 86, loss = 0.18087639\n",
      "Iteration 87, loss = 0.17983010\n",
      "Iteration 88, loss = 0.17874514\n",
      "Iteration 89, loss = 0.17773047\n",
      "Iteration 90, loss = 0.17670554\n",
      "Iteration 91, loss = 0.17558632\n",
      "Iteration 92, loss = 0.17460324\n",
      "Iteration 93, loss = 0.17361961\n",
      "Iteration 94, loss = 0.17262324\n",
      "Iteration 95, loss = 0.17166416\n",
      "Iteration 96, loss = 0.17060132\n",
      "Iteration 97, loss = 0.16973547\n",
      "Iteration 98, loss = 0.16883648\n",
      "Iteration 99, loss = 0.16789287\n",
      "Iteration 100, loss = 0.16691669\n",
      "Iteration 101, loss = 0.16596602\n",
      "Iteration 102, loss = 0.16507393\n",
      "Iteration 103, loss = 0.16418022\n",
      "Iteration 104, loss = 0.16330780\n",
      "Iteration 105, loss = 0.16243732\n",
      "Iteration 106, loss = 0.16158108\n",
      "Iteration 107, loss = 0.16072814\n",
      "Iteration 108, loss = 0.15979243\n",
      "Iteration 109, loss = 0.15902603\n",
      "Iteration 110, loss = 0.15816676\n",
      "Iteration 111, loss = 0.15738578\n",
      "Iteration 112, loss = 0.15659298\n",
      "Iteration 113, loss = 0.15572188\n",
      "Iteration 114, loss = 0.15490845\n",
      "Iteration 115, loss = 0.15412515\n",
      "Iteration 116, loss = 0.15347291\n",
      "Iteration 117, loss = 0.15259589\n",
      "Iteration 118, loss = 0.15184626\n",
      "Iteration 119, loss = 0.15115359\n",
      "Iteration 120, loss = 0.15030316\n",
      "Iteration 121, loss = 0.14963372\n",
      "Iteration 122, loss = 0.14888953\n",
      "Iteration 123, loss = 0.14817862\n",
      "Iteration 124, loss = 0.14741704\n",
      "Iteration 125, loss = 0.14670586\n",
      "Iteration 126, loss = 0.14608643\n",
      "Iteration 127, loss = 0.14534696\n",
      "Iteration 128, loss = 0.14463772\n",
      "Iteration 129, loss = 0.14396282\n",
      "Iteration 130, loss = 0.14332566\n",
      "Iteration 131, loss = 0.14260832\n",
      "Iteration 132, loss = 0.14191025\n",
      "Iteration 133, loss = 0.14129750\n",
      "Iteration 134, loss = 0.14068057\n",
      "Iteration 135, loss = 0.14000584\n",
      "Iteration 136, loss = 0.13941017\n",
      "Iteration 137, loss = 0.13874159\n",
      "Iteration 138, loss = 0.13809653\n",
      "Iteration 139, loss = 0.13748093\n",
      "Iteration 140, loss = 0.13689578\n",
      "Iteration 141, loss = 0.13629049\n",
      "Iteration 142, loss = 0.13563395\n",
      "Iteration 143, loss = 0.13514239\n",
      "Iteration 144, loss = 0.13448439\n",
      "Iteration 145, loss = 0.13390643\n",
      "Iteration 146, loss = 0.13335426\n",
      "Iteration 147, loss = 0.13274122\n",
      "Iteration 148, loss = 0.13214222\n",
      "Iteration 149, loss = 0.13162607\n",
      "Iteration 150, loss = 0.13103763\n",
      "Iteration 151, loss = 0.13048539\n",
      "Iteration 152, loss = 0.12993901\n",
      "Iteration 153, loss = 0.12938978\n",
      "Iteration 154, loss = 0.12885441\n",
      "Iteration 155, loss = 0.12833036\n",
      "Iteration 156, loss = 0.12778453\n",
      "Iteration 157, loss = 0.12725097\n",
      "Iteration 158, loss = 0.12673740\n",
      "Iteration 159, loss = 0.12623098\n",
      "Iteration 160, loss = 0.12572533\n",
      "Iteration 161, loss = 0.12519045\n",
      "Iteration 162, loss = 0.12468547\n",
      "Iteration 163, loss = 0.12414131\n",
      "Iteration 164, loss = 0.12366287\n",
      "Iteration 165, loss = 0.12317798\n",
      "Iteration 166, loss = 0.12268106\n",
      "Iteration 167, loss = 0.12218866\n",
      "Iteration 168, loss = 0.12171022\n",
      "Iteration 169, loss = 0.12127994\n",
      "Iteration 170, loss = 0.12075721\n",
      "Iteration 171, loss = 0.12029651\n",
      "Iteration 172, loss = 0.11979204\n",
      "Iteration 173, loss = 0.11934254\n",
      "Iteration 174, loss = 0.11888494\n",
      "Iteration 175, loss = 0.11843763\n",
      "Iteration 176, loss = 0.11799761\n",
      "Iteration 177, loss = 0.11746492\n",
      "Iteration 178, loss = 0.11704149\n",
      "Iteration 179, loss = 0.11662935\n",
      "Iteration 180, loss = 0.11616285\n",
      "Iteration 181, loss = 0.11572416\n",
      "Iteration 182, loss = 0.11526286\n",
      "Iteration 183, loss = 0.11482997\n",
      "Iteration 184, loss = 0.11442235\n",
      "Iteration 185, loss = 0.11399321\n",
      "Iteration 186, loss = 0.11358382\n",
      "Iteration 187, loss = 0.11313762\n",
      "Iteration 188, loss = 0.11275646\n",
      "Iteration 189, loss = 0.11229603\n",
      "Iteration 190, loss = 0.11191389\n",
      "Iteration 191, loss = 0.11151895\n",
      "Iteration 192, loss = 0.11108143\n",
      "Iteration 193, loss = 0.11060608\n",
      "Iteration 194, loss = 0.11027164\n",
      "Iteration 195, loss = 0.10983098\n",
      "Iteration 196, loss = 0.10950152\n",
      "Iteration 197, loss = 0.10907451\n",
      "Iteration 198, loss = 0.10869385\n",
      "Iteration 199, loss = 0.10831028\n",
      "Iteration 200, loss = 0.10790325\n",
      "[CV] END activation=relu, alpha=0.01, batch_size=200, hidden_layer_sizes=(100,), solver=sgd; total time= 1.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.88052272\n",
      "Iteration 2, loss = 1.09508573\n",
      "Iteration 3, loss = 0.75069755\n",
      "Iteration 4, loss = 0.60946393\n",
      "Iteration 5, loss = 0.53452379\n",
      "Iteration 6, loss = 0.48739530\n",
      "Iteration 7, loss = 0.45455537\n",
      "Iteration 8, loss = 0.42989384\n",
      "Iteration 9, loss = 0.41082835\n",
      "Iteration 10, loss = 0.39521720\n",
      "Iteration 11, loss = 0.38214514\n",
      "Iteration 12, loss = 0.37083561\n",
      "Iteration 13, loss = 0.36118075\n",
      "Iteration 14, loss = 0.35257934\n",
      "Iteration 15, loss = 0.34483975\n",
      "Iteration 16, loss = 0.33776510\n",
      "Iteration 17, loss = 0.33130968\n",
      "Iteration 18, loss = 0.32555274\n",
      "Iteration 19, loss = 0.31995222\n",
      "Iteration 20, loss = 0.31503695\n",
      "Iteration 21, loss = 0.31019772\n",
      "Iteration 22, loss = 0.30589762\n",
      "Iteration 23, loss = 0.30159093\n",
      "Iteration 24, loss = 0.29763664\n",
      "Iteration 25, loss = 0.29384478\n",
      "Iteration 26, loss = 0.29018350\n",
      "Iteration 27, loss = 0.28675137\n",
      "Iteration 28, loss = 0.28347640\n",
      "Iteration 29, loss = 0.28021425\n",
      "Iteration 30, loss = 0.27713471\n",
      "Iteration 31, loss = 0.27420409\n",
      "Iteration 32, loss = 0.27141523\n",
      "Iteration 33, loss = 0.26864177\n",
      "Iteration 34, loss = 0.26591120\n",
      "Iteration 35, loss = 0.26342253\n",
      "Iteration 36, loss = 0.26094398\n",
      "Iteration 37, loss = 0.25842232\n",
      "Iteration 38, loss = 0.25604441\n",
      "Iteration 39, loss = 0.25373908\n",
      "Iteration 40, loss = 0.25157764\n",
      "Iteration 41, loss = 0.24930730\n",
      "Iteration 42, loss = 0.24720104\n",
      "Iteration 43, loss = 0.24513664\n",
      "Iteration 44, loss = 0.24306048\n",
      "Iteration 45, loss = 0.24089225\n",
      "Iteration 46, loss = 0.23896716\n",
      "Iteration 47, loss = 0.23717017\n",
      "Iteration 48, loss = 0.23525594\n",
      "Iteration 49, loss = 0.23328473\n",
      "Iteration 50, loss = 0.23151491\n",
      "Iteration 51, loss = 0.22974239\n",
      "Iteration 52, loss = 0.22804519\n",
      "Iteration 53, loss = 0.22625917\n",
      "Iteration 54, loss = 0.22465825\n",
      "Iteration 55, loss = 0.22299430\n",
      "Iteration 56, loss = 0.22131586\n",
      "Iteration 57, loss = 0.21973531\n",
      "Iteration 58, loss = 0.21820872\n",
      "Iteration 59, loss = 0.21658186\n",
      "Iteration 60, loss = 0.21509211\n",
      "Iteration 61, loss = 0.21358642\n",
      "Iteration 62, loss = 0.21219008\n",
      "Iteration 63, loss = 0.21072205\n",
      "Iteration 64, loss = 0.20932776\n",
      "Iteration 65, loss = 0.20785398\n",
      "Iteration 66, loss = 0.20648823\n",
      "Iteration 67, loss = 0.20502193\n",
      "Iteration 68, loss = 0.20376940\n",
      "Iteration 69, loss = 0.20237731\n",
      "Iteration 70, loss = 0.20108292\n",
      "Iteration 71, loss = 0.19983665\n",
      "Iteration 72, loss = 0.19846945\n",
      "Iteration 73, loss = 0.19726114\n",
      "Iteration 74, loss = 0.19606542\n",
      "Iteration 75, loss = 0.19485616\n",
      "Iteration 76, loss = 0.19360686\n",
      "Iteration 77, loss = 0.19251246\n",
      "Iteration 78, loss = 0.19125075\n",
      "Iteration 79, loss = 0.19017210\n",
      "Iteration 80, loss = 0.18889675\n",
      "Iteration 81, loss = 0.18781010\n",
      "Iteration 82, loss = 0.18663606\n",
      "Iteration 83, loss = 0.18556478\n",
      "Iteration 84, loss = 0.18443792\n",
      "Iteration 85, loss = 0.18343718\n",
      "Iteration 86, loss = 0.18233218\n",
      "Iteration 87, loss = 0.18128679\n",
      "Iteration 88, loss = 0.18030729\n",
      "Iteration 89, loss = 0.17928815\n",
      "Iteration 90, loss = 0.17819648\n",
      "Iteration 91, loss = 0.17721410\n",
      "Iteration 92, loss = 0.17630170\n",
      "Iteration 93, loss = 0.17528734\n",
      "Iteration 94, loss = 0.17432906\n",
      "Iteration 95, loss = 0.17339151\n",
      "Iteration 96, loss = 0.17247654\n",
      "Iteration 97, loss = 0.17157886\n",
      "Iteration 98, loss = 0.17060584\n",
      "Iteration 99, loss = 0.16982479\n",
      "Iteration 100, loss = 0.16888918\n",
      "Iteration 101, loss = 0.16795791\n",
      "Iteration 102, loss = 0.16706210\n",
      "Iteration 103, loss = 0.16625705\n",
      "Iteration 104, loss = 0.16534236\n",
      "Iteration 105, loss = 0.16444408\n",
      "Iteration 106, loss = 0.16365989\n",
      "Iteration 107, loss = 0.16280701\n",
      "Iteration 108, loss = 0.16201964\n",
      "Iteration 109, loss = 0.16124073\n",
      "Iteration 110, loss = 0.16042553\n",
      "Iteration 111, loss = 0.15963419\n",
      "Iteration 112, loss = 0.15885570\n",
      "Iteration 113, loss = 0.15808064\n",
      "Iteration 114, loss = 0.15735704\n",
      "Iteration 115, loss = 0.15650769\n",
      "Iteration 116, loss = 0.15579560\n",
      "Iteration 117, loss = 0.15508438\n",
      "Iteration 118, loss = 0.15437841\n",
      "Iteration 119, loss = 0.15354268\n",
      "Iteration 120, loss = 0.15286803\n",
      "Iteration 121, loss = 0.15220138\n",
      "Iteration 122, loss = 0.15143055\n",
      "Iteration 123, loss = 0.15066629\n",
      "Iteration 124, loss = 0.14998827\n",
      "Iteration 125, loss = 0.14932306\n",
      "Iteration 126, loss = 0.14866284\n",
      "Iteration 127, loss = 0.14798395\n",
      "Iteration 128, loss = 0.14728218\n",
      "Iteration 129, loss = 0.14661406\n",
      "Iteration 130, loss = 0.14596712\n",
      "Iteration 131, loss = 0.14535025\n",
      "Iteration 132, loss = 0.14463520\n",
      "Iteration 133, loss = 0.14400719\n",
      "Iteration 134, loss = 0.14337966\n",
      "Iteration 135, loss = 0.14276368\n",
      "Iteration 136, loss = 0.14210729\n",
      "Iteration 137, loss = 0.14150952\n",
      "Iteration 138, loss = 0.14094182\n",
      "Iteration 139, loss = 0.14015407\n",
      "Iteration 140, loss = 0.13968412\n",
      "Iteration 141, loss = 0.13907819\n",
      "Iteration 142, loss = 0.13846443\n",
      "Iteration 143, loss = 0.13790666\n",
      "Iteration 144, loss = 0.13731932\n",
      "Iteration 145, loss = 0.13674707\n",
      "Iteration 146, loss = 0.13615035\n",
      "Iteration 147, loss = 0.13564622\n",
      "Iteration 148, loss = 0.13505002\n",
      "Iteration 149, loss = 0.13446788\n",
      "Iteration 150, loss = 0.13394237\n",
      "Iteration 151, loss = 0.13334735\n",
      "Iteration 152, loss = 0.13283957\n",
      "Iteration 153, loss = 0.13222242\n",
      "Iteration 154, loss = 0.13184750\n",
      "Iteration 155, loss = 0.13123529\n",
      "Iteration 156, loss = 0.13067486\n",
      "Iteration 157, loss = 0.13017202\n",
      "Iteration 158, loss = 0.12962395\n",
      "Iteration 159, loss = 0.12910898\n",
      "Iteration 160, loss = 0.12859477\n",
      "Iteration 161, loss = 0.12810933\n",
      "Iteration 162, loss = 0.12761693\n",
      "Iteration 163, loss = 0.12707140\n",
      "Iteration 164, loss = 0.12657736\n",
      "Iteration 165, loss = 0.12611741\n",
      "Iteration 166, loss = 0.12562348\n",
      "Iteration 167, loss = 0.12518393\n",
      "Iteration 168, loss = 0.12468108\n",
      "Iteration 169, loss = 0.12422721\n",
      "Iteration 170, loss = 0.12369017\n",
      "Iteration 171, loss = 0.12322932\n",
      "Iteration 172, loss = 0.12276067\n",
      "Iteration 173, loss = 0.12234814\n",
      "Iteration 174, loss = 0.12184482\n",
      "Iteration 175, loss = 0.12142107\n",
      "Iteration 176, loss = 0.12094656\n",
      "Iteration 177, loss = 0.12049911\n",
      "Iteration 178, loss = 0.12007076\n",
      "Iteration 179, loss = 0.11959012\n",
      "Iteration 180, loss = 0.11920088\n",
      "Iteration 181, loss = 0.11880833\n",
      "Iteration 182, loss = 0.11831631\n",
      "Iteration 183, loss = 0.11785147\n",
      "Iteration 184, loss = 0.11744310\n",
      "Iteration 185, loss = 0.11699430\n",
      "Iteration 186, loss = 0.11663841\n",
      "Iteration 187, loss = 0.11623785\n",
      "Iteration 188, loss = 0.11579280\n",
      "Iteration 189, loss = 0.11541892\n",
      "Iteration 190, loss = 0.11493954\n",
      "Iteration 191, loss = 0.11456714\n",
      "Iteration 192, loss = 0.11418242\n",
      "Iteration 193, loss = 0.11374572\n",
      "Iteration 194, loss = 0.11337687\n",
      "Iteration 195, loss = 0.11297477\n",
      "Iteration 196, loss = 0.11256029\n",
      "Iteration 197, loss = 0.11218479\n",
      "Iteration 198, loss = 0.11183122\n",
      "Iteration 199, loss = 0.11143707\n",
      "Iteration 200, loss = 0.11102148\n",
      "[CV] END activation=relu, alpha=0.01, batch_size=200, hidden_layer_sizes=(100,), solver=sgd; total time= 1.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.80900657\n",
      "Iteration 2, loss = 1.02677576\n",
      "Iteration 3, loss = 0.72687748\n",
      "Iteration 4, loss = 0.59942436\n",
      "Iteration 5, loss = 0.52951647\n",
      "Iteration 6, loss = 0.48470977\n",
      "Iteration 7, loss = 0.45316210\n",
      "Iteration 8, loss = 0.42961154\n",
      "Iteration 9, loss = 0.41103965\n",
      "Iteration 10, loss = 0.39589204\n",
      "Iteration 11, loss = 0.38322570\n",
      "Iteration 12, loss = 0.37242604\n",
      "Iteration 13, loss = 0.36302896\n",
      "Iteration 14, loss = 0.35473127\n",
      "Iteration 15, loss = 0.34729298\n",
      "Iteration 16, loss = 0.34082848\n",
      "Iteration 17, loss = 0.33456729\n",
      "Iteration 18, loss = 0.32890725\n",
      "Iteration 19, loss = 0.32376489\n",
      "Iteration 20, loss = 0.31889174\n",
      "Iteration 21, loss = 0.31448042\n",
      "Iteration 22, loss = 0.31003070\n",
      "Iteration 23, loss = 0.30593328\n",
      "Iteration 24, loss = 0.30198862\n",
      "Iteration 25, loss = 0.29843450\n",
      "Iteration 26, loss = 0.29469171\n",
      "Iteration 27, loss = 0.29132505\n",
      "Iteration 28, loss = 0.28816361\n",
      "Iteration 29, loss = 0.28488489\n",
      "Iteration 30, loss = 0.28171422\n",
      "Iteration 31, loss = 0.27884987\n",
      "Iteration 32, loss = 0.27593978\n",
      "Iteration 33, loss = 0.27325685\n",
      "Iteration 34, loss = 0.27059494\n",
      "Iteration 35, loss = 0.26785451\n",
      "Iteration 36, loss = 0.26527766\n",
      "Iteration 37, loss = 0.26282947\n",
      "Iteration 38, loss = 0.26047891\n",
      "Iteration 39, loss = 0.25797369\n",
      "Iteration 40, loss = 0.25575005\n",
      "Iteration 41, loss = 0.25352358\n",
      "Iteration 42, loss = 0.25122329\n",
      "Iteration 43, loss = 0.24903793\n",
      "Iteration 44, loss = 0.24707496\n",
      "Iteration 45, loss = 0.24494841\n",
      "Iteration 46, loss = 0.24292110\n",
      "Iteration 47, loss = 0.24088015\n",
      "Iteration 48, loss = 0.23900743\n",
      "Iteration 49, loss = 0.23700200\n",
      "Iteration 50, loss = 0.23523269\n",
      "Iteration 51, loss = 0.23332370\n",
      "Iteration 52, loss = 0.23161716\n",
      "Iteration 53, loss = 0.22977569\n",
      "Iteration 54, loss = 0.22793744\n",
      "Iteration 55, loss = 0.22628654\n",
      "Iteration 56, loss = 0.22464523\n",
      "Iteration 57, loss = 0.22302035\n",
      "Iteration 58, loss = 0.22125895\n",
      "Iteration 59, loss = 0.21973865\n",
      "Iteration 60, loss = 0.21814628\n",
      "Iteration 61, loss = 0.21660047\n",
      "Iteration 62, loss = 0.21507968\n",
      "Iteration 63, loss = 0.21359856\n",
      "Iteration 64, loss = 0.21207766\n",
      "Iteration 65, loss = 0.21062707\n",
      "Iteration 66, loss = 0.20910339\n",
      "Iteration 67, loss = 0.20781837\n",
      "Iteration 68, loss = 0.20631246\n",
      "Iteration 69, loss = 0.20498474\n",
      "Iteration 70, loss = 0.20354827\n",
      "Iteration 71, loss = 0.20222469\n",
      "Iteration 72, loss = 0.20083448\n",
      "Iteration 73, loss = 0.19969783\n",
      "Iteration 74, loss = 0.19830302\n",
      "Iteration 75, loss = 0.19705419\n",
      "Iteration 76, loss = 0.19586634\n",
      "Iteration 77, loss = 0.19457374\n",
      "Iteration 78, loss = 0.19343678\n",
      "Iteration 79, loss = 0.19214306\n",
      "Iteration 80, loss = 0.19099442\n",
      "Iteration 81, loss = 0.18975304\n",
      "Iteration 82, loss = 0.18858647\n",
      "Iteration 83, loss = 0.18745484\n",
      "Iteration 84, loss = 0.18631290\n",
      "Iteration 85, loss = 0.18532528\n",
      "Iteration 86, loss = 0.18418409\n",
      "Iteration 87, loss = 0.18303086\n",
      "Iteration 88, loss = 0.18195886\n",
      "Iteration 89, loss = 0.18091458\n",
      "Iteration 90, loss = 0.17981769\n",
      "Iteration 91, loss = 0.17877157\n",
      "Iteration 92, loss = 0.17776418\n",
      "Iteration 93, loss = 0.17669106\n",
      "Iteration 94, loss = 0.17568374\n",
      "Iteration 95, loss = 0.17471237\n",
      "Iteration 96, loss = 0.17367675\n",
      "Iteration 97, loss = 0.17264557\n",
      "Iteration 98, loss = 0.17175979\n",
      "Iteration 99, loss = 0.17075022\n",
      "Iteration 100, loss = 0.16982554\n",
      "Iteration 101, loss = 0.16889928\n",
      "Iteration 102, loss = 0.16787548\n",
      "Iteration 103, loss = 0.16702160\n",
      "Iteration 104, loss = 0.16609834\n",
      "Iteration 105, loss = 0.16525215\n",
      "Iteration 106, loss = 0.16438983\n",
      "Iteration 107, loss = 0.16350195\n",
      "Iteration 108, loss = 0.16264879\n",
      "Iteration 109, loss = 0.16177808\n",
      "Iteration 110, loss = 0.16091758\n",
      "Iteration 111, loss = 0.16006927\n",
      "Iteration 112, loss = 0.15921835\n",
      "Iteration 113, loss = 0.15837102\n",
      "Iteration 114, loss = 0.15762492\n",
      "Iteration 115, loss = 0.15676038\n",
      "Iteration 116, loss = 0.15595470\n",
      "Iteration 117, loss = 0.15519099\n",
      "Iteration 118, loss = 0.15440297\n",
      "Iteration 119, loss = 0.15360751\n",
      "Iteration 120, loss = 0.15284179\n",
      "Iteration 121, loss = 0.15206219\n",
      "Iteration 122, loss = 0.15128093\n",
      "Iteration 123, loss = 0.15059948\n",
      "Iteration 124, loss = 0.14982526\n",
      "Iteration 125, loss = 0.14912791\n",
      "Iteration 126, loss = 0.14837249\n",
      "Iteration 127, loss = 0.14764310\n",
      "Iteration 128, loss = 0.14691482\n",
      "Iteration 129, loss = 0.14627774\n",
      "Iteration 130, loss = 0.14556180\n",
      "Iteration 131, loss = 0.14483262\n",
      "Iteration 132, loss = 0.14413175\n",
      "Iteration 133, loss = 0.14343676\n",
      "Iteration 134, loss = 0.14277870\n",
      "Iteration 135, loss = 0.14212019\n",
      "Iteration 136, loss = 0.14144738\n",
      "Iteration 137, loss = 0.14073459\n",
      "Iteration 138, loss = 0.14007637\n",
      "Iteration 139, loss = 0.13951774\n",
      "Iteration 140, loss = 0.13885616\n",
      "Iteration 141, loss = 0.13816971\n",
      "Iteration 142, loss = 0.13760779\n",
      "Iteration 143, loss = 0.13691929\n",
      "Iteration 144, loss = 0.13635637\n",
      "Iteration 145, loss = 0.13577781\n",
      "Iteration 146, loss = 0.13510130\n",
      "Iteration 147, loss = 0.13452733\n",
      "Iteration 148, loss = 0.13393195\n",
      "Iteration 149, loss = 0.13333964\n",
      "Iteration 150, loss = 0.13273188\n",
      "Iteration 151, loss = 0.13224125\n",
      "Iteration 152, loss = 0.13153619\n",
      "Iteration 153, loss = 0.13100008\n",
      "Iteration 154, loss = 0.13044386\n",
      "Iteration 155, loss = 0.12996154\n",
      "Iteration 156, loss = 0.12937088\n",
      "Iteration 157, loss = 0.12876798\n",
      "Iteration 158, loss = 0.12828017\n",
      "Iteration 159, loss = 0.12778657\n",
      "Iteration 160, loss = 0.12720614\n",
      "Iteration 161, loss = 0.12663761\n",
      "Iteration 162, loss = 0.12612645\n",
      "Iteration 163, loss = 0.12565253\n",
      "Iteration 164, loss = 0.12512680\n",
      "Iteration 165, loss = 0.12458511\n",
      "Iteration 166, loss = 0.12408228\n",
      "Iteration 167, loss = 0.12355746\n",
      "Iteration 168, loss = 0.12303484\n",
      "Iteration 169, loss = 0.12254049\n",
      "Iteration 170, loss = 0.12203525\n",
      "Iteration 171, loss = 0.12154708\n",
      "Iteration 172, loss = 0.12108872\n",
      "Iteration 173, loss = 0.12054332\n",
      "Iteration 174, loss = 0.12007518\n",
      "Iteration 175, loss = 0.11961641\n",
      "Iteration 176, loss = 0.11913465\n",
      "Iteration 177, loss = 0.11870842\n",
      "Iteration 178, loss = 0.11819568\n",
      "Iteration 179, loss = 0.11773844\n",
      "Iteration 180, loss = 0.11728638\n",
      "Iteration 181, loss = 0.11683093\n",
      "Iteration 182, loss = 0.11638219\n",
      "Iteration 183, loss = 0.11596050\n",
      "Iteration 184, loss = 0.11549638\n",
      "Iteration 185, loss = 0.11506841\n",
      "Iteration 186, loss = 0.11462586\n",
      "Iteration 187, loss = 0.11421749\n",
      "Iteration 188, loss = 0.11378039\n",
      "Iteration 189, loss = 0.11333727\n",
      "Iteration 190, loss = 0.11283648\n",
      "Iteration 191, loss = 0.11253378\n",
      "Iteration 192, loss = 0.11203691\n",
      "Iteration 193, loss = 0.11166794\n",
      "Iteration 194, loss = 0.11119777\n",
      "Iteration 195, loss = 0.11085463\n",
      "Iteration 196, loss = 0.11038593\n",
      "Iteration 197, loss = 0.11003437\n",
      "Iteration 198, loss = 0.10956001\n",
      "Iteration 199, loss = 0.10920859\n",
      "Iteration 200, loss = 0.10880566\n",
      "[CV] END activation=relu, alpha=0.01, batch_size=200, hidden_layer_sizes=(100,), solver=sgd; total time= 1.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.38460050\n",
      "Iteration 2, loss = 0.15363815\n",
      "Iteration 3, loss = 0.10573297\n",
      "Iteration 4, loss = 0.07862543\n",
      "Iteration 5, loss = 0.06226790\n",
      "Iteration 6, loss = 0.05404123\n",
      "Iteration 7, loss = 0.04168097\n",
      "Iteration 8, loss = 0.03639646\n",
      "Iteration 9, loss = 0.03255397\n",
      "Iteration 10, loss = 0.02877353\n",
      "Iteration 11, loss = 0.02772438\n",
      "Iteration 12, loss = 0.02781404\n",
      "Iteration 13, loss = 0.02435997\n",
      "Iteration 14, loss = 0.02795623\n",
      "Iteration 15, loss = 0.02850863\n",
      "Iteration 16, loss = 0.03569598\n",
      "Iteration 17, loss = 0.02986676\n",
      "Iteration 18, loss = 0.02738733\n",
      "Iteration 19, loss = 0.03281830\n",
      "Iteration 20, loss = 0.02779451\n",
      "Iteration 21, loss = 0.02412009\n",
      "Iteration 22, loss = 0.02150917\n",
      "Iteration 23, loss = 0.01962406\n",
      "Iteration 24, loss = 0.01865907\n",
      "Iteration 25, loss = 0.01786294\n",
      "Iteration 26, loss = 0.01712125\n",
      "Iteration 27, loss = 0.01650216\n",
      "Iteration 28, loss = 0.03892225\n",
      "Iteration 29, loss = 0.04220840\n",
      "Iteration 30, loss = 0.03392207\n",
      "Iteration 31, loss = 0.02567832\n",
      "Iteration 32, loss = 0.02243895\n",
      "Iteration 33, loss = 0.02096167\n",
      "Iteration 34, loss = 0.01944285\n",
      "Iteration 35, loss = 0.01812447\n",
      "Iteration 36, loss = 0.01744639\n",
      "Iteration 37, loss = 0.01637224\n",
      "Iteration 38, loss = 0.01562360\n",
      "Iteration 39, loss = 0.01496994\n",
      "Iteration 40, loss = 0.01436334\n",
      "Iteration 41, loss = 0.01882388\n",
      "Iteration 42, loss = 0.05566076\n",
      "Iteration 43, loss = 0.02710169\n",
      "Iteration 44, loss = 0.02216060\n",
      "Iteration 45, loss = 0.01852312\n",
      "Iteration 46, loss = 0.01711833\n",
      "Iteration 47, loss = 0.01627504\n",
      "Iteration 48, loss = 0.01528221\n",
      "Iteration 49, loss = 0.01457968\n",
      "Iteration 50, loss = 0.01398989\n",
      "Iteration 51, loss = 0.01345551\n",
      "Iteration 52, loss = 0.01296254\n",
      "Iteration 53, loss = 0.01264697\n",
      "Iteration 54, loss = 0.01472257\n",
      "Iteration 55, loss = 0.05236989\n",
      "Iteration 56, loss = 0.02603564\n",
      "Iteration 57, loss = 0.01913201\n",
      "Iteration 58, loss = 0.01639945\n",
      "Iteration 59, loss = 0.01519002\n",
      "Iteration 60, loss = 0.01442667\n",
      "Iteration 61, loss = 0.01380337\n",
      "Iteration 62, loss = 0.01325634\n",
      "Iteration 63, loss = 0.01279853\n",
      "Iteration 64, loss = 0.01238158\n",
      "Iteration 65, loss = 0.01202020\n",
      "Iteration 66, loss = 0.01197535\n",
      "Iteration 67, loss = 0.04140451\n",
      "Iteration 68, loss = 0.03366902\n",
      "Iteration 69, loss = 0.02118149\n",
      "Iteration 70, loss = 0.01737990\n",
      "Iteration 71, loss = 0.01535813\n",
      "Iteration 72, loss = 0.01431710\n",
      "Iteration 73, loss = 0.01362697\n",
      "Iteration 74, loss = 0.01311281\n",
      "Iteration 75, loss = 0.01266498\n",
      "Iteration 76, loss = 0.01222715\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.01, batch_size=200, hidden_layer_sizes=(400, 100), solver=adam; total time= 4.2min\n",
      "Iteration 1, loss = 0.38837938\n",
      "Iteration 2, loss = 0.15999283\n",
      "Iteration 3, loss = 0.11396712\n",
      "Iteration 4, loss = 0.08534395\n",
      "Iteration 5, loss = 0.06842328\n",
      "Iteration 6, loss = 0.05736890\n",
      "Iteration 7, loss = 0.04552512\n",
      "Iteration 8, loss = 0.04043971\n",
      "Iteration 9, loss = 0.03447634\n",
      "Iteration 10, loss = 0.03173842\n",
      "Iteration 11, loss = 0.02838967\n",
      "Iteration 12, loss = 0.02747687\n",
      "Iteration 13, loss = 0.02872673\n",
      "Iteration 14, loss = 0.03175136\n",
      "Iteration 15, loss = 0.03012861\n",
      "Iteration 16, loss = 0.03111877\n",
      "Iteration 17, loss = 0.02814600\n",
      "Iteration 18, loss = 0.03119417\n",
      "Iteration 19, loss = 0.02761396\n",
      "Iteration 20, loss = 0.02279186\n",
      "Iteration 21, loss = 0.02276145\n",
      "Iteration 22, loss = 0.02143381\n",
      "Iteration 23, loss = 0.02141666\n",
      "Iteration 24, loss = 0.02576881\n",
      "Iteration 25, loss = 0.04449957\n",
      "Iteration 26, loss = 0.03448628\n",
      "Iteration 27, loss = 0.02568509\n",
      "Iteration 28, loss = 0.02263266\n",
      "Iteration 29, loss = 0.02113028\n",
      "Iteration 30, loss = 0.01925148\n",
      "Iteration 31, loss = 0.01826873\n",
      "Iteration 32, loss = 0.01743306\n",
      "Iteration 33, loss = 0.01669811\n",
      "Iteration 34, loss = 0.01602512\n",
      "Iteration 35, loss = 0.01544159\n",
      "Iteration 36, loss = 0.01490361\n",
      "Iteration 37, loss = 0.04529496\n",
      "Iteration 38, loss = 0.04591502\n",
      "Iteration 39, loss = 0.02618255\n",
      "Iteration 40, loss = 0.02149321\n",
      "Iteration 41, loss = 0.01847564\n",
      "Iteration 42, loss = 0.01732660\n",
      "Iteration 43, loss = 0.01650680\n",
      "Iteration 44, loss = 0.01578954\n",
      "Iteration 45, loss = 0.01515490\n",
      "Iteration 46, loss = 0.01462841\n",
      "Iteration 47, loss = 0.01406178\n",
      "Iteration 48, loss = 0.01356244\n",
      "Iteration 49, loss = 0.01350927\n",
      "Iteration 50, loss = 0.04952892\n",
      "Iteration 51, loss = 0.03494916\n",
      "Iteration 52, loss = 0.02295359\n",
      "Iteration 53, loss = 0.01981551\n",
      "Iteration 54, loss = 0.01718546\n",
      "Iteration 55, loss = 0.01583067\n",
      "Iteration 56, loss = 0.01504368\n",
      "Iteration 57, loss = 0.01440623\n",
      "Iteration 58, loss = 0.01382013\n",
      "Iteration 59, loss = 0.01333833\n",
      "Iteration 60, loss = 0.01289052\n",
      "Iteration 61, loss = 0.01250264\n",
      "Iteration 62, loss = 0.01225389\n",
      "Iteration 63, loss = 0.01194921\n",
      "Iteration 64, loss = 0.02735498\n",
      "Iteration 65, loss = 0.04574099\n",
      "Iteration 66, loss = 0.02356732\n",
      "Iteration 67, loss = 0.01759867\n",
      "Iteration 68, loss = 0.01544114\n",
      "Iteration 69, loss = 0.01446790\n",
      "Iteration 70, loss = 0.01382211\n",
      "Iteration 71, loss = 0.01329640\n",
      "Iteration 72, loss = 0.01282155\n",
      "Iteration 73, loss = 0.01244050\n",
      "Iteration 74, loss = 0.01209785\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.01, batch_size=200, hidden_layer_sizes=(400, 100), solver=adam; total time= 4.1min\n",
      "Iteration 1, loss = 0.40060603\n",
      "Iteration 2, loss = 0.16079244\n",
      "Iteration 3, loss = 0.11138162\n",
      "Iteration 4, loss = 0.08530051\n",
      "Iteration 5, loss = 0.07069482\n",
      "Iteration 6, loss = 0.05666687\n",
      "Iteration 7, loss = 0.04526494\n",
      "Iteration 8, loss = 0.04099505\n",
      "Iteration 9, loss = 0.03724975\n",
      "Iteration 10, loss = 0.03587341\n",
      "Iteration 11, loss = 0.03115220\n",
      "Iteration 12, loss = 0.03038095\n",
      "Iteration 13, loss = 0.02736560\n",
      "Iteration 14, loss = 0.02419191\n",
      "Iteration 15, loss = 0.02343692\n",
      "Iteration 16, loss = 0.02177362\n",
      "Iteration 17, loss = 0.01995218\n",
      "Iteration 18, loss = 0.01909901\n",
      "Iteration 19, loss = 0.01844326\n",
      "Iteration 20, loss = 0.02955303\n",
      "Iteration 21, loss = 0.05341544\n",
      "Iteration 22, loss = 0.04081209\n",
      "Iteration 23, loss = 0.02594500\n",
      "Iteration 24, loss = 0.02437088\n",
      "Iteration 25, loss = 0.02142394\n",
      "Iteration 26, loss = 0.01954642\n",
      "Iteration 27, loss = 0.01856438\n",
      "Iteration 28, loss = 0.01776436\n",
      "Iteration 29, loss = 0.01707596\n",
      "Iteration 30, loss = 0.01646130\n",
      "Iteration 31, loss = 0.01589474\n",
      "Iteration 32, loss = 0.01539926\n",
      "Iteration 33, loss = 0.01550896\n",
      "Iteration 34, loss = 0.06036948\n",
      "Iteration 35, loss = 0.03764894\n",
      "Iteration 36, loss = 0.02687100\n",
      "Iteration 37, loss = 0.02234576\n",
      "Iteration 38, loss = 0.02064034\n",
      "Iteration 39, loss = 0.01871000\n",
      "Iteration 40, loss = 0.01709338\n",
      "Iteration 41, loss = 0.01624938\n",
      "Iteration 42, loss = 0.01554636\n",
      "Iteration 43, loss = 0.01488834\n",
      "Iteration 44, loss = 0.01431121\n",
      "Iteration 45, loss = 0.01381753\n",
      "Iteration 46, loss = 0.01335622\n",
      "Iteration 47, loss = 0.01303905\n",
      "Iteration 48, loss = 0.03183217\n",
      "Iteration 49, loss = 0.04669746\n",
      "Iteration 50, loss = 0.02795136\n",
      "Iteration 51, loss = 0.01962957\n",
      "Iteration 52, loss = 0.01767972\n",
      "Iteration 53, loss = 0.01743112\n",
      "Iteration 54, loss = 0.01554038\n",
      "Iteration 55, loss = 0.01465573\n",
      "Iteration 56, loss = 0.01401635\n",
      "Iteration 57, loss = 0.01348826\n",
      "Iteration 58, loss = 0.01299869\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.01, batch_size=200, hidden_layer_sizes=(400, 100), solver=adam; total time= 3.3min\n",
      "Iteration 1, loss = 0.39007291\n",
      "Iteration 2, loss = 0.15891410\n",
      "Iteration 3, loss = 0.11019056\n",
      "Iteration 4, loss = 0.08293177\n",
      "Iteration 5, loss = 0.06407962\n",
      "Iteration 6, loss = 0.05459434\n",
      "Iteration 7, loss = 0.04389788\n",
      "Iteration 8, loss = 0.04020006\n",
      "Iteration 9, loss = 0.03565795\n",
      "Iteration 10, loss = 0.03069218\n",
      "Iteration 11, loss = 0.02954729\n",
      "Iteration 12, loss = 0.02835107\n",
      "Iteration 13, loss = 0.03292602\n",
      "Iteration 14, loss = 0.02682109\n",
      "Iteration 15, loss = 0.02858515\n",
      "Iteration 16, loss = 0.02370030\n",
      "Iteration 17, loss = 0.02195644\n",
      "Iteration 18, loss = 0.02182038\n",
      "Iteration 19, loss = 0.02843032\n",
      "Iteration 20, loss = 0.03754550\n",
      "Iteration 21, loss = 0.03950367\n",
      "Iteration 22, loss = 0.02998800\n",
      "Iteration 23, loss = 0.02641920\n",
      "Iteration 24, loss = 0.02246266\n",
      "Iteration 25, loss = 0.02072883\n",
      "Iteration 26, loss = 0.01948993\n",
      "Iteration 27, loss = 0.01850508\n",
      "Iteration 28, loss = 0.01770949\n",
      "Iteration 29, loss = 0.01696508\n",
      "Iteration 30, loss = 0.01630286\n",
      "Iteration 31, loss = 0.01579190\n",
      "Iteration 32, loss = 0.03681616\n",
      "Iteration 33, loss = 0.04829478\n",
      "Iteration 34, loss = 0.03229896\n",
      "Iteration 35, loss = 0.02580066\n",
      "Iteration 36, loss = 0.02235229\n",
      "Iteration 37, loss = 0.01925509\n",
      "Iteration 38, loss = 0.01797788\n",
      "Iteration 39, loss = 0.01710985\n",
      "Iteration 40, loss = 0.01632770\n",
      "Iteration 41, loss = 0.01560753\n",
      "Iteration 42, loss = 0.01498272\n",
      "Iteration 43, loss = 0.01439847\n",
      "Iteration 44, loss = 0.01400089\n",
      "Iteration 45, loss = 0.01508114\n",
      "Iteration 46, loss = 0.04996300\n",
      "Iteration 47, loss = 0.03236490\n",
      "Iteration 48, loss = 0.02118972\n",
      "Iteration 49, loss = 0.01911155\n",
      "Iteration 50, loss = 0.01700030\n",
      "Iteration 51, loss = 0.01578357\n",
      "Iteration 52, loss = 0.01502410\n",
      "Iteration 53, loss = 0.01438147\n",
      "Iteration 54, loss = 0.01383123\n",
      "Iteration 55, loss = 0.01333897\n",
      "Iteration 56, loss = 0.01284868\n",
      "Iteration 57, loss = 0.01241751\n",
      "Iteration 58, loss = 0.01206446\n",
      "Iteration 59, loss = 0.01900134\n",
      "Iteration 60, loss = 0.04587231\n",
      "Iteration 61, loss = 0.02782718\n",
      "Iteration 62, loss = 0.02121824\n",
      "Iteration 63, loss = 0.01715824\n",
      "Iteration 64, loss = 0.01552644\n",
      "Iteration 65, loss = 0.01457189\n",
      "Iteration 66, loss = 0.01394344\n",
      "Iteration 67, loss = 0.01338975\n",
      "Iteration 68, loss = 0.01290146\n",
      "Iteration 69, loss = 0.01247757\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.01, batch_size=200, hidden_layer_sizes=(400, 100), solver=adam; total time= 3.3min\n",
      "Iteration 1, loss = 0.39548743\n",
      "Iteration 2, loss = 0.16199145\n",
      "Iteration 3, loss = 0.11262746\n",
      "Iteration 4, loss = 0.08557739\n",
      "Iteration 5, loss = 0.06892434\n",
      "Iteration 6, loss = 0.05365073\n",
      "Iteration 7, loss = 0.04609245\n",
      "Iteration 8, loss = 0.04086648\n",
      "Iteration 9, loss = 0.03510818\n",
      "Iteration 10, loss = 0.03374654\n",
      "Iteration 11, loss = 0.03089761\n",
      "Iteration 12, loss = 0.02777233\n",
      "Iteration 13, loss = 0.02620383\n",
      "Iteration 14, loss = 0.03095371\n",
      "Iteration 15, loss = 0.03671367\n",
      "Iteration 16, loss = 0.02835229\n",
      "Iteration 17, loss = 0.02441043\n",
      "Iteration 18, loss = 0.02430096\n",
      "Iteration 19, loss = 0.02177394\n",
      "Iteration 20, loss = 0.03642671\n",
      "Iteration 21, loss = 0.03274390\n",
      "Iteration 22, loss = 0.02859318\n",
      "Iteration 23, loss = 0.03044029\n",
      "Iteration 24, loss = 0.02731303\n",
      "Iteration 25, loss = 0.02276000\n",
      "Iteration 26, loss = 0.02022335\n",
      "Iteration 27, loss = 0.01919879\n",
      "Iteration 28, loss = 0.01831596\n",
      "Iteration 29, loss = 0.01752124\n",
      "Iteration 30, loss = 0.01694811\n",
      "Iteration 31, loss = 0.04507809\n",
      "Iteration 32, loss = 0.04329940\n",
      "Iteration 33, loss = 0.02747608\n",
      "Iteration 34, loss = 0.02122017\n",
      "Iteration 35, loss = 0.01956710\n",
      "Iteration 36, loss = 0.01838729\n",
      "Iteration 37, loss = 0.01753997\n",
      "Iteration 38, loss = 0.01676503\n",
      "Iteration 39, loss = 0.01605565\n",
      "Iteration 40, loss = 0.01553796\n",
      "Iteration 41, loss = 0.01534200\n",
      "Iteration 42, loss = 0.04103022\n",
      "Iteration 43, loss = 0.03808133\n",
      "Iteration 44, loss = 0.02472253\n",
      "Iteration 45, loss = 0.02134821\n",
      "Iteration 46, loss = 0.01823805\n",
      "Iteration 47, loss = 0.01693520\n",
      "Iteration 48, loss = 0.01609430\n",
      "Iteration 49, loss = 0.01534534\n",
      "Iteration 50, loss = 0.01470514\n",
      "Iteration 51, loss = 0.01410834\n",
      "Iteration 52, loss = 0.01356489\n",
      "Iteration 53, loss = 0.01387639\n",
      "Iteration 54, loss = 0.04937590\n",
      "Iteration 55, loss = 0.03084576\n",
      "Iteration 56, loss = 0.02296132\n",
      "Iteration 57, loss = 0.01820859\n",
      "Iteration 58, loss = 0.01643796\n",
      "Iteration 59, loss = 0.01543481\n",
      "Iteration 60, loss = 0.01475435\n",
      "Iteration 61, loss = 0.01414909\n",
      "Iteration 62, loss = 0.01363106\n",
      "Iteration 63, loss = 0.01316030\n",
      "Iteration 64, loss = 0.01275927\n",
      "Iteration 65, loss = 0.01234802\n",
      "Iteration 66, loss = 0.01205120\n",
      "Iteration 67, loss = 0.03304374\n",
      "Iteration 68, loss = 0.03780496\n",
      "Iteration 69, loss = 0.02229797\n",
      "Iteration 70, loss = 0.01882269\n",
      "Iteration 71, loss = 0.01575737\n",
      "Iteration 72, loss = 0.01475507\n",
      "Iteration 73, loss = 0.01399192\n",
      "Iteration 74, loss = 0.01343733\n",
      "Iteration 75, loss = 0.01294844\n",
      "Iteration 76, loss = 0.01253820\n",
      "Iteration 77, loss = 0.01213791\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.01, batch_size=200, hidden_layer_sizes=(400, 100), solver=adam; total time= 4.3min\n",
      "Iteration 1, loss = 1.81062026\n",
      "Iteration 2, loss = 0.91342466\n",
      "Iteration 3, loss = 0.60947631\n",
      "Iteration 4, loss = 0.49776240\n",
      "Iteration 5, loss = 0.43967185\n",
      "Iteration 6, loss = 0.40325512\n",
      "Iteration 7, loss = 0.37754612\n",
      "Iteration 8, loss = 0.35809743\n",
      "Iteration 9, loss = 0.34246589\n",
      "Iteration 10, loss = 0.32955223\n",
      "Iteration 11, loss = 0.31848743\n",
      "Iteration 12, loss = 0.30843155\n",
      "Iteration 13, loss = 0.29985630\n",
      "Iteration 14, loss = 0.29149642\n",
      "Iteration 15, loss = 0.28433933\n",
      "Iteration 16, loss = 0.27768761\n",
      "Iteration 17, loss = 0.27130672\n",
      "Iteration 18, loss = 0.26523247\n",
      "Iteration 19, loss = 0.25980280\n",
      "Iteration 20, loss = 0.25447117\n",
      "Iteration 21, loss = 0.24942640\n",
      "Iteration 22, loss = 0.24452754\n",
      "Iteration 23, loss = 0.24016973\n",
      "Iteration 24, loss = 0.23557641\n",
      "Iteration 25, loss = 0.23149857\n",
      "Iteration 26, loss = 0.22714092\n",
      "Iteration 27, loss = 0.22339974\n",
      "Iteration 28, loss = 0.21955474\n",
      "Iteration 29, loss = 0.21589931\n",
      "Iteration 30, loss = 0.21223619\n",
      "Iteration 31, loss = 0.20880610\n",
      "Iteration 32, loss = 0.20556067\n",
      "Iteration 33, loss = 0.20236819\n",
      "Iteration 34, loss = 0.19913459\n",
      "Iteration 35, loss = 0.19617352\n",
      "Iteration 36, loss = 0.19328051\n",
      "Iteration 37, loss = 0.19034451\n",
      "Iteration 38, loss = 0.18761694\n",
      "Iteration 39, loss = 0.18481934\n",
      "Iteration 40, loss = 0.18230602\n",
      "Iteration 41, loss = 0.17977004\n",
      "Iteration 42, loss = 0.17737916\n",
      "Iteration 43, loss = 0.17491106\n",
      "Iteration 44, loss = 0.17246623\n",
      "Iteration 45, loss = 0.17026278\n",
      "Iteration 46, loss = 0.16800694\n",
      "Iteration 47, loss = 0.16590908\n",
      "Iteration 48, loss = 0.16365728\n",
      "Iteration 49, loss = 0.16167105\n",
      "Iteration 50, loss = 0.15947277\n",
      "Iteration 51, loss = 0.15765265\n",
      "Iteration 52, loss = 0.15581149\n",
      "Iteration 53, loss = 0.15375110\n",
      "Iteration 54, loss = 0.15195428\n",
      "Iteration 55, loss = 0.15015986\n",
      "Iteration 56, loss = 0.14830793\n",
      "Iteration 57, loss = 0.14664844\n",
      "Iteration 58, loss = 0.14490685\n",
      "Iteration 59, loss = 0.14333373\n",
      "Iteration 60, loss = 0.14161279\n",
      "Iteration 61, loss = 0.14018194\n",
      "Iteration 62, loss = 0.13849390\n",
      "Iteration 63, loss = 0.13699411\n",
      "Iteration 64, loss = 0.13542595\n",
      "Iteration 65, loss = 0.13397752\n",
      "Iteration 66, loss = 0.13269158\n",
      "Iteration 67, loss = 0.13115802\n",
      "Iteration 68, loss = 0.12974132\n",
      "Iteration 69, loss = 0.12843149\n",
      "Iteration 70, loss = 0.12692016\n",
      "Iteration 71, loss = 0.12578381\n",
      "Iteration 72, loss = 0.12429265\n",
      "Iteration 73, loss = 0.12311615\n",
      "Iteration 74, loss = 0.12180208\n",
      "Iteration 75, loss = 0.12066458\n",
      "Iteration 76, loss = 0.11945932\n",
      "Iteration 77, loss = 0.11829568\n",
      "Iteration 78, loss = 0.11732029\n",
      "Iteration 79, loss = 0.11591554\n",
      "Iteration 80, loss = 0.11492638\n",
      "Iteration 81, loss = 0.11379067\n",
      "Iteration 82, loss = 0.11266946\n",
      "Iteration 83, loss = 0.11133467\n",
      "Iteration 84, loss = 0.11047569\n",
      "Iteration 85, loss = 0.10932403\n",
      "Iteration 86, loss = 0.10838011\n",
      "Iteration 87, loss = 0.10735146\n",
      "Iteration 88, loss = 0.10641639\n",
      "Iteration 89, loss = 0.10519120\n",
      "Iteration 90, loss = 0.10444046\n",
      "Iteration 91, loss = 0.10342037\n",
      "Iteration 92, loss = 0.10259423\n",
      "Iteration 93, loss = 0.10165919\n",
      "Iteration 94, loss = 0.10074509\n",
      "Iteration 95, loss = 0.09986134\n",
      "Iteration 96, loss = 0.09896656\n",
      "Iteration 97, loss = 0.09811390\n",
      "Iteration 98, loss = 0.09699684\n",
      "Iteration 99, loss = 0.09643458\n",
      "Iteration 100, loss = 0.09545292\n",
      "Iteration 101, loss = 0.09480920\n",
      "Iteration 102, loss = 0.09388091\n",
      "Iteration 103, loss = 0.09317350\n",
      "Iteration 104, loss = 0.09234469\n",
      "Iteration 105, loss = 0.09149481\n",
      "Iteration 106, loss = 0.09074284\n",
      "Iteration 107, loss = 0.08997053\n",
      "Iteration 108, loss = 0.08931107\n",
      "Iteration 109, loss = 0.08849690\n",
      "Iteration 110, loss = 0.08782573\n",
      "Iteration 111, loss = 0.08701684\n",
      "Iteration 112, loss = 0.08638088\n",
      "Iteration 113, loss = 0.08573258\n",
      "Iteration 114, loss = 0.08501206\n",
      "Iteration 115, loss = 0.08435105\n",
      "Iteration 116, loss = 0.08365762\n",
      "Iteration 117, loss = 0.08301518\n",
      "Iteration 118, loss = 0.08233038\n",
      "Iteration 119, loss = 0.08166421\n",
      "Iteration 120, loss = 0.08100608\n",
      "Iteration 121, loss = 0.08044974\n",
      "Iteration 122, loss = 0.07979021\n",
      "Iteration 123, loss = 0.07921062\n",
      "Iteration 124, loss = 0.07864276\n",
      "Iteration 125, loss = 0.07804783\n",
      "Iteration 126, loss = 0.07746790\n",
      "Iteration 127, loss = 0.07680815\n",
      "Iteration 128, loss = 0.07638828\n",
      "Iteration 129, loss = 0.07581209\n",
      "Iteration 130, loss = 0.07521815\n",
      "Iteration 131, loss = 0.07470044\n",
      "Iteration 132, loss = 0.07409616\n",
      "Iteration 133, loss = 0.07363329\n",
      "Iteration 134, loss = 0.07318729\n",
      "Iteration 135, loss = 0.07255540\n",
      "Iteration 136, loss = 0.07210668\n",
      "Iteration 137, loss = 0.07157978\n",
      "Iteration 138, loss = 0.07106011\n",
      "Iteration 139, loss = 0.07052035\n",
      "Iteration 140, loss = 0.06996767\n",
      "Iteration 141, loss = 0.06961335\n",
      "Iteration 142, loss = 0.06902618\n",
      "Iteration 143, loss = 0.06871808\n",
      "Iteration 144, loss = 0.06822677\n",
      "Iteration 145, loss = 0.06768839\n",
      "Iteration 146, loss = 0.06726225\n",
      "Iteration 147, loss = 0.06682168\n",
      "Iteration 148, loss = 0.06631474\n",
      "Iteration 149, loss = 0.06593699\n",
      "Iteration 150, loss = 0.06557846\n",
      "Iteration 151, loss = 0.06511205\n",
      "Iteration 152, loss = 0.06472533\n",
      "Iteration 153, loss = 0.06429504\n",
      "Iteration 154, loss = 0.06389904\n",
      "Iteration 155, loss = 0.06346436\n",
      "Iteration 156, loss = 0.06304226\n",
      "Iteration 157, loss = 0.06266720\n",
      "Iteration 158, loss = 0.06227233\n",
      "Iteration 159, loss = 0.06187576\n",
      "Iteration 160, loss = 0.06152290\n",
      "Iteration 161, loss = 0.06109870\n",
      "Iteration 162, loss = 0.06082243\n",
      "Iteration 163, loss = 0.06040918\n",
      "Iteration 164, loss = 0.05998377\n",
      "Iteration 165, loss = 0.05962888\n",
      "Iteration 166, loss = 0.05937367\n",
      "Iteration 167, loss = 0.05888736\n",
      "Iteration 168, loss = 0.05856709\n",
      "Iteration 169, loss = 0.05828914\n",
      "Iteration 170, loss = 0.05793612\n",
      "Iteration 171, loss = 0.05758632\n",
      "Iteration 172, loss = 0.05725960\n",
      "Iteration 173, loss = 0.05692396\n",
      "Iteration 174, loss = 0.05657713\n",
      "Iteration 175, loss = 0.05626905\n",
      "Iteration 176, loss = 0.05595052\n",
      "Iteration 177, loss = 0.05563606\n",
      "Iteration 178, loss = 0.05530710\n",
      "Iteration 179, loss = 0.05498618\n",
      "Iteration 180, loss = 0.05473876\n",
      "Iteration 181, loss = 0.05437573\n",
      "Iteration 182, loss = 0.05412677\n",
      "Iteration 183, loss = 0.05390466\n",
      "Iteration 184, loss = 0.05352313\n",
      "Iteration 185, loss = 0.05324851\n",
      "Iteration 186, loss = 0.05296932\n",
      "Iteration 187, loss = 0.05272456\n",
      "Iteration 188, loss = 0.05234596\n",
      "Iteration 189, loss = 0.05216775\n",
      "Iteration 190, loss = 0.05188710\n",
      "Iteration 191, loss = 0.05153574\n",
      "Iteration 192, loss = 0.05136021\n",
      "Iteration 193, loss = 0.05103531\n",
      "Iteration 194, loss = 0.05085964\n",
      "Iteration 195, loss = 0.05052524\n",
      "Iteration 196, loss = 0.05032125\n",
      "Iteration 197, loss = 0.05005096\n",
      "Iteration 198, loss = 0.04973945\n",
      "Iteration 199, loss = 0.04958374\n",
      "Iteration 200, loss = 0.04939448\n",
      "[CV] END activation=relu, alpha=0.01, batch_size=200, hidden_layer_sizes=(400, 100), solver=sgd; total time= 3.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.83344586\n",
      "Iteration 2, loss = 0.93102600\n",
      "Iteration 3, loss = 0.61448070\n",
      "Iteration 4, loss = 0.49679853\n",
      "Iteration 5, loss = 0.43747152\n",
      "Iteration 6, loss = 0.40130910\n",
      "Iteration 7, loss = 0.37593941\n",
      "Iteration 8, loss = 0.35702398\n",
      "Iteration 9, loss = 0.34154798\n",
      "Iteration 10, loss = 0.32880350\n",
      "Iteration 11, loss = 0.31834631\n",
      "Iteration 12, loss = 0.30889226\n",
      "Iteration 13, loss = 0.30019978\n",
      "Iteration 14, loss = 0.29285472\n",
      "Iteration 15, loss = 0.28561952\n",
      "Iteration 16, loss = 0.27925915\n",
      "Iteration 17, loss = 0.27319219\n",
      "Iteration 18, loss = 0.26773500\n",
      "Iteration 19, loss = 0.26231598\n",
      "Iteration 20, loss = 0.25723051\n",
      "Iteration 21, loss = 0.25222537\n",
      "Iteration 22, loss = 0.24763624\n",
      "Iteration 23, loss = 0.24332135\n",
      "Iteration 24, loss = 0.23908024\n",
      "Iteration 25, loss = 0.23491812\n",
      "Iteration 26, loss = 0.23108019\n",
      "Iteration 27, loss = 0.22692296\n",
      "Iteration 28, loss = 0.22347390\n",
      "Iteration 29, loss = 0.21991433\n",
      "Iteration 30, loss = 0.21648074\n",
      "Iteration 31, loss = 0.21336249\n",
      "Iteration 32, loss = 0.21011771\n",
      "Iteration 33, loss = 0.20703913\n",
      "Iteration 34, loss = 0.20372665\n",
      "Iteration 35, loss = 0.20091725\n",
      "Iteration 36, loss = 0.19797076\n",
      "Iteration 37, loss = 0.19519975\n",
      "Iteration 38, loss = 0.19251292\n",
      "Iteration 39, loss = 0.18964294\n",
      "Iteration 40, loss = 0.18733456\n",
      "Iteration 41, loss = 0.18470515\n",
      "Iteration 42, loss = 0.18234465\n",
      "Iteration 43, loss = 0.17975089\n",
      "Iteration 44, loss = 0.17734639\n",
      "Iteration 45, loss = 0.17507302\n",
      "Iteration 46, loss = 0.17272275\n",
      "Iteration 47, loss = 0.17079273\n",
      "Iteration 48, loss = 0.16845248\n",
      "Iteration 49, loss = 0.16643702\n",
      "Iteration 50, loss = 0.16450681\n",
      "Iteration 51, loss = 0.16241046\n",
      "Iteration 52, loss = 0.16030464\n",
      "Iteration 53, loss = 0.15837441\n",
      "Iteration 54, loss = 0.15659867\n",
      "Iteration 55, loss = 0.15456370\n",
      "Iteration 56, loss = 0.15288779\n",
      "Iteration 57, loss = 0.15116492\n",
      "Iteration 58, loss = 0.14936617\n",
      "Iteration 59, loss = 0.14768006\n",
      "Iteration 60, loss = 0.14619819\n",
      "Iteration 61, loss = 0.14440465\n",
      "Iteration 62, loss = 0.14283114\n",
      "Iteration 63, loss = 0.14134518\n",
      "Iteration 64, loss = 0.13974513\n",
      "Iteration 65, loss = 0.13816878\n",
      "Iteration 66, loss = 0.13674754\n",
      "Iteration 67, loss = 0.13534781\n",
      "Iteration 68, loss = 0.13377042\n",
      "Iteration 69, loss = 0.13244122\n",
      "Iteration 70, loss = 0.13113875\n",
      "Iteration 71, loss = 0.12960345\n",
      "Iteration 72, loss = 0.12839117\n",
      "Iteration 73, loss = 0.12710458\n",
      "Iteration 74, loss = 0.12585783\n",
      "Iteration 75, loss = 0.12459616\n",
      "Iteration 76, loss = 0.12332986\n",
      "Iteration 77, loss = 0.12217211\n",
      "Iteration 78, loss = 0.12097641\n",
      "Iteration 79, loss = 0.11974286\n",
      "Iteration 80, loss = 0.11869436\n",
      "Iteration 81, loss = 0.11745852\n",
      "Iteration 82, loss = 0.11635707\n",
      "Iteration 83, loss = 0.11510832\n",
      "Iteration 84, loss = 0.11397876\n",
      "Iteration 85, loss = 0.11309140\n",
      "Iteration 86, loss = 0.11217818\n",
      "Iteration 87, loss = 0.11084901\n",
      "Iteration 88, loss = 0.10980809\n",
      "Iteration 89, loss = 0.10890712\n",
      "Iteration 90, loss = 0.10816762\n",
      "Iteration 91, loss = 0.10698241\n",
      "Iteration 92, loss = 0.10612003\n",
      "Iteration 93, loss = 0.10514666\n",
      "Iteration 94, loss = 0.10425546\n",
      "Iteration 95, loss = 0.10323756\n",
      "Iteration 96, loss = 0.10243715\n",
      "Iteration 97, loss = 0.10159819\n",
      "Iteration 98, loss = 0.10054121\n",
      "Iteration 99, loss = 0.09979052\n",
      "Iteration 100, loss = 0.09891382\n",
      "Iteration 101, loss = 0.09808689\n",
      "Iteration 102, loss = 0.09727237\n",
      "Iteration 103, loss = 0.09638933\n",
      "Iteration 104, loss = 0.09574048\n",
      "Iteration 105, loss = 0.09495055\n",
      "Iteration 106, loss = 0.09399802\n",
      "Iteration 107, loss = 0.09332536\n",
      "Iteration 108, loss = 0.09266226\n",
      "Iteration 109, loss = 0.09176336\n",
      "Iteration 110, loss = 0.09099396\n",
      "Iteration 111, loss = 0.09019432\n",
      "Iteration 112, loss = 0.08956802\n",
      "Iteration 113, loss = 0.08895542\n",
      "Iteration 114, loss = 0.08817975\n",
      "Iteration 115, loss = 0.08744913\n",
      "Iteration 116, loss = 0.08692452\n",
      "Iteration 117, loss = 0.08611145\n",
      "Iteration 118, loss = 0.08555184\n",
      "Iteration 119, loss = 0.08481790\n",
      "Iteration 120, loss = 0.08418282\n",
      "Iteration 121, loss = 0.08351055\n",
      "Iteration 122, loss = 0.08304210\n",
      "Iteration 123, loss = 0.08238218\n",
      "Iteration 124, loss = 0.08163773\n",
      "Iteration 125, loss = 0.08118311\n",
      "Iteration 126, loss = 0.08044461\n",
      "Iteration 127, loss = 0.07996630\n",
      "Iteration 128, loss = 0.07946385\n",
      "Iteration 129, loss = 0.07884692\n",
      "Iteration 130, loss = 0.07819586\n",
      "Iteration 131, loss = 0.07770606\n",
      "Iteration 132, loss = 0.07708913\n",
      "Iteration 133, loss = 0.07657245\n",
      "Iteration 134, loss = 0.07600841\n",
      "Iteration 135, loss = 0.07547819\n",
      "Iteration 136, loss = 0.07486796\n",
      "Iteration 137, loss = 0.07437945\n",
      "Iteration 138, loss = 0.07389703\n",
      "Iteration 139, loss = 0.07345433\n",
      "Iteration 140, loss = 0.07290970\n",
      "Iteration 141, loss = 0.07239937\n",
      "Iteration 142, loss = 0.07201292\n",
      "Iteration 143, loss = 0.07140711\n",
      "Iteration 144, loss = 0.07092699\n",
      "Iteration 145, loss = 0.07051853\n",
      "Iteration 146, loss = 0.07006037\n",
      "Iteration 147, loss = 0.06963168\n",
      "Iteration 148, loss = 0.06914750\n",
      "Iteration 149, loss = 0.06876121\n",
      "Iteration 150, loss = 0.06816234\n",
      "Iteration 151, loss = 0.06785001\n",
      "Iteration 152, loss = 0.06735446\n",
      "Iteration 153, loss = 0.06709677\n",
      "Iteration 154, loss = 0.06664586\n",
      "Iteration 155, loss = 0.06611347\n",
      "Iteration 156, loss = 0.06575003\n",
      "Iteration 157, loss = 0.06531044\n",
      "Iteration 158, loss = 0.06506792\n",
      "Iteration 159, loss = 0.06453449\n",
      "Iteration 160, loss = 0.06417369\n",
      "Iteration 161, loss = 0.06374917\n",
      "Iteration 162, loss = 0.06338336\n",
      "Iteration 163, loss = 0.06303283\n",
      "Iteration 164, loss = 0.06259731\n",
      "Iteration 165, loss = 0.06222348\n",
      "Iteration 166, loss = 0.06190338\n",
      "Iteration 167, loss = 0.06148441\n",
      "Iteration 168, loss = 0.06119006\n",
      "Iteration 169, loss = 0.06085759\n",
      "Iteration 170, loss = 0.06048616\n",
      "Iteration 171, loss = 0.06007213\n",
      "Iteration 172, loss = 0.05975899\n",
      "Iteration 173, loss = 0.05942120\n",
      "Iteration 174, loss = 0.05906354\n",
      "Iteration 175, loss = 0.05882843\n",
      "Iteration 176, loss = 0.05842689\n",
      "Iteration 177, loss = 0.05806475\n",
      "Iteration 178, loss = 0.05782035\n",
      "Iteration 179, loss = 0.05746469\n",
      "Iteration 180, loss = 0.05708869\n",
      "Iteration 181, loss = 0.05674828\n",
      "Iteration 182, loss = 0.05659712\n",
      "Iteration 183, loss = 0.05625795\n",
      "Iteration 184, loss = 0.05590904\n",
      "Iteration 185, loss = 0.05562410\n",
      "Iteration 186, loss = 0.05536307\n",
      "Iteration 187, loss = 0.05505221\n",
      "Iteration 188, loss = 0.05475098\n",
      "Iteration 189, loss = 0.05448597\n",
      "Iteration 190, loss = 0.05412798\n",
      "Iteration 191, loss = 0.05387047\n",
      "Iteration 192, loss = 0.05361665\n",
      "Iteration 193, loss = 0.05335177\n",
      "Iteration 194, loss = 0.05308380\n",
      "Iteration 195, loss = 0.05287127\n",
      "Iteration 196, loss = 0.05254542\n",
      "Iteration 197, loss = 0.05225081\n",
      "Iteration 198, loss = 0.05203346\n",
      "Iteration 199, loss = 0.05178093\n",
      "Iteration 200, loss = 0.05155843\n",
      "[CV] END activation=relu, alpha=0.01, batch_size=200, hidden_layer_sizes=(400, 100), solver=sgd; total time= 3.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.80720088\n",
      "Iteration 2, loss = 0.92057419\n",
      "Iteration 3, loss = 0.62569632\n",
      "Iteration 4, loss = 0.51150724\n",
      "Iteration 5, loss = 0.44970596\n",
      "Iteration 6, loss = 0.41044274\n",
      "Iteration 7, loss = 0.38285092\n",
      "Iteration 8, loss = 0.36174137\n",
      "Iteration 9, loss = 0.34526047\n",
      "Iteration 10, loss = 0.33172325\n",
      "Iteration 11, loss = 0.32032064\n",
      "Iteration 12, loss = 0.31006944\n",
      "Iteration 13, loss = 0.30119014\n",
      "Iteration 14, loss = 0.29319850\n",
      "Iteration 15, loss = 0.28568721\n",
      "Iteration 16, loss = 0.27903736\n",
      "Iteration 17, loss = 0.27289040\n",
      "Iteration 18, loss = 0.26686073\n",
      "Iteration 19, loss = 0.26154598\n",
      "Iteration 20, loss = 0.25632251\n",
      "Iteration 21, loss = 0.25144983\n",
      "Iteration 22, loss = 0.24666474\n",
      "Iteration 23, loss = 0.24221239\n",
      "Iteration 24, loss = 0.23762085\n",
      "Iteration 25, loss = 0.23371276\n",
      "Iteration 26, loss = 0.22988549\n",
      "Iteration 27, loss = 0.22597068\n",
      "Iteration 28, loss = 0.22232443\n",
      "Iteration 29, loss = 0.21863431\n",
      "Iteration 30, loss = 0.21537310\n",
      "Iteration 31, loss = 0.21222831\n",
      "Iteration 32, loss = 0.20898531\n",
      "Iteration 33, loss = 0.20578766\n",
      "Iteration 34, loss = 0.20271475\n",
      "Iteration 35, loss = 0.20004720\n",
      "Iteration 36, loss = 0.19712078\n",
      "Iteration 37, loss = 0.19432923\n",
      "Iteration 38, loss = 0.19162366\n",
      "Iteration 39, loss = 0.18922538\n",
      "Iteration 40, loss = 0.18663550\n",
      "Iteration 41, loss = 0.18400343\n",
      "Iteration 42, loss = 0.18159104\n",
      "Iteration 43, loss = 0.17922458\n",
      "Iteration 44, loss = 0.17691033\n",
      "Iteration 45, loss = 0.17476889\n",
      "Iteration 46, loss = 0.17261707\n",
      "Iteration 47, loss = 0.17033183\n",
      "Iteration 48, loss = 0.16817102\n",
      "Iteration 49, loss = 0.16625258\n",
      "Iteration 50, loss = 0.16417035\n",
      "Iteration 51, loss = 0.16227594\n",
      "Iteration 52, loss = 0.16027148\n",
      "Iteration 53, loss = 0.15847562\n",
      "Iteration 54, loss = 0.15662471\n",
      "Iteration 55, loss = 0.15480967\n",
      "Iteration 56, loss = 0.15296126\n",
      "Iteration 57, loss = 0.15106679\n",
      "Iteration 58, loss = 0.14946575\n",
      "Iteration 59, loss = 0.14774535\n",
      "Iteration 60, loss = 0.14618513\n",
      "Iteration 61, loss = 0.14460678\n",
      "Iteration 62, loss = 0.14293941\n",
      "Iteration 63, loss = 0.14135379\n",
      "Iteration 64, loss = 0.13978719\n",
      "Iteration 65, loss = 0.13848579\n",
      "Iteration 66, loss = 0.13694870\n",
      "Iteration 67, loss = 0.13542094\n",
      "Iteration 68, loss = 0.13408710\n",
      "Iteration 69, loss = 0.13267226\n",
      "Iteration 70, loss = 0.13135983\n",
      "Iteration 71, loss = 0.12988004\n",
      "Iteration 72, loss = 0.12858816\n",
      "Iteration 73, loss = 0.12726715\n",
      "Iteration 74, loss = 0.12607559\n",
      "Iteration 75, loss = 0.12493349\n",
      "Iteration 76, loss = 0.12369636\n",
      "Iteration 77, loss = 0.12250850\n",
      "Iteration 78, loss = 0.12117565\n",
      "Iteration 79, loss = 0.12009552\n",
      "Iteration 80, loss = 0.11893500\n",
      "Iteration 81, loss = 0.11782224\n",
      "Iteration 82, loss = 0.11664926\n",
      "Iteration 83, loss = 0.11563491\n",
      "Iteration 84, loss = 0.11461023\n",
      "Iteration 85, loss = 0.11330147\n",
      "Iteration 86, loss = 0.11240306\n",
      "Iteration 87, loss = 0.11134402\n",
      "Iteration 88, loss = 0.11030056\n",
      "Iteration 89, loss = 0.10921780\n",
      "Iteration 90, loss = 0.10834292\n",
      "Iteration 91, loss = 0.10749326\n",
      "Iteration 92, loss = 0.10647451\n",
      "Iteration 93, loss = 0.10558300\n",
      "Iteration 94, loss = 0.10465341\n",
      "Iteration 95, loss = 0.10367767\n",
      "Iteration 96, loss = 0.10285009\n",
      "Iteration 97, loss = 0.10201604\n",
      "Iteration 98, loss = 0.10104510\n",
      "Iteration 99, loss = 0.10038177\n",
      "Iteration 100, loss = 0.09946654\n",
      "Iteration 101, loss = 0.09856301\n",
      "Iteration 102, loss = 0.09778391\n",
      "Iteration 103, loss = 0.09701683\n",
      "Iteration 104, loss = 0.09620483\n",
      "Iteration 105, loss = 0.09537803\n",
      "Iteration 106, loss = 0.09469608\n",
      "Iteration 107, loss = 0.09393099\n",
      "Iteration 108, loss = 0.09320481\n",
      "Iteration 109, loss = 0.09241908\n",
      "Iteration 110, loss = 0.09168922\n",
      "Iteration 111, loss = 0.09109596\n",
      "Iteration 112, loss = 0.09032785\n",
      "Iteration 113, loss = 0.08953233\n",
      "Iteration 114, loss = 0.08883259\n",
      "Iteration 115, loss = 0.08815317\n",
      "Iteration 116, loss = 0.08743927\n",
      "Iteration 117, loss = 0.08677274\n",
      "Iteration 118, loss = 0.08611834\n",
      "Iteration 119, loss = 0.08552253\n",
      "Iteration 120, loss = 0.08483719\n",
      "Iteration 121, loss = 0.08415967\n",
      "Iteration 122, loss = 0.08364583\n",
      "Iteration 123, loss = 0.08303443\n",
      "Iteration 124, loss = 0.08241140\n",
      "Iteration 125, loss = 0.08189001\n",
      "Iteration 126, loss = 0.08124833\n",
      "Iteration 127, loss = 0.08060716\n",
      "Iteration 128, loss = 0.08005507\n",
      "Iteration 129, loss = 0.07942889\n",
      "Iteration 130, loss = 0.07882811\n",
      "Iteration 131, loss = 0.07838251\n",
      "Iteration 132, loss = 0.07764312\n",
      "Iteration 133, loss = 0.07733075\n",
      "Iteration 134, loss = 0.07665604\n",
      "Iteration 135, loss = 0.07618245\n",
      "Iteration 136, loss = 0.07567118\n",
      "Iteration 137, loss = 0.07509732\n",
      "Iteration 138, loss = 0.07456757\n",
      "Iteration 139, loss = 0.07420393\n",
      "Iteration 140, loss = 0.07359875\n",
      "Iteration 141, loss = 0.07309493\n",
      "Iteration 142, loss = 0.07250827\n",
      "Iteration 143, loss = 0.07221392\n",
      "Iteration 144, loss = 0.07163650\n",
      "Iteration 145, loss = 0.07116509\n",
      "Iteration 146, loss = 0.07062969\n",
      "Iteration 147, loss = 0.07033580\n",
      "Iteration 148, loss = 0.06980054\n",
      "Iteration 149, loss = 0.06932818\n",
      "Iteration 150, loss = 0.06889406\n",
      "Iteration 151, loss = 0.06841747\n",
      "Iteration 152, loss = 0.06800150\n",
      "Iteration 153, loss = 0.06756325\n",
      "Iteration 154, loss = 0.06714419\n",
      "Iteration 155, loss = 0.06675407\n",
      "Iteration 156, loss = 0.06631675\n",
      "Iteration 157, loss = 0.06584604\n",
      "Iteration 158, loss = 0.06553671\n",
      "Iteration 159, loss = 0.06507742\n",
      "Iteration 160, loss = 0.06468225\n",
      "Iteration 161, loss = 0.06432255\n",
      "Iteration 162, loss = 0.06379700\n",
      "Iteration 163, loss = 0.06349397\n",
      "Iteration 164, loss = 0.06305560\n",
      "Iteration 165, loss = 0.06272263\n",
      "Iteration 166, loss = 0.06235289\n",
      "Iteration 167, loss = 0.06194850\n",
      "Iteration 168, loss = 0.06162388\n",
      "Iteration 169, loss = 0.06122617\n",
      "Iteration 170, loss = 0.06096535\n",
      "Iteration 171, loss = 0.06056871\n",
      "Iteration 172, loss = 0.06023452\n",
      "Iteration 173, loss = 0.05976882\n",
      "Iteration 174, loss = 0.05952168\n",
      "Iteration 175, loss = 0.05921295\n",
      "Iteration 176, loss = 0.05883157\n",
      "Iteration 177, loss = 0.05847708\n",
      "Iteration 178, loss = 0.05808125\n",
      "Iteration 179, loss = 0.05780684\n",
      "Iteration 180, loss = 0.05759681\n",
      "Iteration 181, loss = 0.05721088\n",
      "Iteration 182, loss = 0.05680686\n",
      "Iteration 183, loss = 0.05653350\n",
      "Iteration 184, loss = 0.05628154\n",
      "Iteration 185, loss = 0.05591665\n",
      "Iteration 186, loss = 0.05564777\n",
      "Iteration 187, loss = 0.05536358\n",
      "Iteration 188, loss = 0.05504902\n",
      "Iteration 189, loss = 0.05477673\n",
      "Iteration 190, loss = 0.05449966\n",
      "Iteration 191, loss = 0.05424460\n",
      "Iteration 192, loss = 0.05393179\n",
      "Iteration 193, loss = 0.05361044\n",
      "Iteration 194, loss = 0.05333531\n",
      "Iteration 195, loss = 0.05310730\n",
      "Iteration 196, loss = 0.05279711\n",
      "Iteration 197, loss = 0.05251997\n",
      "Iteration 198, loss = 0.05221544\n",
      "Iteration 199, loss = 0.05198701\n",
      "Iteration 200, loss = 0.05170490\n",
      "[CV] END activation=relu, alpha=0.01, batch_size=200, hidden_layer_sizes=(400, 100), solver=sgd; total time= 3.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.77636481\n",
      "Iteration 2, loss = 0.89491240\n",
      "Iteration 3, loss = 0.59624995\n",
      "Iteration 4, loss = 0.48876227\n",
      "Iteration 5, loss = 0.43286302\n",
      "Iteration 6, loss = 0.39827558\n",
      "Iteration 7, loss = 0.37332594\n",
      "Iteration 8, loss = 0.35510770\n",
      "Iteration 9, loss = 0.33998401\n",
      "Iteration 10, loss = 0.32735625\n",
      "Iteration 11, loss = 0.31649046\n",
      "Iteration 12, loss = 0.30720705\n",
      "Iteration 13, loss = 0.29852882\n",
      "Iteration 14, loss = 0.29065654\n",
      "Iteration 15, loss = 0.28355412\n",
      "Iteration 16, loss = 0.27682800\n",
      "Iteration 17, loss = 0.27081012\n",
      "Iteration 18, loss = 0.26491338\n",
      "Iteration 19, loss = 0.25930057\n",
      "Iteration 20, loss = 0.25410108\n",
      "Iteration 21, loss = 0.24912810\n",
      "Iteration 22, loss = 0.24429064\n",
      "Iteration 23, loss = 0.23971280\n",
      "Iteration 24, loss = 0.23525676\n",
      "Iteration 25, loss = 0.23118545\n",
      "Iteration 26, loss = 0.22700750\n",
      "Iteration 27, loss = 0.22301503\n",
      "Iteration 28, loss = 0.21916539\n",
      "Iteration 29, loss = 0.21553729\n",
      "Iteration 30, loss = 0.21216527\n",
      "Iteration 31, loss = 0.20879330\n",
      "Iteration 32, loss = 0.20549432\n",
      "Iteration 33, loss = 0.20225968\n",
      "Iteration 34, loss = 0.19919755\n",
      "Iteration 35, loss = 0.19623876\n",
      "Iteration 36, loss = 0.19331982\n",
      "Iteration 37, loss = 0.19042161\n",
      "Iteration 38, loss = 0.18788284\n",
      "Iteration 39, loss = 0.18522764\n",
      "Iteration 40, loss = 0.18254438\n",
      "Iteration 41, loss = 0.18008492\n",
      "Iteration 42, loss = 0.17761172\n",
      "Iteration 43, loss = 0.17522581\n",
      "Iteration 44, loss = 0.17293117\n",
      "Iteration 45, loss = 0.17056485\n",
      "Iteration 46, loss = 0.16862973\n",
      "Iteration 47, loss = 0.16635454\n",
      "Iteration 48, loss = 0.16429953\n",
      "Iteration 49, loss = 0.16215619\n",
      "Iteration 50, loss = 0.16009263\n",
      "Iteration 51, loss = 0.15825243\n",
      "Iteration 52, loss = 0.15648163\n",
      "Iteration 53, loss = 0.15447697\n",
      "Iteration 54, loss = 0.15280408\n",
      "Iteration 55, loss = 0.15119717\n",
      "Iteration 56, loss = 0.14923541\n",
      "Iteration 57, loss = 0.14779134\n",
      "Iteration 58, loss = 0.14613687\n",
      "Iteration 59, loss = 0.14441143\n",
      "Iteration 60, loss = 0.14277127\n",
      "Iteration 61, loss = 0.14138711\n",
      "Iteration 62, loss = 0.13990746\n",
      "Iteration 63, loss = 0.13827783\n",
      "Iteration 64, loss = 0.13680110\n",
      "Iteration 65, loss = 0.13549555\n",
      "Iteration 66, loss = 0.13416305\n",
      "Iteration 67, loss = 0.13287095\n",
      "Iteration 68, loss = 0.13144577\n",
      "Iteration 69, loss = 0.13004867\n",
      "Iteration 70, loss = 0.12872906\n",
      "Iteration 71, loss = 0.12748803\n",
      "Iteration 72, loss = 0.12630073\n",
      "Iteration 73, loss = 0.12519486\n",
      "Iteration 74, loss = 0.12389491\n",
      "Iteration 75, loss = 0.12262743\n",
      "Iteration 76, loss = 0.12164690\n",
      "Iteration 77, loss = 0.12042211\n",
      "Iteration 78, loss = 0.11939044\n",
      "Iteration 79, loss = 0.11819519\n",
      "Iteration 80, loss = 0.11722263\n",
      "Iteration 81, loss = 0.11608701\n",
      "Iteration 82, loss = 0.11513535\n",
      "Iteration 83, loss = 0.11404324\n",
      "Iteration 84, loss = 0.11325423\n",
      "Iteration 85, loss = 0.11203043\n",
      "Iteration 86, loss = 0.11095618\n",
      "Iteration 87, loss = 0.11021816\n",
      "Iteration 88, loss = 0.10919136\n",
      "Iteration 89, loss = 0.10824777\n",
      "Iteration 90, loss = 0.10714546\n",
      "Iteration 91, loss = 0.10643347\n",
      "Iteration 92, loss = 0.10546760\n",
      "Iteration 93, loss = 0.10448015\n",
      "Iteration 94, loss = 0.10379024\n",
      "Iteration 95, loss = 0.10295913\n",
      "Iteration 96, loss = 0.10206919\n",
      "Iteration 97, loss = 0.10115435\n",
      "Iteration 98, loss = 0.10039366\n",
      "Iteration 99, loss = 0.09950969\n",
      "Iteration 100, loss = 0.09869140\n",
      "Iteration 101, loss = 0.09797079\n",
      "Iteration 102, loss = 0.09703476\n",
      "Iteration 103, loss = 0.09636554\n",
      "Iteration 104, loss = 0.09557261\n",
      "Iteration 105, loss = 0.09480589\n",
      "Iteration 106, loss = 0.09417378\n",
      "Iteration 107, loss = 0.09350708\n",
      "Iteration 108, loss = 0.09269146\n",
      "Iteration 109, loss = 0.09197278\n",
      "Iteration 110, loss = 0.09133411\n",
      "Iteration 111, loss = 0.09057961\n",
      "Iteration 112, loss = 0.08992725\n",
      "Iteration 113, loss = 0.08913677\n",
      "Iteration 114, loss = 0.08855158\n",
      "Iteration 115, loss = 0.08784177\n",
      "Iteration 116, loss = 0.08723629\n",
      "Iteration 117, loss = 0.08668702\n",
      "Iteration 118, loss = 0.08606514\n",
      "Iteration 119, loss = 0.08534436\n",
      "Iteration 120, loss = 0.08477291\n",
      "Iteration 121, loss = 0.08419681\n",
      "Iteration 122, loss = 0.08346686\n",
      "Iteration 123, loss = 0.08291096\n",
      "Iteration 124, loss = 0.08231367\n",
      "Iteration 125, loss = 0.08179493\n",
      "Iteration 126, loss = 0.08119050\n",
      "Iteration 127, loss = 0.08062683\n",
      "Iteration 128, loss = 0.08002374\n",
      "Iteration 129, loss = 0.07952011\n",
      "Iteration 130, loss = 0.07903567\n",
      "Iteration 131, loss = 0.07843591\n",
      "Iteration 132, loss = 0.07791774\n",
      "Iteration 133, loss = 0.07727784\n",
      "Iteration 134, loss = 0.07684721\n",
      "Iteration 135, loss = 0.07628130\n",
      "Iteration 136, loss = 0.07584673\n",
      "Iteration 137, loss = 0.07528078\n",
      "Iteration 138, loss = 0.07482619\n",
      "Iteration 139, loss = 0.07432410\n",
      "Iteration 140, loss = 0.07379381\n",
      "Iteration 141, loss = 0.07335233\n",
      "Iteration 142, loss = 0.07285815\n",
      "Iteration 143, loss = 0.07236040\n",
      "Iteration 144, loss = 0.07192810\n",
      "Iteration 145, loss = 0.07146229\n",
      "Iteration 146, loss = 0.07100714\n",
      "Iteration 147, loss = 0.07060935\n",
      "Iteration 148, loss = 0.07007818\n",
      "Iteration 149, loss = 0.06969898\n",
      "Iteration 150, loss = 0.06920167\n",
      "Iteration 151, loss = 0.06878374\n",
      "Iteration 152, loss = 0.06835048\n",
      "Iteration 153, loss = 0.06799281\n",
      "Iteration 154, loss = 0.06761696\n",
      "Iteration 155, loss = 0.06705511\n",
      "Iteration 156, loss = 0.06682851\n",
      "Iteration 157, loss = 0.06628681\n",
      "Iteration 158, loss = 0.06597812\n",
      "Iteration 159, loss = 0.06550325\n",
      "Iteration 160, loss = 0.06512811\n",
      "Iteration 161, loss = 0.06479228\n",
      "Iteration 162, loss = 0.06439470\n",
      "Iteration 163, loss = 0.06401041\n",
      "Iteration 164, loss = 0.06366351\n",
      "Iteration 165, loss = 0.06324724\n",
      "Iteration 166, loss = 0.06295833\n",
      "Iteration 167, loss = 0.06257061\n",
      "Iteration 168, loss = 0.06216548\n",
      "Iteration 169, loss = 0.06176910\n",
      "Iteration 170, loss = 0.06144133\n",
      "Iteration 171, loss = 0.06112050\n",
      "Iteration 172, loss = 0.06075752\n",
      "Iteration 173, loss = 0.06042774\n",
      "Iteration 174, loss = 0.06005753\n",
      "Iteration 175, loss = 0.05967842\n",
      "Iteration 176, loss = 0.05942557\n",
      "Iteration 177, loss = 0.05905287\n",
      "Iteration 178, loss = 0.05874027\n",
      "Iteration 179, loss = 0.05840733\n",
      "Iteration 180, loss = 0.05815261\n",
      "Iteration 181, loss = 0.05772740\n",
      "Iteration 182, loss = 0.05751158\n",
      "Iteration 183, loss = 0.05712044\n",
      "Iteration 184, loss = 0.05687261\n",
      "Iteration 185, loss = 0.05658125\n",
      "Iteration 186, loss = 0.05624908\n",
      "Iteration 187, loss = 0.05591050\n",
      "Iteration 188, loss = 0.05575280\n",
      "Iteration 189, loss = 0.05543376\n",
      "Iteration 190, loss = 0.05509347\n",
      "Iteration 191, loss = 0.05477260\n",
      "Iteration 192, loss = 0.05461878\n",
      "Iteration 193, loss = 0.05434190\n",
      "Iteration 194, loss = 0.05401585\n",
      "Iteration 195, loss = 0.05376692\n",
      "Iteration 196, loss = 0.05341906\n",
      "Iteration 197, loss = 0.05315775\n",
      "Iteration 198, loss = 0.05295703\n",
      "Iteration 199, loss = 0.05268216\n",
      "Iteration 200, loss = 0.05243181\n",
      "[CV] END activation=relu, alpha=0.01, batch_size=200, hidden_layer_sizes=(400, 100), solver=sgd; total time= 3.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.81019294\n",
      "Iteration 2, loss = 0.93552933\n",
      "Iteration 3, loss = 0.62195596\n",
      "Iteration 4, loss = 0.50864492\n",
      "Iteration 5, loss = 0.45006474\n",
      "Iteration 6, loss = 0.41280277\n",
      "Iteration 7, loss = 0.38680189\n",
      "Iteration 8, loss = 0.36672048\n",
      "Iteration 9, loss = 0.35092818\n",
      "Iteration 10, loss = 0.33752218\n",
      "Iteration 11, loss = 0.32605740\n",
      "Iteration 12, loss = 0.31598900\n",
      "Iteration 13, loss = 0.30700890\n",
      "Iteration 14, loss = 0.29889507\n",
      "Iteration 15, loss = 0.29137943\n",
      "Iteration 16, loss = 0.28445533\n",
      "Iteration 17, loss = 0.27813147\n",
      "Iteration 18, loss = 0.27232492\n",
      "Iteration 19, loss = 0.26642796\n",
      "Iteration 20, loss = 0.26122852\n",
      "Iteration 21, loss = 0.25623358\n",
      "Iteration 22, loss = 0.25125282\n",
      "Iteration 23, loss = 0.24673854\n",
      "Iteration 24, loss = 0.24248560\n",
      "Iteration 25, loss = 0.23814649\n",
      "Iteration 26, loss = 0.23404157\n",
      "Iteration 27, loss = 0.23022728\n",
      "Iteration 28, loss = 0.22652458\n",
      "Iteration 29, loss = 0.22301764\n",
      "Iteration 30, loss = 0.21945527\n",
      "Iteration 31, loss = 0.21601012\n",
      "Iteration 32, loss = 0.21255653\n",
      "Iteration 33, loss = 0.20961656\n",
      "Iteration 34, loss = 0.20655780\n",
      "Iteration 35, loss = 0.20369886\n",
      "Iteration 36, loss = 0.20052788\n",
      "Iteration 37, loss = 0.19781432\n",
      "Iteration 38, loss = 0.19515496\n",
      "Iteration 39, loss = 0.19237923\n",
      "Iteration 40, loss = 0.18985321\n",
      "Iteration 41, loss = 0.18748380\n",
      "Iteration 42, loss = 0.18500748\n",
      "Iteration 43, loss = 0.18245444\n",
      "Iteration 44, loss = 0.18022004\n",
      "Iteration 45, loss = 0.17793832\n",
      "Iteration 46, loss = 0.17573478\n",
      "Iteration 47, loss = 0.17345501\n",
      "Iteration 48, loss = 0.17142485\n",
      "Iteration 49, loss = 0.16939414\n",
      "Iteration 50, loss = 0.16735894\n",
      "Iteration 51, loss = 0.16544922\n",
      "Iteration 52, loss = 0.16351572\n",
      "Iteration 53, loss = 0.16173666\n",
      "Iteration 54, loss = 0.15988918\n",
      "Iteration 55, loss = 0.15799084\n",
      "Iteration 56, loss = 0.15633487\n",
      "Iteration 57, loss = 0.15460975\n",
      "Iteration 58, loss = 0.15294049\n",
      "Iteration 59, loss = 0.15117945\n",
      "Iteration 60, loss = 0.14962354\n",
      "Iteration 61, loss = 0.14814140\n",
      "Iteration 62, loss = 0.14652948\n",
      "Iteration 63, loss = 0.14489840\n",
      "Iteration 64, loss = 0.14344459\n",
      "Iteration 65, loss = 0.14202774\n",
      "Iteration 66, loss = 0.14072436\n",
      "Iteration 67, loss = 0.13921608\n",
      "Iteration 68, loss = 0.13787979\n",
      "Iteration 69, loss = 0.13651558\n",
      "Iteration 70, loss = 0.13515855\n",
      "Iteration 71, loss = 0.13376010\n",
      "Iteration 72, loss = 0.13241873\n",
      "Iteration 73, loss = 0.13117220\n",
      "Iteration 74, loss = 0.12999253\n",
      "Iteration 75, loss = 0.12877068\n",
      "Iteration 76, loss = 0.12754768\n",
      "Iteration 77, loss = 0.12623596\n",
      "Iteration 78, loss = 0.12530411\n",
      "Iteration 79, loss = 0.12408691\n",
      "Iteration 80, loss = 0.12288798\n",
      "Iteration 81, loss = 0.12167454\n",
      "Iteration 82, loss = 0.12065020\n",
      "Iteration 83, loss = 0.11935913\n",
      "Iteration 84, loss = 0.11840019\n",
      "Iteration 85, loss = 0.11734399\n",
      "Iteration 86, loss = 0.11636754\n",
      "Iteration 87, loss = 0.11549089\n",
      "Iteration 88, loss = 0.11439922\n",
      "Iteration 89, loss = 0.11330030\n",
      "Iteration 90, loss = 0.11235354\n",
      "Iteration 91, loss = 0.11149116\n",
      "Iteration 92, loss = 0.11046304\n",
      "Iteration 93, loss = 0.10960806\n",
      "Iteration 94, loss = 0.10865936\n",
      "Iteration 95, loss = 0.10767928\n",
      "Iteration 96, loss = 0.10685442\n",
      "Iteration 97, loss = 0.10592431\n",
      "Iteration 98, loss = 0.10504438\n",
      "Iteration 99, loss = 0.10420575\n",
      "Iteration 100, loss = 0.10332544\n",
      "Iteration 101, loss = 0.10258885\n",
      "Iteration 102, loss = 0.10177097\n",
      "Iteration 103, loss = 0.10080854\n",
      "Iteration 104, loss = 0.10002223\n",
      "Iteration 105, loss = 0.09929998\n",
      "Iteration 106, loss = 0.09848425\n",
      "Iteration 107, loss = 0.09774948\n",
      "Iteration 108, loss = 0.09697413\n",
      "Iteration 109, loss = 0.09627906\n",
      "Iteration 110, loss = 0.09548466\n",
      "Iteration 111, loss = 0.09459504\n",
      "Iteration 112, loss = 0.09404298\n",
      "Iteration 113, loss = 0.09319272\n",
      "Iteration 114, loss = 0.09250717\n",
      "Iteration 115, loss = 0.09176056\n",
      "Iteration 116, loss = 0.09104997\n",
      "Iteration 117, loss = 0.09034035\n",
      "Iteration 118, loss = 0.08970648\n",
      "Iteration 119, loss = 0.08909781\n",
      "Iteration 120, loss = 0.08847759\n",
      "Iteration 121, loss = 0.08775503\n",
      "Iteration 122, loss = 0.08720951\n",
      "Iteration 123, loss = 0.08654766\n",
      "Iteration 124, loss = 0.08589572\n",
      "Iteration 125, loss = 0.08518715\n",
      "Iteration 126, loss = 0.08478110\n",
      "Iteration 127, loss = 0.08401722\n",
      "Iteration 128, loss = 0.08336715\n",
      "Iteration 129, loss = 0.08286855\n",
      "Iteration 130, loss = 0.08213439\n",
      "Iteration 131, loss = 0.08160313\n",
      "Iteration 132, loss = 0.08096531\n",
      "Iteration 133, loss = 0.08038877\n",
      "Iteration 134, loss = 0.07997571\n",
      "Iteration 135, loss = 0.07942022\n",
      "Iteration 136, loss = 0.07875624\n",
      "Iteration 137, loss = 0.07816854\n",
      "Iteration 138, loss = 0.07766428\n",
      "Iteration 139, loss = 0.07713159\n",
      "Iteration 140, loss = 0.07653332\n",
      "Iteration 141, loss = 0.07611772\n",
      "Iteration 142, loss = 0.07560820\n",
      "Iteration 143, loss = 0.07498029\n",
      "Iteration 144, loss = 0.07465035\n",
      "Iteration 145, loss = 0.07395569\n",
      "Iteration 146, loss = 0.07362820\n",
      "Iteration 147, loss = 0.07293389\n",
      "Iteration 148, loss = 0.07260809\n",
      "Iteration 149, loss = 0.07205864\n",
      "Iteration 150, loss = 0.07159373\n",
      "Iteration 151, loss = 0.07116287\n",
      "Iteration 152, loss = 0.07059790\n",
      "Iteration 153, loss = 0.07027929\n",
      "Iteration 154, loss = 0.06971628\n",
      "Iteration 155, loss = 0.06931763\n",
      "Iteration 156, loss = 0.06880484\n",
      "Iteration 157, loss = 0.06844876\n",
      "Iteration 158, loss = 0.06805358\n",
      "Iteration 159, loss = 0.06769395\n",
      "Iteration 160, loss = 0.06717621\n",
      "Iteration 161, loss = 0.06678834\n",
      "Iteration 162, loss = 0.06633123\n",
      "Iteration 163, loss = 0.06582782\n",
      "Iteration 164, loss = 0.06539059\n",
      "Iteration 165, loss = 0.06501158\n",
      "Iteration 166, loss = 0.06468583\n",
      "Iteration 167, loss = 0.06420021\n",
      "Iteration 168, loss = 0.06388246\n",
      "Iteration 169, loss = 0.06349721\n",
      "Iteration 170, loss = 0.06320837\n",
      "Iteration 171, loss = 0.06268436\n",
      "Iteration 172, loss = 0.06239890\n",
      "Iteration 173, loss = 0.06199691\n",
      "Iteration 174, loss = 0.06159313\n",
      "Iteration 175, loss = 0.06123754\n",
      "Iteration 176, loss = 0.06095309\n",
      "Iteration 177, loss = 0.06052811\n",
      "Iteration 178, loss = 0.06029113\n",
      "Iteration 179, loss = 0.05985440\n",
      "Iteration 180, loss = 0.05951350\n",
      "Iteration 181, loss = 0.05917282\n",
      "Iteration 182, loss = 0.05884766\n",
      "Iteration 183, loss = 0.05847379\n",
      "Iteration 184, loss = 0.05819561\n",
      "Iteration 185, loss = 0.05785195\n",
      "Iteration 186, loss = 0.05757557\n",
      "Iteration 187, loss = 0.05721962\n",
      "Iteration 188, loss = 0.05688245\n",
      "Iteration 189, loss = 0.05661335\n",
      "Iteration 190, loss = 0.05639010\n",
      "Iteration 191, loss = 0.05602031\n",
      "Iteration 192, loss = 0.05566929\n",
      "Iteration 193, loss = 0.05543333\n",
      "Iteration 194, loss = 0.05511089\n",
      "Iteration 195, loss = 0.05477979\n",
      "Iteration 196, loss = 0.05450444\n",
      "Iteration 197, loss = 0.05423951\n",
      "Iteration 198, loss = 0.05385863\n",
      "Iteration 199, loss = 0.05366373\n",
      "Iteration 200, loss = 0.05343571\n",
      "[CV] END activation=relu, alpha=0.01, batch_size=200, hidden_layer_sizes=(400, 100), solver=sgd; total time= 4.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.51008459\n",
      "Iteration 2, loss = 0.19869913\n",
      "Iteration 3, loss = 0.13891823\n",
      "Iteration 4, loss = 0.10979359\n",
      "Iteration 5, loss = 0.09049802\n",
      "Iteration 6, loss = 0.07208514\n",
      "Iteration 7, loss = 0.06214605\n",
      "Iteration 8, loss = 0.05271416\n",
      "Iteration 9, loss = 0.04577106\n",
      "Iteration 10, loss = 0.04129226\n",
      "Iteration 11, loss = 0.03511359\n",
      "Iteration 12, loss = 0.03262965\n",
      "Iteration 13, loss = 0.03071004\n",
      "Iteration 14, loss = 0.02766723\n",
      "Iteration 15, loss = 0.03150895\n",
      "Iteration 16, loss = 0.02503575\n",
      "Iteration 17, loss = 0.02670390\n",
      "Iteration 18, loss = 0.02812424\n",
      "Iteration 19, loss = 0.02515445\n",
      "Iteration 20, loss = 0.02539831\n",
      "Iteration 21, loss = 0.02777888\n",
      "Iteration 22, loss = 0.02652610\n",
      "Iteration 23, loss = 0.02138057\n",
      "Iteration 24, loss = 0.01805779\n",
      "Iteration 25, loss = 0.01629231\n",
      "Iteration 26, loss = 0.01539591\n",
      "Iteration 27, loss = 0.01502620\n",
      "Iteration 28, loss = 0.01470571\n",
      "Iteration 29, loss = 0.01440404\n",
      "Iteration 30, loss = 0.01411664\n",
      "Iteration 31, loss = 0.01385538\n",
      "Iteration 32, loss = 0.01363572\n",
      "Iteration 33, loss = 0.01351873\n",
      "Iteration 34, loss = 0.01339432\n",
      "Iteration 35, loss = 0.06675839\n",
      "Iteration 36, loss = 0.03275636\n",
      "Iteration 37, loss = 0.02436471\n",
      "Iteration 38, loss = 0.02004583\n",
      "Iteration 39, loss = 0.01928186\n",
      "Iteration 40, loss = 0.01587169\n",
      "Iteration 41, loss = 0.01456948\n",
      "Iteration 42, loss = 0.01416999\n",
      "Iteration 43, loss = 0.01383782\n",
      "Iteration 44, loss = 0.01353220\n",
      "Iteration 45, loss = 0.01324284\n",
      "Iteration 46, loss = 0.01295146\n",
      "Iteration 47, loss = 0.01268385\n",
      "Iteration 48, loss = 0.01240811\n",
      "Iteration 49, loss = 0.01217194\n",
      "Iteration 50, loss = 0.01199185\n",
      "Iteration 51, loss = 0.04820779\n",
      "Iteration 52, loss = 0.04094980\n",
      "Iteration 53, loss = 0.02628645\n",
      "Iteration 54, loss = 0.02045071\n",
      "Iteration 55, loss = 0.01679810\n",
      "Iteration 56, loss = 0.01493914\n",
      "Iteration 57, loss = 0.01491060\n",
      "Iteration 58, loss = 0.01433685\n",
      "Iteration 59, loss = 0.01338402\n",
      "Iteration 60, loss = 0.01296985\n",
      "Iteration 61, loss = 0.01264494\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.01, batch_size=200, hidden_layer_sizes=(100, 100, 100), solver=adam; total time= 1.3min\n",
      "Iteration 1, loss = 0.51811965\n",
      "Iteration 2, loss = 0.19621457\n",
      "Iteration 3, loss = 0.14565603\n",
      "Iteration 4, loss = 0.11643604\n",
      "Iteration 5, loss = 0.09704823\n",
      "Iteration 6, loss = 0.08345067\n",
      "Iteration 7, loss = 0.06769244\n",
      "Iteration 8, loss = 0.05843324\n",
      "Iteration 9, loss = 0.05230308\n",
      "Iteration 10, loss = 0.04692204\n",
      "Iteration 11, loss = 0.04189201\n",
      "Iteration 12, loss = 0.04120007\n",
      "Iteration 13, loss = 0.03685047\n",
      "Iteration 14, loss = 0.03097461\n",
      "Iteration 15, loss = 0.02978178\n",
      "Iteration 16, loss = 0.02697004\n",
      "Iteration 17, loss = 0.02425170\n",
      "Iteration 18, loss = 0.02680281\n",
      "Iteration 19, loss = 0.03169224\n",
      "Iteration 20, loss = 0.02453582\n",
      "Iteration 21, loss = 0.02298670\n",
      "Iteration 22, loss = 0.02170247\n",
      "Iteration 23, loss = 0.02126378\n",
      "Iteration 24, loss = 0.01745409\n",
      "Iteration 25, loss = 0.01629561\n",
      "Iteration 26, loss = 0.01534098\n",
      "Iteration 27, loss = 0.01497844\n",
      "Iteration 28, loss = 0.01467499\n",
      "Iteration 29, loss = 0.01430732\n",
      "Iteration 30, loss = 0.01402058\n",
      "Iteration 31, loss = 0.01375267\n",
      "Iteration 32, loss = 0.01354547\n",
      "Iteration 33, loss = 0.06245870\n",
      "Iteration 34, loss = 0.03815226\n",
      "Iteration 35, loss = 0.02511372\n",
      "Iteration 36, loss = 0.02188630\n",
      "Iteration 37, loss = 0.01872413\n",
      "Iteration 38, loss = 0.01642345\n",
      "Iteration 39, loss = 0.01545160\n",
      "Iteration 40, loss = 0.01462470\n",
      "Iteration 41, loss = 0.01428050\n",
      "Iteration 42, loss = 0.01397941\n",
      "Iteration 43, loss = 0.01367995\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.01, batch_size=200, hidden_layer_sizes=(100, 100, 100), solver=adam; total time=  44.8s\n",
      "Iteration 1, loss = 0.51285808\n",
      "Iteration 2, loss = 0.19282965\n",
      "Iteration 3, loss = 0.14415889\n",
      "Iteration 4, loss = 0.11273957\n",
      "Iteration 5, loss = 0.09326986\n",
      "Iteration 6, loss = 0.08000235\n",
      "Iteration 7, loss = 0.06620815\n",
      "Iteration 8, loss = 0.05597657\n",
      "Iteration 9, loss = 0.04976084\n",
      "Iteration 10, loss = 0.04263994\n",
      "Iteration 11, loss = 0.04034059\n",
      "Iteration 12, loss = 0.03611874\n",
      "Iteration 13, loss = 0.02828498\n",
      "Iteration 14, loss = 0.02714296\n",
      "Iteration 15, loss = 0.02739068\n",
      "Iteration 16, loss = 0.02752364\n",
      "Iteration 17, loss = 0.03130486\n",
      "Iteration 18, loss = 0.02369897\n",
      "Iteration 19, loss = 0.02312853\n",
      "Iteration 20, loss = 0.02657059\n",
      "Iteration 21, loss = 0.02597802\n",
      "Iteration 22, loss = 0.02467088\n",
      "Iteration 23, loss = 0.02640171\n",
      "Iteration 24, loss = 0.02362643\n",
      "Iteration 25, loss = 0.01906697\n",
      "Iteration 26, loss = 0.01757485\n",
      "Iteration 27, loss = 0.01578713\n",
      "Iteration 28, loss = 0.01570135\n",
      "Iteration 29, loss = 0.01500976\n",
      "Iteration 30, loss = 0.01466994\n",
      "Iteration 31, loss = 0.01438499\n",
      "Iteration 32, loss = 0.01408886\n",
      "Iteration 33, loss = 0.01381636\n",
      "Iteration 34, loss = 0.01356813\n",
      "Iteration 35, loss = 0.01338640\n",
      "Iteration 36, loss = 0.07011454\n",
      "Iteration 37, loss = 0.03721697\n",
      "Iteration 38, loss = 0.02482319\n",
      "Iteration 39, loss = 0.02143701\n",
      "Iteration 40, loss = 0.01892999\n",
      "Iteration 41, loss = 0.02174490\n",
      "Iteration 42, loss = 0.02108269\n",
      "Iteration 43, loss = 0.02234345\n",
      "Iteration 44, loss = 0.01901917\n",
      "Iteration 45, loss = 0.01977215\n",
      "Iteration 46, loss = 0.02200702\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.01, batch_size=200, hidden_layer_sizes=(100, 100, 100), solver=adam; total time=  51.4s\n",
      "Iteration 1, loss = 0.53332744\n",
      "Iteration 2, loss = 0.19540587\n",
      "Iteration 3, loss = 0.14126525\n",
      "Iteration 4, loss = 0.10980960\n",
      "Iteration 5, loss = 0.08903552\n",
      "Iteration 6, loss = 0.07379087\n",
      "Iteration 7, loss = 0.06160312\n",
      "Iteration 8, loss = 0.05377398\n",
      "Iteration 9, loss = 0.04751917\n",
      "Iteration 10, loss = 0.04023162\n",
      "Iteration 11, loss = 0.03706874\n",
      "Iteration 12, loss = 0.03468731\n",
      "Iteration 13, loss = 0.03301885\n",
      "Iteration 14, loss = 0.02917214\n",
      "Iteration 15, loss = 0.02938153\n",
      "Iteration 16, loss = 0.02452916\n",
      "Iteration 17, loss = 0.02384088\n",
      "Iteration 18, loss = 0.02703392\n",
      "Iteration 19, loss = 0.02260475\n",
      "Iteration 20, loss = 0.03285539\n",
      "Iteration 21, loss = 0.02564919\n",
      "Iteration 22, loss = 0.01890789\n",
      "Iteration 23, loss = 0.02772812\n",
      "Iteration 24, loss = 0.02953938\n",
      "Iteration 25, loss = 0.02150468\n",
      "Iteration 26, loss = 0.01863753\n",
      "Iteration 27, loss = 0.01609512\n",
      "Iteration 28, loss = 0.01541901\n",
      "Iteration 29, loss = 0.01506171\n",
      "Iteration 30, loss = 0.01474891\n",
      "Iteration 31, loss = 0.01442875\n",
      "Iteration 32, loss = 0.01416313\n",
      "Iteration 33, loss = 0.01393848\n",
      "Iteration 34, loss = 0.01370236\n",
      "Iteration 35, loss = 0.01340898\n",
      "Iteration 36, loss = 0.01333179\n",
      "Iteration 37, loss = 0.03690336\n",
      "Iteration 38, loss = 0.05720743\n",
      "Iteration 39, loss = 0.02938393\n",
      "Iteration 40, loss = 0.02367310\n",
      "Iteration 41, loss = 0.02310610\n",
      "Iteration 42, loss = 0.02209172\n",
      "Iteration 43, loss = 0.01704528\n",
      "Iteration 44, loss = 0.01530725\n",
      "Iteration 45, loss = 0.01465617\n",
      "Iteration 46, loss = 0.01428203\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.01, batch_size=200, hidden_layer_sizes=(100, 100, 100), solver=adam; total time=  45.7s\n",
      "Iteration 1, loss = 0.53068520\n",
      "Iteration 2, loss = 0.19309400\n",
      "Iteration 3, loss = 0.14029386\n",
      "Iteration 4, loss = 0.10789691\n",
      "Iteration 5, loss = 0.08887243\n",
      "Iteration 6, loss = 0.07122485\n",
      "Iteration 7, loss = 0.06505150\n",
      "Iteration 8, loss = 0.05390115\n",
      "Iteration 9, loss = 0.04669620\n",
      "Iteration 10, loss = 0.04306529\n",
      "Iteration 11, loss = 0.03873893\n",
      "Iteration 12, loss = 0.03401370\n",
      "Iteration 13, loss = 0.03371451\n",
      "Iteration 14, loss = 0.02994674\n",
      "Iteration 15, loss = 0.02743881\n",
      "Iteration 16, loss = 0.02859496\n",
      "Iteration 17, loss = 0.02422117\n",
      "Iteration 18, loss = 0.02726523\n",
      "Iteration 19, loss = 0.03171019\n",
      "Iteration 20, loss = 0.02479192\n",
      "Iteration 21, loss = 0.02306917\n",
      "Iteration 22, loss = 0.02376626\n",
      "Iteration 23, loss = 0.02455017\n",
      "Iteration 24, loss = 0.02761195\n",
      "Iteration 25, loss = 0.02691412\n",
      "Iteration 26, loss = 0.02314301\n",
      "Iteration 27, loss = 0.01813740\n",
      "Iteration 28, loss = 0.01757130\n",
      "Iteration 29, loss = 0.01887491\n",
      "Iteration 30, loss = 0.02031671\n",
      "Iteration 31, loss = 0.03169715\n",
      "Iteration 32, loss = 0.03474451\n",
      "Iteration 33, loss = 0.02314413\n",
      "Iteration 34, loss = 0.02500237\n",
      "Iteration 35, loss = 0.01953779\n",
      "Iteration 36, loss = 0.01683035\n",
      "Iteration 37, loss = 0.01618193\n",
      "Iteration 38, loss = 0.01569129\n",
      "Iteration 39, loss = 0.01532804\n",
      "Iteration 40, loss = 0.01498019\n",
      "Iteration 41, loss = 0.01463744\n",
      "Iteration 42, loss = 0.01431007\n",
      "Iteration 43, loss = 0.01398646\n",
      "Iteration 44, loss = 0.01367323\n",
      "Iteration 45, loss = 0.01336514\n",
      "Iteration 46, loss = 0.01350962\n",
      "Iteration 47, loss = 0.06869562\n",
      "Iteration 48, loss = 0.02847207\n",
      "Iteration 49, loss = 0.02183802\n",
      "Iteration 50, loss = 0.02282832\n",
      "Iteration 51, loss = 0.02316882\n",
      "Iteration 52, loss = 0.01742915\n",
      "Iteration 53, loss = 0.01590340\n",
      "Iteration 54, loss = 0.01971290\n",
      "Iteration 55, loss = 0.02197124\n",
      "Iteration 56, loss = 0.02110034\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.01, batch_size=200, hidden_layer_sizes=(100, 100, 100), solver=adam; total time= 1.1min\n",
      "Iteration 1, loss = 2.14483872\n",
      "Iteration 2, loss = 1.46482528\n",
      "Iteration 3, loss = 0.83901192\n",
      "Iteration 4, loss = 0.59435563\n",
      "Iteration 5, loss = 0.48786826\n",
      "Iteration 6, loss = 0.43009371\n",
      "Iteration 7, loss = 0.39387430\n",
      "Iteration 8, loss = 0.36804185\n",
      "Iteration 9, loss = 0.34865344\n",
      "Iteration 10, loss = 0.33308134\n",
      "Iteration 11, loss = 0.31970671\n",
      "Iteration 12, loss = 0.30870062\n",
      "Iteration 13, loss = 0.29849953\n",
      "Iteration 14, loss = 0.28917432\n",
      "Iteration 15, loss = 0.28096555\n",
      "Iteration 16, loss = 0.27354015\n",
      "Iteration 17, loss = 0.26626527\n",
      "Iteration 18, loss = 0.25956773\n",
      "Iteration 19, loss = 0.25266985\n",
      "Iteration 20, loss = 0.24698661\n",
      "Iteration 21, loss = 0.24132741\n",
      "Iteration 22, loss = 0.23567386\n",
      "Iteration 23, loss = 0.23082332\n",
      "Iteration 24, loss = 0.22537337\n",
      "Iteration 25, loss = 0.22055794\n",
      "Iteration 26, loss = 0.21579030\n",
      "Iteration 27, loss = 0.21127952\n",
      "Iteration 28, loss = 0.20688001\n",
      "Iteration 29, loss = 0.20255118\n",
      "Iteration 30, loss = 0.19838209\n",
      "Iteration 31, loss = 0.19456153\n",
      "Iteration 32, loss = 0.19065869\n",
      "Iteration 33, loss = 0.18674550\n",
      "Iteration 34, loss = 0.18357604\n",
      "Iteration 35, loss = 0.18004965\n",
      "Iteration 36, loss = 0.17672681\n",
      "Iteration 37, loss = 0.17352558\n",
      "Iteration 38, loss = 0.17085800\n",
      "Iteration 39, loss = 0.16757276\n",
      "Iteration 40, loss = 0.16507163\n",
      "Iteration 41, loss = 0.16206079\n",
      "Iteration 42, loss = 0.15935603\n",
      "Iteration 43, loss = 0.15675953\n",
      "Iteration 44, loss = 0.15432694\n",
      "Iteration 45, loss = 0.15163588\n",
      "Iteration 46, loss = 0.14936610\n",
      "Iteration 47, loss = 0.14733150\n",
      "Iteration 48, loss = 0.14500877\n",
      "Iteration 49, loss = 0.14229032\n",
      "Iteration 50, loss = 0.14037666\n",
      "Iteration 51, loss = 0.13834469\n",
      "Iteration 52, loss = 0.13631813\n",
      "Iteration 53, loss = 0.13439909\n",
      "Iteration 54, loss = 0.13241373\n",
      "Iteration 55, loss = 0.13033739\n",
      "Iteration 56, loss = 0.12876482\n",
      "Iteration 57, loss = 0.12681052\n",
      "Iteration 58, loss = 0.12502525\n",
      "Iteration 59, loss = 0.12326206\n",
      "Iteration 60, loss = 0.12159303\n",
      "Iteration 61, loss = 0.11944199\n",
      "Iteration 62, loss = 0.11831267\n",
      "Iteration 63, loss = 0.11694646\n",
      "Iteration 64, loss = 0.11515320\n",
      "Iteration 65, loss = 0.11380752\n",
      "Iteration 66, loss = 0.11234221\n",
      "Iteration 67, loss = 0.11091392\n",
      "Iteration 68, loss = 0.10932804\n",
      "Iteration 69, loss = 0.10801532\n",
      "Iteration 70, loss = 0.10664896\n",
      "Iteration 71, loss = 0.10536605\n",
      "Iteration 72, loss = 0.10410822\n",
      "Iteration 73, loss = 0.10269956\n",
      "Iteration 74, loss = 0.10146658\n",
      "Iteration 75, loss = 0.10018822\n",
      "Iteration 76, loss = 0.09897329\n",
      "Iteration 77, loss = 0.09782267\n",
      "Iteration 78, loss = 0.09659959\n",
      "Iteration 79, loss = 0.09541111\n",
      "Iteration 80, loss = 0.09415527\n",
      "Iteration 81, loss = 0.09321713\n",
      "Iteration 82, loss = 0.09207290\n",
      "Iteration 83, loss = 0.09113283\n",
      "Iteration 84, loss = 0.09010412\n",
      "Iteration 85, loss = 0.08907330\n",
      "Iteration 86, loss = 0.08809614\n",
      "Iteration 87, loss = 0.08691644\n",
      "Iteration 88, loss = 0.08608113\n",
      "Iteration 89, loss = 0.08486833\n",
      "Iteration 90, loss = 0.08426262\n",
      "Iteration 91, loss = 0.08320136\n",
      "Iteration 92, loss = 0.08222361\n",
      "Iteration 93, loss = 0.08133016\n",
      "Iteration 94, loss = 0.08065666\n",
      "Iteration 95, loss = 0.07944190\n",
      "Iteration 96, loss = 0.07861951\n",
      "Iteration 97, loss = 0.07772018\n",
      "Iteration 98, loss = 0.07692537\n",
      "Iteration 99, loss = 0.07629880\n",
      "Iteration 100, loss = 0.07552830\n",
      "Iteration 101, loss = 0.07481580\n",
      "Iteration 102, loss = 0.07389007\n",
      "Iteration 103, loss = 0.07306272\n",
      "Iteration 104, loss = 0.07239125\n",
      "Iteration 105, loss = 0.07169000\n",
      "Iteration 106, loss = 0.07101430\n",
      "Iteration 107, loss = 0.07028544\n",
      "Iteration 108, loss = 0.06939045\n",
      "Iteration 109, loss = 0.06881882\n",
      "Iteration 110, loss = 0.06805903\n",
      "Iteration 111, loss = 0.06736973\n",
      "Iteration 112, loss = 0.06698284\n",
      "Iteration 113, loss = 0.06631645\n",
      "Iteration 114, loss = 0.06539060\n",
      "Iteration 115, loss = 0.06479421\n",
      "Iteration 116, loss = 0.06430369\n",
      "Iteration 117, loss = 0.06369471\n",
      "Iteration 118, loss = 0.06305077\n",
      "Iteration 119, loss = 0.06249545\n",
      "Iteration 120, loss = 0.06205798\n",
      "Iteration 121, loss = 0.06137563\n",
      "Iteration 122, loss = 0.06082341\n",
      "Iteration 123, loss = 0.06021362\n",
      "Iteration 124, loss = 0.05972743\n",
      "Iteration 125, loss = 0.05918479\n",
      "Iteration 126, loss = 0.05859845\n",
      "Iteration 127, loss = 0.05824811\n",
      "Iteration 128, loss = 0.05745747\n",
      "Iteration 129, loss = 0.05726425\n",
      "Iteration 130, loss = 0.05661494\n",
      "Iteration 131, loss = 0.05608853\n",
      "Iteration 132, loss = 0.05544028\n",
      "Iteration 133, loss = 0.05500125\n",
      "Iteration 134, loss = 0.05488645\n",
      "Iteration 135, loss = 0.05401953\n",
      "Iteration 136, loss = 0.05364261\n",
      "Iteration 137, loss = 0.05319591\n",
      "Iteration 138, loss = 0.05262348\n",
      "Iteration 139, loss = 0.05237045\n",
      "Iteration 140, loss = 0.05174970\n",
      "Iteration 141, loss = 0.05159833\n",
      "Iteration 142, loss = 0.05091443\n",
      "Iteration 143, loss = 0.05050078\n",
      "Iteration 144, loss = 0.05006790\n",
      "Iteration 145, loss = 0.04977705\n",
      "Iteration 146, loss = 0.04919741\n",
      "Iteration 147, loss = 0.04883649\n",
      "Iteration 148, loss = 0.04831409\n",
      "Iteration 149, loss = 0.04787313\n",
      "Iteration 150, loss = 0.04785701\n",
      "Iteration 151, loss = 0.04715249\n",
      "Iteration 152, loss = 0.04695786\n",
      "Iteration 153, loss = 0.04654725\n",
      "Iteration 154, loss = 0.04611822\n",
      "Iteration 155, loss = 0.04585639\n",
      "Iteration 156, loss = 0.04551316\n",
      "Iteration 157, loss = 0.04497803\n",
      "Iteration 158, loss = 0.04475206\n",
      "Iteration 159, loss = 0.04433634\n",
      "Iteration 160, loss = 0.04386064\n",
      "Iteration 161, loss = 0.04363721\n",
      "Iteration 162, loss = 0.04338174\n",
      "Iteration 163, loss = 0.04287277\n",
      "Iteration 164, loss = 0.04264019\n",
      "Iteration 165, loss = 0.04220557\n",
      "Iteration 166, loss = 0.04192479\n",
      "Iteration 167, loss = 0.04163246\n",
      "Iteration 168, loss = 0.04132391\n",
      "Iteration 169, loss = 0.04094701\n",
      "Iteration 170, loss = 0.04070417\n",
      "Iteration 171, loss = 0.04037453\n",
      "Iteration 172, loss = 0.04016656\n",
      "Iteration 173, loss = 0.03980500\n",
      "Iteration 174, loss = 0.03956755\n",
      "Iteration 175, loss = 0.03923751\n",
      "Iteration 176, loss = 0.03909622\n",
      "Iteration 177, loss = 0.03869823\n",
      "Iteration 178, loss = 0.03834877\n",
      "Iteration 179, loss = 0.03809983\n",
      "Iteration 180, loss = 0.03778071\n",
      "Iteration 181, loss = 0.03757089\n",
      "Iteration 182, loss = 0.03723644\n",
      "Iteration 183, loss = 0.03704233\n",
      "Iteration 184, loss = 0.03684924\n",
      "Iteration 185, loss = 0.03651483\n",
      "Iteration 186, loss = 0.03626606\n",
      "Iteration 187, loss = 0.03605531\n",
      "Iteration 188, loss = 0.03581459\n",
      "Iteration 189, loss = 0.03551924\n",
      "Iteration 190, loss = 0.03537975\n",
      "Iteration 191, loss = 0.03506191\n",
      "Iteration 192, loss = 0.03490811\n",
      "Iteration 193, loss = 0.03460869\n",
      "Iteration 194, loss = 0.03437618\n",
      "Iteration 195, loss = 0.03414243\n",
      "Iteration 196, loss = 0.03392960\n",
      "Iteration 197, loss = 0.03366693\n",
      "Iteration 198, loss = 0.03351959\n",
      "Iteration 199, loss = 0.03319776\n",
      "Iteration 200, loss = 0.03317098\n",
      "[CV] END activation=relu, alpha=0.01, batch_size=200, hidden_layer_sizes=(100, 100, 100), solver=sgd; total time= 1.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.14143803\n",
      "Iteration 2, loss = 1.45747565\n",
      "Iteration 3, loss = 0.81013551\n",
      "Iteration 4, loss = 0.58136893\n",
      "Iteration 5, loss = 0.48228156\n",
      "Iteration 6, loss = 0.42545156\n",
      "Iteration 7, loss = 0.38898312\n",
      "Iteration 8, loss = 0.36327724\n",
      "Iteration 9, loss = 0.34405415\n",
      "Iteration 10, loss = 0.32844565\n",
      "Iteration 11, loss = 0.31572643\n",
      "Iteration 12, loss = 0.30501043\n",
      "Iteration 13, loss = 0.29523107\n",
      "Iteration 14, loss = 0.28637248\n",
      "Iteration 15, loss = 0.27846445\n",
      "Iteration 16, loss = 0.27098543\n",
      "Iteration 17, loss = 0.26473831\n",
      "Iteration 18, loss = 0.25864248\n",
      "Iteration 19, loss = 0.25263620\n",
      "Iteration 20, loss = 0.24724047\n",
      "Iteration 21, loss = 0.24161951\n",
      "Iteration 22, loss = 0.23687295\n",
      "Iteration 23, loss = 0.23229654\n",
      "Iteration 24, loss = 0.22785198\n",
      "Iteration 25, loss = 0.22364213\n",
      "Iteration 26, loss = 0.21914636\n",
      "Iteration 27, loss = 0.21509081\n",
      "Iteration 28, loss = 0.21128211\n",
      "Iteration 29, loss = 0.20736801\n",
      "Iteration 30, loss = 0.20414269\n",
      "Iteration 31, loss = 0.20028252\n",
      "Iteration 32, loss = 0.19697695\n",
      "Iteration 33, loss = 0.19379724\n",
      "Iteration 34, loss = 0.19038336\n",
      "Iteration 35, loss = 0.18765325\n",
      "Iteration 36, loss = 0.18436582\n",
      "Iteration 37, loss = 0.18135724\n",
      "Iteration 38, loss = 0.17866449\n",
      "Iteration 39, loss = 0.17599925\n",
      "Iteration 40, loss = 0.17368587\n",
      "Iteration 41, loss = 0.17064161\n",
      "Iteration 42, loss = 0.16833708\n",
      "Iteration 43, loss = 0.16560830\n",
      "Iteration 44, loss = 0.16328394\n",
      "Iteration 45, loss = 0.16116176\n",
      "Iteration 46, loss = 0.15892603\n",
      "Iteration 47, loss = 0.15665030\n",
      "Iteration 48, loss = 0.15437431\n",
      "Iteration 49, loss = 0.15258230\n",
      "Iteration 50, loss = 0.15053868\n",
      "Iteration 51, loss = 0.14840203\n",
      "Iteration 52, loss = 0.14638630\n",
      "Iteration 53, loss = 0.14480778\n",
      "Iteration 54, loss = 0.14241723\n",
      "Iteration 55, loss = 0.14118296\n",
      "Iteration 56, loss = 0.13924772\n",
      "Iteration 57, loss = 0.13722150\n",
      "Iteration 58, loss = 0.13554161\n",
      "Iteration 59, loss = 0.13410225\n",
      "Iteration 60, loss = 0.13242250\n",
      "Iteration 61, loss = 0.13062425\n",
      "Iteration 62, loss = 0.12937311\n",
      "Iteration 63, loss = 0.12786681\n",
      "Iteration 64, loss = 0.12622976\n",
      "Iteration 65, loss = 0.12469636\n",
      "Iteration 66, loss = 0.12330050\n",
      "Iteration 67, loss = 0.12179428\n",
      "Iteration 68, loss = 0.12052321\n",
      "Iteration 69, loss = 0.11902892\n",
      "Iteration 70, loss = 0.11802119\n",
      "Iteration 71, loss = 0.11619240\n",
      "Iteration 72, loss = 0.11549468\n",
      "Iteration 73, loss = 0.11368123\n",
      "Iteration 74, loss = 0.11223013\n",
      "Iteration 75, loss = 0.11141473\n",
      "Iteration 76, loss = 0.10997246\n",
      "Iteration 77, loss = 0.10877420\n",
      "Iteration 78, loss = 0.10770823\n",
      "Iteration 79, loss = 0.10650880\n",
      "Iteration 80, loss = 0.10536211\n",
      "Iteration 81, loss = 0.10405187\n",
      "Iteration 82, loss = 0.10319254\n",
      "Iteration 83, loss = 0.10210324\n",
      "Iteration 84, loss = 0.10102242\n",
      "Iteration 85, loss = 0.09983352\n",
      "Iteration 86, loss = 0.09876737\n",
      "Iteration 87, loss = 0.09769176\n",
      "Iteration 88, loss = 0.09703228\n",
      "Iteration 89, loss = 0.09575224\n",
      "Iteration 90, loss = 0.09482706\n",
      "Iteration 91, loss = 0.09403210\n",
      "Iteration 92, loss = 0.09293273\n",
      "Iteration 93, loss = 0.09221345\n",
      "Iteration 94, loss = 0.09111763\n",
      "Iteration 95, loss = 0.09012472\n",
      "Iteration 96, loss = 0.08951554\n",
      "Iteration 97, loss = 0.08825942\n",
      "Iteration 98, loss = 0.08742153\n",
      "Iteration 99, loss = 0.08661144\n",
      "Iteration 100, loss = 0.08569793\n",
      "Iteration 101, loss = 0.08508523\n",
      "Iteration 102, loss = 0.08400115\n",
      "Iteration 103, loss = 0.08325946\n",
      "Iteration 104, loss = 0.08272550\n",
      "Iteration 105, loss = 0.08184004\n",
      "Iteration 106, loss = 0.08088061\n",
      "Iteration 107, loss = 0.08020374\n",
      "Iteration 108, loss = 0.07914770\n",
      "Iteration 109, loss = 0.07854441\n",
      "Iteration 110, loss = 0.07778612\n",
      "Iteration 111, loss = 0.07707290\n",
      "Iteration 112, loss = 0.07633664\n",
      "Iteration 113, loss = 0.07531882\n",
      "Iteration 114, loss = 0.07501332\n",
      "Iteration 115, loss = 0.07430580\n",
      "Iteration 116, loss = 0.07345989\n",
      "Iteration 117, loss = 0.07278583\n",
      "Iteration 118, loss = 0.07217525\n",
      "Iteration 119, loss = 0.07146087\n",
      "Iteration 120, loss = 0.07097422\n",
      "Iteration 121, loss = 0.07026209\n",
      "Iteration 122, loss = 0.06950330\n",
      "Iteration 123, loss = 0.06887209\n",
      "Iteration 124, loss = 0.06824498\n",
      "Iteration 125, loss = 0.06760813\n",
      "Iteration 126, loss = 0.06704955\n",
      "Iteration 127, loss = 0.06633707\n",
      "Iteration 128, loss = 0.06591825\n",
      "Iteration 129, loss = 0.06526421\n",
      "Iteration 130, loss = 0.06462547\n",
      "Iteration 131, loss = 0.06406907\n",
      "Iteration 132, loss = 0.06353893\n",
      "Iteration 133, loss = 0.06289831\n",
      "Iteration 134, loss = 0.06241149\n",
      "Iteration 135, loss = 0.06176318\n",
      "Iteration 136, loss = 0.06130511\n",
      "Iteration 137, loss = 0.06063143\n",
      "Iteration 138, loss = 0.06019781\n",
      "Iteration 139, loss = 0.05971706\n",
      "Iteration 140, loss = 0.05923213\n",
      "Iteration 141, loss = 0.05868650\n",
      "Iteration 142, loss = 0.05820124\n",
      "Iteration 143, loss = 0.05757795\n",
      "Iteration 144, loss = 0.05708740\n",
      "Iteration 145, loss = 0.05668874\n",
      "Iteration 146, loss = 0.05617575\n",
      "Iteration 147, loss = 0.05558570\n",
      "Iteration 148, loss = 0.05510972\n",
      "Iteration 149, loss = 0.05496330\n",
      "Iteration 150, loss = 0.05432770\n",
      "Iteration 151, loss = 0.05401604\n",
      "Iteration 152, loss = 0.05342626\n",
      "Iteration 153, loss = 0.05307594\n",
      "Iteration 154, loss = 0.05229802\n",
      "Iteration 155, loss = 0.05223972\n",
      "Iteration 156, loss = 0.05172103\n",
      "Iteration 157, loss = 0.05112550\n",
      "Iteration 158, loss = 0.05086110\n",
      "Iteration 159, loss = 0.05028865\n",
      "Iteration 160, loss = 0.04999540\n",
      "Iteration 161, loss = 0.04947695\n",
      "Iteration 162, loss = 0.04912243\n",
      "Iteration 163, loss = 0.04871308\n",
      "Iteration 164, loss = 0.04836560\n",
      "Iteration 165, loss = 0.04799582\n",
      "Iteration 166, loss = 0.04752070\n",
      "Iteration 167, loss = 0.04736813\n",
      "Iteration 168, loss = 0.04676650\n",
      "Iteration 169, loss = 0.04663287\n",
      "Iteration 170, loss = 0.04603174\n",
      "Iteration 171, loss = 0.04576787\n",
      "Iteration 172, loss = 0.04542784\n",
      "Iteration 173, loss = 0.04503117\n",
      "Iteration 174, loss = 0.04460951\n",
      "Iteration 175, loss = 0.04436958\n",
      "Iteration 176, loss = 0.04397736\n",
      "Iteration 177, loss = 0.04368768\n",
      "Iteration 178, loss = 0.04335581\n",
      "Iteration 179, loss = 0.04324133\n",
      "Iteration 180, loss = 0.04267841\n",
      "Iteration 181, loss = 0.04238897\n",
      "Iteration 182, loss = 0.04205212\n",
      "Iteration 183, loss = 0.04178267\n",
      "Iteration 184, loss = 0.04145087\n",
      "Iteration 185, loss = 0.04111875\n",
      "Iteration 186, loss = 0.04066676\n",
      "Iteration 187, loss = 0.04055706\n",
      "Iteration 188, loss = 0.04015962\n",
      "Iteration 189, loss = 0.03981775\n",
      "Iteration 190, loss = 0.03955848\n",
      "Iteration 191, loss = 0.03931719\n",
      "Iteration 192, loss = 0.03903150\n",
      "Iteration 193, loss = 0.03871089\n",
      "Iteration 194, loss = 0.03857208\n",
      "Iteration 195, loss = 0.03830540\n",
      "Iteration 196, loss = 0.03791440\n",
      "Iteration 197, loss = 0.03770238\n",
      "Iteration 198, loss = 0.03743150\n",
      "Iteration 199, loss = 0.03717687\n",
      "Iteration 200, loss = 0.03694746\n",
      "[CV] END activation=relu, alpha=0.01, batch_size=200, hidden_layer_sizes=(100, 100, 100), solver=sgd; total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.16609367\n",
      "Iteration 2, loss = 1.45249049\n",
      "Iteration 3, loss = 0.80212687\n",
      "Iteration 4, loss = 0.57796399\n",
      "Iteration 5, loss = 0.48062568\n",
      "Iteration 6, loss = 0.42425837\n",
      "Iteration 7, loss = 0.38760885\n",
      "Iteration 8, loss = 0.36161082\n",
      "Iteration 9, loss = 0.34205607\n",
      "Iteration 10, loss = 0.32615349\n",
      "Iteration 11, loss = 0.31330864\n",
      "Iteration 12, loss = 0.30163435\n",
      "Iteration 13, loss = 0.29162100\n",
      "Iteration 14, loss = 0.28280250\n",
      "Iteration 15, loss = 0.27426732\n",
      "Iteration 16, loss = 0.26692791\n",
      "Iteration 17, loss = 0.25980741\n",
      "Iteration 18, loss = 0.25356051\n",
      "Iteration 19, loss = 0.24707930\n",
      "Iteration 20, loss = 0.24126793\n",
      "Iteration 21, loss = 0.23572859\n",
      "Iteration 22, loss = 0.23041758\n",
      "Iteration 23, loss = 0.22568162\n",
      "Iteration 24, loss = 0.22109153\n",
      "Iteration 25, loss = 0.21630770\n",
      "Iteration 26, loss = 0.21221417\n",
      "Iteration 27, loss = 0.20752057\n",
      "Iteration 28, loss = 0.20377709\n",
      "Iteration 29, loss = 0.19993257\n",
      "Iteration 30, loss = 0.19649014\n",
      "Iteration 31, loss = 0.19282474\n",
      "Iteration 32, loss = 0.18930169\n",
      "Iteration 33, loss = 0.18601660\n",
      "Iteration 34, loss = 0.18315700\n",
      "Iteration 35, loss = 0.18001950\n",
      "Iteration 36, loss = 0.17685393\n",
      "Iteration 37, loss = 0.17400409\n",
      "Iteration 38, loss = 0.17147077\n",
      "Iteration 39, loss = 0.16872661\n",
      "Iteration 40, loss = 0.16601749\n",
      "Iteration 41, loss = 0.16391613\n",
      "Iteration 42, loss = 0.16111051\n",
      "Iteration 43, loss = 0.15853275\n",
      "Iteration 44, loss = 0.15630172\n",
      "Iteration 45, loss = 0.15409148\n",
      "Iteration 46, loss = 0.15182732\n",
      "Iteration 47, loss = 0.14977254\n",
      "Iteration 48, loss = 0.14758761\n",
      "Iteration 49, loss = 0.14554224\n",
      "Iteration 50, loss = 0.14361335\n",
      "Iteration 51, loss = 0.14165165\n",
      "Iteration 52, loss = 0.13991800\n",
      "Iteration 53, loss = 0.13770634\n",
      "Iteration 54, loss = 0.13582263\n",
      "Iteration 55, loss = 0.13404110\n",
      "Iteration 56, loss = 0.13241327\n",
      "Iteration 57, loss = 0.13057471\n",
      "Iteration 58, loss = 0.12908693\n",
      "Iteration 59, loss = 0.12742307\n",
      "Iteration 60, loss = 0.12589442\n",
      "Iteration 61, loss = 0.12397855\n",
      "Iteration 62, loss = 0.12253726\n",
      "Iteration 63, loss = 0.12103445\n",
      "Iteration 64, loss = 0.11946464\n",
      "Iteration 65, loss = 0.11796158\n",
      "Iteration 66, loss = 0.11688681\n",
      "Iteration 67, loss = 0.11556152\n",
      "Iteration 68, loss = 0.11405392\n",
      "Iteration 69, loss = 0.11259688\n",
      "Iteration 70, loss = 0.11137254\n",
      "Iteration 71, loss = 0.11019348\n",
      "Iteration 72, loss = 0.10888458\n",
      "Iteration 73, loss = 0.10761913\n",
      "Iteration 74, loss = 0.10638779\n",
      "Iteration 75, loss = 0.10496393\n",
      "Iteration 76, loss = 0.10391592\n",
      "Iteration 77, loss = 0.10296457\n",
      "Iteration 78, loss = 0.10181660\n",
      "Iteration 79, loss = 0.10059625\n",
      "Iteration 80, loss = 0.09959645\n",
      "Iteration 81, loss = 0.09844825\n",
      "Iteration 82, loss = 0.09750634\n",
      "Iteration 83, loss = 0.09630600\n",
      "Iteration 84, loss = 0.09534114\n",
      "Iteration 85, loss = 0.09430154\n",
      "Iteration 86, loss = 0.09322400\n",
      "Iteration 87, loss = 0.09237352\n",
      "Iteration 88, loss = 0.09100050\n",
      "Iteration 89, loss = 0.09035266\n",
      "Iteration 90, loss = 0.08934719\n",
      "Iteration 91, loss = 0.08835413\n",
      "Iteration 92, loss = 0.08744672\n",
      "Iteration 93, loss = 0.08669330\n",
      "Iteration 94, loss = 0.08563280\n",
      "Iteration 95, loss = 0.08485603\n",
      "Iteration 96, loss = 0.08397223\n",
      "Iteration 97, loss = 0.08279273\n",
      "Iteration 98, loss = 0.08237842\n",
      "Iteration 99, loss = 0.08123671\n",
      "Iteration 100, loss = 0.08043955\n",
      "Iteration 101, loss = 0.07966993\n",
      "Iteration 102, loss = 0.07865138\n",
      "Iteration 103, loss = 0.07834296\n",
      "Iteration 104, loss = 0.07727638\n",
      "Iteration 105, loss = 0.07657235\n",
      "Iteration 106, loss = 0.07581025\n",
      "Iteration 107, loss = 0.07508703\n",
      "Iteration 108, loss = 0.07439128\n",
      "Iteration 109, loss = 0.07353349\n",
      "Iteration 110, loss = 0.07295361\n",
      "Iteration 111, loss = 0.07222912\n",
      "Iteration 112, loss = 0.07128222\n",
      "Iteration 113, loss = 0.07075232\n",
      "Iteration 114, loss = 0.06982634\n",
      "Iteration 115, loss = 0.06923726\n",
      "Iteration 116, loss = 0.06858893\n",
      "Iteration 117, loss = 0.06796770\n",
      "Iteration 118, loss = 0.06727399\n",
      "Iteration 119, loss = 0.06663382\n",
      "Iteration 120, loss = 0.06595945\n",
      "Iteration 121, loss = 0.06545396\n",
      "Iteration 122, loss = 0.06471818\n",
      "Iteration 123, loss = 0.06420744\n",
      "Iteration 124, loss = 0.06350742\n",
      "Iteration 125, loss = 0.06288306\n",
      "Iteration 126, loss = 0.06233915\n",
      "Iteration 127, loss = 0.06183765\n",
      "Iteration 128, loss = 0.06108332\n",
      "Iteration 129, loss = 0.06068997\n",
      "Iteration 130, loss = 0.06013360\n",
      "Iteration 131, loss = 0.05940562\n",
      "Iteration 132, loss = 0.05902631\n",
      "Iteration 133, loss = 0.05849096\n",
      "Iteration 134, loss = 0.05789948\n",
      "Iteration 135, loss = 0.05749240\n",
      "Iteration 136, loss = 0.05691382\n",
      "Iteration 137, loss = 0.05642829\n",
      "Iteration 138, loss = 0.05595859\n",
      "Iteration 139, loss = 0.05554293\n",
      "Iteration 140, loss = 0.05487732\n",
      "Iteration 141, loss = 0.05429543\n",
      "Iteration 142, loss = 0.05379102\n",
      "Iteration 143, loss = 0.05344551\n",
      "Iteration 144, loss = 0.05298801\n",
      "Iteration 145, loss = 0.05256513\n",
      "Iteration 146, loss = 0.05208655\n",
      "Iteration 147, loss = 0.05170577\n",
      "Iteration 148, loss = 0.05117473\n",
      "Iteration 149, loss = 0.05072287\n",
      "Iteration 150, loss = 0.05035746\n",
      "Iteration 151, loss = 0.04982753\n",
      "Iteration 152, loss = 0.04947081\n",
      "Iteration 153, loss = 0.04906265\n",
      "Iteration 154, loss = 0.04858738\n",
      "Iteration 155, loss = 0.04823130\n",
      "Iteration 156, loss = 0.04777634\n",
      "Iteration 157, loss = 0.04746109\n",
      "Iteration 158, loss = 0.04714555\n",
      "Iteration 159, loss = 0.04669907\n",
      "Iteration 160, loss = 0.04627832\n",
      "Iteration 161, loss = 0.04584306\n",
      "Iteration 162, loss = 0.04559944\n",
      "Iteration 163, loss = 0.04522754\n",
      "Iteration 164, loss = 0.04466526\n",
      "Iteration 165, loss = 0.04441814\n",
      "Iteration 166, loss = 0.04397466\n",
      "Iteration 167, loss = 0.04377008\n",
      "Iteration 168, loss = 0.04364881\n",
      "Iteration 169, loss = 0.04309563\n",
      "Iteration 170, loss = 0.04293837\n",
      "Iteration 171, loss = 0.04237591\n",
      "Iteration 172, loss = 0.04211721\n",
      "Iteration 173, loss = 0.04179638\n",
      "Iteration 174, loss = 0.04134910\n",
      "Iteration 175, loss = 0.04108038\n",
      "Iteration 176, loss = 0.04088246\n",
      "Iteration 177, loss = 0.04054475\n",
      "Iteration 178, loss = 0.04024266\n",
      "Iteration 179, loss = 0.04002763\n",
      "Iteration 180, loss = 0.03958538\n",
      "Iteration 181, loss = 0.03945109\n",
      "Iteration 182, loss = 0.03905885\n",
      "Iteration 183, loss = 0.03884306\n",
      "Iteration 184, loss = 0.03855538\n",
      "Iteration 185, loss = 0.03836044\n",
      "Iteration 186, loss = 0.03790656\n",
      "Iteration 187, loss = 0.03773124\n",
      "Iteration 188, loss = 0.03750159\n",
      "Iteration 189, loss = 0.03711493\n",
      "Iteration 190, loss = 0.03686933\n",
      "Iteration 191, loss = 0.03655419\n",
      "Iteration 192, loss = 0.03636395\n",
      "Iteration 193, loss = 0.03617461\n",
      "Iteration 194, loss = 0.03582915\n",
      "Iteration 195, loss = 0.03564325\n",
      "Iteration 196, loss = 0.03536286\n",
      "Iteration 197, loss = 0.03515212\n",
      "Iteration 198, loss = 0.03472381\n",
      "Iteration 199, loss = 0.03478061\n",
      "Iteration 200, loss = 0.03450343\n",
      "[CV] END activation=relu, alpha=0.01, batch_size=200, hidden_layer_sizes=(100, 100, 100), solver=sgd; total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.12741955\n",
      "Iteration 2, loss = 1.39787467\n",
      "Iteration 3, loss = 0.79273692\n",
      "Iteration 4, loss = 0.57463189\n",
      "Iteration 5, loss = 0.47866886\n",
      "Iteration 6, loss = 0.42527414\n",
      "Iteration 7, loss = 0.39060658\n",
      "Iteration 8, loss = 0.36600790\n",
      "Iteration 9, loss = 0.34759917\n",
      "Iteration 10, loss = 0.33233373\n",
      "Iteration 11, loss = 0.31978830\n",
      "Iteration 12, loss = 0.30934005\n",
      "Iteration 13, loss = 0.29952386\n",
      "Iteration 14, loss = 0.29104613\n",
      "Iteration 15, loss = 0.28301740\n",
      "Iteration 16, loss = 0.27638700\n",
      "Iteration 17, loss = 0.26923796\n",
      "Iteration 18, loss = 0.26300648\n",
      "Iteration 19, loss = 0.25719288\n",
      "Iteration 20, loss = 0.25164303\n",
      "Iteration 21, loss = 0.24656606\n",
      "Iteration 22, loss = 0.24153824\n",
      "Iteration 23, loss = 0.23672482\n",
      "Iteration 24, loss = 0.23213761\n",
      "Iteration 25, loss = 0.22762702\n",
      "Iteration 26, loss = 0.22373768\n",
      "Iteration 27, loss = 0.21909188\n",
      "Iteration 28, loss = 0.21502554\n",
      "Iteration 29, loss = 0.21106516\n",
      "Iteration 30, loss = 0.20774170\n",
      "Iteration 31, loss = 0.20376563\n",
      "Iteration 32, loss = 0.20039578\n",
      "Iteration 33, loss = 0.19662612\n",
      "Iteration 34, loss = 0.19331263\n",
      "Iteration 35, loss = 0.18991680\n",
      "Iteration 36, loss = 0.18705688\n",
      "Iteration 37, loss = 0.18382012\n",
      "Iteration 38, loss = 0.18060935\n",
      "Iteration 39, loss = 0.17761592\n",
      "Iteration 40, loss = 0.17485771\n",
      "Iteration 41, loss = 0.17214261\n",
      "Iteration 42, loss = 0.16930350\n",
      "Iteration 43, loss = 0.16705240\n",
      "Iteration 44, loss = 0.16444823\n",
      "Iteration 45, loss = 0.16203070\n",
      "Iteration 46, loss = 0.15935403\n",
      "Iteration 47, loss = 0.15739957\n",
      "Iteration 48, loss = 0.15496058\n",
      "Iteration 49, loss = 0.15271968\n",
      "Iteration 50, loss = 0.15074168\n",
      "Iteration 51, loss = 0.14841751\n",
      "Iteration 52, loss = 0.14591277\n",
      "Iteration 53, loss = 0.14407079\n",
      "Iteration 54, loss = 0.14215357\n",
      "Iteration 55, loss = 0.14020965\n",
      "Iteration 56, loss = 0.13824559\n",
      "Iteration 57, loss = 0.13635975\n",
      "Iteration 58, loss = 0.13471666\n",
      "Iteration 59, loss = 0.13303095\n",
      "Iteration 60, loss = 0.13146329\n",
      "Iteration 61, loss = 0.12948070\n",
      "Iteration 62, loss = 0.12829933\n",
      "Iteration 63, loss = 0.12626116\n",
      "Iteration 64, loss = 0.12488459\n",
      "Iteration 65, loss = 0.12310496\n",
      "Iteration 66, loss = 0.12175281\n",
      "Iteration 67, loss = 0.12048670\n",
      "Iteration 68, loss = 0.11882344\n",
      "Iteration 69, loss = 0.11750523\n",
      "Iteration 70, loss = 0.11588400\n",
      "Iteration 71, loss = 0.11504858\n",
      "Iteration 72, loss = 0.11350706\n",
      "Iteration 73, loss = 0.11206212\n",
      "Iteration 74, loss = 0.11078453\n",
      "Iteration 75, loss = 0.10953838\n",
      "Iteration 76, loss = 0.10858458\n",
      "Iteration 77, loss = 0.10703600\n",
      "Iteration 78, loss = 0.10587737\n",
      "Iteration 79, loss = 0.10457935\n",
      "Iteration 80, loss = 0.10344620\n",
      "Iteration 81, loss = 0.10257512\n",
      "Iteration 82, loss = 0.10112313\n",
      "Iteration 83, loss = 0.10010452\n",
      "Iteration 84, loss = 0.09921546\n",
      "Iteration 85, loss = 0.09793137\n",
      "Iteration 86, loss = 0.09692603\n",
      "Iteration 87, loss = 0.09605960\n",
      "Iteration 88, loss = 0.09504931\n",
      "Iteration 89, loss = 0.09413994\n",
      "Iteration 90, loss = 0.09318574\n",
      "Iteration 91, loss = 0.09221575\n",
      "Iteration 92, loss = 0.09128021\n",
      "Iteration 93, loss = 0.09023499\n",
      "Iteration 94, loss = 0.08919995\n",
      "Iteration 95, loss = 0.08839138\n",
      "Iteration 96, loss = 0.08745116\n",
      "Iteration 97, loss = 0.08641389\n",
      "Iteration 98, loss = 0.08568888\n",
      "Iteration 99, loss = 0.08493283\n",
      "Iteration 100, loss = 0.08391732\n",
      "Iteration 101, loss = 0.08332665\n",
      "Iteration 102, loss = 0.08216615\n",
      "Iteration 103, loss = 0.08155358\n",
      "Iteration 104, loss = 0.08059448\n",
      "Iteration 105, loss = 0.08000396\n",
      "Iteration 106, loss = 0.07923807\n",
      "Iteration 107, loss = 0.07846316\n",
      "Iteration 108, loss = 0.07776776\n",
      "Iteration 109, loss = 0.07681031\n",
      "Iteration 110, loss = 0.07627312\n",
      "Iteration 111, loss = 0.07560600\n",
      "Iteration 112, loss = 0.07481493\n",
      "Iteration 113, loss = 0.07414424\n",
      "Iteration 114, loss = 0.07349698\n",
      "Iteration 115, loss = 0.07277337\n",
      "Iteration 116, loss = 0.07200648\n",
      "Iteration 117, loss = 0.07133536\n",
      "Iteration 118, loss = 0.07069251\n",
      "Iteration 119, loss = 0.07009791\n",
      "Iteration 120, loss = 0.06917391\n",
      "Iteration 121, loss = 0.06887314\n",
      "Iteration 122, loss = 0.06817180\n",
      "Iteration 123, loss = 0.06740469\n",
      "Iteration 124, loss = 0.06674923\n",
      "Iteration 125, loss = 0.06624471\n",
      "Iteration 126, loss = 0.06580513\n",
      "Iteration 127, loss = 0.06539033\n",
      "Iteration 128, loss = 0.06441148\n",
      "Iteration 129, loss = 0.06399195\n",
      "Iteration 130, loss = 0.06338306\n",
      "Iteration 131, loss = 0.06275881\n",
      "Iteration 132, loss = 0.06252325\n",
      "Iteration 133, loss = 0.06176232\n",
      "Iteration 134, loss = 0.06108562\n",
      "Iteration 135, loss = 0.06057129\n",
      "Iteration 136, loss = 0.05999247\n",
      "Iteration 137, loss = 0.05967726\n",
      "Iteration 138, loss = 0.05937272\n",
      "Iteration 139, loss = 0.05854991\n",
      "Iteration 140, loss = 0.05832728\n",
      "Iteration 141, loss = 0.05757875\n",
      "Iteration 142, loss = 0.05706640\n",
      "Iteration 143, loss = 0.05655471\n",
      "Iteration 144, loss = 0.05624811\n",
      "Iteration 145, loss = 0.05575874\n",
      "Iteration 146, loss = 0.05535231\n",
      "Iteration 147, loss = 0.05490124\n",
      "Iteration 148, loss = 0.05427679\n",
      "Iteration 149, loss = 0.05381424\n",
      "Iteration 150, loss = 0.05344977\n",
      "Iteration 151, loss = 0.05292423\n",
      "Iteration 152, loss = 0.05266460\n",
      "Iteration 153, loss = 0.05216968\n",
      "Iteration 154, loss = 0.05182770\n",
      "Iteration 155, loss = 0.05122048\n",
      "Iteration 156, loss = 0.05088553\n",
      "Iteration 157, loss = 0.05050262\n",
      "Iteration 158, loss = 0.05009710\n",
      "Iteration 159, loss = 0.04963425\n",
      "Iteration 160, loss = 0.04934823\n",
      "Iteration 161, loss = 0.04893642\n",
      "Iteration 162, loss = 0.04857536\n",
      "Iteration 163, loss = 0.04816406\n",
      "Iteration 164, loss = 0.04773073\n",
      "Iteration 165, loss = 0.04735002\n",
      "Iteration 166, loss = 0.04701660\n",
      "Iteration 167, loss = 0.04657230\n",
      "Iteration 168, loss = 0.04621830\n",
      "Iteration 169, loss = 0.04603050\n",
      "Iteration 170, loss = 0.04563351\n",
      "Iteration 171, loss = 0.04517820\n",
      "Iteration 172, loss = 0.04483091\n",
      "Iteration 173, loss = 0.04463688\n",
      "Iteration 174, loss = 0.04428956\n",
      "Iteration 175, loss = 0.04386908\n",
      "Iteration 176, loss = 0.04354996\n",
      "Iteration 177, loss = 0.04327536\n",
      "Iteration 178, loss = 0.04277788\n",
      "Iteration 179, loss = 0.04244893\n",
      "Iteration 180, loss = 0.04223461\n",
      "Iteration 181, loss = 0.04195266\n",
      "Iteration 182, loss = 0.04162691\n",
      "Iteration 183, loss = 0.04138195\n",
      "Iteration 184, loss = 0.04094865\n",
      "Iteration 185, loss = 0.04074543\n",
      "Iteration 186, loss = 0.04040737\n",
      "Iteration 187, loss = 0.04015930\n",
      "Iteration 188, loss = 0.03968380\n",
      "Iteration 189, loss = 0.03968971\n",
      "Iteration 190, loss = 0.03928906\n",
      "Iteration 191, loss = 0.03902133\n",
      "Iteration 192, loss = 0.03883796\n",
      "Iteration 193, loss = 0.03836994\n",
      "Iteration 194, loss = 0.03823537\n",
      "Iteration 195, loss = 0.03792995\n",
      "Iteration 196, loss = 0.03773043\n",
      "Iteration 197, loss = 0.03737724\n",
      "Iteration 198, loss = 0.03704946\n",
      "Iteration 199, loss = 0.03704051\n",
      "Iteration 200, loss = 0.03657865\n",
      "[CV] END activation=relu, alpha=0.01, batch_size=200, hidden_layer_sizes=(100, 100, 100), solver=sgd; total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.11926354\n",
      "Iteration 2, loss = 1.39427736\n",
      "Iteration 3, loss = 0.80148108\n",
      "Iteration 4, loss = 0.57379602\n",
      "Iteration 5, loss = 0.47353576\n",
      "Iteration 6, loss = 0.42113403\n",
      "Iteration 7, loss = 0.38858791\n",
      "Iteration 8, loss = 0.36615796\n",
      "Iteration 9, loss = 0.34886847\n",
      "Iteration 10, loss = 0.33521559\n",
      "Iteration 11, loss = 0.32340689\n",
      "Iteration 12, loss = 0.31360929\n",
      "Iteration 13, loss = 0.30390339\n",
      "Iteration 14, loss = 0.29563819\n",
      "Iteration 15, loss = 0.28836780\n",
      "Iteration 16, loss = 0.28175599\n",
      "Iteration 17, loss = 0.27521226\n",
      "Iteration 18, loss = 0.26900412\n",
      "Iteration 19, loss = 0.26346832\n",
      "Iteration 20, loss = 0.25757205\n",
      "Iteration 21, loss = 0.25220588\n",
      "Iteration 22, loss = 0.24732930\n",
      "Iteration 23, loss = 0.24237494\n",
      "Iteration 24, loss = 0.23776590\n",
      "Iteration 25, loss = 0.23297931\n",
      "Iteration 26, loss = 0.22843803\n",
      "Iteration 27, loss = 0.22453267\n",
      "Iteration 28, loss = 0.22001519\n",
      "Iteration 29, loss = 0.21624704\n",
      "Iteration 30, loss = 0.21203237\n",
      "Iteration 31, loss = 0.20847509\n",
      "Iteration 32, loss = 0.20508517\n",
      "Iteration 33, loss = 0.20146334\n",
      "Iteration 34, loss = 0.19846603\n",
      "Iteration 35, loss = 0.19473540\n",
      "Iteration 36, loss = 0.19175734\n",
      "Iteration 37, loss = 0.18841902\n",
      "Iteration 38, loss = 0.18550233\n",
      "Iteration 39, loss = 0.18262297\n",
      "Iteration 40, loss = 0.17993561\n",
      "Iteration 41, loss = 0.17664242\n",
      "Iteration 42, loss = 0.17406623\n",
      "Iteration 43, loss = 0.17139929\n",
      "Iteration 44, loss = 0.16901411\n",
      "Iteration 45, loss = 0.16643497\n",
      "Iteration 46, loss = 0.16395476\n",
      "Iteration 47, loss = 0.16133556\n",
      "Iteration 48, loss = 0.15922624\n",
      "Iteration 49, loss = 0.15697450\n",
      "Iteration 50, loss = 0.15512168\n",
      "Iteration 51, loss = 0.15260462\n",
      "Iteration 52, loss = 0.15061141\n",
      "Iteration 53, loss = 0.14839141\n",
      "Iteration 54, loss = 0.14653855\n",
      "Iteration 55, loss = 0.14445960\n",
      "Iteration 56, loss = 0.14239526\n",
      "Iteration 57, loss = 0.14084816\n",
      "Iteration 58, loss = 0.13889259\n",
      "Iteration 59, loss = 0.13709815\n",
      "Iteration 60, loss = 0.13545055\n",
      "Iteration 61, loss = 0.13380788\n",
      "Iteration 62, loss = 0.13178834\n",
      "Iteration 63, loss = 0.12990988\n",
      "Iteration 64, loss = 0.12863107\n",
      "Iteration 65, loss = 0.12725843\n",
      "Iteration 66, loss = 0.12538414\n",
      "Iteration 67, loss = 0.12402604\n",
      "Iteration 68, loss = 0.12216051\n",
      "Iteration 69, loss = 0.12081061\n",
      "Iteration 70, loss = 0.11924620\n",
      "Iteration 71, loss = 0.11803845\n",
      "Iteration 72, loss = 0.11679250\n",
      "Iteration 73, loss = 0.11525103\n",
      "Iteration 74, loss = 0.11399530\n",
      "Iteration 75, loss = 0.11233909\n",
      "Iteration 76, loss = 0.11101977\n",
      "Iteration 77, loss = 0.10995569\n",
      "Iteration 78, loss = 0.10878373\n",
      "Iteration 79, loss = 0.10737091\n",
      "Iteration 80, loss = 0.10605937\n",
      "Iteration 81, loss = 0.10494764\n",
      "Iteration 82, loss = 0.10376779\n",
      "Iteration 83, loss = 0.10257437\n",
      "Iteration 84, loss = 0.10159750\n",
      "Iteration 85, loss = 0.10025820\n",
      "Iteration 86, loss = 0.09938287\n",
      "Iteration 87, loss = 0.09838665\n",
      "Iteration 88, loss = 0.09714854\n",
      "Iteration 89, loss = 0.09624323\n",
      "Iteration 90, loss = 0.09508021\n",
      "Iteration 91, loss = 0.09392218\n",
      "Iteration 92, loss = 0.09284506\n",
      "Iteration 93, loss = 0.09172545\n",
      "Iteration 94, loss = 0.09114598\n",
      "Iteration 95, loss = 0.09009292\n",
      "Iteration 96, loss = 0.08905836\n",
      "Iteration 97, loss = 0.08808204\n",
      "Iteration 98, loss = 0.08719101\n",
      "Iteration 99, loss = 0.08625638\n",
      "Iteration 100, loss = 0.08557787\n",
      "Iteration 101, loss = 0.08451234\n",
      "Iteration 102, loss = 0.08391186\n",
      "Iteration 103, loss = 0.08298176\n",
      "Iteration 104, loss = 0.08212482\n",
      "Iteration 105, loss = 0.08112491\n",
      "Iteration 106, loss = 0.08021568\n",
      "Iteration 107, loss = 0.07972528\n",
      "Iteration 108, loss = 0.07883213\n",
      "Iteration 109, loss = 0.07790534\n",
      "Iteration 110, loss = 0.07720513\n",
      "Iteration 111, loss = 0.07665418\n",
      "Iteration 112, loss = 0.07575050\n",
      "Iteration 113, loss = 0.07491869\n",
      "Iteration 114, loss = 0.07410910\n",
      "Iteration 115, loss = 0.07354227\n",
      "Iteration 116, loss = 0.07295103\n",
      "Iteration 117, loss = 0.07223517\n",
      "Iteration 118, loss = 0.07141409\n",
      "Iteration 119, loss = 0.07058484\n",
      "Iteration 120, loss = 0.07011192\n",
      "Iteration 121, loss = 0.06923253\n",
      "Iteration 122, loss = 0.06883231\n",
      "Iteration 123, loss = 0.06810537\n",
      "Iteration 124, loss = 0.06734968\n",
      "Iteration 125, loss = 0.06688224\n",
      "Iteration 126, loss = 0.06617572\n",
      "Iteration 127, loss = 0.06567779\n",
      "Iteration 128, loss = 0.06523817\n",
      "Iteration 129, loss = 0.06442231\n",
      "Iteration 130, loss = 0.06362101\n",
      "Iteration 131, loss = 0.06320894\n",
      "Iteration 132, loss = 0.06268833\n",
      "Iteration 133, loss = 0.06206734\n",
      "Iteration 134, loss = 0.06154356\n",
      "Iteration 135, loss = 0.06081958\n",
      "Iteration 136, loss = 0.06042040\n",
      "Iteration 137, loss = 0.05996095\n",
      "Iteration 138, loss = 0.05928091\n",
      "Iteration 139, loss = 0.05883443\n",
      "Iteration 140, loss = 0.05844479\n",
      "Iteration 141, loss = 0.05781031\n",
      "Iteration 142, loss = 0.05735094\n",
      "Iteration 143, loss = 0.05701653\n",
      "Iteration 144, loss = 0.05660275\n",
      "Iteration 145, loss = 0.05588394\n",
      "Iteration 146, loss = 0.05540717\n",
      "Iteration 147, loss = 0.05488832\n",
      "Iteration 148, loss = 0.05452056\n",
      "Iteration 149, loss = 0.05384170\n",
      "Iteration 150, loss = 0.05367489\n",
      "Iteration 151, loss = 0.05314582\n",
      "Iteration 152, loss = 0.05284649\n",
      "Iteration 153, loss = 0.05224575\n",
      "Iteration 154, loss = 0.05186427\n",
      "Iteration 155, loss = 0.05140750\n",
      "Iteration 156, loss = 0.05117202\n",
      "Iteration 157, loss = 0.05057729\n",
      "Iteration 158, loss = 0.05028257\n",
      "Iteration 159, loss = 0.04964674\n",
      "Iteration 160, loss = 0.04924291\n",
      "Iteration 161, loss = 0.04896082\n",
      "Iteration 162, loss = 0.04849733\n",
      "Iteration 163, loss = 0.04803677\n",
      "Iteration 164, loss = 0.04781522\n",
      "Iteration 165, loss = 0.04735774\n",
      "Iteration 166, loss = 0.04694214\n",
      "Iteration 167, loss = 0.04653426\n",
      "Iteration 168, loss = 0.04625768\n",
      "Iteration 169, loss = 0.04605809\n",
      "Iteration 170, loss = 0.04539933\n",
      "Iteration 171, loss = 0.04507013\n",
      "Iteration 172, loss = 0.04479186\n",
      "Iteration 173, loss = 0.04446286\n",
      "Iteration 174, loss = 0.04420357\n",
      "Iteration 175, loss = 0.04383202\n",
      "Iteration 176, loss = 0.04343952\n",
      "Iteration 177, loss = 0.04319226\n",
      "Iteration 178, loss = 0.04273476\n",
      "Iteration 179, loss = 0.04254682\n",
      "Iteration 180, loss = 0.04215960\n",
      "Iteration 181, loss = 0.04200209\n",
      "Iteration 182, loss = 0.04152097\n",
      "Iteration 183, loss = 0.04126208\n",
      "Iteration 184, loss = 0.04098261\n",
      "Iteration 185, loss = 0.04073930\n",
      "Iteration 186, loss = 0.04028697\n",
      "Iteration 187, loss = 0.03999362\n",
      "Iteration 188, loss = 0.03976746\n",
      "Iteration 189, loss = 0.03950713\n",
      "Iteration 190, loss = 0.03919214\n",
      "Iteration 191, loss = 0.03901049\n",
      "Iteration 192, loss = 0.03859433\n",
      "Iteration 193, loss = 0.03856026\n",
      "Iteration 194, loss = 0.03808854\n",
      "Iteration 195, loss = 0.03791217\n",
      "Iteration 196, loss = 0.03758757\n",
      "Iteration 197, loss = 0.03739672\n",
      "Iteration 198, loss = 0.03713414\n",
      "Iteration 199, loss = 0.03666222\n",
      "Iteration 200, loss = 0.03647796\n",
      "[CV] END activation=relu, alpha=0.01, batch_size=200, hidden_layer_sizes=(100, 100, 100), solver=sgd; total time= 1.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.66680840\n",
      "Iteration 2, loss = 0.28125999\n",
      "Iteration 3, loss = 0.22149735\n",
      "Iteration 4, loss = 0.18570409\n",
      "Iteration 5, loss = 0.15975374\n",
      "Iteration 6, loss = 0.14005084\n",
      "Iteration 7, loss = 0.12282710\n",
      "Iteration 8, loss = 0.10886358\n",
      "Iteration 9, loss = 0.09714875\n",
      "Iteration 10, loss = 0.08704749\n",
      "Iteration 11, loss = 0.07808200\n",
      "Iteration 12, loss = 0.06965358\n",
      "Iteration 13, loss = 0.06304568\n",
      "Iteration 14, loss = 0.05667243\n",
      "Iteration 15, loss = 0.05162604\n",
      "Iteration 16, loss = 0.04629539\n",
      "Iteration 17, loss = 0.04200556\n",
      "Iteration 18, loss = 0.03816899\n",
      "Iteration 19, loss = 0.03459677\n",
      "Iteration 20, loss = 0.03158163\n",
      "Iteration 21, loss = 0.02833901\n",
      "Iteration 22, loss = 0.02600757\n",
      "Iteration 23, loss = 0.02356638\n",
      "Iteration 24, loss = 0.02173999\n",
      "Iteration 25, loss = 0.01995428\n",
      "Iteration 26, loss = 0.01835028\n",
      "Iteration 27, loss = 0.01674820\n",
      "Iteration 28, loss = 0.01553246\n",
      "Iteration 29, loss = 0.01440044\n",
      "Iteration 30, loss = 0.01336356\n",
      "Iteration 31, loss = 0.01257004\n",
      "Iteration 32, loss = 0.01177236\n",
      "Iteration 33, loss = 0.01134004\n",
      "Iteration 34, loss = 0.01058182\n",
      "Iteration 35, loss = 0.00991321\n",
      "Iteration 36, loss = 0.00960355\n",
      "Iteration 37, loss = 0.00912790\n",
      "Iteration 38, loss = 0.00883123\n",
      "Iteration 39, loss = 0.00855716\n",
      "Iteration 40, loss = 0.00826266\n",
      "Iteration 41, loss = 0.00806725\n",
      "Iteration 42, loss = 0.00765022\n",
      "Iteration 43, loss = 0.00766444\n",
      "Iteration 44, loss = 0.00744873\n",
      "Iteration 45, loss = 0.00723063\n",
      "Iteration 46, loss = 0.00750033\n",
      "Iteration 47, loss = 0.00696422\n",
      "Iteration 48, loss = 0.00664422\n",
      "Iteration 49, loss = 0.00663196\n",
      "Iteration 50, loss = 0.00664333\n",
      "Iteration 51, loss = 0.00646341\n",
      "Iteration 52, loss = 0.00661685\n",
      "Iteration 53, loss = 0.00663435\n",
      "Iteration 54, loss = 0.00625000\n",
      "Iteration 55, loss = 0.00603897\n",
      "Iteration 56, loss = 0.00645591\n",
      "Iteration 57, loss = 0.00609065\n",
      "Iteration 58, loss = 0.00593770\n",
      "Iteration 59, loss = 0.00584149\n",
      "Iteration 60, loss = 0.00702148\n",
      "Iteration 61, loss = 0.00587129\n",
      "Iteration 62, loss = 0.00561247\n",
      "Iteration 63, loss = 0.00558813\n",
      "Iteration 64, loss = 0.00567566\n",
      "Iteration 65, loss = 0.00625087\n",
      "Iteration 66, loss = 0.00676545\n",
      "Iteration 67, loss = 0.00547253\n",
      "Iteration 68, loss = 0.00538773\n",
      "Iteration 69, loss = 0.00546302\n",
      "Iteration 70, loss = 0.00601191\n",
      "Iteration 71, loss = 0.00684928\n",
      "Iteration 72, loss = 0.00544479\n",
      "Iteration 73, loss = 0.00529603\n",
      "Iteration 74, loss = 0.00529937\n",
      "Iteration 75, loss = 0.00543723\n",
      "Iteration 76, loss = 0.00691659\n",
      "Iteration 77, loss = 0.00534335\n",
      "Iteration 78, loss = 0.00520981\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.0001, batch_size=50, hidden_layer_sizes=(100,), solver=adam; total time= 2.0min\n",
      "Iteration 1, loss = 0.67208524\n",
      "Iteration 2, loss = 0.27988198\n",
      "Iteration 3, loss = 0.22197350\n",
      "Iteration 4, loss = 0.18669867\n",
      "Iteration 5, loss = 0.16064551\n",
      "Iteration 6, loss = 0.14087563\n",
      "Iteration 7, loss = 0.12479807\n",
      "Iteration 8, loss = 0.11130023\n",
      "Iteration 9, loss = 0.09927539\n",
      "Iteration 10, loss = 0.08899829\n",
      "Iteration 11, loss = 0.08003538\n",
      "Iteration 12, loss = 0.07198315\n",
      "Iteration 13, loss = 0.06537597\n",
      "Iteration 14, loss = 0.05935400\n",
      "Iteration 15, loss = 0.05343854\n",
      "Iteration 16, loss = 0.04843164\n",
      "Iteration 17, loss = 0.04394797\n",
      "Iteration 18, loss = 0.04004336\n",
      "Iteration 19, loss = 0.03650200\n",
      "Iteration 20, loss = 0.03330689\n",
      "Iteration 21, loss = 0.03024243\n",
      "Iteration 22, loss = 0.02747967\n",
      "Iteration 23, loss = 0.02501383\n",
      "Iteration 24, loss = 0.02315481\n",
      "Iteration 25, loss = 0.02097822\n",
      "Iteration 26, loss = 0.01926723\n",
      "Iteration 27, loss = 0.01794731\n",
      "Iteration 28, loss = 0.01649148\n",
      "Iteration 29, loss = 0.01534359\n",
      "Iteration 30, loss = 0.01423022\n",
      "Iteration 31, loss = 0.01327767\n",
      "Iteration 32, loss = 0.01245226\n",
      "Iteration 33, loss = 0.01182543\n",
      "Iteration 34, loss = 0.01114406\n",
      "Iteration 35, loss = 0.01052371\n",
      "Iteration 36, loss = 0.00998787\n",
      "Iteration 37, loss = 0.00971288\n",
      "Iteration 38, loss = 0.00924962\n",
      "Iteration 39, loss = 0.00899023\n",
      "Iteration 40, loss = 0.00851354\n",
      "Iteration 41, loss = 0.00844955\n",
      "Iteration 42, loss = 0.00807412\n",
      "Iteration 43, loss = 0.00808411\n",
      "Iteration 44, loss = 0.00759692\n",
      "Iteration 45, loss = 0.00743844\n",
      "Iteration 46, loss = 0.00726758\n",
      "Iteration 47, loss = 0.00720727\n",
      "Iteration 48, loss = 0.00722179\n",
      "Iteration 49, loss = 0.00693584\n",
      "Iteration 50, loss = 0.00669527\n",
      "Iteration 51, loss = 0.00739607\n",
      "Iteration 52, loss = 0.00660614\n",
      "Iteration 53, loss = 0.00631327\n",
      "Iteration 54, loss = 0.00657675\n",
      "Iteration 55, loss = 0.00655026\n",
      "Iteration 56, loss = 0.00613662\n",
      "Iteration 57, loss = 0.00649948\n",
      "Iteration 58, loss = 0.00671045\n",
      "Iteration 59, loss = 0.00596593\n",
      "Iteration 60, loss = 0.00592606\n",
      "Iteration 61, loss = 0.00594370\n",
      "Iteration 62, loss = 0.00631168\n",
      "Iteration 63, loss = 0.00674461\n",
      "Iteration 64, loss = 0.00591406\n",
      "Iteration 65, loss = 0.00562997\n",
      "Iteration 66, loss = 0.00610329\n",
      "Iteration 67, loss = 0.00586524\n",
      "Iteration 68, loss = 0.00581778\n",
      "Iteration 69, loss = 0.00568758\n",
      "Iteration 70, loss = 0.00583997\n",
      "Iteration 71, loss = 0.00574264\n",
      "Iteration 72, loss = 0.00601665\n",
      "Iteration 73, loss = 0.00618758\n",
      "Iteration 74, loss = 0.00563698\n",
      "Iteration 75, loss = 0.00542425\n",
      "Iteration 76, loss = 0.00547026\n",
      "Iteration 77, loss = 0.00725697\n",
      "Iteration 78, loss = 0.00614037\n",
      "Iteration 79, loss = 0.00534659\n",
      "Iteration 80, loss = 0.00525947\n",
      "Iteration 81, loss = 0.00525514\n",
      "Iteration 82, loss = 0.00542539\n",
      "Iteration 83, loss = 0.00639960\n",
      "Iteration 84, loss = 0.00530007\n",
      "Iteration 85, loss = 0.00535523\n",
      "Iteration 86, loss = 0.00692491\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.0001, batch_size=50, hidden_layer_sizes=(100,), solver=adam; total time= 2.2min\n",
      "Iteration 1, loss = 0.66716934\n",
      "Iteration 2, loss = 0.28031632\n",
      "Iteration 3, loss = 0.21973570\n",
      "Iteration 4, loss = 0.18289521\n",
      "Iteration 5, loss = 0.15757441\n",
      "Iteration 6, loss = 0.13743618\n",
      "Iteration 7, loss = 0.12190835\n",
      "Iteration 8, loss = 0.10883994\n",
      "Iteration 9, loss = 0.09730680\n",
      "Iteration 10, loss = 0.08695154\n",
      "Iteration 11, loss = 0.07846285\n",
      "Iteration 12, loss = 0.07019446\n",
      "Iteration 13, loss = 0.06374860\n",
      "Iteration 14, loss = 0.05773738\n",
      "Iteration 15, loss = 0.05203921\n",
      "Iteration 16, loss = 0.04726373\n",
      "Iteration 17, loss = 0.04277073\n",
      "Iteration 18, loss = 0.03840786\n",
      "Iteration 19, loss = 0.03520342\n",
      "Iteration 20, loss = 0.03210815\n",
      "Iteration 21, loss = 0.02899699\n",
      "Iteration 22, loss = 0.02649924\n",
      "Iteration 23, loss = 0.02418286\n",
      "Iteration 24, loss = 0.02196661\n",
      "Iteration 25, loss = 0.02008018\n",
      "Iteration 26, loss = 0.01834125\n",
      "Iteration 27, loss = 0.01683109\n",
      "Iteration 28, loss = 0.01554416\n",
      "Iteration 29, loss = 0.01451710\n",
      "Iteration 30, loss = 0.01342292\n",
      "Iteration 31, loss = 0.01249677\n",
      "Iteration 32, loss = 0.01195520\n",
      "Iteration 33, loss = 0.01111792\n",
      "Iteration 34, loss = 0.01048936\n",
      "Iteration 35, loss = 0.00997029\n",
      "Iteration 36, loss = 0.00945526\n",
      "Iteration 37, loss = 0.00910890\n",
      "Iteration 38, loss = 0.00881962\n",
      "Iteration 39, loss = 0.00863104\n",
      "Iteration 40, loss = 0.00810496\n",
      "Iteration 41, loss = 0.00815050\n",
      "Iteration 42, loss = 0.00775668\n",
      "Iteration 43, loss = 0.00760241\n",
      "Iteration 44, loss = 0.00729396\n",
      "Iteration 45, loss = 0.00719356\n",
      "Iteration 46, loss = 0.00704635\n",
      "Iteration 47, loss = 0.00712479\n",
      "Iteration 48, loss = 0.00680317\n",
      "Iteration 49, loss = 0.00667962\n",
      "Iteration 50, loss = 0.00649672\n",
      "Iteration 51, loss = 0.00689820\n",
      "Iteration 52, loss = 0.00707016\n",
      "Iteration 53, loss = 0.00610261\n",
      "Iteration 54, loss = 0.00605973\n",
      "Iteration 55, loss = 0.00613011\n",
      "Iteration 56, loss = 0.00669110\n",
      "Iteration 57, loss = 0.00675783\n",
      "Iteration 58, loss = 0.00583236\n",
      "Iteration 59, loss = 0.00575312\n",
      "Iteration 60, loss = 0.00603905\n",
      "Iteration 61, loss = 0.00778518\n",
      "Iteration 62, loss = 0.00573323\n",
      "Iteration 63, loss = 0.00559348\n",
      "Iteration 64, loss = 0.00559950\n",
      "Iteration 65, loss = 0.00651841\n",
      "Iteration 66, loss = 0.00642670\n",
      "Iteration 67, loss = 0.00553395\n",
      "Iteration 68, loss = 0.00543117\n",
      "Iteration 69, loss = 0.00576217\n",
      "Iteration 70, loss = 0.00571070\n",
      "Iteration 71, loss = 0.00548762\n",
      "Iteration 72, loss = 0.00602980\n",
      "Iteration 73, loss = 0.00666759\n",
      "Iteration 74, loss = 0.00538210\n",
      "Iteration 75, loss = 0.00530506\n",
      "Iteration 76, loss = 0.00535823\n",
      "Iteration 77, loss = 0.00562764\n",
      "Iteration 78, loss = 0.00682589\n",
      "Iteration 79, loss = 0.00535293\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.0001, batch_size=50, hidden_layer_sizes=(100,), solver=adam; total time= 2.0min\n",
      "Iteration 1, loss = 0.66445987\n",
      "Iteration 2, loss = 0.27964794\n",
      "Iteration 3, loss = 0.22128329\n",
      "Iteration 4, loss = 0.18436745\n",
      "Iteration 5, loss = 0.15825858\n",
      "Iteration 6, loss = 0.13820500\n",
      "Iteration 7, loss = 0.12210057\n",
      "Iteration 8, loss = 0.10886443\n",
      "Iteration 9, loss = 0.09701892\n",
      "Iteration 10, loss = 0.08762656\n",
      "Iteration 11, loss = 0.07892494\n",
      "Iteration 12, loss = 0.07137401\n",
      "Iteration 13, loss = 0.06451547\n",
      "Iteration 14, loss = 0.05849959\n",
      "Iteration 15, loss = 0.05302009\n",
      "Iteration 16, loss = 0.04783043\n",
      "Iteration 17, loss = 0.04357248\n",
      "Iteration 18, loss = 0.03959336\n",
      "Iteration 19, loss = 0.03606361\n",
      "Iteration 20, loss = 0.03292698\n",
      "Iteration 21, loss = 0.02992370\n",
      "Iteration 22, loss = 0.02726700\n",
      "Iteration 23, loss = 0.02491248\n",
      "Iteration 24, loss = 0.02301053\n",
      "Iteration 25, loss = 0.02098250\n",
      "Iteration 26, loss = 0.01910600\n",
      "Iteration 27, loss = 0.01799157\n",
      "Iteration 28, loss = 0.01660770\n",
      "Iteration 29, loss = 0.01547287\n",
      "Iteration 30, loss = 0.01436461\n",
      "Iteration 31, loss = 0.01344543\n",
      "Iteration 32, loss = 0.01271795\n",
      "Iteration 33, loss = 0.01193395\n",
      "Iteration 34, loss = 0.01136066\n",
      "Iteration 35, loss = 0.01067825\n",
      "Iteration 36, loss = 0.01008953\n",
      "Iteration 37, loss = 0.00976475\n",
      "Iteration 38, loss = 0.00933823\n",
      "Iteration 39, loss = 0.00895939\n",
      "Iteration 40, loss = 0.00873531\n",
      "Iteration 41, loss = 0.00843854\n",
      "Iteration 42, loss = 0.00810390\n",
      "Iteration 43, loss = 0.00784558\n",
      "Iteration 44, loss = 0.00780511\n",
      "Iteration 45, loss = 0.00775621\n",
      "Iteration 46, loss = 0.00745802\n",
      "Iteration 47, loss = 0.00725623\n",
      "Iteration 48, loss = 0.00712137\n",
      "Iteration 49, loss = 0.00702457\n",
      "Iteration 50, loss = 0.00672831\n",
      "Iteration 51, loss = 0.00750925\n",
      "Iteration 52, loss = 0.00672696\n",
      "Iteration 53, loss = 0.00670532\n",
      "Iteration 54, loss = 0.00680674\n",
      "Iteration 55, loss = 0.00630404\n",
      "Iteration 56, loss = 0.00635078\n",
      "Iteration 57, loss = 0.00660488\n",
      "Iteration 58, loss = 0.00613711\n",
      "Iteration 59, loss = 0.00608614\n",
      "Iteration 60, loss = 0.00612625\n",
      "Iteration 61, loss = 0.00745088\n",
      "Iteration 62, loss = 0.00639406\n",
      "Iteration 63, loss = 0.00577997\n",
      "Iteration 64, loss = 0.00573438\n",
      "Iteration 65, loss = 0.00699105\n",
      "Iteration 66, loss = 0.00629698\n",
      "Iteration 67, loss = 0.00564125\n",
      "Iteration 68, loss = 0.00557877\n",
      "Iteration 69, loss = 0.00563013\n",
      "Iteration 70, loss = 0.00682667\n",
      "Iteration 71, loss = 0.00571517\n",
      "Iteration 72, loss = 0.00548748\n",
      "Iteration 73, loss = 0.00552514\n",
      "Iteration 74, loss = 0.00569323\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.0001, batch_size=50, hidden_layer_sizes=(100,), solver=adam; total time= 1.9min\n",
      "Iteration 1, loss = 0.68338187\n",
      "Iteration 2, loss = 0.28340019\n",
      "Iteration 3, loss = 0.22339256\n",
      "Iteration 4, loss = 0.18529923\n",
      "Iteration 5, loss = 0.15829107\n",
      "Iteration 6, loss = 0.13731433\n",
      "Iteration 7, loss = 0.12048388\n",
      "Iteration 8, loss = 0.10688752\n",
      "Iteration 9, loss = 0.09486709\n",
      "Iteration 10, loss = 0.08450102\n",
      "Iteration 11, loss = 0.07575519\n",
      "Iteration 12, loss = 0.06789527\n",
      "Iteration 13, loss = 0.06109306\n",
      "Iteration 14, loss = 0.05499441\n",
      "Iteration 15, loss = 0.04983009\n",
      "Iteration 16, loss = 0.04466690\n",
      "Iteration 17, loss = 0.04087166\n",
      "Iteration 18, loss = 0.03647028\n",
      "Iteration 19, loss = 0.03325322\n",
      "Iteration 20, loss = 0.03017662\n",
      "Iteration 21, loss = 0.02752163\n",
      "Iteration 22, loss = 0.02498679\n",
      "Iteration 23, loss = 0.02280923\n",
      "Iteration 24, loss = 0.02095200\n",
      "Iteration 25, loss = 0.01906978\n",
      "Iteration 26, loss = 0.01771312\n",
      "Iteration 27, loss = 0.01613141\n",
      "Iteration 28, loss = 0.01500718\n",
      "Iteration 29, loss = 0.01388417\n",
      "Iteration 30, loss = 0.01302733\n",
      "Iteration 31, loss = 0.01216569\n",
      "Iteration 32, loss = 0.01144539\n",
      "Iteration 33, loss = 0.01080139\n",
      "Iteration 34, loss = 0.01026994\n",
      "Iteration 35, loss = 0.00971488\n",
      "Iteration 36, loss = 0.00940697\n",
      "Iteration 37, loss = 0.00902431\n",
      "Iteration 38, loss = 0.00847238\n",
      "Iteration 39, loss = 0.00835366\n",
      "Iteration 40, loss = 0.00816810\n",
      "Iteration 41, loss = 0.00777626\n",
      "Iteration 42, loss = 0.00754703\n",
      "Iteration 43, loss = 0.00783629\n",
      "Iteration 44, loss = 0.00730536\n",
      "Iteration 45, loss = 0.00698505\n",
      "Iteration 46, loss = 0.00697361\n",
      "Iteration 47, loss = 0.00688098\n",
      "Iteration 48, loss = 0.00671280\n",
      "Iteration 49, loss = 0.00666069\n",
      "Iteration 50, loss = 0.00752447\n",
      "Iteration 51, loss = 0.00656967\n",
      "Iteration 52, loss = 0.00612215\n",
      "Iteration 53, loss = 0.00631073\n",
      "Iteration 54, loss = 0.00634142\n",
      "Iteration 55, loss = 0.00610121\n",
      "Iteration 56, loss = 0.00678346\n",
      "Iteration 57, loss = 0.00591712\n",
      "Iteration 58, loss = 0.00585359\n",
      "Iteration 59, loss = 0.00632611\n",
      "Iteration 60, loss = 0.00593006\n",
      "Iteration 61, loss = 0.00664279\n",
      "Iteration 62, loss = 0.00597982\n",
      "Iteration 63, loss = 0.00561006\n",
      "Iteration 64, loss = 0.00558602\n",
      "Iteration 65, loss = 0.00589305\n",
      "Iteration 66, loss = 0.00589682\n",
      "Iteration 67, loss = 0.00562237\n",
      "Iteration 68, loss = 0.00578545\n",
      "Iteration 69, loss = 0.00707835\n",
      "Iteration 70, loss = 0.00551174\n",
      "Iteration 71, loss = 0.00534647\n",
      "Iteration 72, loss = 0.00535056\n",
      "Iteration 73, loss = 0.00596508\n",
      "Iteration 74, loss = 0.00803790\n",
      "Iteration 75, loss = 0.00545879\n",
      "Iteration 76, loss = 0.00530931\n",
      "Iteration 77, loss = 0.00526343\n",
      "Iteration 78, loss = 0.00546217\n",
      "Iteration 79, loss = 0.00782185\n",
      "Iteration 80, loss = 0.00540531\n",
      "Iteration 81, loss = 0.00523433\n",
      "Iteration 82, loss = 0.00520922\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.0001, batch_size=50, hidden_layer_sizes=(100,), solver=adam; total time= 2.1min\n",
      "Iteration 1, loss = 2.16283878\n",
      "Iteration 2, loss = 1.76582959\n",
      "Iteration 3, loss = 1.31723575\n",
      "Iteration 4, loss = 1.00794316\n",
      "Iteration 5, loss = 0.82315803\n",
      "Iteration 6, loss = 0.70726045\n",
      "Iteration 7, loss = 0.62899194\n",
      "Iteration 8, loss = 0.57302783\n",
      "Iteration 9, loss = 0.53107945\n",
      "Iteration 10, loss = 0.49844365\n",
      "Iteration 11, loss = 0.47249442\n",
      "Iteration 12, loss = 0.45130642\n",
      "Iteration 13, loss = 0.43366718\n",
      "Iteration 14, loss = 0.41868467\n",
      "Iteration 15, loss = 0.40602515\n",
      "Iteration 16, loss = 0.39471889\n",
      "Iteration 17, loss = 0.38502548\n",
      "Iteration 18, loss = 0.37648756\n",
      "Iteration 19, loss = 0.36862828\n",
      "Iteration 20, loss = 0.36178966\n",
      "Iteration 21, loss = 0.35535497\n",
      "Iteration 22, loss = 0.34951419\n",
      "Iteration 23, loss = 0.34426642\n",
      "Iteration 24, loss = 0.33915872\n",
      "Iteration 25, loss = 0.33466476\n",
      "Iteration 26, loss = 0.33042340\n",
      "Iteration 27, loss = 0.32644429\n",
      "Iteration 28, loss = 0.32257787\n",
      "Iteration 29, loss = 0.31897436\n",
      "Iteration 30, loss = 0.31558146\n",
      "Iteration 31, loss = 0.31245811\n",
      "Iteration 32, loss = 0.30932831\n",
      "Iteration 33, loss = 0.30641961\n",
      "Iteration 34, loss = 0.30366521\n",
      "Iteration 35, loss = 0.30092924\n",
      "Iteration 36, loss = 0.29833268\n",
      "Iteration 37, loss = 0.29583960\n",
      "Iteration 38, loss = 0.29334002\n",
      "Iteration 39, loss = 0.29104716\n",
      "Iteration 40, loss = 0.28878563\n",
      "Iteration 41, loss = 0.28657214\n",
      "Iteration 42, loss = 0.28434332\n",
      "Iteration 43, loss = 0.28229909\n",
      "Iteration 44, loss = 0.28026793\n",
      "Iteration 45, loss = 0.27823364\n",
      "Iteration 46, loss = 0.27626682\n",
      "Iteration 47, loss = 0.27442115\n",
      "Iteration 48, loss = 0.27250557\n",
      "Iteration 49, loss = 0.27060621\n",
      "Iteration 50, loss = 0.26889099\n",
      "Iteration 51, loss = 0.26707682\n",
      "Iteration 52, loss = 0.26540137\n",
      "Iteration 53, loss = 0.26369503\n",
      "Iteration 54, loss = 0.26197329\n",
      "Iteration 55, loss = 0.26027927\n",
      "Iteration 56, loss = 0.25870294\n",
      "Iteration 57, loss = 0.25709094\n",
      "Iteration 58, loss = 0.25545830\n",
      "Iteration 59, loss = 0.25399150\n",
      "Iteration 60, loss = 0.25244104\n",
      "Iteration 61, loss = 0.25089343\n",
      "Iteration 62, loss = 0.24936303\n",
      "Iteration 63, loss = 0.24789824\n",
      "Iteration 64, loss = 0.24635094\n",
      "Iteration 65, loss = 0.24492200\n",
      "Iteration 66, loss = 0.24346215\n",
      "Iteration 67, loss = 0.24213249\n",
      "Iteration 68, loss = 0.24072272\n",
      "Iteration 69, loss = 0.23920315\n",
      "Iteration 70, loss = 0.23798118\n",
      "Iteration 71, loss = 0.23654532\n",
      "Iteration 72, loss = 0.23519443\n",
      "Iteration 73, loss = 0.23388577\n",
      "Iteration 74, loss = 0.23263024\n",
      "Iteration 75, loss = 0.23127564\n",
      "Iteration 76, loss = 0.22993097\n",
      "Iteration 77, loss = 0.22868307\n",
      "Iteration 78, loss = 0.22749644\n",
      "Iteration 79, loss = 0.22626003\n",
      "Iteration 80, loss = 0.22502236\n",
      "Iteration 81, loss = 0.22378945\n",
      "Iteration 82, loss = 0.22257018\n",
      "Iteration 83, loss = 0.22138392\n",
      "Iteration 84, loss = 0.22025978\n",
      "Iteration 85, loss = 0.21898620\n",
      "Iteration 86, loss = 0.21785740\n",
      "Iteration 87, loss = 0.21676000\n",
      "Iteration 88, loss = 0.21561675\n",
      "Iteration 89, loss = 0.21442689\n",
      "Iteration 90, loss = 0.21338837\n",
      "Iteration 91, loss = 0.21222540\n",
      "Iteration 92, loss = 0.21113847\n",
      "Iteration 93, loss = 0.21010170\n",
      "Iteration 94, loss = 0.20895891\n",
      "Iteration 95, loss = 0.20788974\n",
      "Iteration 96, loss = 0.20687243\n",
      "Iteration 97, loss = 0.20587896\n",
      "Iteration 98, loss = 0.20483728\n",
      "Iteration 99, loss = 0.20384338\n",
      "Iteration 100, loss = 0.20278776\n",
      "Iteration 101, loss = 0.20175643\n",
      "Iteration 102, loss = 0.20078863\n",
      "Iteration 103, loss = 0.19984425\n",
      "Iteration 104, loss = 0.19878704\n",
      "Iteration 105, loss = 0.19801187\n",
      "Iteration 106, loss = 0.19691748\n",
      "Iteration 107, loss = 0.19600526\n",
      "Iteration 108, loss = 0.19504007\n",
      "Iteration 109, loss = 0.19411203\n",
      "Iteration 110, loss = 0.19313604\n",
      "Iteration 111, loss = 0.19225515\n",
      "Iteration 112, loss = 0.19138755\n",
      "Iteration 113, loss = 0.19047155\n",
      "Iteration 114, loss = 0.18959478\n",
      "Iteration 115, loss = 0.18866704\n",
      "Iteration 116, loss = 0.18790070\n",
      "Iteration 117, loss = 0.18696516\n",
      "Iteration 118, loss = 0.18605316\n",
      "Iteration 119, loss = 0.18530923\n",
      "Iteration 120, loss = 0.18443886\n",
      "Iteration 121, loss = 0.18357212\n",
      "Iteration 122, loss = 0.18272557\n",
      "Iteration 123, loss = 0.18195899\n",
      "Iteration 124, loss = 0.18116754\n",
      "Iteration 125, loss = 0.18036005\n",
      "Iteration 126, loss = 0.17952100\n",
      "Iteration 127, loss = 0.17870908\n",
      "Iteration 128, loss = 0.17794218\n",
      "Iteration 129, loss = 0.17716837\n",
      "Iteration 130, loss = 0.17641648\n",
      "Iteration 131, loss = 0.17561365\n",
      "Iteration 132, loss = 0.17482922\n",
      "Iteration 133, loss = 0.17411333\n",
      "Iteration 134, loss = 0.17336014\n",
      "Iteration 135, loss = 0.17258370\n",
      "Iteration 136, loss = 0.17192219\n",
      "Iteration 137, loss = 0.17112744\n",
      "Iteration 138, loss = 0.17038222\n",
      "Iteration 139, loss = 0.16969238\n",
      "Iteration 140, loss = 0.16891567\n",
      "Iteration 141, loss = 0.16832475\n",
      "Iteration 142, loss = 0.16752841\n",
      "Iteration 143, loss = 0.16689436\n",
      "Iteration 144, loss = 0.16615031\n",
      "Iteration 145, loss = 0.16543859\n",
      "Iteration 146, loss = 0.16479119\n",
      "Iteration 147, loss = 0.16409657\n",
      "Iteration 148, loss = 0.16346901\n",
      "Iteration 149, loss = 0.16278357\n",
      "Iteration 150, loss = 0.16209477\n",
      "Iteration 151, loss = 0.16143824\n",
      "Iteration 152, loss = 0.16073941\n",
      "Iteration 153, loss = 0.16007535\n",
      "Iteration 154, loss = 0.15947701\n",
      "Iteration 155, loss = 0.15889796\n",
      "Iteration 156, loss = 0.15822112\n",
      "Iteration 157, loss = 0.15756208\n",
      "Iteration 158, loss = 0.15693882\n",
      "Iteration 159, loss = 0.15634209\n",
      "Iteration 160, loss = 0.15568308\n",
      "Iteration 161, loss = 0.15516364\n",
      "Iteration 162, loss = 0.15446687\n",
      "Iteration 163, loss = 0.15384124\n",
      "Iteration 164, loss = 0.15327883\n",
      "Iteration 165, loss = 0.15268097\n",
      "Iteration 166, loss = 0.15209214\n",
      "Iteration 167, loss = 0.15147758\n",
      "Iteration 168, loss = 0.15088193\n",
      "Iteration 169, loss = 0.15028845\n",
      "Iteration 170, loss = 0.14973309\n",
      "Iteration 171, loss = 0.14918792\n",
      "Iteration 172, loss = 0.14862982\n",
      "Iteration 173, loss = 0.14804961\n",
      "Iteration 174, loss = 0.14751532\n",
      "Iteration 175, loss = 0.14686751\n",
      "Iteration 176, loss = 0.14639160\n",
      "Iteration 177, loss = 0.14585420\n",
      "Iteration 178, loss = 0.14526769\n",
      "Iteration 179, loss = 0.14472358\n",
      "Iteration 180, loss = 0.14422449\n",
      "Iteration 181, loss = 0.14368806\n",
      "Iteration 182, loss = 0.14312348\n",
      "Iteration 183, loss = 0.14264424\n",
      "Iteration 184, loss = 0.14204526\n",
      "Iteration 185, loss = 0.14155069\n",
      "Iteration 186, loss = 0.14102107\n",
      "Iteration 187, loss = 0.14055028\n",
      "Iteration 188, loss = 0.13996863\n",
      "Iteration 189, loss = 0.13957019\n",
      "Iteration 190, loss = 0.13902572\n",
      "Iteration 191, loss = 0.13854500\n",
      "Iteration 192, loss = 0.13800702\n",
      "Iteration 193, loss = 0.13748436\n",
      "Iteration 194, loss = 0.13702639\n",
      "Iteration 195, loss = 0.13653012\n",
      "Iteration 196, loss = 0.13604140\n",
      "Iteration 197, loss = 0.13558457\n",
      "Iteration 198, loss = 0.13509889\n",
      "Iteration 199, loss = 0.13463980\n",
      "Iteration 200, loss = 0.13411577\n",
      "[CV] END activation=logistic, alpha=0.0001, batch_size=50, hidden_layer_sizes=(100,), solver=sgd; total time= 3.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.14737219\n",
      "Iteration 2, loss = 1.73813705\n",
      "Iteration 3, loss = 1.29490838\n",
      "Iteration 4, loss = 0.99372749\n",
      "Iteration 5, loss = 0.81384253\n",
      "Iteration 6, loss = 0.70124319\n",
      "Iteration 7, loss = 0.62511300\n",
      "Iteration 8, loss = 0.57070340\n",
      "Iteration 9, loss = 0.52946007\n",
      "Iteration 10, loss = 0.49742316\n",
      "Iteration 11, loss = 0.47156219\n",
      "Iteration 12, loss = 0.45055904\n",
      "Iteration 13, loss = 0.43294311\n",
      "Iteration 14, loss = 0.41794698\n",
      "Iteration 15, loss = 0.40507198\n",
      "Iteration 16, loss = 0.39394034\n",
      "Iteration 17, loss = 0.38427747\n",
      "Iteration 18, loss = 0.37569015\n",
      "Iteration 19, loss = 0.36789078\n",
      "Iteration 20, loss = 0.36084748\n",
      "Iteration 21, loss = 0.35440925\n",
      "Iteration 22, loss = 0.34877718\n",
      "Iteration 23, loss = 0.34353886\n",
      "Iteration 24, loss = 0.33858427\n",
      "Iteration 25, loss = 0.33396606\n",
      "Iteration 26, loss = 0.32983448\n",
      "Iteration 27, loss = 0.32582388\n",
      "Iteration 28, loss = 0.32211955\n",
      "Iteration 29, loss = 0.31851451\n",
      "Iteration 30, loss = 0.31519946\n",
      "Iteration 31, loss = 0.31202403\n",
      "Iteration 32, loss = 0.30890264\n",
      "Iteration 33, loss = 0.30610467\n",
      "Iteration 34, loss = 0.30332329\n",
      "Iteration 35, loss = 0.30065386\n",
      "Iteration 36, loss = 0.29811166\n",
      "Iteration 37, loss = 0.29565695\n",
      "Iteration 38, loss = 0.29319637\n",
      "Iteration 39, loss = 0.29081564\n",
      "Iteration 40, loss = 0.28876681\n",
      "Iteration 41, loss = 0.28640910\n",
      "Iteration 42, loss = 0.28442961\n",
      "Iteration 43, loss = 0.28240028\n",
      "Iteration 44, loss = 0.28020529\n",
      "Iteration 45, loss = 0.27839124\n",
      "Iteration 46, loss = 0.27631718\n",
      "Iteration 47, loss = 0.27462653\n",
      "Iteration 48, loss = 0.27266376\n",
      "Iteration 49, loss = 0.27082754\n",
      "Iteration 50, loss = 0.26913196\n",
      "Iteration 51, loss = 0.26737143\n",
      "Iteration 52, loss = 0.26567590\n",
      "Iteration 53, loss = 0.26398910\n",
      "Iteration 54, loss = 0.26227303\n",
      "Iteration 55, loss = 0.26066963\n",
      "Iteration 56, loss = 0.25901906\n",
      "Iteration 57, loss = 0.25751475\n",
      "Iteration 58, loss = 0.25594256\n",
      "Iteration 59, loss = 0.25437187\n",
      "Iteration 60, loss = 0.25291954\n",
      "Iteration 61, loss = 0.25138187\n",
      "Iteration 62, loss = 0.24986350\n",
      "Iteration 63, loss = 0.24838181\n",
      "Iteration 64, loss = 0.24691900\n",
      "Iteration 65, loss = 0.24556715\n",
      "Iteration 66, loss = 0.24412522\n",
      "Iteration 67, loss = 0.24269174\n",
      "Iteration 68, loss = 0.24140171\n",
      "Iteration 69, loss = 0.23994098\n",
      "Iteration 70, loss = 0.23868186\n",
      "Iteration 71, loss = 0.23731517\n",
      "Iteration 72, loss = 0.23594360\n",
      "Iteration 73, loss = 0.23469139\n",
      "Iteration 74, loss = 0.23339520\n",
      "Iteration 75, loss = 0.23209242\n",
      "Iteration 76, loss = 0.23074007\n",
      "Iteration 77, loss = 0.22954047\n",
      "Iteration 78, loss = 0.22835011\n",
      "Iteration 79, loss = 0.22708964\n",
      "Iteration 80, loss = 0.22589152\n",
      "Iteration 81, loss = 0.22461402\n",
      "Iteration 82, loss = 0.22344226\n",
      "Iteration 83, loss = 0.22218701\n",
      "Iteration 84, loss = 0.22113049\n",
      "Iteration 85, loss = 0.21988051\n",
      "Iteration 86, loss = 0.21883189\n",
      "Iteration 87, loss = 0.21770259\n",
      "Iteration 88, loss = 0.21656208\n",
      "Iteration 89, loss = 0.21542515\n",
      "Iteration 90, loss = 0.21425563\n",
      "Iteration 91, loss = 0.21322720\n",
      "Iteration 92, loss = 0.21211251\n",
      "Iteration 93, loss = 0.21106636\n",
      "Iteration 94, loss = 0.20998039\n",
      "Iteration 95, loss = 0.20894344\n",
      "Iteration 96, loss = 0.20794176\n",
      "Iteration 97, loss = 0.20693857\n",
      "Iteration 98, loss = 0.20579757\n",
      "Iteration 99, loss = 0.20490248\n",
      "Iteration 100, loss = 0.20381316\n",
      "Iteration 101, loss = 0.20289622\n",
      "Iteration 102, loss = 0.20185708\n",
      "Iteration 103, loss = 0.20086661\n",
      "Iteration 104, loss = 0.19993491\n",
      "Iteration 105, loss = 0.19897869\n",
      "Iteration 106, loss = 0.19806232\n",
      "Iteration 107, loss = 0.19704366\n",
      "Iteration 108, loss = 0.19623002\n",
      "Iteration 109, loss = 0.19526006\n",
      "Iteration 110, loss = 0.19439798\n",
      "Iteration 111, loss = 0.19340897\n",
      "Iteration 112, loss = 0.19261736\n",
      "Iteration 113, loss = 0.19170336\n",
      "Iteration 114, loss = 0.19083419\n",
      "Iteration 115, loss = 0.18992203\n",
      "Iteration 116, loss = 0.18913580\n",
      "Iteration 117, loss = 0.18824570\n",
      "Iteration 118, loss = 0.18745367\n",
      "Iteration 119, loss = 0.18656554\n",
      "Iteration 120, loss = 0.18573318\n",
      "Iteration 121, loss = 0.18495397\n",
      "Iteration 122, loss = 0.18412998\n",
      "Iteration 123, loss = 0.18327003\n",
      "Iteration 124, loss = 0.18245479\n",
      "Iteration 125, loss = 0.18174565\n",
      "Iteration 126, loss = 0.18087369\n",
      "Iteration 127, loss = 0.18017764\n",
      "Iteration 128, loss = 0.17941587\n",
      "Iteration 129, loss = 0.17863008\n",
      "Iteration 130, loss = 0.17783698\n",
      "Iteration 131, loss = 0.17710702\n",
      "Iteration 132, loss = 0.17634252\n",
      "Iteration 133, loss = 0.17561779\n",
      "Iteration 134, loss = 0.17489573\n",
      "Iteration 135, loss = 0.17415600\n",
      "Iteration 136, loss = 0.17351618\n",
      "Iteration 137, loss = 0.17275331\n",
      "Iteration 138, loss = 0.17201298\n",
      "Iteration 139, loss = 0.17131756\n",
      "Iteration 140, loss = 0.17064336\n",
      "Iteration 141, loss = 0.16993336\n",
      "Iteration 142, loss = 0.16927305\n",
      "Iteration 143, loss = 0.16857397\n",
      "Iteration 144, loss = 0.16791870\n",
      "Iteration 145, loss = 0.16726845\n",
      "Iteration 146, loss = 0.16649734\n",
      "Iteration 147, loss = 0.16591373\n",
      "Iteration 148, loss = 0.16523879\n",
      "Iteration 149, loss = 0.16454079\n",
      "Iteration 150, loss = 0.16398020\n",
      "Iteration 151, loss = 0.16335197\n",
      "Iteration 152, loss = 0.16270990\n",
      "Iteration 153, loss = 0.16208714\n",
      "Iteration 154, loss = 0.16144938\n",
      "Iteration 155, loss = 0.16078744\n",
      "Iteration 156, loss = 0.16020574\n",
      "Iteration 157, loss = 0.15958610\n",
      "Iteration 158, loss = 0.15896058\n",
      "Iteration 159, loss = 0.15843887\n",
      "Iteration 160, loss = 0.15781437\n",
      "Iteration 161, loss = 0.15715121\n",
      "Iteration 162, loss = 0.15663327\n",
      "Iteration 163, loss = 0.15599965\n",
      "Iteration 164, loss = 0.15547576\n",
      "Iteration 165, loss = 0.15469208\n",
      "Iteration 166, loss = 0.15437000\n",
      "Iteration 167, loss = 0.15364119\n",
      "Iteration 168, loss = 0.15315481\n",
      "Iteration 169, loss = 0.15261295\n",
      "Iteration 170, loss = 0.15204040\n",
      "Iteration 171, loss = 0.15145392\n",
      "Iteration 172, loss = 0.15096230\n",
      "Iteration 173, loss = 0.15038481\n",
      "Iteration 174, loss = 0.14980756\n",
      "Iteration 175, loss = 0.14928642\n",
      "Iteration 176, loss = 0.14882161\n",
      "Iteration 177, loss = 0.14825183\n",
      "Iteration 178, loss = 0.14778674\n",
      "Iteration 179, loss = 0.14722144\n",
      "Iteration 180, loss = 0.14671374\n",
      "Iteration 181, loss = 0.14617018\n",
      "Iteration 182, loss = 0.14567393\n",
      "Iteration 183, loss = 0.14515001\n",
      "Iteration 184, loss = 0.14463496\n",
      "Iteration 185, loss = 0.14415290\n",
      "Iteration 186, loss = 0.14367358\n",
      "Iteration 187, loss = 0.14314196\n",
      "Iteration 188, loss = 0.14264187\n",
      "Iteration 189, loss = 0.14215954\n",
      "Iteration 190, loss = 0.14168938\n",
      "Iteration 191, loss = 0.14124077\n",
      "Iteration 192, loss = 0.14069140\n",
      "Iteration 193, loss = 0.14028140\n",
      "Iteration 194, loss = 0.13971909\n",
      "Iteration 195, loss = 0.13930192\n",
      "Iteration 196, loss = 0.13879101\n",
      "Iteration 197, loss = 0.13836805\n",
      "Iteration 198, loss = 0.13787943\n",
      "Iteration 199, loss = 0.13739363\n",
      "Iteration 200, loss = 0.13700010\n",
      "[CV] END activation=logistic, alpha=0.0001, batch_size=50, hidden_layer_sizes=(100,), solver=sgd; total time= 3.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.12877878\n",
      "Iteration 2, loss = 1.68483593\n",
      "Iteration 3, loss = 1.24836065\n",
      "Iteration 4, loss = 0.96601664\n",
      "Iteration 5, loss = 0.79781171\n",
      "Iteration 6, loss = 0.69077564\n",
      "Iteration 7, loss = 0.61753574\n",
      "Iteration 8, loss = 0.56427202\n",
      "Iteration 9, loss = 0.52371378\n",
      "Iteration 10, loss = 0.49196310\n",
      "Iteration 11, loss = 0.46626083\n",
      "Iteration 12, loss = 0.44538418\n",
      "Iteration 13, loss = 0.42791063\n",
      "Iteration 14, loss = 0.41296941\n",
      "Iteration 15, loss = 0.40036316\n",
      "Iteration 16, loss = 0.38914185\n",
      "Iteration 17, loss = 0.37953514\n",
      "Iteration 18, loss = 0.37088545\n",
      "Iteration 19, loss = 0.36314461\n",
      "Iteration 20, loss = 0.35635005\n",
      "Iteration 21, loss = 0.34995220\n",
      "Iteration 22, loss = 0.34435502\n",
      "Iteration 23, loss = 0.33890663\n",
      "Iteration 24, loss = 0.33413566\n",
      "Iteration 25, loss = 0.32954728\n",
      "Iteration 26, loss = 0.32536927\n",
      "Iteration 27, loss = 0.32130545\n",
      "Iteration 28, loss = 0.31759377\n",
      "Iteration 29, loss = 0.31412071\n",
      "Iteration 30, loss = 0.31081607\n",
      "Iteration 31, loss = 0.30754184\n",
      "Iteration 32, loss = 0.30463739\n",
      "Iteration 33, loss = 0.30172912\n",
      "Iteration 34, loss = 0.29895107\n",
      "Iteration 35, loss = 0.29623299\n",
      "Iteration 36, loss = 0.29362641\n",
      "Iteration 37, loss = 0.29126943\n",
      "Iteration 38, loss = 0.28878837\n",
      "Iteration 39, loss = 0.28666456\n",
      "Iteration 40, loss = 0.28425546\n",
      "Iteration 41, loss = 0.28218361\n",
      "Iteration 42, loss = 0.28000567\n",
      "Iteration 43, loss = 0.27797052\n",
      "Iteration 44, loss = 0.27601841\n",
      "Iteration 45, loss = 0.27403399\n",
      "Iteration 46, loss = 0.27217997\n",
      "Iteration 47, loss = 0.27021297\n",
      "Iteration 48, loss = 0.26850057\n",
      "Iteration 49, loss = 0.26667563\n",
      "Iteration 50, loss = 0.26499221\n",
      "Iteration 51, loss = 0.26319608\n",
      "Iteration 52, loss = 0.26155484\n",
      "Iteration 53, loss = 0.25990325\n",
      "Iteration 54, loss = 0.25819595\n",
      "Iteration 55, loss = 0.25667970\n",
      "Iteration 56, loss = 0.25507612\n",
      "Iteration 57, loss = 0.25347040\n",
      "Iteration 58, loss = 0.25204506\n",
      "Iteration 59, loss = 0.25047320\n",
      "Iteration 60, loss = 0.24899362\n",
      "Iteration 61, loss = 0.24755426\n",
      "Iteration 62, loss = 0.24607364\n",
      "Iteration 63, loss = 0.24462010\n",
      "Iteration 64, loss = 0.24336740\n",
      "Iteration 65, loss = 0.24189939\n",
      "Iteration 66, loss = 0.24056868\n",
      "Iteration 67, loss = 0.23914946\n",
      "Iteration 68, loss = 0.23788714\n",
      "Iteration 69, loss = 0.23642361\n",
      "Iteration 70, loss = 0.23511390\n",
      "Iteration 71, loss = 0.23380627\n",
      "Iteration 72, loss = 0.23266856\n",
      "Iteration 73, loss = 0.23131077\n",
      "Iteration 74, loss = 0.23010364\n",
      "Iteration 75, loss = 0.22874694\n",
      "Iteration 76, loss = 0.22764256\n",
      "Iteration 77, loss = 0.22638794\n",
      "Iteration 78, loss = 0.22517150\n",
      "Iteration 79, loss = 0.22400307\n",
      "Iteration 80, loss = 0.22278881\n",
      "Iteration 81, loss = 0.22163395\n",
      "Iteration 82, loss = 0.22052565\n",
      "Iteration 83, loss = 0.21935904\n",
      "Iteration 84, loss = 0.21823204\n",
      "Iteration 85, loss = 0.21705246\n",
      "Iteration 86, loss = 0.21597102\n",
      "Iteration 87, loss = 0.21485788\n",
      "Iteration 88, loss = 0.21378259\n",
      "Iteration 89, loss = 0.21268157\n",
      "Iteration 90, loss = 0.21159522\n",
      "Iteration 91, loss = 0.21053213\n",
      "Iteration 92, loss = 0.20948733\n",
      "Iteration 93, loss = 0.20847350\n",
      "Iteration 94, loss = 0.20736538\n",
      "Iteration 95, loss = 0.20637768\n",
      "Iteration 96, loss = 0.20535387\n",
      "Iteration 97, loss = 0.20434741\n",
      "Iteration 98, loss = 0.20335883\n",
      "Iteration 99, loss = 0.20238653\n",
      "Iteration 100, loss = 0.20139934\n",
      "Iteration 101, loss = 0.20040755\n",
      "Iteration 102, loss = 0.19942697\n",
      "Iteration 103, loss = 0.19844425\n",
      "Iteration 104, loss = 0.19743522\n",
      "Iteration 105, loss = 0.19665441\n",
      "Iteration 106, loss = 0.19569783\n",
      "Iteration 107, loss = 0.19475946\n",
      "Iteration 108, loss = 0.19388472\n",
      "Iteration 109, loss = 0.19299950\n",
      "Iteration 110, loss = 0.19207842\n",
      "Iteration 111, loss = 0.19116567\n",
      "Iteration 112, loss = 0.19025905\n",
      "Iteration 113, loss = 0.18939706\n",
      "Iteration 114, loss = 0.18864071\n",
      "Iteration 115, loss = 0.18767609\n",
      "Iteration 116, loss = 0.18686106\n",
      "Iteration 117, loss = 0.18607244\n",
      "Iteration 118, loss = 0.18516743\n",
      "Iteration 119, loss = 0.18433226\n",
      "Iteration 120, loss = 0.18351635\n",
      "Iteration 121, loss = 0.18275892\n",
      "Iteration 122, loss = 0.18187700\n",
      "Iteration 123, loss = 0.18101813\n",
      "Iteration 124, loss = 0.18031313\n",
      "Iteration 125, loss = 0.17951521\n",
      "Iteration 126, loss = 0.17870973\n",
      "Iteration 127, loss = 0.17793905\n",
      "Iteration 128, loss = 0.17718026\n",
      "Iteration 129, loss = 0.17639613\n",
      "Iteration 130, loss = 0.17562748\n",
      "Iteration 131, loss = 0.17491467\n",
      "Iteration 132, loss = 0.17412662\n",
      "Iteration 133, loss = 0.17339927\n",
      "Iteration 134, loss = 0.17266660\n",
      "Iteration 135, loss = 0.17192583\n",
      "Iteration 136, loss = 0.17122785\n",
      "Iteration 137, loss = 0.17055170\n",
      "Iteration 138, loss = 0.16981159\n",
      "Iteration 139, loss = 0.16904115\n",
      "Iteration 140, loss = 0.16845364\n",
      "Iteration 141, loss = 0.16771090\n",
      "Iteration 142, loss = 0.16703983\n",
      "Iteration 143, loss = 0.16633410\n",
      "Iteration 144, loss = 0.16567067\n",
      "Iteration 145, loss = 0.16493491\n",
      "Iteration 146, loss = 0.16437871\n",
      "Iteration 147, loss = 0.16362095\n",
      "Iteration 148, loss = 0.16299786\n",
      "Iteration 149, loss = 0.16236731\n",
      "Iteration 150, loss = 0.16175347\n",
      "Iteration 151, loss = 0.16102146\n",
      "Iteration 152, loss = 0.16042932\n",
      "Iteration 153, loss = 0.15978071\n",
      "Iteration 154, loss = 0.15914774\n",
      "Iteration 155, loss = 0.15851930\n",
      "Iteration 156, loss = 0.15789418\n",
      "Iteration 157, loss = 0.15729879\n",
      "Iteration 158, loss = 0.15667677\n",
      "Iteration 159, loss = 0.15607013\n",
      "Iteration 160, loss = 0.15544387\n",
      "Iteration 161, loss = 0.15486493\n",
      "Iteration 162, loss = 0.15428256\n",
      "Iteration 163, loss = 0.15366238\n",
      "Iteration 164, loss = 0.15309016\n",
      "Iteration 165, loss = 0.15244386\n",
      "Iteration 166, loss = 0.15194854\n",
      "Iteration 167, loss = 0.15136079\n",
      "Iteration 168, loss = 0.15075231\n",
      "Iteration 169, loss = 0.15021980\n",
      "Iteration 170, loss = 0.14963373\n",
      "Iteration 171, loss = 0.14903692\n",
      "Iteration 172, loss = 0.14854766\n",
      "Iteration 173, loss = 0.14793938\n",
      "Iteration 174, loss = 0.14744525\n",
      "Iteration 175, loss = 0.14683835\n",
      "Iteration 176, loss = 0.14635336\n",
      "Iteration 177, loss = 0.14583189\n",
      "Iteration 178, loss = 0.14521428\n",
      "Iteration 179, loss = 0.14471607\n",
      "Iteration 180, loss = 0.14421015\n",
      "Iteration 181, loss = 0.14368795\n",
      "Iteration 182, loss = 0.14314860\n",
      "Iteration 183, loss = 0.14256959\n",
      "Iteration 184, loss = 0.14217051\n",
      "Iteration 185, loss = 0.14161504\n",
      "Iteration 186, loss = 0.14109240\n",
      "Iteration 187, loss = 0.14057025\n",
      "Iteration 188, loss = 0.14008777\n",
      "Iteration 189, loss = 0.13958244\n",
      "Iteration 190, loss = 0.13914585\n",
      "Iteration 191, loss = 0.13861647\n",
      "Iteration 192, loss = 0.13816572\n",
      "Iteration 193, loss = 0.13764353\n",
      "Iteration 194, loss = 0.13717938\n",
      "Iteration 195, loss = 0.13672755\n",
      "Iteration 196, loss = 0.13616769\n",
      "Iteration 197, loss = 0.13573644\n",
      "Iteration 198, loss = 0.13527940\n",
      "Iteration 199, loss = 0.13479448\n",
      "Iteration 200, loss = 0.13433691\n",
      "[CV] END activation=logistic, alpha=0.0001, batch_size=50, hidden_layer_sizes=(100,), solver=sgd; total time= 3.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.12989203\n",
      "Iteration 2, loss = 1.70570734\n",
      "Iteration 3, loss = 1.26976766\n",
      "Iteration 4, loss = 0.98120970\n",
      "Iteration 5, loss = 0.80867642\n",
      "Iteration 6, loss = 0.69873004\n",
      "Iteration 7, loss = 0.62374066\n",
      "Iteration 8, loss = 0.56944137\n",
      "Iteration 9, loss = 0.52838393\n",
      "Iteration 10, loss = 0.49619350\n",
      "Iteration 11, loss = 0.47040754\n",
      "Iteration 12, loss = 0.44938430\n",
      "Iteration 13, loss = 0.43199404\n",
      "Iteration 14, loss = 0.41714320\n",
      "Iteration 15, loss = 0.40430527\n",
      "Iteration 16, loss = 0.39324653\n",
      "Iteration 17, loss = 0.38357193\n",
      "Iteration 18, loss = 0.37490086\n",
      "Iteration 19, loss = 0.36720323\n",
      "Iteration 20, loss = 0.36038069\n",
      "Iteration 21, loss = 0.35385686\n",
      "Iteration 22, loss = 0.34816823\n",
      "Iteration 23, loss = 0.34278778\n",
      "Iteration 24, loss = 0.33793709\n",
      "Iteration 25, loss = 0.33331974\n",
      "Iteration 26, loss = 0.32904726\n",
      "Iteration 27, loss = 0.32497584\n",
      "Iteration 28, loss = 0.32118482\n",
      "Iteration 29, loss = 0.31752286\n",
      "Iteration 30, loss = 0.31421507\n",
      "Iteration 31, loss = 0.31095882\n",
      "Iteration 32, loss = 0.30788915\n",
      "Iteration 33, loss = 0.30503375\n",
      "Iteration 34, loss = 0.30213174\n",
      "Iteration 35, loss = 0.29946613\n",
      "Iteration 36, loss = 0.29683555\n",
      "Iteration 37, loss = 0.29432209\n",
      "Iteration 38, loss = 0.29187526\n",
      "Iteration 39, loss = 0.28954558\n",
      "Iteration 40, loss = 0.28730362\n",
      "Iteration 41, loss = 0.28506164\n",
      "Iteration 42, loss = 0.28290890\n",
      "Iteration 43, loss = 0.28087669\n",
      "Iteration 44, loss = 0.27891619\n",
      "Iteration 45, loss = 0.27679161\n",
      "Iteration 46, loss = 0.27490778\n",
      "Iteration 47, loss = 0.27309606\n",
      "Iteration 48, loss = 0.27114310\n",
      "Iteration 49, loss = 0.26939711\n",
      "Iteration 50, loss = 0.26752324\n",
      "Iteration 51, loss = 0.26579606\n",
      "Iteration 52, loss = 0.26418964\n",
      "Iteration 53, loss = 0.26237031\n",
      "Iteration 54, loss = 0.26077146\n",
      "Iteration 55, loss = 0.25907144\n",
      "Iteration 56, loss = 0.25748989\n",
      "Iteration 57, loss = 0.25593730\n",
      "Iteration 58, loss = 0.25438861\n",
      "Iteration 59, loss = 0.25285341\n",
      "Iteration 60, loss = 0.25125635\n",
      "Iteration 61, loss = 0.24983150\n",
      "Iteration 62, loss = 0.24832066\n",
      "Iteration 63, loss = 0.24690654\n",
      "Iteration 64, loss = 0.24543052\n",
      "Iteration 65, loss = 0.24405996\n",
      "Iteration 66, loss = 0.24251529\n",
      "Iteration 67, loss = 0.24120686\n",
      "Iteration 68, loss = 0.23990373\n",
      "Iteration 69, loss = 0.23851681\n",
      "Iteration 70, loss = 0.23724228\n",
      "Iteration 71, loss = 0.23585319\n",
      "Iteration 72, loss = 0.23449190\n",
      "Iteration 73, loss = 0.23327003\n",
      "Iteration 74, loss = 0.23195146\n",
      "Iteration 75, loss = 0.23069980\n",
      "Iteration 76, loss = 0.22941141\n",
      "Iteration 77, loss = 0.22817242\n",
      "Iteration 78, loss = 0.22693506\n",
      "Iteration 79, loss = 0.22571770\n",
      "Iteration 80, loss = 0.22453447\n",
      "Iteration 81, loss = 0.22334409\n",
      "Iteration 82, loss = 0.22215312\n",
      "Iteration 83, loss = 0.22097084\n",
      "Iteration 84, loss = 0.21981763\n",
      "Iteration 85, loss = 0.21868286\n",
      "Iteration 86, loss = 0.21755874\n",
      "Iteration 87, loss = 0.21646490\n",
      "Iteration 88, loss = 0.21532717\n",
      "Iteration 89, loss = 0.21418888\n",
      "Iteration 90, loss = 0.21306005\n",
      "Iteration 91, loss = 0.21201677\n",
      "Iteration 92, loss = 0.21090750\n",
      "Iteration 93, loss = 0.20989854\n",
      "Iteration 94, loss = 0.20880068\n",
      "Iteration 95, loss = 0.20778044\n",
      "Iteration 96, loss = 0.20675571\n",
      "Iteration 97, loss = 0.20572095\n",
      "Iteration 98, loss = 0.20474348\n",
      "Iteration 99, loss = 0.20363934\n",
      "Iteration 100, loss = 0.20268099\n",
      "Iteration 101, loss = 0.20172341\n",
      "Iteration 102, loss = 0.20067328\n",
      "Iteration 103, loss = 0.19969841\n",
      "Iteration 104, loss = 0.19867086\n",
      "Iteration 105, loss = 0.19785578\n",
      "Iteration 106, loss = 0.19684737\n",
      "Iteration 107, loss = 0.19588718\n",
      "Iteration 108, loss = 0.19498972\n",
      "Iteration 109, loss = 0.19396445\n",
      "Iteration 110, loss = 0.19308091\n",
      "Iteration 111, loss = 0.19229156\n",
      "Iteration 112, loss = 0.19125954\n",
      "Iteration 113, loss = 0.19043645\n",
      "Iteration 114, loss = 0.18950999\n",
      "Iteration 115, loss = 0.18862498\n",
      "Iteration 116, loss = 0.18772146\n",
      "Iteration 117, loss = 0.18687608\n",
      "Iteration 118, loss = 0.18605401\n",
      "Iteration 119, loss = 0.18515731\n",
      "Iteration 120, loss = 0.18429324\n",
      "Iteration 121, loss = 0.18347421\n",
      "Iteration 122, loss = 0.18265111\n",
      "Iteration 123, loss = 0.18184090\n",
      "Iteration 124, loss = 0.18095402\n",
      "Iteration 125, loss = 0.18020381\n",
      "Iteration 126, loss = 0.17939461\n",
      "Iteration 127, loss = 0.17857183\n",
      "Iteration 128, loss = 0.17771334\n",
      "Iteration 129, loss = 0.17702087\n",
      "Iteration 130, loss = 0.17626335\n",
      "Iteration 131, loss = 0.17541400\n",
      "Iteration 132, loss = 0.17464981\n",
      "Iteration 133, loss = 0.17389619\n",
      "Iteration 134, loss = 0.17315814\n",
      "Iteration 135, loss = 0.17237781\n",
      "Iteration 136, loss = 0.17166968\n",
      "Iteration 137, loss = 0.17090483\n",
      "Iteration 138, loss = 0.17015368\n",
      "Iteration 139, loss = 0.16942355\n",
      "Iteration 140, loss = 0.16869184\n",
      "Iteration 141, loss = 0.16799996\n",
      "Iteration 142, loss = 0.16725046\n",
      "Iteration 143, loss = 0.16656978\n",
      "Iteration 144, loss = 0.16589899\n",
      "Iteration 145, loss = 0.16515492\n",
      "Iteration 146, loss = 0.16448676\n",
      "Iteration 147, loss = 0.16380364\n",
      "Iteration 148, loss = 0.16308530\n",
      "Iteration 149, loss = 0.16237539\n",
      "Iteration 150, loss = 0.16173438\n",
      "Iteration 151, loss = 0.16105993\n",
      "Iteration 152, loss = 0.16044955\n",
      "Iteration 153, loss = 0.15974456\n",
      "Iteration 154, loss = 0.15904658\n",
      "Iteration 155, loss = 0.15853213\n",
      "Iteration 156, loss = 0.15781003\n",
      "Iteration 157, loss = 0.15720851\n",
      "Iteration 158, loss = 0.15655746\n",
      "Iteration 159, loss = 0.15591554\n",
      "Iteration 160, loss = 0.15523325\n",
      "Iteration 161, loss = 0.15471138\n",
      "Iteration 162, loss = 0.15404522\n",
      "Iteration 163, loss = 0.15345270\n",
      "Iteration 164, loss = 0.15284070\n",
      "Iteration 165, loss = 0.15222697\n",
      "Iteration 166, loss = 0.15163210\n",
      "Iteration 167, loss = 0.15101019\n",
      "Iteration 168, loss = 0.15037040\n",
      "Iteration 169, loss = 0.14985196\n",
      "Iteration 170, loss = 0.14927116\n",
      "Iteration 171, loss = 0.14869727\n",
      "Iteration 172, loss = 0.14814738\n",
      "Iteration 173, loss = 0.14749614\n",
      "Iteration 174, loss = 0.14705630\n",
      "Iteration 175, loss = 0.14639280\n",
      "Iteration 176, loss = 0.14590205\n",
      "Iteration 177, loss = 0.14529826\n",
      "Iteration 178, loss = 0.14483258\n",
      "Iteration 179, loss = 0.14424545\n",
      "Iteration 180, loss = 0.14370999\n",
      "Iteration 181, loss = 0.14311120\n",
      "Iteration 182, loss = 0.14264456\n",
      "Iteration 183, loss = 0.14209189\n",
      "Iteration 184, loss = 0.14157029\n",
      "Iteration 185, loss = 0.14110501\n",
      "Iteration 186, loss = 0.14053086\n",
      "Iteration 187, loss = 0.14002309\n",
      "Iteration 188, loss = 0.13947854\n",
      "Iteration 189, loss = 0.13896002\n",
      "Iteration 190, loss = 0.13846068\n",
      "Iteration 191, loss = 0.13800382\n",
      "Iteration 192, loss = 0.13748176\n",
      "Iteration 193, loss = 0.13707055\n",
      "Iteration 194, loss = 0.13643500\n",
      "Iteration 195, loss = 0.13603805\n",
      "Iteration 196, loss = 0.13544147\n",
      "Iteration 197, loss = 0.13510164\n",
      "Iteration 198, loss = 0.13455760\n",
      "Iteration 199, loss = 0.13413405\n",
      "Iteration 200, loss = 0.13361675\n",
      "[CV] END activation=logistic, alpha=0.0001, batch_size=50, hidden_layer_sizes=(100,), solver=sgd; total time= 3.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.16000913\n",
      "Iteration 2, loss = 1.75496361\n",
      "Iteration 3, loss = 1.31331723\n",
      "Iteration 4, loss = 1.00546911\n",
      "Iteration 5, loss = 0.82071389\n",
      "Iteration 6, loss = 0.70553503\n",
      "Iteration 7, loss = 0.62799136\n",
      "Iteration 8, loss = 0.57229735\n",
      "Iteration 9, loss = 0.53052417\n",
      "Iteration 10, loss = 0.49792576\n",
      "Iteration 11, loss = 0.47179961\n",
      "Iteration 12, loss = 0.45047995\n",
      "Iteration 13, loss = 0.43272691\n",
      "Iteration 14, loss = 0.41781240\n",
      "Iteration 15, loss = 0.40493472\n",
      "Iteration 16, loss = 0.39377649\n",
      "Iteration 17, loss = 0.38406360\n",
      "Iteration 18, loss = 0.37537666\n",
      "Iteration 19, loss = 0.36761469\n",
      "Iteration 20, loss = 0.36076566\n",
      "Iteration 21, loss = 0.35448398\n",
      "Iteration 22, loss = 0.34861145\n",
      "Iteration 23, loss = 0.34342184\n",
      "Iteration 24, loss = 0.33842680\n",
      "Iteration 25, loss = 0.33386313\n",
      "Iteration 26, loss = 0.32973575\n",
      "Iteration 27, loss = 0.32575262\n",
      "Iteration 28, loss = 0.32203197\n",
      "Iteration 29, loss = 0.31860824\n",
      "Iteration 30, loss = 0.31520818\n",
      "Iteration 31, loss = 0.31199020\n",
      "Iteration 32, loss = 0.30899284\n",
      "Iteration 33, loss = 0.30615033\n",
      "Iteration 34, loss = 0.30344580\n",
      "Iteration 35, loss = 0.30069521\n",
      "Iteration 36, loss = 0.29829082\n",
      "Iteration 37, loss = 0.29587014\n",
      "Iteration 38, loss = 0.29347673\n",
      "Iteration 39, loss = 0.29113513\n",
      "Iteration 40, loss = 0.28888441\n",
      "Iteration 41, loss = 0.28682749\n",
      "Iteration 42, loss = 0.28476333\n",
      "Iteration 43, loss = 0.28273359\n",
      "Iteration 44, loss = 0.28068293\n",
      "Iteration 45, loss = 0.27881527\n",
      "Iteration 46, loss = 0.27689704\n",
      "Iteration 47, loss = 0.27505990\n",
      "Iteration 48, loss = 0.27326252\n",
      "Iteration 49, loss = 0.27143653\n",
      "Iteration 50, loss = 0.26973150\n",
      "Iteration 51, loss = 0.26800988\n",
      "Iteration 52, loss = 0.26635984\n",
      "Iteration 53, loss = 0.26454561\n",
      "Iteration 54, loss = 0.26296347\n",
      "Iteration 55, loss = 0.26143658\n",
      "Iteration 56, loss = 0.25993656\n",
      "Iteration 57, loss = 0.25836756\n",
      "Iteration 58, loss = 0.25666990\n",
      "Iteration 59, loss = 0.25533035\n",
      "Iteration 60, loss = 0.25379576\n",
      "Iteration 61, loss = 0.25235854\n",
      "Iteration 62, loss = 0.25090034\n",
      "Iteration 63, loss = 0.24945024\n",
      "Iteration 64, loss = 0.24806177\n",
      "Iteration 65, loss = 0.24670156\n",
      "Iteration 66, loss = 0.24527729\n",
      "Iteration 67, loss = 0.24389095\n",
      "Iteration 68, loss = 0.24255147\n",
      "Iteration 69, loss = 0.24121592\n",
      "Iteration 70, loss = 0.23987251\n",
      "Iteration 71, loss = 0.23854608\n",
      "Iteration 72, loss = 0.23732475\n",
      "Iteration 73, loss = 0.23599518\n",
      "Iteration 74, loss = 0.23476091\n",
      "Iteration 75, loss = 0.23353699\n",
      "Iteration 76, loss = 0.23238473\n",
      "Iteration 77, loss = 0.23104416\n",
      "Iteration 78, loss = 0.22989333\n",
      "Iteration 79, loss = 0.22862867\n",
      "Iteration 80, loss = 0.22740742\n",
      "Iteration 81, loss = 0.22632456\n",
      "Iteration 82, loss = 0.22518469\n",
      "Iteration 83, loss = 0.22400689\n",
      "Iteration 84, loss = 0.22284598\n",
      "Iteration 85, loss = 0.22166267\n",
      "Iteration 86, loss = 0.22058009\n",
      "Iteration 87, loss = 0.21951962\n",
      "Iteration 88, loss = 0.21840088\n",
      "Iteration 89, loss = 0.21729829\n",
      "Iteration 90, loss = 0.21623022\n",
      "Iteration 91, loss = 0.21519791\n",
      "Iteration 92, loss = 0.21404672\n",
      "Iteration 93, loss = 0.21310045\n",
      "Iteration 94, loss = 0.21200246\n",
      "Iteration 95, loss = 0.21101848\n",
      "Iteration 96, loss = 0.21002598\n",
      "Iteration 97, loss = 0.20894889\n",
      "Iteration 98, loss = 0.20798278\n",
      "Iteration 99, loss = 0.20700060\n",
      "Iteration 100, loss = 0.20603175\n",
      "Iteration 101, loss = 0.20503653\n",
      "Iteration 102, loss = 0.20402004\n",
      "Iteration 103, loss = 0.20309573\n",
      "Iteration 104, loss = 0.20212472\n",
      "Iteration 105, loss = 0.20123349\n",
      "Iteration 106, loss = 0.20025621\n",
      "Iteration 107, loss = 0.19939684\n",
      "Iteration 108, loss = 0.19846449\n",
      "Iteration 109, loss = 0.19752064\n",
      "Iteration 110, loss = 0.19669252\n",
      "Iteration 111, loss = 0.19573086\n",
      "Iteration 112, loss = 0.19476190\n",
      "Iteration 113, loss = 0.19404796\n",
      "Iteration 114, loss = 0.19309241\n",
      "Iteration 115, loss = 0.19220936\n",
      "Iteration 116, loss = 0.19139747\n",
      "Iteration 117, loss = 0.19045322\n",
      "Iteration 118, loss = 0.18968383\n",
      "Iteration 119, loss = 0.18883914\n",
      "Iteration 120, loss = 0.18800881\n",
      "Iteration 121, loss = 0.18723909\n",
      "Iteration 122, loss = 0.18641108\n",
      "Iteration 123, loss = 0.18560943\n",
      "Iteration 124, loss = 0.18478379\n",
      "Iteration 125, loss = 0.18396365\n",
      "Iteration 126, loss = 0.18320358\n",
      "Iteration 127, loss = 0.18239047\n",
      "Iteration 128, loss = 0.18165695\n",
      "Iteration 129, loss = 0.18094131\n",
      "Iteration 130, loss = 0.18009911\n",
      "Iteration 131, loss = 0.17931584\n",
      "Iteration 132, loss = 0.17857359\n",
      "Iteration 133, loss = 0.17784057\n",
      "Iteration 134, loss = 0.17717216\n",
      "Iteration 135, loss = 0.17640606\n",
      "Iteration 136, loss = 0.17566714\n",
      "Iteration 137, loss = 0.17488887\n",
      "Iteration 138, loss = 0.17423397\n",
      "Iteration 139, loss = 0.17345359\n",
      "Iteration 140, loss = 0.17278774\n",
      "Iteration 141, loss = 0.17204427\n",
      "Iteration 142, loss = 0.17127821\n",
      "Iteration 143, loss = 0.17073507\n",
      "Iteration 144, loss = 0.17001284\n",
      "Iteration 145, loss = 0.16921505\n",
      "Iteration 146, loss = 0.16861550\n",
      "Iteration 147, loss = 0.16793922\n",
      "Iteration 148, loss = 0.16727954\n",
      "Iteration 149, loss = 0.16662660\n",
      "Iteration 150, loss = 0.16598344\n",
      "Iteration 151, loss = 0.16531662\n",
      "Iteration 152, loss = 0.16461776\n",
      "Iteration 153, loss = 0.16404985\n",
      "Iteration 154, loss = 0.16340776\n",
      "Iteration 155, loss = 0.16274336\n",
      "Iteration 156, loss = 0.16207842\n",
      "Iteration 157, loss = 0.16153187\n",
      "Iteration 158, loss = 0.16089533\n",
      "Iteration 159, loss = 0.16021622\n",
      "Iteration 160, loss = 0.15962698\n",
      "Iteration 161, loss = 0.15905165\n",
      "Iteration 162, loss = 0.15839824\n",
      "Iteration 163, loss = 0.15783314\n",
      "Iteration 164, loss = 0.15721490\n",
      "Iteration 165, loss = 0.15659861\n",
      "Iteration 166, loss = 0.15599106\n",
      "Iteration 167, loss = 0.15546645\n",
      "Iteration 168, loss = 0.15484106\n",
      "Iteration 169, loss = 0.15425510\n",
      "Iteration 170, loss = 0.15366786\n",
      "Iteration 171, loss = 0.15316888\n",
      "Iteration 172, loss = 0.15259649\n",
      "Iteration 173, loss = 0.15203141\n",
      "Iteration 174, loss = 0.15148791\n",
      "Iteration 175, loss = 0.15090538\n",
      "Iteration 176, loss = 0.15032569\n",
      "Iteration 177, loss = 0.14980224\n",
      "Iteration 178, loss = 0.14929180\n",
      "Iteration 179, loss = 0.14869361\n",
      "Iteration 180, loss = 0.14819070\n",
      "Iteration 181, loss = 0.14765394\n",
      "Iteration 182, loss = 0.14706066\n",
      "Iteration 183, loss = 0.14656704\n",
      "Iteration 184, loss = 0.14606531\n",
      "Iteration 185, loss = 0.14552220\n",
      "Iteration 186, loss = 0.14499936\n",
      "Iteration 187, loss = 0.14450476\n",
      "Iteration 188, loss = 0.14403273\n",
      "Iteration 189, loss = 0.14349881\n",
      "Iteration 190, loss = 0.14301253\n",
      "Iteration 191, loss = 0.14246653\n",
      "Iteration 192, loss = 0.14194198\n",
      "Iteration 193, loss = 0.14138892\n",
      "Iteration 194, loss = 0.14095729\n",
      "Iteration 195, loss = 0.14048556\n",
      "Iteration 196, loss = 0.14001925\n",
      "Iteration 197, loss = 0.13948660\n",
      "Iteration 198, loss = 0.13899580\n",
      "Iteration 199, loss = 0.13857062\n",
      "Iteration 200, loss = 0.13806715\n",
      "[CV] END activation=logistic, alpha=0.0001, batch_size=50, hidden_layer_sizes=(100,), solver=sgd; total time= 3.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.62705980\n",
      "Iteration 2, loss = 0.22544785\n",
      "Iteration 3, loss = 0.16454067\n",
      "Iteration 4, loss = 0.12616184\n",
      "Iteration 5, loss = 0.10063084\n",
      "Iteration 6, loss = 0.08058360\n",
      "Iteration 7, loss = 0.06481062\n",
      "Iteration 8, loss = 0.05182396\n",
      "Iteration 9, loss = 0.04330356\n",
      "Iteration 10, loss = 0.03418943\n",
      "Iteration 11, loss = 0.02800438\n",
      "Iteration 12, loss = 0.02458164\n",
      "Iteration 13, loss = 0.02013118\n",
      "Iteration 14, loss = 0.01749034\n",
      "Iteration 15, loss = 0.01410777\n",
      "Iteration 16, loss = 0.01373684\n",
      "Iteration 17, loss = 0.01294144\n",
      "Iteration 18, loss = 0.01255158\n",
      "Iteration 19, loss = 0.01245428\n",
      "Iteration 20, loss = 0.01272202\n",
      "Iteration 21, loss = 0.01185646\n",
      "Iteration 22, loss = 0.00816880\n",
      "Iteration 23, loss = 0.00785316\n",
      "Iteration 24, loss = 0.02000622\n",
      "Iteration 25, loss = 0.01018797\n",
      "Iteration 26, loss = 0.00796335\n",
      "Iteration 27, loss = 0.00741793\n",
      "Iteration 28, loss = 0.00706224\n",
      "Iteration 29, loss = 0.00685637\n",
      "Iteration 30, loss = 0.02645248\n",
      "Iteration 31, loss = 0.00981177\n",
      "Iteration 32, loss = 0.00761415\n",
      "Iteration 33, loss = 0.00709905\n",
      "Iteration 34, loss = 0.00760349\n",
      "Iteration 35, loss = 0.02140815\n",
      "Iteration 36, loss = 0.00837226\n",
      "Iteration 37, loss = 0.00725669\n",
      "Iteration 38, loss = 0.00696295\n",
      "Iteration 39, loss = 0.00673342\n",
      "Iteration 40, loss = 0.02223441\n",
      "Iteration 41, loss = 0.00932436\n",
      "Iteration 42, loss = 0.00729941\n",
      "Iteration 43, loss = 0.00696638\n",
      "Iteration 44, loss = 0.00670484\n",
      "Iteration 45, loss = 0.00642563\n",
      "Iteration 46, loss = 0.01165612\n",
      "Iteration 47, loss = 0.01764515\n",
      "Iteration 48, loss = 0.00794686\n",
      "Iteration 49, loss = 0.00825936\n",
      "Iteration 50, loss = 0.01237929\n",
      "Iteration 51, loss = 0.00723645\n",
      "Iteration 52, loss = 0.00665675\n",
      "Iteration 53, loss = 0.00640576\n",
      "Iteration 54, loss = 0.00615546\n",
      "Iteration 55, loss = 0.00639214\n",
      "Iteration 56, loss = 0.02399602\n",
      "Iteration 57, loss = 0.00732836\n",
      "Iteration 58, loss = 0.00668872\n",
      "Iteration 59, loss = 0.00632490\n",
      "Iteration 60, loss = 0.00612197\n",
      "Iteration 61, loss = 0.00591804\n",
      "Iteration 62, loss = 0.02200918\n",
      "Iteration 63, loss = 0.01214222\n",
      "Iteration 64, loss = 0.00773052\n",
      "Iteration 65, loss = 0.00659009\n",
      "Iteration 66, loss = 0.00629493\n",
      "Iteration 67, loss = 0.00610051\n",
      "Iteration 68, loss = 0.00590663\n",
      "Iteration 69, loss = 0.00580262\n",
      "Iteration 70, loss = 0.02246361\n",
      "Iteration 71, loss = 0.00684759\n",
      "Iteration 72, loss = 0.00620685\n",
      "Iteration 73, loss = 0.00599749\n",
      "Iteration 74, loss = 0.00582555\n",
      "Iteration 75, loss = 0.00562550\n",
      "Iteration 76, loss = 0.01841366\n",
      "Iteration 77, loss = 0.01105140\n",
      "Iteration 78, loss = 0.00645794\n",
      "Iteration 79, loss = 0.00605161\n",
      "Iteration 80, loss = 0.00588624\n",
      "Iteration 81, loss = 0.01481322\n",
      "Iteration 82, loss = 0.01299429\n",
      "Iteration 83, loss = 0.00661692\n",
      "Iteration 84, loss = 0.00613825\n",
      "Iteration 85, loss = 0.00592914\n",
      "Iteration 86, loss = 0.00574089\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.0001, batch_size=50, hidden_layer_sizes=(400, 100), solver=adam; total time= 7.4min\n",
      "Iteration 1, loss = 0.62625365\n",
      "Iteration 2, loss = 0.22616985\n",
      "Iteration 3, loss = 0.16642183\n",
      "Iteration 4, loss = 0.12927564\n",
      "Iteration 5, loss = 0.10186916\n",
      "Iteration 6, loss = 0.08235802\n",
      "Iteration 7, loss = 0.06624382\n",
      "Iteration 8, loss = 0.05493760\n",
      "Iteration 9, loss = 0.04305821\n",
      "Iteration 10, loss = 0.03568620\n",
      "Iteration 11, loss = 0.02898387\n",
      "Iteration 12, loss = 0.02423582\n",
      "Iteration 13, loss = 0.02013044\n",
      "Iteration 14, loss = 0.01749158\n",
      "Iteration 15, loss = 0.01676686\n",
      "Iteration 16, loss = 0.01376820\n",
      "Iteration 17, loss = 0.01212220\n",
      "Iteration 18, loss = 0.01616909\n",
      "Iteration 19, loss = 0.01268030\n",
      "Iteration 20, loss = 0.01016154\n",
      "Iteration 21, loss = 0.00862485\n",
      "Iteration 22, loss = 0.01052648\n",
      "Iteration 23, loss = 0.01789353\n",
      "Iteration 24, loss = 0.00971918\n",
      "Iteration 25, loss = 0.00780572\n",
      "Iteration 26, loss = 0.00742367\n",
      "Iteration 27, loss = 0.00713112\n",
      "Iteration 28, loss = 0.02450670\n",
      "Iteration 29, loss = 0.01018170\n",
      "Iteration 30, loss = 0.00780557\n",
      "Iteration 31, loss = 0.00730750\n",
      "Iteration 32, loss = 0.00703166\n",
      "Iteration 33, loss = 0.00731971\n",
      "Iteration 34, loss = 0.02201614\n",
      "Iteration 35, loss = 0.00874281\n",
      "Iteration 36, loss = 0.00735228\n",
      "Iteration 37, loss = 0.00703825\n",
      "Iteration 38, loss = 0.00673202\n",
      "Iteration 39, loss = 0.00654995\n",
      "Iteration 40, loss = 0.02319915\n",
      "Iteration 41, loss = 0.00806068\n",
      "Iteration 42, loss = 0.00706323\n",
      "Iteration 43, loss = 0.00678193\n",
      "Iteration 44, loss = 0.00656433\n",
      "Iteration 45, loss = 0.02212811\n",
      "Iteration 46, loss = 0.00967514\n",
      "Iteration 47, loss = 0.00776776\n",
      "Iteration 48, loss = 0.00708714\n",
      "Iteration 49, loss = 0.00673556\n",
      "Iteration 50, loss = 0.00646468\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.0001, batch_size=50, hidden_layer_sizes=(400, 100), solver=adam; total time= 4.2min\n",
      "Iteration 1, loss = 0.62679290\n",
      "Iteration 2, loss = 0.22595204\n",
      "Iteration 3, loss = 0.16474681\n",
      "Iteration 4, loss = 0.12661643\n",
      "Iteration 5, loss = 0.10054748\n",
      "Iteration 6, loss = 0.07979405\n",
      "Iteration 7, loss = 0.06452977\n",
      "Iteration 8, loss = 0.05175262\n",
      "Iteration 9, loss = 0.04252789\n",
      "Iteration 10, loss = 0.03450927\n",
      "Iteration 11, loss = 0.02809658\n",
      "Iteration 12, loss = 0.02416799\n",
      "Iteration 13, loss = 0.01979280\n",
      "Iteration 14, loss = 0.01775950\n",
      "Iteration 15, loss = 0.01714003\n",
      "Iteration 16, loss = 0.01315457\n",
      "Iteration 17, loss = 0.01401177\n",
      "Iteration 18, loss = 0.01429103\n",
      "Iteration 19, loss = 0.01274514\n",
      "Iteration 20, loss = 0.01006968\n",
      "Iteration 21, loss = 0.01100906\n",
      "Iteration 22, loss = 0.01216162\n",
      "Iteration 23, loss = 0.01313633\n",
      "Iteration 24, loss = 0.01193651\n",
      "Iteration 25, loss = 0.00798193\n",
      "Iteration 26, loss = 0.00751911\n",
      "Iteration 27, loss = 0.00727851\n",
      "Iteration 28, loss = 0.02159642\n",
      "Iteration 29, loss = 0.00865130\n",
      "Iteration 30, loss = 0.00755687\n",
      "Iteration 31, loss = 0.00723336\n",
      "Iteration 32, loss = 0.00689923\n",
      "Iteration 33, loss = 0.01822295\n",
      "Iteration 34, loss = 0.01512052\n",
      "Iteration 35, loss = 0.00883908\n",
      "Iteration 36, loss = 0.00762972\n",
      "Iteration 37, loss = 0.00708248\n",
      "Iteration 38, loss = 0.00682455\n",
      "Iteration 39, loss = 0.00653759\n",
      "Iteration 40, loss = 0.01617423\n",
      "Iteration 41, loss = 0.01656399\n",
      "Iteration 42, loss = 0.00787654\n",
      "Iteration 43, loss = 0.00691211\n",
      "Iteration 44, loss = 0.00666604\n",
      "Iteration 45, loss = 0.00644202\n",
      "Iteration 46, loss = 0.00619914\n",
      "Iteration 47, loss = 0.01802897\n",
      "Iteration 48, loss = 0.01535785\n",
      "Iteration 49, loss = 0.00733812\n",
      "Iteration 50, loss = 0.00660704\n",
      "Iteration 51, loss = 0.00637685\n",
      "Iteration 52, loss = 0.00616844\n",
      "Iteration 53, loss = 0.00594182\n",
      "Iteration 54, loss = 0.01687196\n",
      "Iteration 55, loss = 0.01498686\n",
      "Iteration 56, loss = 0.00819382\n",
      "Iteration 57, loss = 0.00649675\n",
      "Iteration 58, loss = 0.00622991\n",
      "Iteration 59, loss = 0.00602689\n",
      "Iteration 60, loss = 0.00580267\n",
      "Iteration 61, loss = 0.00558512\n",
      "Iteration 62, loss = 0.02371808\n",
      "Iteration 63, loss = 0.01002062\n",
      "Iteration 64, loss = 0.01072606\n",
      "Iteration 65, loss = 0.00796587\n",
      "Iteration 66, loss = 0.00625228\n",
      "Iteration 67, loss = 0.00596245\n",
      "Iteration 68, loss = 0.00577391\n",
      "Iteration 69, loss = 0.00556287\n",
      "Iteration 70, loss = 0.00536455\n",
      "Iteration 71, loss = 0.02549165\n",
      "Iteration 72, loss = 0.00995846\n",
      "Iteration 73, loss = 0.00629457\n",
      "Iteration 74, loss = 0.00580050\n",
      "Iteration 75, loss = 0.00563287\n",
      "Iteration 76, loss = 0.00547306\n",
      "Iteration 77, loss = 0.00530791\n",
      "Iteration 78, loss = 0.02114812\n",
      "Iteration 79, loss = 0.01048866\n",
      "Iteration 80, loss = 0.00675540\n",
      "Iteration 81, loss = 0.00579629\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.0001, batch_size=50, hidden_layer_sizes=(400, 100), solver=adam; total time= 7.0min\n",
      "Iteration 1, loss = 0.62637841\n",
      "Iteration 2, loss = 0.22405930\n",
      "Iteration 3, loss = 0.16360570\n",
      "Iteration 4, loss = 0.12769173\n",
      "Iteration 5, loss = 0.10078994\n",
      "Iteration 6, loss = 0.08141249\n",
      "Iteration 7, loss = 0.06576271\n",
      "Iteration 8, loss = 0.05337470\n",
      "Iteration 9, loss = 0.04454479\n",
      "Iteration 10, loss = 0.03651148\n",
      "Iteration 11, loss = 0.03027533\n",
      "Iteration 12, loss = 0.02573199\n",
      "Iteration 13, loss = 0.02028517\n",
      "Iteration 14, loss = 0.01919853\n",
      "Iteration 15, loss = 0.01629808\n",
      "Iteration 16, loss = 0.01343928\n",
      "Iteration 17, loss = 0.01368610\n",
      "Iteration 18, loss = 0.01469514\n",
      "Iteration 19, loss = 0.01299346\n",
      "Iteration 20, loss = 0.01045037\n",
      "Iteration 21, loss = 0.00924847\n",
      "Iteration 22, loss = 0.00807447\n",
      "Iteration 23, loss = 0.01250777\n",
      "Iteration 24, loss = 0.02204228\n",
      "Iteration 25, loss = 0.00878855\n",
      "Iteration 26, loss = 0.00793472\n",
      "Iteration 27, loss = 0.01303306\n",
      "Iteration 28, loss = 0.01727563\n",
      "Iteration 29, loss = 0.00825366\n",
      "Iteration 30, loss = 0.00764070\n",
      "Iteration 31, loss = 0.00734327\n",
      "Iteration 32, loss = 0.00704000\n",
      "Iteration 33, loss = 0.00693803\n",
      "Iteration 34, loss = 0.02729875\n",
      "Iteration 35, loss = 0.00899838\n",
      "Iteration 36, loss = 0.00749576\n",
      "Iteration 37, loss = 0.00711380\n",
      "Iteration 38, loss = 0.00684573\n",
      "Iteration 39, loss = 0.01726659\n",
      "Iteration 40, loss = 0.01555887\n",
      "Iteration 41, loss = 0.00768280\n",
      "Iteration 42, loss = 0.00712948\n",
      "Iteration 43, loss = 0.00684791\n",
      "Iteration 44, loss = 0.00659691\n",
      "Iteration 45, loss = 0.00632333\n",
      "Iteration 46, loss = 0.02246965\n",
      "Iteration 47, loss = 0.01183808\n",
      "Iteration 48, loss = 0.00906768\n",
      "Iteration 49, loss = 0.00857743\n",
      "Iteration 50, loss = 0.00680999\n",
      "Iteration 51, loss = 0.00649043\n",
      "Iteration 52, loss = 0.00623968\n",
      "Iteration 53, loss = 0.00598230\n",
      "Iteration 54, loss = 0.01998742\n",
      "Iteration 55, loss = 0.01357343\n",
      "Iteration 56, loss = 0.00801454\n",
      "Iteration 57, loss = 0.00665940\n",
      "Iteration 58, loss = 0.01032871\n",
      "Iteration 59, loss = 0.01336588\n",
      "Iteration 60, loss = 0.00794260\n",
      "Iteration 61, loss = 0.00939848\n",
      "Iteration 62, loss = 0.01075238\n",
      "Iteration 63, loss = 0.00835344\n",
      "Iteration 64, loss = 0.00678141\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.0001, batch_size=50, hidden_layer_sizes=(400, 100), solver=adam; total time= 5.4min\n",
      "Iteration 1, loss = 0.62723934\n",
      "Iteration 2, loss = 0.23230133\n",
      "Iteration 3, loss = 0.17069755\n",
      "Iteration 4, loss = 0.13234150\n",
      "Iteration 5, loss = 0.10508538\n",
      "Iteration 6, loss = 0.08403278\n",
      "Iteration 7, loss = 0.06901073\n",
      "Iteration 8, loss = 0.05557547\n",
      "Iteration 9, loss = 0.04535312\n",
      "Iteration 10, loss = 0.03668393\n",
      "Iteration 11, loss = 0.03047936\n",
      "Iteration 12, loss = 0.02606769\n",
      "Iteration 13, loss = 0.02114127\n",
      "Iteration 14, loss = 0.01805257\n",
      "Iteration 15, loss = 0.01548645\n",
      "Iteration 16, loss = 0.01790675\n",
      "Iteration 17, loss = 0.01359263\n",
      "Iteration 18, loss = 0.01204312\n",
      "Iteration 19, loss = 0.01213300\n",
      "Iteration 20, loss = 0.01424783\n",
      "Iteration 21, loss = 0.01028895\n",
      "Iteration 22, loss = 0.01492310\n",
      "Iteration 23, loss = 0.01023401\n",
      "Iteration 24, loss = 0.01237232\n",
      "Iteration 25, loss = 0.01454504\n",
      "Iteration 26, loss = 0.01021095\n",
      "Iteration 27, loss = 0.00869161\n",
      "Iteration 28, loss = 0.01202741\n",
      "Iteration 29, loss = 0.01335584\n",
      "Iteration 30, loss = 0.01202790\n",
      "Iteration 31, loss = 0.00845441\n",
      "Iteration 32, loss = 0.00778536\n",
      "Iteration 33, loss = 0.00742434\n",
      "Iteration 34, loss = 0.01133084\n",
      "Iteration 35, loss = 0.01972498\n",
      "Iteration 36, loss = 0.00832400\n",
      "Iteration 37, loss = 0.00761229\n",
      "Iteration 38, loss = 0.00735577\n",
      "Iteration 39, loss = 0.00709588\n",
      "Iteration 40, loss = 0.02108459\n",
      "Iteration 41, loss = 0.01226730\n",
      "Iteration 42, loss = 0.00790749\n",
      "Iteration 43, loss = 0.00731266\n",
      "Iteration 44, loss = 0.00702538\n",
      "Iteration 45, loss = 0.00676027\n",
      "Iteration 46, loss = 0.01950562\n",
      "Iteration 47, loss = 0.01213715\n",
      "Iteration 48, loss = 0.01048484\n",
      "Iteration 49, loss = 0.00848831\n",
      "Iteration 50, loss = 0.00715631\n",
      "Iteration 51, loss = 0.00682532\n",
      "Iteration 52, loss = 0.00656846\n",
      "Iteration 53, loss = 0.00627061\n",
      "Iteration 54, loss = 0.02243603\n",
      "Iteration 55, loss = 0.01041233\n",
      "Iteration 56, loss = 0.00737304\n",
      "Iteration 57, loss = 0.00671627\n",
      "Iteration 58, loss = 0.00646809\n",
      "Iteration 59, loss = 0.00625032\n",
      "Iteration 60, loss = 0.02164289\n",
      "Iteration 61, loss = 0.01104725\n",
      "Iteration 62, loss = 0.00790568\n",
      "Iteration 63, loss = 0.00721362\n",
      "Iteration 64, loss = 0.01454948\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.0001, batch_size=50, hidden_layer_sizes=(400, 100), solver=adam; total time= 5.4min\n",
      "Iteration 1, loss = 2.29946863\n",
      "Iteration 2, loss = 2.28395185\n",
      "Iteration 3, loss = 2.26612609\n",
      "Iteration 4, loss = 2.23927002\n",
      "Iteration 5, loss = 2.19340578\n",
      "Iteration 6, loss = 2.10587654\n",
      "Iteration 7, loss = 1.93872078\n",
      "Iteration 8, loss = 1.68742720\n",
      "Iteration 9, loss = 1.43677170\n",
      "Iteration 10, loss = 1.23866583\n",
      "Iteration 11, loss = 1.08531525\n",
      "Iteration 12, loss = 0.96704003\n",
      "Iteration 13, loss = 0.87416398\n",
      "Iteration 14, loss = 0.79967359\n",
      "Iteration 15, loss = 0.73750957\n",
      "Iteration 16, loss = 0.68575251\n",
      "Iteration 17, loss = 0.64194579\n",
      "Iteration 18, loss = 0.60469409\n",
      "Iteration 19, loss = 0.57301703\n",
      "Iteration 20, loss = 0.54618160\n",
      "Iteration 21, loss = 0.52293876\n",
      "Iteration 22, loss = 0.50330127\n",
      "Iteration 23, loss = 0.48569855\n",
      "Iteration 24, loss = 0.47036208\n",
      "Iteration 25, loss = 0.45691194\n",
      "Iteration 26, loss = 0.44482579\n",
      "Iteration 27, loss = 0.43406549\n",
      "Iteration 28, loss = 0.42455747\n",
      "Iteration 29, loss = 0.41609757\n",
      "Iteration 30, loss = 0.40839748\n",
      "Iteration 31, loss = 0.40123655\n",
      "Iteration 32, loss = 0.39476007\n",
      "Iteration 33, loss = 0.38882395\n",
      "Iteration 34, loss = 0.38349219\n",
      "Iteration 35, loss = 0.37852573\n",
      "Iteration 36, loss = 0.37360235\n",
      "Iteration 37, loss = 0.36940930\n",
      "Iteration 38, loss = 0.36535192\n",
      "Iteration 39, loss = 0.36149388\n",
      "Iteration 40, loss = 0.35781762\n",
      "Iteration 41, loss = 0.35446199\n",
      "Iteration 42, loss = 0.35113433\n",
      "Iteration 43, loss = 0.34784701\n",
      "Iteration 44, loss = 0.34498821\n",
      "Iteration 45, loss = 0.34203441\n",
      "Iteration 46, loss = 0.33919962\n",
      "Iteration 47, loss = 0.33662626\n",
      "Iteration 48, loss = 0.33415952\n",
      "Iteration 49, loss = 0.33182430\n",
      "Iteration 50, loss = 0.32930878\n",
      "Iteration 51, loss = 0.32704237\n",
      "Iteration 52, loss = 0.32483048\n",
      "Iteration 53, loss = 0.32246329\n",
      "Iteration 54, loss = 0.32048054\n",
      "Iteration 55, loss = 0.31822037\n",
      "Iteration 56, loss = 0.31616690\n",
      "Iteration 57, loss = 0.31429058\n",
      "Iteration 58, loss = 0.31235669\n",
      "Iteration 59, loss = 0.31062139\n",
      "Iteration 60, loss = 0.30859367\n",
      "Iteration 61, loss = 0.30670799\n",
      "Iteration 62, loss = 0.30483235\n",
      "Iteration 63, loss = 0.30326665\n",
      "Iteration 64, loss = 0.30131800\n",
      "Iteration 65, loss = 0.29954953\n",
      "Iteration 66, loss = 0.29801339\n",
      "Iteration 67, loss = 0.29633797\n",
      "Iteration 68, loss = 0.29487918\n",
      "Iteration 69, loss = 0.29319141\n",
      "Iteration 70, loss = 0.29169909\n",
      "Iteration 71, loss = 0.29020051\n",
      "Iteration 72, loss = 0.28873673\n",
      "Iteration 73, loss = 0.28694717\n",
      "Iteration 74, loss = 0.28563835\n",
      "Iteration 75, loss = 0.28408835\n",
      "Iteration 76, loss = 0.28266884\n",
      "Iteration 77, loss = 0.28109316\n",
      "Iteration 78, loss = 0.27990152\n",
      "Iteration 79, loss = 0.27843536\n",
      "Iteration 80, loss = 0.27698830\n",
      "Iteration 81, loss = 0.27587052\n",
      "Iteration 82, loss = 0.27435131\n",
      "Iteration 83, loss = 0.27323068\n",
      "Iteration 84, loss = 0.27182821\n",
      "Iteration 85, loss = 0.27034028\n",
      "Iteration 86, loss = 0.26927575\n",
      "Iteration 87, loss = 0.26794670\n",
      "Iteration 88, loss = 0.26676083\n",
      "Iteration 89, loss = 0.26559060\n",
      "Iteration 90, loss = 0.26434964\n",
      "Iteration 91, loss = 0.26307174\n",
      "Iteration 92, loss = 0.26197656\n",
      "Iteration 93, loss = 0.26075379\n",
      "Iteration 94, loss = 0.25956327\n",
      "Iteration 95, loss = 0.25848473\n",
      "Iteration 96, loss = 0.25724297\n",
      "Iteration 97, loss = 0.25601472\n",
      "Iteration 98, loss = 0.25499695\n",
      "Iteration 99, loss = 0.25379891\n",
      "Iteration 100, loss = 0.25273443\n",
      "Iteration 101, loss = 0.25171656\n",
      "Iteration 102, loss = 0.25056486\n",
      "Iteration 103, loss = 0.24955257\n",
      "Iteration 104, loss = 0.24849043\n",
      "Iteration 105, loss = 0.24741659\n",
      "Iteration 106, loss = 0.24631887\n",
      "Iteration 107, loss = 0.24525956\n",
      "Iteration 108, loss = 0.24404660\n",
      "Iteration 109, loss = 0.24310779\n",
      "Iteration 110, loss = 0.24218851\n",
      "Iteration 111, loss = 0.24120217\n",
      "Iteration 112, loss = 0.24006567\n",
      "Iteration 113, loss = 0.23911467\n",
      "Iteration 114, loss = 0.23815388\n",
      "Iteration 115, loss = 0.23707552\n",
      "Iteration 116, loss = 0.23617106\n",
      "Iteration 117, loss = 0.23515827\n",
      "Iteration 118, loss = 0.23399374\n",
      "Iteration 119, loss = 0.23322959\n",
      "Iteration 120, loss = 0.23212423\n",
      "Iteration 121, loss = 0.23121335\n",
      "Iteration 122, loss = 0.23035823\n",
      "Iteration 123, loss = 0.22932308\n",
      "Iteration 124, loss = 0.22851709\n",
      "Iteration 125, loss = 0.22740228\n",
      "Iteration 126, loss = 0.22644699\n",
      "Iteration 127, loss = 0.22558755\n",
      "Iteration 128, loss = 0.22492866\n",
      "Iteration 129, loss = 0.22368041\n",
      "Iteration 130, loss = 0.22298480\n",
      "Iteration 131, loss = 0.22193768\n",
      "Iteration 132, loss = 0.22108581\n",
      "Iteration 133, loss = 0.22018005\n",
      "Iteration 134, loss = 0.21920690\n",
      "Iteration 135, loss = 0.21838862\n",
      "Iteration 136, loss = 0.21750540\n",
      "Iteration 137, loss = 0.21650845\n",
      "Iteration 138, loss = 0.21582264\n",
      "Iteration 139, loss = 0.21481590\n",
      "Iteration 140, loss = 0.21406116\n",
      "Iteration 141, loss = 0.21320761\n",
      "Iteration 142, loss = 0.21213126\n",
      "Iteration 143, loss = 0.21135876\n",
      "Iteration 144, loss = 0.21051957\n",
      "Iteration 145, loss = 0.20970545\n",
      "Iteration 146, loss = 0.20879829\n",
      "Iteration 147, loss = 0.20806219\n",
      "Iteration 148, loss = 0.20721953\n",
      "Iteration 149, loss = 0.20627173\n",
      "Iteration 150, loss = 0.20549504\n",
      "Iteration 151, loss = 0.20468465\n",
      "Iteration 152, loss = 0.20392859\n",
      "Iteration 153, loss = 0.20311774\n",
      "Iteration 154, loss = 0.20222864\n",
      "Iteration 155, loss = 0.20152780\n",
      "Iteration 156, loss = 0.20063048\n",
      "Iteration 157, loss = 0.19986303\n",
      "Iteration 158, loss = 0.19897094\n",
      "Iteration 159, loss = 0.19812944\n",
      "Iteration 160, loss = 0.19733581\n",
      "Iteration 161, loss = 0.19656531\n",
      "Iteration 162, loss = 0.19582263\n",
      "Iteration 163, loss = 0.19499200\n",
      "Iteration 164, loss = 0.19425514\n",
      "Iteration 165, loss = 0.19360352\n",
      "Iteration 166, loss = 0.19269458\n",
      "Iteration 167, loss = 0.19183861\n",
      "Iteration 168, loss = 0.19113631\n",
      "Iteration 169, loss = 0.19045372\n",
      "Iteration 170, loss = 0.18968408\n",
      "Iteration 171, loss = 0.18884175\n",
      "Iteration 172, loss = 0.18824916\n",
      "Iteration 173, loss = 0.18747825\n",
      "Iteration 174, loss = 0.18668155\n",
      "Iteration 175, loss = 0.18614508\n",
      "Iteration 176, loss = 0.18510444\n",
      "Iteration 177, loss = 0.18455378\n",
      "Iteration 178, loss = 0.18373435\n",
      "Iteration 179, loss = 0.18312245\n",
      "Iteration 180, loss = 0.18237745\n",
      "Iteration 181, loss = 0.18145114\n",
      "Iteration 182, loss = 0.18095276\n",
      "Iteration 183, loss = 0.18034233\n",
      "Iteration 184, loss = 0.17936906\n",
      "Iteration 185, loss = 0.17892938\n",
      "Iteration 186, loss = 0.17793045\n",
      "Iteration 187, loss = 0.17747410\n",
      "Iteration 188, loss = 0.17659082\n",
      "Iteration 189, loss = 0.17595202\n",
      "Iteration 190, loss = 0.17535777\n",
      "Iteration 191, loss = 0.17458040\n",
      "Iteration 192, loss = 0.17394883\n",
      "Iteration 193, loss = 0.17331963\n",
      "Iteration 194, loss = 0.17261407\n",
      "Iteration 195, loss = 0.17187951\n",
      "Iteration 196, loss = 0.17121290\n",
      "Iteration 197, loss = 0.17068741\n",
      "Iteration 198, loss = 0.16991161\n",
      "Iteration 199, loss = 0.16920354\n",
      "Iteration 200, loss = 0.16871893\n",
      "[CV] END activation=logistic, alpha=0.0001, batch_size=50, hidden_layer_sizes=(400, 100), solver=sgd; total time=10.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.29957891\n",
      "Iteration 2, loss = 2.28255815\n",
      "Iteration 3, loss = 2.26353282\n",
      "Iteration 4, loss = 2.23364056\n",
      "Iteration 5, loss = 2.18016116\n",
      "Iteration 6, loss = 2.07773888\n",
      "Iteration 7, loss = 1.88892042\n",
      "Iteration 8, loss = 1.62467276\n",
      "Iteration 9, loss = 1.36974129\n",
      "Iteration 10, loss = 1.17423621\n",
      "Iteration 11, loss = 1.03479953\n",
      "Iteration 12, loss = 0.93344517\n",
      "Iteration 13, loss = 0.85501443\n",
      "Iteration 14, loss = 0.79042820\n",
      "Iteration 15, loss = 0.73542859\n",
      "Iteration 16, loss = 0.68747049\n",
      "Iteration 17, loss = 0.64637076\n",
      "Iteration 18, loss = 0.61061485\n",
      "Iteration 19, loss = 0.58010690\n",
      "Iteration 20, loss = 0.55396625\n",
      "Iteration 21, loss = 0.53145301\n",
      "Iteration 22, loss = 0.51155296\n",
      "Iteration 23, loss = 0.49439950\n",
      "Iteration 24, loss = 0.47913469\n",
      "Iteration 25, loss = 0.46575359\n",
      "Iteration 26, loss = 0.45351256\n",
      "Iteration 27, loss = 0.44246999\n",
      "Iteration 28, loss = 0.43252999\n",
      "Iteration 29, loss = 0.42351288\n",
      "Iteration 30, loss = 0.41560865\n",
      "Iteration 31, loss = 0.40821786\n",
      "Iteration 32, loss = 0.40128067\n",
      "Iteration 33, loss = 0.39478555\n",
      "Iteration 34, loss = 0.38909122\n",
      "Iteration 35, loss = 0.38373150\n",
      "Iteration 36, loss = 0.37862586\n",
      "Iteration 37, loss = 0.37382680\n",
      "Iteration 38, loss = 0.36941471\n",
      "Iteration 39, loss = 0.36517917\n",
      "Iteration 40, loss = 0.36113528\n",
      "Iteration 41, loss = 0.35739347\n",
      "Iteration 42, loss = 0.35404488\n",
      "Iteration 43, loss = 0.35075203\n",
      "Iteration 44, loss = 0.34736434\n",
      "Iteration 45, loss = 0.34451203\n",
      "Iteration 46, loss = 0.34153936\n",
      "Iteration 47, loss = 0.33847158\n",
      "Iteration 48, loss = 0.33598250\n",
      "Iteration 49, loss = 0.33334532\n",
      "Iteration 50, loss = 0.33084166\n",
      "Iteration 51, loss = 0.32843094\n",
      "Iteration 52, loss = 0.32609989\n",
      "Iteration 53, loss = 0.32370923\n",
      "Iteration 54, loss = 0.32179581\n",
      "Iteration 55, loss = 0.31951711\n",
      "Iteration 56, loss = 0.31734093\n",
      "Iteration 57, loss = 0.31538202\n",
      "Iteration 58, loss = 0.31342643\n",
      "Iteration 59, loss = 0.31160205\n",
      "Iteration 60, loss = 0.30967583\n",
      "Iteration 61, loss = 0.30792187\n",
      "Iteration 62, loss = 0.30610543\n",
      "Iteration 63, loss = 0.30434919\n",
      "Iteration 64, loss = 0.30260985\n",
      "Iteration 65, loss = 0.30090688\n",
      "Iteration 66, loss = 0.29956231\n",
      "Iteration 67, loss = 0.29777264\n",
      "Iteration 68, loss = 0.29625766\n",
      "Iteration 69, loss = 0.29459706\n",
      "Iteration 70, loss = 0.29325161\n",
      "Iteration 71, loss = 0.29170700\n",
      "Iteration 72, loss = 0.29023249\n",
      "Iteration 73, loss = 0.28881028\n",
      "Iteration 74, loss = 0.28728221\n",
      "Iteration 75, loss = 0.28606407\n",
      "Iteration 76, loss = 0.28479431\n",
      "Iteration 77, loss = 0.28342472\n",
      "Iteration 78, loss = 0.28200250\n",
      "Iteration 79, loss = 0.28084966\n",
      "Iteration 80, loss = 0.27938115\n",
      "Iteration 81, loss = 0.27814416\n",
      "Iteration 82, loss = 0.27698718\n",
      "Iteration 83, loss = 0.27582024\n",
      "Iteration 84, loss = 0.27445669\n",
      "Iteration 85, loss = 0.27323697\n",
      "Iteration 86, loss = 0.27209504\n",
      "Iteration 87, loss = 0.27085040\n",
      "Iteration 88, loss = 0.26982509\n",
      "Iteration 89, loss = 0.26852167\n",
      "Iteration 90, loss = 0.26756872\n",
      "Iteration 91, loss = 0.26645856\n",
      "Iteration 92, loss = 0.26511878\n",
      "Iteration 93, loss = 0.26412829\n",
      "Iteration 94, loss = 0.26303597\n",
      "Iteration 95, loss = 0.26183963\n",
      "Iteration 96, loss = 0.26064451\n",
      "Iteration 97, loss = 0.25959518\n",
      "Iteration 98, loss = 0.25856476\n",
      "Iteration 99, loss = 0.25757994\n",
      "Iteration 100, loss = 0.25663574\n",
      "Iteration 101, loss = 0.25541856\n",
      "Iteration 102, loss = 0.25433556\n",
      "Iteration 103, loss = 0.25342770\n",
      "Iteration 104, loss = 0.25240782\n",
      "Iteration 105, loss = 0.25141013\n",
      "Iteration 106, loss = 0.25018128\n",
      "Iteration 107, loss = 0.24931617\n",
      "Iteration 108, loss = 0.24821883\n",
      "Iteration 109, loss = 0.24732115\n",
      "Iteration 110, loss = 0.24636063\n",
      "Iteration 111, loss = 0.24530296\n",
      "Iteration 112, loss = 0.24433875\n",
      "Iteration 113, loss = 0.24327863\n",
      "Iteration 114, loss = 0.24235954\n",
      "Iteration 115, loss = 0.24147868\n",
      "Iteration 116, loss = 0.24051746\n",
      "Iteration 117, loss = 0.23943819\n",
      "Iteration 118, loss = 0.23861813\n",
      "Iteration 119, loss = 0.23779646\n",
      "Iteration 120, loss = 0.23667993\n",
      "Iteration 121, loss = 0.23559801\n",
      "Iteration 122, loss = 0.23482049\n",
      "Iteration 123, loss = 0.23381211\n",
      "Iteration 124, loss = 0.23293757\n",
      "Iteration 125, loss = 0.23199459\n",
      "Iteration 126, loss = 0.23096050\n",
      "Iteration 127, loss = 0.23019183\n",
      "Iteration 128, loss = 0.22932844\n",
      "Iteration 129, loss = 0.22835719\n",
      "Iteration 130, loss = 0.22736203\n",
      "Iteration 131, loss = 0.22660449\n",
      "Iteration 132, loss = 0.22568238\n",
      "Iteration 133, loss = 0.22472769\n",
      "Iteration 134, loss = 0.22385091\n",
      "Iteration 135, loss = 0.22301354\n",
      "Iteration 136, loss = 0.22214031\n",
      "Iteration 137, loss = 0.22135720\n",
      "Iteration 138, loss = 0.22029876\n",
      "Iteration 139, loss = 0.21953695\n",
      "Iteration 140, loss = 0.21859117\n",
      "Iteration 141, loss = 0.21785398\n",
      "Iteration 142, loss = 0.21700965\n",
      "Iteration 143, loss = 0.21607528\n",
      "Iteration 144, loss = 0.21515528\n",
      "Iteration 145, loss = 0.21441336\n",
      "Iteration 146, loss = 0.21357347\n",
      "Iteration 147, loss = 0.21257087\n",
      "Iteration 148, loss = 0.21180891\n",
      "Iteration 149, loss = 0.21085211\n",
      "Iteration 150, loss = 0.21010146\n",
      "Iteration 151, loss = 0.20920058\n",
      "Iteration 152, loss = 0.20837054\n",
      "Iteration 153, loss = 0.20775205\n",
      "Iteration 154, loss = 0.20690224\n",
      "Iteration 155, loss = 0.20595637\n",
      "Iteration 156, loss = 0.20533269\n",
      "Iteration 157, loss = 0.20442292\n",
      "Iteration 158, loss = 0.20368283\n",
      "Iteration 159, loss = 0.20271682\n",
      "Iteration 160, loss = 0.20204986\n",
      "Iteration 161, loss = 0.20113176\n",
      "Iteration 162, loss = 0.20024217\n",
      "Iteration 163, loss = 0.19950045\n",
      "Iteration 164, loss = 0.19890832\n",
      "Iteration 165, loss = 0.19788225\n",
      "Iteration 166, loss = 0.19720834\n",
      "Iteration 167, loss = 0.19639502\n",
      "Iteration 168, loss = 0.19562611\n",
      "Iteration 169, loss = 0.19483693\n",
      "Iteration 170, loss = 0.19400236\n",
      "Iteration 171, loss = 0.19336402\n",
      "Iteration 172, loss = 0.19263059\n",
      "Iteration 173, loss = 0.19186794\n",
      "Iteration 174, loss = 0.19103148\n",
      "Iteration 175, loss = 0.19029495\n",
      "Iteration 176, loss = 0.18966306\n",
      "Iteration 177, loss = 0.18887255\n",
      "Iteration 178, loss = 0.18804870\n",
      "Iteration 179, loss = 0.18739798\n",
      "Iteration 180, loss = 0.18657944\n",
      "Iteration 181, loss = 0.18590412\n",
      "Iteration 182, loss = 0.18521232\n",
      "Iteration 183, loss = 0.18446657\n",
      "Iteration 184, loss = 0.18366040\n",
      "Iteration 185, loss = 0.18294764\n",
      "Iteration 186, loss = 0.18222361\n",
      "Iteration 187, loss = 0.18144965\n",
      "Iteration 188, loss = 0.18071523\n",
      "Iteration 189, loss = 0.18012353\n",
      "Iteration 190, loss = 0.17933216\n",
      "Iteration 191, loss = 0.17869460\n",
      "Iteration 192, loss = 0.17803101\n",
      "Iteration 193, loss = 0.17736028\n",
      "Iteration 194, loss = 0.17667800\n",
      "Iteration 195, loss = 0.17602619\n",
      "Iteration 196, loss = 0.17520621\n",
      "Iteration 197, loss = 0.17470329\n",
      "Iteration 198, loss = 0.17386927\n",
      "Iteration 199, loss = 0.17331740\n",
      "Iteration 200, loss = 0.17248785\n",
      "[CV] END activation=logistic, alpha=0.0001, batch_size=50, hidden_layer_sizes=(400, 100), solver=sgd; total time=10.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.30030013\n",
      "Iteration 2, loss = 2.28435751\n",
      "Iteration 3, loss = 2.26663153\n",
      "Iteration 4, loss = 2.23916679\n",
      "Iteration 5, loss = 2.19168643\n",
      "Iteration 6, loss = 2.10033273\n",
      "Iteration 7, loss = 1.92642634\n",
      "Iteration 8, loss = 1.66855099\n",
      "Iteration 9, loss = 1.41057634\n",
      "Iteration 10, loss = 1.20921894\n",
      "Iteration 11, loss = 1.06406522\n",
      "Iteration 12, loss = 0.95934138\n",
      "Iteration 13, loss = 0.87910135\n",
      "Iteration 14, loss = 0.81338579\n",
      "Iteration 15, loss = 0.75691965\n",
      "Iteration 16, loss = 0.70767446\n",
      "Iteration 17, loss = 0.66483750\n",
      "Iteration 18, loss = 0.62781069\n",
      "Iteration 19, loss = 0.59608706\n",
      "Iteration 20, loss = 0.56867920\n",
      "Iteration 21, loss = 0.54479709\n",
      "Iteration 22, loss = 0.52387317\n",
      "Iteration 23, loss = 0.50560194\n",
      "Iteration 24, loss = 0.48897036\n",
      "Iteration 25, loss = 0.47448205\n",
      "Iteration 26, loss = 0.46129695\n",
      "Iteration 27, loss = 0.44943516\n",
      "Iteration 28, loss = 0.43862652\n",
      "Iteration 29, loss = 0.42885347\n",
      "Iteration 30, loss = 0.42017956\n",
      "Iteration 31, loss = 0.41218897\n",
      "Iteration 32, loss = 0.40462327\n",
      "Iteration 33, loss = 0.39776986\n",
      "Iteration 34, loss = 0.39172122\n",
      "Iteration 35, loss = 0.38593596\n",
      "Iteration 36, loss = 0.38030734\n",
      "Iteration 37, loss = 0.37558735\n",
      "Iteration 38, loss = 0.37072390\n",
      "Iteration 39, loss = 0.36641434\n",
      "Iteration 40, loss = 0.36230033\n",
      "Iteration 41, loss = 0.35831279\n",
      "Iteration 42, loss = 0.35466160\n",
      "Iteration 43, loss = 0.35110995\n",
      "Iteration 44, loss = 0.34775157\n",
      "Iteration 45, loss = 0.34450098\n",
      "Iteration 46, loss = 0.34159875\n",
      "Iteration 47, loss = 0.33865245\n",
      "Iteration 48, loss = 0.33581030\n",
      "Iteration 49, loss = 0.33307385\n",
      "Iteration 50, loss = 0.33047074\n",
      "Iteration 51, loss = 0.32789305\n",
      "Iteration 52, loss = 0.32559296\n",
      "Iteration 53, loss = 0.32318896\n",
      "Iteration 54, loss = 0.32093693\n",
      "Iteration 55, loss = 0.31853126\n",
      "Iteration 56, loss = 0.31657907\n",
      "Iteration 57, loss = 0.31451599\n",
      "Iteration 58, loss = 0.31232580\n",
      "Iteration 59, loss = 0.31033156\n",
      "Iteration 60, loss = 0.30849229\n",
      "Iteration 61, loss = 0.30663938\n",
      "Iteration 62, loss = 0.30465975\n",
      "Iteration 63, loss = 0.30286114\n",
      "Iteration 64, loss = 0.30115413\n",
      "Iteration 65, loss = 0.29922127\n",
      "Iteration 66, loss = 0.29780052\n",
      "Iteration 67, loss = 0.29589789\n",
      "Iteration 68, loss = 0.29418798\n",
      "Iteration 69, loss = 0.29262063\n",
      "Iteration 70, loss = 0.29098691\n",
      "Iteration 71, loss = 0.28950476\n",
      "Iteration 72, loss = 0.28800009\n",
      "Iteration 73, loss = 0.28637805\n",
      "Iteration 74, loss = 0.28488099\n",
      "Iteration 75, loss = 0.28331994\n",
      "Iteration 76, loss = 0.28180084\n",
      "Iteration 77, loss = 0.28039343\n",
      "Iteration 78, loss = 0.27896610\n",
      "Iteration 79, loss = 0.27761930\n",
      "Iteration 80, loss = 0.27612794\n",
      "Iteration 81, loss = 0.27494803\n",
      "Iteration 82, loss = 0.27352987\n",
      "Iteration 83, loss = 0.27215218\n",
      "Iteration 84, loss = 0.27104797\n",
      "Iteration 85, loss = 0.26955234\n",
      "Iteration 86, loss = 0.26816589\n",
      "Iteration 87, loss = 0.26695305\n",
      "Iteration 88, loss = 0.26567326\n",
      "Iteration 89, loss = 0.26449134\n",
      "Iteration 90, loss = 0.26311848\n",
      "Iteration 91, loss = 0.26190910\n",
      "Iteration 92, loss = 0.26072811\n",
      "Iteration 93, loss = 0.25930445\n",
      "Iteration 94, loss = 0.25838619\n",
      "Iteration 95, loss = 0.25718937\n",
      "Iteration 96, loss = 0.25614497\n",
      "Iteration 97, loss = 0.25463124\n",
      "Iteration 98, loss = 0.25359208\n",
      "Iteration 99, loss = 0.25253998\n",
      "Iteration 100, loss = 0.25165241\n",
      "Iteration 101, loss = 0.25026396\n",
      "Iteration 102, loss = 0.24909960\n",
      "Iteration 103, loss = 0.24823403\n",
      "Iteration 104, loss = 0.24704293\n",
      "Iteration 105, loss = 0.24596898\n",
      "Iteration 106, loss = 0.24499897\n",
      "Iteration 107, loss = 0.24382492\n",
      "Iteration 108, loss = 0.24266889\n",
      "Iteration 109, loss = 0.24166748\n",
      "Iteration 110, loss = 0.24051366\n",
      "Iteration 111, loss = 0.23959810\n",
      "Iteration 112, loss = 0.23855679\n",
      "Iteration 113, loss = 0.23740848\n",
      "Iteration 114, loss = 0.23633338\n",
      "Iteration 115, loss = 0.23556588\n",
      "Iteration 116, loss = 0.23455267\n",
      "Iteration 117, loss = 0.23356290\n",
      "Iteration 118, loss = 0.23246108\n",
      "Iteration 119, loss = 0.23156881\n",
      "Iteration 120, loss = 0.23048420\n",
      "Iteration 121, loss = 0.22936410\n",
      "Iteration 122, loss = 0.22873526\n",
      "Iteration 123, loss = 0.22772357\n",
      "Iteration 124, loss = 0.22665139\n",
      "Iteration 125, loss = 0.22559728\n",
      "Iteration 126, loss = 0.22486179\n",
      "Iteration 127, loss = 0.22365363\n",
      "Iteration 128, loss = 0.22297482\n",
      "Iteration 129, loss = 0.22182160\n",
      "Iteration 130, loss = 0.22105230\n",
      "Iteration 131, loss = 0.22005165\n",
      "Iteration 132, loss = 0.21929060\n",
      "Iteration 133, loss = 0.21822897\n",
      "Iteration 134, loss = 0.21733677\n",
      "Iteration 135, loss = 0.21649218\n",
      "Iteration 136, loss = 0.21562515\n",
      "Iteration 137, loss = 0.21472370\n",
      "Iteration 138, loss = 0.21370161\n",
      "Iteration 139, loss = 0.21296152\n",
      "Iteration 140, loss = 0.21199210\n",
      "Iteration 141, loss = 0.21116168\n",
      "Iteration 142, loss = 0.21037613\n",
      "Iteration 143, loss = 0.20954174\n",
      "Iteration 144, loss = 0.20844496\n",
      "Iteration 145, loss = 0.20772194\n",
      "Iteration 146, loss = 0.20686520\n",
      "Iteration 147, loss = 0.20604127\n",
      "Iteration 148, loss = 0.20520072\n",
      "Iteration 149, loss = 0.20434789\n",
      "Iteration 150, loss = 0.20349511\n",
      "Iteration 151, loss = 0.20279920\n",
      "Iteration 152, loss = 0.20181559\n",
      "Iteration 153, loss = 0.20115458\n",
      "Iteration 154, loss = 0.20033561\n",
      "Iteration 155, loss = 0.19938699\n",
      "Iteration 156, loss = 0.19863138\n",
      "Iteration 157, loss = 0.19788295\n",
      "Iteration 158, loss = 0.19701722\n",
      "Iteration 159, loss = 0.19615690\n",
      "Iteration 160, loss = 0.19535225\n",
      "Iteration 161, loss = 0.19457549\n",
      "Iteration 162, loss = 0.19401255\n",
      "Iteration 163, loss = 0.19322667\n",
      "Iteration 164, loss = 0.19252130\n",
      "Iteration 165, loss = 0.19159204\n",
      "Iteration 166, loss = 0.19077893\n",
      "Iteration 167, loss = 0.19002232\n",
      "Iteration 168, loss = 0.18937258\n",
      "Iteration 169, loss = 0.18851864\n",
      "Iteration 170, loss = 0.18780196\n",
      "Iteration 171, loss = 0.18715054\n",
      "Iteration 172, loss = 0.18626352\n",
      "Iteration 173, loss = 0.18551298\n",
      "Iteration 174, loss = 0.18479462\n",
      "Iteration 175, loss = 0.18411909\n",
      "Iteration 176, loss = 0.18342276\n",
      "Iteration 177, loss = 0.18269009\n",
      "Iteration 178, loss = 0.18199495\n",
      "Iteration 179, loss = 0.18112804\n",
      "Iteration 180, loss = 0.18054842\n",
      "Iteration 181, loss = 0.17980502\n",
      "Iteration 182, loss = 0.17912112\n",
      "Iteration 183, loss = 0.17842470\n",
      "Iteration 184, loss = 0.17773245\n",
      "Iteration 185, loss = 0.17706192\n",
      "Iteration 186, loss = 0.17632596\n",
      "Iteration 187, loss = 0.17557430\n",
      "Iteration 188, loss = 0.17501097\n",
      "Iteration 189, loss = 0.17421897\n",
      "Iteration 190, loss = 0.17356521\n",
      "Iteration 191, loss = 0.17316210\n",
      "Iteration 192, loss = 0.17233627\n",
      "Iteration 193, loss = 0.17176252\n",
      "Iteration 194, loss = 0.17094524\n",
      "Iteration 195, loss = 0.17039797\n",
      "Iteration 196, loss = 0.16963508\n",
      "Iteration 197, loss = 0.16898104\n",
      "Iteration 198, loss = 0.16828054\n",
      "Iteration 199, loss = 0.16766474\n",
      "Iteration 200, loss = 0.16711879\n",
      "[CV] END activation=logistic, alpha=0.0001, batch_size=50, hidden_layer_sizes=(400, 100), solver=sgd; total time=10.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.29904628\n",
      "Iteration 2, loss = 2.28108608\n",
      "Iteration 3, loss = 2.26010568\n",
      "Iteration 4, loss = 2.22697723\n",
      "Iteration 5, loss = 2.16769938\n",
      "Iteration 6, loss = 2.05291200\n",
      "Iteration 7, loss = 1.85423943\n",
      "Iteration 8, loss = 1.59970787\n",
      "Iteration 9, loss = 1.36647846\n",
      "Iteration 10, loss = 1.18162621\n",
      "Iteration 11, loss = 1.03892639\n",
      "Iteration 12, loss = 0.92950091\n",
      "Iteration 13, loss = 0.84288224\n",
      "Iteration 14, loss = 0.77250409\n",
      "Iteration 15, loss = 0.71379423\n",
      "Iteration 16, loss = 0.66487978\n",
      "Iteration 17, loss = 0.62394309\n",
      "Iteration 18, loss = 0.58977140\n",
      "Iteration 19, loss = 0.56089995\n",
      "Iteration 20, loss = 0.53638784\n",
      "Iteration 21, loss = 0.51548503\n",
      "Iteration 22, loss = 0.49714461\n",
      "Iteration 23, loss = 0.48139184\n",
      "Iteration 24, loss = 0.46728014\n",
      "Iteration 25, loss = 0.45481463\n",
      "Iteration 26, loss = 0.44376176\n",
      "Iteration 27, loss = 0.43368524\n",
      "Iteration 28, loss = 0.42480000\n",
      "Iteration 29, loss = 0.41663400\n",
      "Iteration 30, loss = 0.40934463\n",
      "Iteration 31, loss = 0.40245842\n",
      "Iteration 32, loss = 0.39624981\n",
      "Iteration 33, loss = 0.39047939\n",
      "Iteration 34, loss = 0.38518126\n",
      "Iteration 35, loss = 0.38031762\n",
      "Iteration 36, loss = 0.37551512\n",
      "Iteration 37, loss = 0.37120978\n",
      "Iteration 38, loss = 0.36711572\n",
      "Iteration 39, loss = 0.36308233\n",
      "Iteration 40, loss = 0.35949178\n",
      "Iteration 41, loss = 0.35603830\n",
      "Iteration 42, loss = 0.35271048\n",
      "Iteration 43, loss = 0.34956594\n",
      "Iteration 44, loss = 0.34664220\n",
      "Iteration 45, loss = 0.34381983\n",
      "Iteration 46, loss = 0.34101367\n",
      "Iteration 47, loss = 0.33827736\n",
      "Iteration 48, loss = 0.33575039\n",
      "Iteration 49, loss = 0.33315177\n",
      "Iteration 50, loss = 0.33083654\n",
      "Iteration 51, loss = 0.32836943\n",
      "Iteration 52, loss = 0.32615520\n",
      "Iteration 53, loss = 0.32405506\n",
      "Iteration 54, loss = 0.32166368\n",
      "Iteration 55, loss = 0.31960907\n",
      "Iteration 56, loss = 0.31747332\n",
      "Iteration 57, loss = 0.31569282\n",
      "Iteration 58, loss = 0.31377904\n",
      "Iteration 59, loss = 0.31171772\n",
      "Iteration 60, loss = 0.30989487\n",
      "Iteration 61, loss = 0.30817162\n",
      "Iteration 62, loss = 0.30632265\n",
      "Iteration 63, loss = 0.30447112\n",
      "Iteration 64, loss = 0.30285836\n",
      "Iteration 65, loss = 0.30121273\n",
      "Iteration 66, loss = 0.29929536\n",
      "Iteration 67, loss = 0.29786256\n",
      "Iteration 68, loss = 0.29600107\n",
      "Iteration 69, loss = 0.29443836\n",
      "Iteration 70, loss = 0.29312820\n",
      "Iteration 71, loss = 0.29151216\n",
      "Iteration 72, loss = 0.29006746\n",
      "Iteration 73, loss = 0.28858708\n",
      "Iteration 74, loss = 0.28709079\n",
      "Iteration 75, loss = 0.28552299\n",
      "Iteration 76, loss = 0.28434172\n",
      "Iteration 77, loss = 0.28278594\n",
      "Iteration 78, loss = 0.28144957\n",
      "Iteration 79, loss = 0.28003819\n",
      "Iteration 80, loss = 0.27851121\n",
      "Iteration 81, loss = 0.27744678\n",
      "Iteration 82, loss = 0.27608051\n",
      "Iteration 83, loss = 0.27470341\n",
      "Iteration 84, loss = 0.27341543\n",
      "Iteration 85, loss = 0.27214594\n",
      "Iteration 86, loss = 0.27091410\n",
      "Iteration 87, loss = 0.26946350\n",
      "Iteration 88, loss = 0.26861970\n",
      "Iteration 89, loss = 0.26734749\n",
      "Iteration 90, loss = 0.26608793\n",
      "Iteration 91, loss = 0.26485123\n",
      "Iteration 92, loss = 0.26406485\n",
      "Iteration 93, loss = 0.26243595\n",
      "Iteration 94, loss = 0.26148307\n",
      "Iteration 95, loss = 0.26026199\n",
      "Iteration 96, loss = 0.25908101\n",
      "Iteration 97, loss = 0.25817452\n",
      "Iteration 98, loss = 0.25705163\n",
      "Iteration 99, loss = 0.25584893\n",
      "Iteration 100, loss = 0.25457745\n",
      "Iteration 101, loss = 0.25362082\n",
      "Iteration 102, loss = 0.25240022\n",
      "Iteration 103, loss = 0.25145319\n",
      "Iteration 104, loss = 0.25040160\n",
      "Iteration 105, loss = 0.24945762\n",
      "Iteration 106, loss = 0.24825908\n",
      "Iteration 107, loss = 0.24743672\n",
      "Iteration 108, loss = 0.24608199\n",
      "Iteration 109, loss = 0.24522343\n",
      "Iteration 110, loss = 0.24407784\n",
      "Iteration 111, loss = 0.24324186\n",
      "Iteration 112, loss = 0.24226851\n",
      "Iteration 113, loss = 0.24109622\n",
      "Iteration 114, loss = 0.24007290\n",
      "Iteration 115, loss = 0.23910214\n",
      "Iteration 116, loss = 0.23809373\n",
      "Iteration 117, loss = 0.23721103\n",
      "Iteration 118, loss = 0.23612621\n",
      "Iteration 119, loss = 0.23520931\n",
      "Iteration 120, loss = 0.23413590\n",
      "Iteration 121, loss = 0.23336601\n",
      "Iteration 122, loss = 0.23230746\n",
      "Iteration 123, loss = 0.23138624\n",
      "Iteration 124, loss = 0.23037752\n",
      "Iteration 125, loss = 0.22949279\n",
      "Iteration 126, loss = 0.22843316\n",
      "Iteration 127, loss = 0.22754397\n",
      "Iteration 128, loss = 0.22651596\n",
      "Iteration 129, loss = 0.22586626\n",
      "Iteration 130, loss = 0.22491935\n",
      "Iteration 131, loss = 0.22402606\n",
      "Iteration 132, loss = 0.22305816\n",
      "Iteration 133, loss = 0.22212736\n",
      "Iteration 134, loss = 0.22121978\n",
      "Iteration 135, loss = 0.22037709\n",
      "Iteration 136, loss = 0.21942574\n",
      "Iteration 137, loss = 0.21855025\n",
      "Iteration 138, loss = 0.21774618\n",
      "Iteration 139, loss = 0.21674580\n",
      "Iteration 140, loss = 0.21584170\n",
      "Iteration 141, loss = 0.21510822\n",
      "Iteration 142, loss = 0.21410108\n",
      "Iteration 143, loss = 0.21339489\n",
      "Iteration 144, loss = 0.21240989\n",
      "Iteration 145, loss = 0.21160219\n",
      "Iteration 146, loss = 0.21082406\n",
      "Iteration 147, loss = 0.20987939\n",
      "Iteration 148, loss = 0.20896622\n",
      "Iteration 149, loss = 0.20823839\n",
      "Iteration 150, loss = 0.20731582\n",
      "Iteration 151, loss = 0.20645511\n",
      "Iteration 152, loss = 0.20537814\n",
      "Iteration 153, loss = 0.20485460\n",
      "Iteration 154, loss = 0.20403514\n",
      "Iteration 155, loss = 0.20315756\n",
      "Iteration 156, loss = 0.20252952\n",
      "Iteration 157, loss = 0.20143616\n",
      "Iteration 158, loss = 0.20081825\n",
      "Iteration 159, loss = 0.19999021\n",
      "Iteration 160, loss = 0.19902103\n",
      "Iteration 161, loss = 0.19851116\n",
      "Iteration 162, loss = 0.19759949\n",
      "Iteration 163, loss = 0.19668833\n",
      "Iteration 164, loss = 0.19609053\n",
      "Iteration 165, loss = 0.19517405\n",
      "Iteration 166, loss = 0.19445074\n",
      "Iteration 167, loss = 0.19365861\n",
      "Iteration 168, loss = 0.19296753\n",
      "Iteration 169, loss = 0.19225721\n",
      "Iteration 170, loss = 0.19129520\n",
      "Iteration 171, loss = 0.19058830\n",
      "Iteration 172, loss = 0.18992355\n",
      "Iteration 173, loss = 0.18905733\n",
      "Iteration 174, loss = 0.18830921\n",
      "Iteration 175, loss = 0.18764732\n",
      "Iteration 176, loss = 0.18691847\n",
      "Iteration 177, loss = 0.18608413\n",
      "Iteration 178, loss = 0.18548410\n",
      "Iteration 179, loss = 0.18479102\n",
      "Iteration 180, loss = 0.18400360\n",
      "Iteration 181, loss = 0.18320043\n",
      "Iteration 182, loss = 0.18247809\n",
      "Iteration 183, loss = 0.18191765\n",
      "Iteration 184, loss = 0.18097644\n",
      "Iteration 185, loss = 0.18033174\n",
      "Iteration 186, loss = 0.17971206\n",
      "Iteration 187, loss = 0.17909848\n",
      "Iteration 188, loss = 0.17832685\n",
      "Iteration 189, loss = 0.17753976\n",
      "Iteration 190, loss = 0.17698542\n",
      "Iteration 191, loss = 0.17618855\n",
      "Iteration 192, loss = 0.17548378\n",
      "Iteration 193, loss = 0.17490623\n",
      "Iteration 194, loss = 0.17432251\n",
      "Iteration 195, loss = 0.17351430\n",
      "Iteration 196, loss = 0.17284128\n",
      "Iteration 197, loss = 0.17226544\n",
      "Iteration 198, loss = 0.17161558\n",
      "Iteration 199, loss = 0.17101716\n",
      "Iteration 200, loss = 0.17027759\n",
      "[CV] END activation=logistic, alpha=0.0001, batch_size=50, hidden_layer_sizes=(400, 100), solver=sgd; total time=10.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.29548019\n",
      "Iteration 2, loss = 2.27856766\n",
      "Iteration 3, loss = 2.25701395\n",
      "Iteration 4, loss = 2.22147512\n",
      "Iteration 5, loss = 2.15636119\n",
      "Iteration 6, loss = 2.02895082\n",
      "Iteration 7, loss = 1.81464481\n",
      "Iteration 8, loss = 1.56530108\n",
      "Iteration 9, loss = 1.35608868\n",
      "Iteration 10, loss = 1.19133625\n",
      "Iteration 11, loss = 1.05922721\n",
      "Iteration 12, loss = 0.95609463\n",
      "Iteration 13, loss = 0.87485102\n",
      "Iteration 14, loss = 0.80824724\n",
      "Iteration 15, loss = 0.75215543\n",
      "Iteration 16, loss = 0.70387581\n",
      "Iteration 17, loss = 0.66189508\n",
      "Iteration 18, loss = 0.62576872\n",
      "Iteration 19, loss = 0.59442437\n",
      "Iteration 20, loss = 0.56702201\n",
      "Iteration 21, loss = 0.54342885\n",
      "Iteration 22, loss = 0.52232716\n",
      "Iteration 23, loss = 0.50367198\n",
      "Iteration 24, loss = 0.48701218\n",
      "Iteration 25, loss = 0.47209791\n",
      "Iteration 26, loss = 0.45890689\n",
      "Iteration 27, loss = 0.44698310\n",
      "Iteration 28, loss = 0.43635846\n",
      "Iteration 29, loss = 0.42658486\n",
      "Iteration 30, loss = 0.41783126\n",
      "Iteration 31, loss = 0.41007013\n",
      "Iteration 32, loss = 0.40267982\n",
      "Iteration 33, loss = 0.39619283\n",
      "Iteration 34, loss = 0.39011893\n",
      "Iteration 35, loss = 0.38437676\n",
      "Iteration 36, loss = 0.37941820\n",
      "Iteration 37, loss = 0.37446361\n",
      "Iteration 38, loss = 0.36992346\n",
      "Iteration 39, loss = 0.36568448\n",
      "Iteration 40, loss = 0.36182888\n",
      "Iteration 41, loss = 0.35809350\n",
      "Iteration 42, loss = 0.35460733\n",
      "Iteration 43, loss = 0.35107081\n",
      "Iteration 44, loss = 0.34786894\n",
      "Iteration 45, loss = 0.34517180\n",
      "Iteration 46, loss = 0.34209614\n",
      "Iteration 47, loss = 0.33917077\n",
      "Iteration 48, loss = 0.33668958\n",
      "Iteration 49, loss = 0.33403321\n",
      "Iteration 50, loss = 0.33154998\n",
      "Iteration 51, loss = 0.32909555\n",
      "Iteration 52, loss = 0.32703170\n",
      "Iteration 53, loss = 0.32461000\n",
      "Iteration 54, loss = 0.32229761\n",
      "Iteration 55, loss = 0.32036192\n",
      "Iteration 56, loss = 0.31817902\n",
      "Iteration 57, loss = 0.31625121\n",
      "Iteration 58, loss = 0.31438229\n",
      "Iteration 59, loss = 0.31240560\n",
      "Iteration 60, loss = 0.31041291\n",
      "Iteration 61, loss = 0.30881500\n",
      "Iteration 62, loss = 0.30697156\n",
      "Iteration 63, loss = 0.30539136\n",
      "Iteration 64, loss = 0.30364423\n",
      "Iteration 65, loss = 0.30185433\n",
      "Iteration 66, loss = 0.30040047\n",
      "Iteration 67, loss = 0.29867946\n",
      "Iteration 68, loss = 0.29727131\n",
      "Iteration 69, loss = 0.29560559\n",
      "Iteration 70, loss = 0.29393645\n",
      "Iteration 71, loss = 0.29258631\n",
      "Iteration 72, loss = 0.29128042\n",
      "Iteration 73, loss = 0.28951703\n",
      "Iteration 74, loss = 0.28831910\n",
      "Iteration 75, loss = 0.28683459\n",
      "Iteration 76, loss = 0.28546710\n",
      "Iteration 77, loss = 0.28415568\n",
      "Iteration 78, loss = 0.28287068\n",
      "Iteration 79, loss = 0.28143752\n",
      "Iteration 80, loss = 0.28026262\n",
      "Iteration 81, loss = 0.27882844\n",
      "Iteration 82, loss = 0.27759137\n",
      "Iteration 83, loss = 0.27637415\n",
      "Iteration 84, loss = 0.27522367\n",
      "Iteration 85, loss = 0.27400909\n",
      "Iteration 86, loss = 0.27265185\n",
      "Iteration 87, loss = 0.27148830\n",
      "Iteration 88, loss = 0.27035157\n",
      "Iteration 89, loss = 0.26913540\n",
      "Iteration 90, loss = 0.26775554\n",
      "Iteration 91, loss = 0.26684623\n",
      "Iteration 92, loss = 0.26580313\n",
      "Iteration 93, loss = 0.26437277\n",
      "Iteration 94, loss = 0.26341727\n",
      "Iteration 95, loss = 0.26218389\n",
      "Iteration 96, loss = 0.26132402\n",
      "Iteration 97, loss = 0.26027082\n",
      "Iteration 98, loss = 0.25890372\n",
      "Iteration 99, loss = 0.25796298\n",
      "Iteration 100, loss = 0.25691480\n",
      "Iteration 101, loss = 0.25581322\n",
      "Iteration 102, loss = 0.25495966\n",
      "Iteration 103, loss = 0.25396541\n",
      "Iteration 104, loss = 0.25278246\n",
      "Iteration 105, loss = 0.25177221\n",
      "Iteration 106, loss = 0.25098208\n",
      "Iteration 107, loss = 0.24992230\n",
      "Iteration 108, loss = 0.24869503\n",
      "Iteration 109, loss = 0.24809768\n",
      "Iteration 110, loss = 0.24699774\n",
      "Iteration 111, loss = 0.24578841\n",
      "Iteration 112, loss = 0.24497449\n",
      "Iteration 113, loss = 0.24411684\n",
      "Iteration 114, loss = 0.24309257\n",
      "Iteration 115, loss = 0.24217208\n",
      "Iteration 116, loss = 0.24121071\n",
      "Iteration 117, loss = 0.24026179\n",
      "Iteration 118, loss = 0.23931933\n",
      "Iteration 119, loss = 0.23844369\n",
      "Iteration 120, loss = 0.23748109\n",
      "Iteration 121, loss = 0.23660574\n",
      "Iteration 122, loss = 0.23554668\n",
      "Iteration 123, loss = 0.23480478\n",
      "Iteration 124, loss = 0.23373570\n",
      "Iteration 125, loss = 0.23300157\n",
      "Iteration 126, loss = 0.23215437\n",
      "Iteration 127, loss = 0.23124743\n",
      "Iteration 128, loss = 0.23044312\n",
      "Iteration 129, loss = 0.22955168\n",
      "Iteration 130, loss = 0.22859237\n",
      "Iteration 131, loss = 0.22786863\n",
      "Iteration 132, loss = 0.22696204\n",
      "Iteration 133, loss = 0.22604049\n",
      "Iteration 134, loss = 0.22510282\n",
      "Iteration 135, loss = 0.22444773\n",
      "Iteration 136, loss = 0.22354657\n",
      "Iteration 137, loss = 0.22247223\n",
      "Iteration 138, loss = 0.22181240\n",
      "Iteration 139, loss = 0.22100952\n",
      "Iteration 140, loss = 0.22015773\n",
      "Iteration 141, loss = 0.21930655\n",
      "Iteration 142, loss = 0.21839724\n",
      "Iteration 143, loss = 0.21771268\n",
      "Iteration 144, loss = 0.21681251\n",
      "Iteration 145, loss = 0.21592286\n",
      "Iteration 146, loss = 0.21517140\n",
      "Iteration 147, loss = 0.21427465\n",
      "Iteration 148, loss = 0.21363778\n",
      "Iteration 149, loss = 0.21280080\n",
      "Iteration 150, loss = 0.21194658\n",
      "Iteration 151, loss = 0.21122650\n",
      "Iteration 152, loss = 0.21041388\n",
      "Iteration 153, loss = 0.20958065\n",
      "Iteration 154, loss = 0.20893002\n",
      "Iteration 155, loss = 0.20796174\n",
      "Iteration 156, loss = 0.20728346\n",
      "Iteration 157, loss = 0.20647082\n",
      "Iteration 158, loss = 0.20561681\n",
      "Iteration 159, loss = 0.20490725\n",
      "Iteration 160, loss = 0.20411015\n",
      "Iteration 161, loss = 0.20310270\n",
      "Iteration 162, loss = 0.20256496\n",
      "Iteration 163, loss = 0.20180257\n",
      "Iteration 164, loss = 0.20104473\n",
      "Iteration 165, loss = 0.20019459\n",
      "Iteration 166, loss = 0.19948748\n",
      "Iteration 167, loss = 0.19879775\n",
      "Iteration 168, loss = 0.19788259\n",
      "Iteration 169, loss = 0.19728095\n",
      "Iteration 170, loss = 0.19649935\n",
      "Iteration 171, loss = 0.19584049\n",
      "Iteration 172, loss = 0.19500097\n",
      "Iteration 173, loss = 0.19427099\n",
      "Iteration 174, loss = 0.19353335\n",
      "Iteration 175, loss = 0.19277632\n",
      "Iteration 176, loss = 0.19191582\n",
      "Iteration 177, loss = 0.19122959\n",
      "Iteration 178, loss = 0.19054876\n",
      "Iteration 179, loss = 0.18980887\n",
      "Iteration 180, loss = 0.18913846\n",
      "Iteration 181, loss = 0.18837904\n",
      "Iteration 182, loss = 0.18762077\n",
      "Iteration 183, loss = 0.18682048\n",
      "Iteration 184, loss = 0.18621936\n",
      "Iteration 185, loss = 0.18541923\n",
      "Iteration 186, loss = 0.18480684\n",
      "Iteration 187, loss = 0.18408847\n",
      "Iteration 188, loss = 0.18354130\n",
      "Iteration 189, loss = 0.18260858\n",
      "Iteration 190, loss = 0.18189667\n",
      "Iteration 191, loss = 0.18131169\n",
      "Iteration 192, loss = 0.18059380\n",
      "Iteration 193, loss = 0.17984897\n",
      "Iteration 194, loss = 0.17932817\n",
      "Iteration 195, loss = 0.17862882\n",
      "Iteration 196, loss = 0.17785165\n",
      "Iteration 197, loss = 0.17713272\n",
      "Iteration 198, loss = 0.17656437\n",
      "Iteration 199, loss = 0.17582224\n",
      "Iteration 200, loss = 0.17516681\n",
      "[CV] END activation=logistic, alpha=0.0001, batch_size=50, hidden_layer_sizes=(400, 100), solver=sgd; total time=10.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.04685644\n",
      "Iteration 2, loss = 0.33216926\n",
      "Iteration 3, loss = 0.21840938\n",
      "Iteration 4, loss = 0.16607256\n",
      "Iteration 5, loss = 0.13346918\n",
      "Iteration 6, loss = 0.11019886\n",
      "Iteration 7, loss = 0.09057031\n",
      "Iteration 8, loss = 0.07765650\n",
      "Iteration 9, loss = 0.06509116\n",
      "Iteration 10, loss = 0.05703500\n",
      "Iteration 11, loss = 0.04984016\n",
      "Iteration 12, loss = 0.04237940\n",
      "Iteration 13, loss = 0.03562654\n",
      "Iteration 14, loss = 0.03070728\n",
      "Iteration 15, loss = 0.02813374\n",
      "Iteration 16, loss = 0.02371595\n",
      "Iteration 17, loss = 0.01996819\n",
      "Iteration 18, loss = 0.02010147\n",
      "Iteration 19, loss = 0.01903057\n",
      "Iteration 20, loss = 0.01607078\n",
      "Iteration 21, loss = 0.01417604\n",
      "Iteration 22, loss = 0.01163691\n",
      "Iteration 23, loss = 0.01245543\n",
      "Iteration 24, loss = 0.01832013\n",
      "Iteration 25, loss = 0.01009249\n",
      "Iteration 26, loss = 0.00947177\n",
      "Iteration 27, loss = 0.00836260\n",
      "Iteration 28, loss = 0.01877604\n",
      "Iteration 29, loss = 0.00998095\n",
      "Iteration 30, loss = 0.00729255\n",
      "Iteration 31, loss = 0.00772713\n",
      "Iteration 32, loss = 0.01858261\n",
      "Iteration 33, loss = 0.00781627\n",
      "Iteration 34, loss = 0.00623657\n",
      "Iteration 35, loss = 0.00565581\n",
      "Iteration 36, loss = 0.00548461\n",
      "Iteration 37, loss = 0.02370491\n",
      "Iteration 38, loss = 0.00863110\n",
      "Iteration 39, loss = 0.00637656\n",
      "Iteration 40, loss = 0.00544721\n",
      "Iteration 41, loss = 0.00532113\n",
      "Iteration 42, loss = 0.00520892\n",
      "Iteration 43, loss = 0.00505302\n",
      "Iteration 44, loss = 0.02797117\n",
      "Iteration 45, loss = 0.00897634\n",
      "Iteration 46, loss = 0.00916767\n",
      "Iteration 47, loss = 0.00707569\n",
      "Iteration 48, loss = 0.01264329\n",
      "Iteration 49, loss = 0.00732187\n",
      "Iteration 50, loss = 0.00641513\n",
      "Iteration 51, loss = 0.01285807\n",
      "Iteration 52, loss = 0.00691646\n",
      "Iteration 53, loss = 0.00572764\n",
      "Iteration 54, loss = 0.00545752\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.0001, batch_size=50, hidden_layer_sizes=(100, 100, 100), solver=adam; total time= 1.9min\n",
      "Iteration 1, loss = 1.02835928\n",
      "Iteration 2, loss = 0.31460932\n",
      "Iteration 3, loss = 0.21528998\n",
      "Iteration 4, loss = 0.16568381\n",
      "Iteration 5, loss = 0.13221330\n",
      "Iteration 6, loss = 0.11057984\n",
      "Iteration 7, loss = 0.09404030\n",
      "Iteration 8, loss = 0.07820300\n",
      "Iteration 9, loss = 0.06647820\n",
      "Iteration 10, loss = 0.05734422\n",
      "Iteration 11, loss = 0.04798471\n",
      "Iteration 12, loss = 0.04044352\n",
      "Iteration 13, loss = 0.03478982\n",
      "Iteration 14, loss = 0.03010085\n",
      "Iteration 15, loss = 0.02654975\n",
      "Iteration 16, loss = 0.02228678\n",
      "Iteration 17, loss = 0.01935622\n",
      "Iteration 18, loss = 0.01677457\n",
      "Iteration 19, loss = 0.01507929\n",
      "Iteration 20, loss = 0.01618841\n",
      "Iteration 21, loss = 0.01289343\n",
      "Iteration 22, loss = 0.01328312\n",
      "Iteration 23, loss = 0.01145357\n",
      "Iteration 24, loss = 0.01166740\n",
      "Iteration 25, loss = 0.00911956\n",
      "Iteration 26, loss = 0.01256560\n",
      "Iteration 27, loss = 0.01287993\n",
      "Iteration 28, loss = 0.00832777\n",
      "Iteration 29, loss = 0.00798971\n",
      "Iteration 30, loss = 0.00835459\n",
      "Iteration 31, loss = 0.01754289\n",
      "Iteration 32, loss = 0.00730027\n",
      "Iteration 33, loss = 0.00648916\n",
      "Iteration 34, loss = 0.01356325\n",
      "Iteration 35, loss = 0.00792765\n",
      "Iteration 36, loss = 0.00568927\n",
      "Iteration 37, loss = 0.00529807\n",
      "Iteration 38, loss = 0.00515674\n",
      "Iteration 39, loss = 0.02057774\n",
      "Iteration 40, loss = 0.01013455\n",
      "Iteration 41, loss = 0.00604977\n",
      "Iteration 42, loss = 0.00539300\n",
      "Iteration 43, loss = 0.00518151\n",
      "Iteration 44, loss = 0.00506690\n",
      "Iteration 45, loss = 0.00494695\n",
      "Iteration 46, loss = 0.00761946\n",
      "Iteration 47, loss = 0.02071943\n",
      "Iteration 48, loss = 0.00803450\n",
      "Iteration 49, loss = 0.00578248\n",
      "Iteration 50, loss = 0.00505283\n",
      "Iteration 51, loss = 0.00494943\n",
      "Iteration 52, loss = 0.00485035\n",
      "Iteration 53, loss = 0.00475191\n",
      "Iteration 54, loss = 0.00729424\n",
      "Iteration 55, loss = 0.02297499\n",
      "Iteration 56, loss = 0.00650654\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.0001, batch_size=50, hidden_layer_sizes=(100, 100, 100), solver=adam; total time= 2.1min\n",
      "Iteration 1, loss = 1.00379825\n",
      "Iteration 2, loss = 0.33273669\n",
      "Iteration 3, loss = 0.22650259\n",
      "Iteration 4, loss = 0.17389517\n",
      "Iteration 5, loss = 0.14017082\n",
      "Iteration 6, loss = 0.11563583\n",
      "Iteration 7, loss = 0.09844356\n",
      "Iteration 8, loss = 0.08260166\n",
      "Iteration 9, loss = 0.07094044\n",
      "Iteration 10, loss = 0.06115292\n",
      "Iteration 11, loss = 0.05133549\n",
      "Iteration 12, loss = 0.04409919\n",
      "Iteration 13, loss = 0.03874224\n",
      "Iteration 14, loss = 0.03313562\n",
      "Iteration 15, loss = 0.02726515\n",
      "Iteration 16, loss = 0.02297986\n",
      "Iteration 17, loss = 0.02471942\n",
      "Iteration 18, loss = 0.01945158\n",
      "Iteration 19, loss = 0.01540282\n",
      "Iteration 20, loss = 0.01810600\n",
      "Iteration 21, loss = 0.01397540\n",
      "Iteration 22, loss = 0.01398990\n",
      "Iteration 23, loss = 0.01326946\n",
      "Iteration 24, loss = 0.01605350\n",
      "Iteration 25, loss = 0.01073563\n",
      "Iteration 26, loss = 0.01178756\n",
      "Iteration 27, loss = 0.01085701\n",
      "Iteration 28, loss = 0.01294543\n",
      "Iteration 29, loss = 0.00857838\n",
      "Iteration 30, loss = 0.01193275\n",
      "Iteration 31, loss = 0.01191037\n",
      "Iteration 32, loss = 0.00869390\n",
      "Iteration 33, loss = 0.00876698\n",
      "Iteration 34, loss = 0.01256486\n",
      "Iteration 35, loss = 0.01293277\n",
      "Iteration 36, loss = 0.00650564\n",
      "Iteration 37, loss = 0.00605284\n",
      "Iteration 38, loss = 0.01609740\n",
      "Iteration 39, loss = 0.00813412\n",
      "Iteration 40, loss = 0.00644913\n",
      "Iteration 41, loss = 0.00544589\n",
      "Iteration 42, loss = 0.00530809\n",
      "Iteration 43, loss = 0.00518999\n",
      "Iteration 44, loss = 0.00505998\n",
      "Iteration 45, loss = 0.01486016\n",
      "Iteration 46, loss = 0.01555919\n",
      "Iteration 47, loss = 0.00587640\n",
      "Iteration 48, loss = 0.00529551\n",
      "Iteration 49, loss = 0.00512670\n",
      "Iteration 50, loss = 0.00499428\n",
      "Iteration 51, loss = 0.00489608\n",
      "Iteration 52, loss = 0.00479305\n",
      "Iteration 53, loss = 0.02382480\n",
      "Iteration 54, loss = 0.00950231\n",
      "Iteration 55, loss = 0.00614973\n",
      "Iteration 56, loss = 0.00503740\n",
      "Iteration 57, loss = 0.00489506\n",
      "Iteration 58, loss = 0.00481326\n",
      "Iteration 59, loss = 0.00471967\n",
      "Iteration 60, loss = 0.00461657\n",
      "Iteration 61, loss = 0.02912069\n",
      "Iteration 62, loss = 0.00689961\n",
      "Iteration 63, loss = 0.00503976\n",
      "Iteration 64, loss = 0.00484854\n",
      "Iteration 65, loss = 0.00475880\n",
      "Iteration 66, loss = 0.00467765\n",
      "Iteration 67, loss = 0.00458452\n",
      "Iteration 68, loss = 0.01027957\n",
      "Iteration 69, loss = 0.02161085\n",
      "Iteration 70, loss = 0.00554215\n",
      "Iteration 71, loss = 0.00485448\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.0001, batch_size=50, hidden_layer_sizes=(100, 100, 100), solver=adam; total time= 2.6min\n",
      "Iteration 1, loss = 0.95658324\n",
      "Iteration 2, loss = 0.29419243\n",
      "Iteration 3, loss = 0.20786673\n",
      "Iteration 4, loss = 0.16135657\n",
      "Iteration 5, loss = 0.13010912\n",
      "Iteration 6, loss = 0.10914323\n",
      "Iteration 7, loss = 0.09016392\n",
      "Iteration 8, loss = 0.07563762\n",
      "Iteration 9, loss = 0.06351994\n",
      "Iteration 10, loss = 0.05532258\n",
      "Iteration 11, loss = 0.04712882\n",
      "Iteration 12, loss = 0.04028842\n",
      "Iteration 13, loss = 0.03610527\n",
      "Iteration 14, loss = 0.02951501\n",
      "Iteration 15, loss = 0.02560259\n",
      "Iteration 16, loss = 0.02260190\n",
      "Iteration 17, loss = 0.01914076\n",
      "Iteration 18, loss = 0.01837681\n",
      "Iteration 19, loss = 0.01678878\n",
      "Iteration 20, loss = 0.01315192\n",
      "Iteration 21, loss = 0.01379755\n",
      "Iteration 22, loss = 0.01419996\n",
      "Iteration 23, loss = 0.01227114\n",
      "Iteration 24, loss = 0.00896613\n",
      "Iteration 25, loss = 0.01186526\n",
      "Iteration 26, loss = 0.00971954\n",
      "Iteration 27, loss = 0.01517965\n",
      "Iteration 28, loss = 0.00762618\n",
      "Iteration 29, loss = 0.00601537\n",
      "Iteration 30, loss = 0.00569421\n",
      "Iteration 31, loss = 0.01936813\n",
      "Iteration 32, loss = 0.01049960\n",
      "Iteration 33, loss = 0.00848530\n",
      "Iteration 34, loss = 0.00627387\n",
      "Iteration 35, loss = 0.00617012\n",
      "Iteration 36, loss = 0.00550118\n",
      "Iteration 37, loss = 0.01841027\n",
      "Iteration 38, loss = 0.00759081\n",
      "Iteration 39, loss = 0.00556027\n",
      "Iteration 40, loss = 0.00769061\n",
      "Iteration 41, loss = 0.01676038\n",
      "Iteration 42, loss = 0.00621385\n",
      "Iteration 43, loss = 0.00529441\n",
      "Iteration 44, loss = 0.00514645\n",
      "Iteration 45, loss = 0.00503662\n",
      "Iteration 46, loss = 0.00492204\n",
      "Iteration 47, loss = 0.01608369\n",
      "Iteration 48, loss = 0.01499634\n",
      "Iteration 49, loss = 0.00577964\n",
      "Iteration 50, loss = 0.00892482\n",
      "Iteration 51, loss = 0.01351109\n",
      "Iteration 52, loss = 0.00615643\n",
      "Iteration 53, loss = 0.00517670\n",
      "Iteration 54, loss = 0.00500529\n",
      "Iteration 55, loss = 0.00490800\n",
      "Iteration 56, loss = 0.00479899\n",
      "Iteration 57, loss = 0.00467533\n",
      "Iteration 58, loss = 0.00453420\n",
      "Iteration 59, loss = 0.02214271\n",
      "Iteration 60, loss = 0.00811433\n",
      "Iteration 61, loss = 0.00509406\n",
      "Iteration 62, loss = 0.00466863\n",
      "Iteration 63, loss = 0.00455951\n",
      "Iteration 64, loss = 0.00447924\n",
      "Iteration 65, loss = 0.00439891\n",
      "Iteration 66, loss = 0.00496815\n",
      "Iteration 67, loss = 0.02545518\n",
      "Iteration 68, loss = 0.00559176\n",
      "Iteration 69, loss = 0.00468993\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.0001, batch_size=50, hidden_layer_sizes=(100, 100, 100), solver=adam; total time= 2.6min\n",
      "Iteration 1, loss = 1.00612727\n",
      "Iteration 2, loss = 0.31488995\n",
      "Iteration 3, loss = 0.21331103\n",
      "Iteration 4, loss = 0.16408970\n",
      "Iteration 5, loss = 0.13436569\n",
      "Iteration 6, loss = 0.11085225\n",
      "Iteration 7, loss = 0.09237241\n",
      "Iteration 8, loss = 0.08021857\n",
      "Iteration 9, loss = 0.06707496\n",
      "Iteration 10, loss = 0.05753937\n",
      "Iteration 11, loss = 0.04995918\n",
      "Iteration 12, loss = 0.04417302\n",
      "Iteration 13, loss = 0.03735550\n",
      "Iteration 14, loss = 0.03332924\n",
      "Iteration 15, loss = 0.02853215\n",
      "Iteration 16, loss = 0.02402834\n",
      "Iteration 17, loss = 0.02418249\n",
      "Iteration 18, loss = 0.02109044\n",
      "Iteration 19, loss = 0.01905075\n",
      "Iteration 20, loss = 0.01428143\n",
      "Iteration 21, loss = 0.01697032\n",
      "Iteration 22, loss = 0.01492206\n",
      "Iteration 23, loss = 0.01129641\n",
      "Iteration 24, loss = 0.01493543\n",
      "Iteration 25, loss = 0.01220389\n",
      "Iteration 26, loss = 0.01181700\n",
      "Iteration 27, loss = 0.00927319\n",
      "Iteration 28, loss = 0.00856250\n",
      "Iteration 29, loss = 0.00744003\n",
      "Iteration 30, loss = 0.01788629\n",
      "Iteration 31, loss = 0.00995460\n",
      "Iteration 32, loss = 0.00941853\n",
      "Iteration 33, loss = 0.00983257\n",
      "Iteration 34, loss = 0.01114516\n",
      "Iteration 35, loss = 0.00881897\n",
      "Iteration 36, loss = 0.00623893\n",
      "Iteration 37, loss = 0.00551098\n",
      "Iteration 38, loss = 0.00527929\n",
      "Iteration 39, loss = 0.01365908\n",
      "Iteration 40, loss = 0.01594197\n",
      "Iteration 41, loss = 0.00601429\n",
      "Iteration 42, loss = 0.00538255\n",
      "Iteration 43, loss = 0.00523340\n",
      "Iteration 44, loss = 0.00511407\n",
      "Iteration 45, loss = 0.00499993\n",
      "Iteration 46, loss = 0.00486444\n",
      "Iteration 47, loss = 0.02349163\n",
      "Iteration 48, loss = 0.01147913\n",
      "Iteration 49, loss = 0.00616675\n",
      "Iteration 50, loss = 0.00724352\n",
      "Iteration 51, loss = 0.01353667\n",
      "Iteration 52, loss = 0.00621301\n",
      "Iteration 53, loss = 0.00508666\n",
      "Iteration 54, loss = 0.00495230\n",
      "Iteration 55, loss = 0.00485977\n",
      "Iteration 56, loss = 0.00476823\n",
      "Iteration 57, loss = 0.00465912\n",
      "Iteration 58, loss = 0.00636636\n",
      "Iteration 59, loss = 0.02485284\n",
      "Iteration 60, loss = 0.00689133\n",
      "Iteration 61, loss = 0.00488596\n",
      "Iteration 62, loss = 0.00474186\n",
      "Iteration 63, loss = 0.00467018\n",
      "Iteration 64, loss = 0.00459637\n",
      "Iteration 65, loss = 0.00450866\n",
      "Iteration 66, loss = 0.00441139\n",
      "Iteration 67, loss = 0.02312521\n",
      "Iteration 68, loss = 0.00765216\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.0001, batch_size=50, hidden_layer_sizes=(100, 100, 100), solver=adam; total time= 2.5min\n",
      "Iteration 1, loss = 2.30462588\n",
      "Iteration 2, loss = 2.30236736\n",
      "Iteration 3, loss = 2.30212826\n",
      "Iteration 4, loss = 2.30220879\n",
      "Iteration 5, loss = 2.30191958\n",
      "Iteration 6, loss = 2.30171422\n",
      "Iteration 7, loss = 2.30129548\n",
      "Iteration 8, loss = 2.30103202\n",
      "Iteration 9, loss = 2.30135408\n",
      "Iteration 10, loss = 2.30112861\n",
      "Iteration 11, loss = 2.30087366\n",
      "Iteration 12, loss = 2.30054381\n",
      "Iteration 13, loss = 2.30028047\n",
      "Iteration 14, loss = 2.29980076\n",
      "Iteration 15, loss = 2.29966424\n",
      "Iteration 16, loss = 2.29931770\n",
      "Iteration 17, loss = 2.29866611\n",
      "Iteration 18, loss = 2.29851754\n",
      "Iteration 19, loss = 2.29818205\n",
      "Iteration 20, loss = 2.29745027\n",
      "Iteration 21, loss = 2.29686422\n",
      "Iteration 22, loss = 2.29578080\n",
      "Iteration 23, loss = 2.29481811\n",
      "Iteration 24, loss = 2.29370419\n",
      "Iteration 25, loss = 2.29237263\n",
      "Iteration 26, loss = 2.29037709\n",
      "Iteration 27, loss = 2.28803783\n",
      "Iteration 28, loss = 2.28499332\n",
      "Iteration 29, loss = 2.28047718\n",
      "Iteration 30, loss = 2.27443234\n",
      "Iteration 31, loss = 2.26528587\n",
      "Iteration 32, loss = 2.25093601\n",
      "Iteration 33, loss = 2.22671998\n",
      "Iteration 34, loss = 2.18531524\n",
      "Iteration 35, loss = 2.11791828\n",
      "Iteration 36, loss = 2.03113771\n",
      "Iteration 37, loss = 1.94861820\n",
      "Iteration 38, loss = 1.88110947\n",
      "Iteration 39, loss = 1.82281416\n",
      "Iteration 40, loss = 1.76760121\n",
      "Iteration 41, loss = 1.71081652\n",
      "Iteration 42, loss = 1.64655107\n",
      "Iteration 43, loss = 1.56818075\n",
      "Iteration 44, loss = 1.47490354\n",
      "Iteration 45, loss = 1.38291566\n",
      "Iteration 46, loss = 1.31042505\n",
      "Iteration 47, loss = 1.25766333\n",
      "Iteration 48, loss = 1.21682235\n",
      "Iteration 49, loss = 1.18083819\n",
      "Iteration 50, loss = 1.14583497\n",
      "Iteration 51, loss = 1.11008426\n",
      "Iteration 52, loss = 1.07200525\n",
      "Iteration 53, loss = 1.03074437\n",
      "Iteration 54, loss = 0.98662231\n",
      "Iteration 55, loss = 0.94196862\n",
      "Iteration 56, loss = 0.89884172\n",
      "Iteration 57, loss = 0.85919978\n",
      "Iteration 58, loss = 0.82427690\n",
      "Iteration 59, loss = 0.79411832\n",
      "Iteration 60, loss = 0.76801811\n",
      "Iteration 61, loss = 0.74476021\n",
      "Iteration 62, loss = 0.72394372\n",
      "Iteration 63, loss = 0.70474952\n",
      "Iteration 64, loss = 0.68679841\n",
      "Iteration 65, loss = 0.67006727\n",
      "Iteration 66, loss = 0.65415079\n",
      "Iteration 67, loss = 0.63910073\n",
      "Iteration 68, loss = 0.62432772\n",
      "Iteration 69, loss = 0.61074631\n",
      "Iteration 70, loss = 0.59747350\n",
      "Iteration 71, loss = 0.58480410\n",
      "Iteration 72, loss = 0.57279083\n",
      "Iteration 73, loss = 0.56113124\n",
      "Iteration 74, loss = 0.55027582\n",
      "Iteration 75, loss = 0.53973023\n",
      "Iteration 76, loss = 0.52946340\n",
      "Iteration 77, loss = 0.51995859\n",
      "Iteration 78, loss = 0.51069716\n",
      "Iteration 79, loss = 0.50195210\n",
      "Iteration 80, loss = 0.49347674\n",
      "Iteration 81, loss = 0.48534415\n",
      "Iteration 82, loss = 0.47756472\n",
      "Iteration 83, loss = 0.47010718\n",
      "Iteration 84, loss = 0.46288410\n",
      "Iteration 85, loss = 0.45583440\n",
      "Iteration 86, loss = 0.44899534\n",
      "Iteration 87, loss = 0.44243415\n",
      "Iteration 88, loss = 0.43605488\n",
      "Iteration 89, loss = 0.42996465\n",
      "Iteration 90, loss = 0.42397509\n",
      "Iteration 91, loss = 0.41821186\n",
      "Iteration 92, loss = 0.41228187\n",
      "Iteration 93, loss = 0.40687678\n",
      "Iteration 94, loss = 0.40135580\n",
      "Iteration 95, loss = 0.39613248\n",
      "Iteration 96, loss = 0.39093161\n",
      "Iteration 97, loss = 0.38605369\n",
      "Iteration 98, loss = 0.38108748\n",
      "Iteration 99, loss = 0.37629350\n",
      "Iteration 100, loss = 0.37155683\n",
      "Iteration 101, loss = 0.36716465\n",
      "Iteration 102, loss = 0.36287950\n",
      "Iteration 103, loss = 0.35841304\n",
      "Iteration 104, loss = 0.35451558\n",
      "Iteration 105, loss = 0.35020024\n",
      "Iteration 106, loss = 0.34629746\n",
      "Iteration 107, loss = 0.34263350\n",
      "Iteration 108, loss = 0.33863107\n",
      "Iteration 109, loss = 0.33516910\n",
      "Iteration 110, loss = 0.33188939\n",
      "Iteration 111, loss = 0.32831810\n",
      "Iteration 112, loss = 0.32481064\n",
      "Iteration 113, loss = 0.32130194\n",
      "Iteration 114, loss = 0.31834024\n",
      "Iteration 115, loss = 0.31535605\n",
      "Iteration 116, loss = 0.31198455\n",
      "Iteration 117, loss = 0.30903361\n",
      "Iteration 118, loss = 0.30613589\n",
      "Iteration 119, loss = 0.30334656\n",
      "Iteration 120, loss = 0.30031690\n",
      "Iteration 121, loss = 0.29768978\n",
      "Iteration 122, loss = 0.29484940\n",
      "Iteration 123, loss = 0.29247249\n",
      "Iteration 124, loss = 0.28976843\n",
      "Iteration 125, loss = 0.28713564\n",
      "Iteration 126, loss = 0.28465027\n",
      "Iteration 127, loss = 0.28226887\n",
      "Iteration 128, loss = 0.27973814\n",
      "Iteration 129, loss = 0.27757115\n",
      "Iteration 130, loss = 0.27498455\n",
      "Iteration 131, loss = 0.27258381\n",
      "Iteration 132, loss = 0.27032270\n",
      "Iteration 133, loss = 0.26812835\n",
      "Iteration 134, loss = 0.26566477\n",
      "Iteration 135, loss = 0.26360946\n",
      "Iteration 136, loss = 0.26127130\n",
      "Iteration 137, loss = 0.25920041\n",
      "Iteration 138, loss = 0.25694598\n",
      "Iteration 139, loss = 0.25494756\n",
      "Iteration 140, loss = 0.25292739\n",
      "Iteration 141, loss = 0.25051061\n",
      "Iteration 142, loss = 0.24860942\n",
      "Iteration 143, loss = 0.24649922\n",
      "Iteration 144, loss = 0.24467022\n",
      "Iteration 145, loss = 0.24256934\n",
      "Iteration 146, loss = 0.24049675\n",
      "Iteration 147, loss = 0.23856961\n",
      "Iteration 148, loss = 0.23676566\n",
      "Iteration 149, loss = 0.23466009\n",
      "Iteration 150, loss = 0.23257999\n",
      "Iteration 151, loss = 0.23073553\n",
      "Iteration 152, loss = 0.22891716\n",
      "Iteration 153, loss = 0.22697909\n",
      "Iteration 154, loss = 0.22511824\n",
      "Iteration 155, loss = 0.22317147\n",
      "Iteration 156, loss = 0.22145476\n",
      "Iteration 157, loss = 0.21959763\n",
      "Iteration 158, loss = 0.21769717\n",
      "Iteration 159, loss = 0.21594763\n",
      "Iteration 160, loss = 0.21425146\n",
      "Iteration 161, loss = 0.21253048\n",
      "Iteration 162, loss = 0.21085168\n",
      "Iteration 163, loss = 0.20890697\n",
      "Iteration 164, loss = 0.20719694\n",
      "Iteration 165, loss = 0.20546063\n",
      "Iteration 166, loss = 0.20376334\n",
      "Iteration 167, loss = 0.20207182\n",
      "Iteration 168, loss = 0.20037443\n",
      "Iteration 169, loss = 0.19870245\n",
      "Iteration 170, loss = 0.19720012\n",
      "Iteration 171, loss = 0.19514447\n",
      "Iteration 172, loss = 0.19364120\n",
      "Iteration 173, loss = 0.19216573\n",
      "Iteration 174, loss = 0.19065469\n",
      "Iteration 175, loss = 0.18871645\n",
      "Iteration 176, loss = 0.18743236\n",
      "Iteration 177, loss = 0.18553350\n",
      "Iteration 178, loss = 0.18416707\n",
      "Iteration 179, loss = 0.18244025\n",
      "Iteration 180, loss = 0.18113328\n",
      "Iteration 181, loss = 0.17961757\n",
      "Iteration 182, loss = 0.17823871\n",
      "Iteration 183, loss = 0.17642906\n",
      "Iteration 184, loss = 0.17517857\n",
      "Iteration 185, loss = 0.17362502\n",
      "Iteration 186, loss = 0.17225365\n",
      "Iteration 187, loss = 0.17076406\n",
      "Iteration 188, loss = 0.16921646\n",
      "Iteration 189, loss = 0.16798410\n",
      "Iteration 190, loss = 0.16648566\n",
      "Iteration 191, loss = 0.16510862\n",
      "Iteration 192, loss = 0.16387904\n",
      "Iteration 193, loss = 0.16236996\n",
      "Iteration 194, loss = 0.16131343\n",
      "Iteration 195, loss = 0.15997091\n",
      "Iteration 196, loss = 0.15844825\n",
      "Iteration 197, loss = 0.15727298\n",
      "Iteration 198, loss = 0.15559352\n",
      "Iteration 199, loss = 0.15471979\n",
      "Iteration 200, loss = 0.15322653\n",
      "[CV] END activation=logistic, alpha=0.0001, batch_size=50, hidden_layer_sizes=(100, 100, 100), solver=sgd; total time= 5.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.30525853\n",
      "Iteration 2, loss = 2.30233566\n",
      "Iteration 3, loss = 2.30232076\n",
      "Iteration 4, loss = 2.30204014\n",
      "Iteration 5, loss = 2.30203761\n",
      "Iteration 6, loss = 2.30166525\n",
      "Iteration 7, loss = 2.30149728\n",
      "Iteration 8, loss = 2.30149767\n",
      "Iteration 9, loss = 2.30095826\n",
      "Iteration 10, loss = 2.30060385\n",
      "Iteration 11, loss = 2.30067005\n",
      "Iteration 12, loss = 2.30014825\n",
      "Iteration 13, loss = 2.30016393\n",
      "Iteration 14, loss = 2.29974511\n",
      "Iteration 15, loss = 2.29936004\n",
      "Iteration 16, loss = 2.29884847\n",
      "Iteration 17, loss = 2.29843912\n",
      "Iteration 18, loss = 2.29783231\n",
      "Iteration 19, loss = 2.29725676\n",
      "Iteration 20, loss = 2.29638624\n",
      "Iteration 21, loss = 2.29571768\n",
      "Iteration 22, loss = 2.29451234\n",
      "Iteration 23, loss = 2.29345787\n",
      "Iteration 24, loss = 2.29183067\n",
      "Iteration 25, loss = 2.28960601\n",
      "Iteration 26, loss = 2.28684497\n",
      "Iteration 27, loss = 2.28368153\n",
      "Iteration 28, loss = 2.27880918\n",
      "Iteration 29, loss = 2.27186666\n",
      "Iteration 30, loss = 2.26056486\n",
      "Iteration 31, loss = 2.24193099\n",
      "Iteration 32, loss = 2.20868594\n",
      "Iteration 33, loss = 2.14851447\n",
      "Iteration 34, loss = 2.04915532\n",
      "Iteration 35, loss = 1.92964391\n",
      "Iteration 36, loss = 1.82993864\n",
      "Iteration 37, loss = 1.76085716\n",
      "Iteration 38, loss = 1.71270219\n",
      "Iteration 39, loss = 1.67484726\n",
      "Iteration 40, loss = 1.64089713\n",
      "Iteration 41, loss = 1.60891809\n",
      "Iteration 42, loss = 1.57638697\n",
      "Iteration 43, loss = 1.54162565\n",
      "Iteration 44, loss = 1.50244742\n",
      "Iteration 45, loss = 1.45575067\n",
      "Iteration 46, loss = 1.39954875\n",
      "Iteration 47, loss = 1.33145060\n",
      "Iteration 48, loss = 1.25326781\n",
      "Iteration 49, loss = 1.16848127\n",
      "Iteration 50, loss = 1.08399810\n",
      "Iteration 51, loss = 1.00819659\n",
      "Iteration 52, loss = 0.94424766\n",
      "Iteration 53, loss = 0.89178029\n",
      "Iteration 54, loss = 0.84889323\n",
      "Iteration 55, loss = 0.81318548\n",
      "Iteration 56, loss = 0.78316818\n",
      "Iteration 57, loss = 0.75783982\n",
      "Iteration 58, loss = 0.73537323\n",
      "Iteration 59, loss = 0.71551655\n",
      "Iteration 60, loss = 0.69775161\n",
      "Iteration 61, loss = 0.68149348\n",
      "Iteration 62, loss = 0.66656026\n",
      "Iteration 63, loss = 0.65236940\n",
      "Iteration 64, loss = 0.63914787\n",
      "Iteration 65, loss = 0.62656206\n",
      "Iteration 66, loss = 0.61479149\n",
      "Iteration 67, loss = 0.60345644\n",
      "Iteration 68, loss = 0.59264948\n",
      "Iteration 69, loss = 0.58235896\n",
      "Iteration 70, loss = 0.57252050\n",
      "Iteration 71, loss = 0.56293801\n",
      "Iteration 72, loss = 0.55398565\n",
      "Iteration 73, loss = 0.54541588\n",
      "Iteration 74, loss = 0.53690934\n",
      "Iteration 75, loss = 0.52896501\n",
      "Iteration 76, loss = 0.52110097\n",
      "Iteration 77, loss = 0.51357059\n",
      "Iteration 78, loss = 0.50646211\n",
      "Iteration 79, loss = 0.49942415\n",
      "Iteration 80, loss = 0.49238470\n",
      "Iteration 81, loss = 0.48586732\n",
      "Iteration 82, loss = 0.47940836\n",
      "Iteration 83, loss = 0.47305069\n",
      "Iteration 84, loss = 0.46713022\n",
      "Iteration 85, loss = 0.46113080\n",
      "Iteration 86, loss = 0.45541551\n",
      "Iteration 87, loss = 0.44965681\n",
      "Iteration 88, loss = 0.44415859\n",
      "Iteration 89, loss = 0.43885785\n",
      "Iteration 90, loss = 0.43339626\n",
      "Iteration 91, loss = 0.42827590\n",
      "Iteration 92, loss = 0.42326140\n",
      "Iteration 93, loss = 0.41809343\n",
      "Iteration 94, loss = 0.41301737\n",
      "Iteration 95, loss = 0.40830663\n",
      "Iteration 96, loss = 0.40346153\n",
      "Iteration 97, loss = 0.39864937\n",
      "Iteration 98, loss = 0.39382142\n",
      "Iteration 99, loss = 0.38936877\n",
      "Iteration 100, loss = 0.38493836\n",
      "Iteration 101, loss = 0.38034067\n",
      "Iteration 102, loss = 0.37600966\n",
      "Iteration 103, loss = 0.37158009\n",
      "Iteration 104, loss = 0.36732525\n",
      "Iteration 105, loss = 0.36299486\n",
      "Iteration 106, loss = 0.35871934\n",
      "Iteration 107, loss = 0.35456170\n",
      "Iteration 108, loss = 0.35044436\n",
      "Iteration 109, loss = 0.34648347\n",
      "Iteration 110, loss = 0.34250841\n",
      "Iteration 111, loss = 0.33862358\n",
      "Iteration 112, loss = 0.33483139\n",
      "Iteration 113, loss = 0.33105107\n",
      "Iteration 114, loss = 0.32717544\n",
      "Iteration 115, loss = 0.32356300\n",
      "Iteration 116, loss = 0.32003306\n",
      "Iteration 117, loss = 0.31642063\n",
      "Iteration 118, loss = 0.31319483\n",
      "Iteration 119, loss = 0.30954745\n",
      "Iteration 120, loss = 0.30617249\n",
      "Iteration 121, loss = 0.30275583\n",
      "Iteration 122, loss = 0.29933545\n",
      "Iteration 123, loss = 0.29606801\n",
      "Iteration 124, loss = 0.29285005\n",
      "Iteration 125, loss = 0.28994337\n",
      "Iteration 126, loss = 0.28665247\n",
      "Iteration 127, loss = 0.28368196\n",
      "Iteration 128, loss = 0.28058591\n",
      "Iteration 129, loss = 0.27759757\n",
      "Iteration 130, loss = 0.27461207\n",
      "Iteration 131, loss = 0.27179483\n",
      "Iteration 132, loss = 0.26901457\n",
      "Iteration 133, loss = 0.26597410\n",
      "Iteration 134, loss = 0.26330298\n",
      "Iteration 135, loss = 0.26044614\n",
      "Iteration 136, loss = 0.25787886\n",
      "Iteration 137, loss = 0.25506435\n",
      "Iteration 138, loss = 0.25256804\n",
      "Iteration 139, loss = 0.25001556\n",
      "Iteration 140, loss = 0.24728669\n",
      "Iteration 141, loss = 0.24484734\n",
      "Iteration 142, loss = 0.24237792\n",
      "Iteration 143, loss = 0.23983160\n",
      "Iteration 144, loss = 0.23743541\n",
      "Iteration 145, loss = 0.23505407\n",
      "Iteration 146, loss = 0.23278879\n",
      "Iteration 147, loss = 0.23018605\n",
      "Iteration 148, loss = 0.22802320\n",
      "Iteration 149, loss = 0.22566019\n",
      "Iteration 150, loss = 0.22354318\n",
      "Iteration 151, loss = 0.22130448\n",
      "Iteration 152, loss = 0.21907645\n",
      "Iteration 153, loss = 0.21669425\n",
      "Iteration 154, loss = 0.21491687\n",
      "Iteration 155, loss = 0.21273516\n",
      "Iteration 156, loss = 0.21047486\n",
      "Iteration 157, loss = 0.20859599\n",
      "Iteration 158, loss = 0.20658592\n",
      "Iteration 159, loss = 0.20425924\n",
      "Iteration 160, loss = 0.20242830\n",
      "Iteration 161, loss = 0.20056302\n",
      "Iteration 162, loss = 0.19863021\n",
      "Iteration 163, loss = 0.19656354\n",
      "Iteration 164, loss = 0.19482807\n",
      "Iteration 165, loss = 0.19313758\n",
      "Iteration 166, loss = 0.19115578\n",
      "Iteration 167, loss = 0.18935190\n",
      "Iteration 168, loss = 0.18749619\n",
      "Iteration 169, loss = 0.18585443\n",
      "Iteration 170, loss = 0.18399673\n",
      "Iteration 171, loss = 0.18226729\n",
      "Iteration 172, loss = 0.18062473\n",
      "Iteration 173, loss = 0.17901208\n",
      "Iteration 174, loss = 0.17747331\n",
      "Iteration 175, loss = 0.17566832\n",
      "Iteration 176, loss = 0.17399273\n",
      "Iteration 177, loss = 0.17247651\n",
      "Iteration 178, loss = 0.17106504\n",
      "Iteration 179, loss = 0.16945505\n",
      "Iteration 180, loss = 0.16783503\n",
      "Iteration 181, loss = 0.16634713\n",
      "Iteration 182, loss = 0.16487485\n",
      "Iteration 183, loss = 0.16360333\n",
      "Iteration 184, loss = 0.16193112\n",
      "Iteration 185, loss = 0.16065331\n",
      "Iteration 186, loss = 0.15925366\n",
      "Iteration 187, loss = 0.15783965\n",
      "Iteration 188, loss = 0.15646439\n",
      "Iteration 189, loss = 0.15510598\n",
      "Iteration 190, loss = 0.15372799\n",
      "Iteration 191, loss = 0.15220232\n",
      "Iteration 192, loss = 0.15102825\n",
      "Iteration 193, loss = 0.14962946\n",
      "Iteration 194, loss = 0.14851870\n",
      "Iteration 195, loss = 0.14719008\n",
      "Iteration 196, loss = 0.14586141\n",
      "Iteration 197, loss = 0.14481736\n",
      "Iteration 198, loss = 0.14380558\n",
      "Iteration 199, loss = 0.14237962\n",
      "Iteration 200, loss = 0.14112877\n",
      "[CV] END activation=logistic, alpha=0.0001, batch_size=50, hidden_layer_sizes=(100, 100, 100), solver=sgd; total time= 5.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.30336519\n",
      "Iteration 2, loss = 2.30205113\n",
      "Iteration 3, loss = 2.30188447\n",
      "Iteration 4, loss = 2.30159958\n",
      "Iteration 5, loss = 2.30158553\n",
      "Iteration 6, loss = 2.30123751\n",
      "Iteration 7, loss = 2.30114613\n",
      "Iteration 8, loss = 2.30049898\n",
      "Iteration 9, loss = 2.30059012\n",
      "Iteration 10, loss = 2.30053422\n",
      "Iteration 11, loss = 2.30010827\n",
      "Iteration 12, loss = 2.29964295\n",
      "Iteration 13, loss = 2.29922042\n",
      "Iteration 14, loss = 2.29887636\n",
      "Iteration 15, loss = 2.29847326\n",
      "Iteration 16, loss = 2.29771601\n",
      "Iteration 17, loss = 2.29721142\n",
      "Iteration 18, loss = 2.29628672\n",
      "Iteration 19, loss = 2.29575612\n",
      "Iteration 20, loss = 2.29457175\n",
      "Iteration 21, loss = 2.29352166\n",
      "Iteration 22, loss = 2.29212719\n",
      "Iteration 23, loss = 2.29015991\n",
      "Iteration 24, loss = 2.28770788\n",
      "Iteration 25, loss = 2.28467450\n",
      "Iteration 26, loss = 2.28070130\n",
      "Iteration 27, loss = 2.27488255\n",
      "Iteration 28, loss = 2.26691706\n",
      "Iteration 29, loss = 2.25433747\n",
      "Iteration 30, loss = 2.23422607\n",
      "Iteration 31, loss = 2.19957567\n",
      "Iteration 32, loss = 2.13908879\n",
      "Iteration 33, loss = 2.04531219\n",
      "Iteration 34, loss = 1.93650282\n",
      "Iteration 35, loss = 1.84210011\n",
      "Iteration 36, loss = 1.76908070\n",
      "Iteration 37, loss = 1.70505807\n",
      "Iteration 38, loss = 1.63526134\n",
      "Iteration 39, loss = 1.54752193\n",
      "Iteration 40, loss = 1.44217913\n",
      "Iteration 41, loss = 1.34049578\n",
      "Iteration 42, loss = 1.25982964\n",
      "Iteration 43, loss = 1.19854997\n",
      "Iteration 44, loss = 1.14840839\n",
      "Iteration 45, loss = 1.10280933\n",
      "Iteration 46, loss = 1.05863460\n",
      "Iteration 47, loss = 1.01369486\n",
      "Iteration 48, loss = 0.96860486\n",
      "Iteration 49, loss = 0.92505714\n",
      "Iteration 50, loss = 0.88546362\n",
      "Iteration 51, loss = 0.85135865\n",
      "Iteration 52, loss = 0.82242886\n",
      "Iteration 53, loss = 0.79796962\n",
      "Iteration 54, loss = 0.77687405\n",
      "Iteration 55, loss = 0.75835025\n",
      "Iteration 56, loss = 0.74146889\n",
      "Iteration 57, loss = 0.72599709\n",
      "Iteration 58, loss = 0.71147194\n",
      "Iteration 59, loss = 0.69759815\n",
      "Iteration 60, loss = 0.68446829\n",
      "Iteration 61, loss = 0.67190648\n",
      "Iteration 62, loss = 0.65939594\n",
      "Iteration 63, loss = 0.64765380\n",
      "Iteration 64, loss = 0.63611822\n",
      "Iteration 65, loss = 0.62487214\n",
      "Iteration 66, loss = 0.61381718\n",
      "Iteration 67, loss = 0.60346162\n",
      "Iteration 68, loss = 0.59285582\n",
      "Iteration 69, loss = 0.58303706\n",
      "Iteration 70, loss = 0.57316488\n",
      "Iteration 71, loss = 0.56404779\n",
      "Iteration 72, loss = 0.55476161\n",
      "Iteration 73, loss = 0.54609907\n",
      "Iteration 74, loss = 0.53738162\n",
      "Iteration 75, loss = 0.52912253\n",
      "Iteration 76, loss = 0.52121185\n",
      "Iteration 77, loss = 0.51354173\n",
      "Iteration 78, loss = 0.50607204\n",
      "Iteration 79, loss = 0.49908054\n",
      "Iteration 80, loss = 0.49193922\n",
      "Iteration 81, loss = 0.48520209\n",
      "Iteration 82, loss = 0.47865785\n",
      "Iteration 83, loss = 0.47242347\n",
      "Iteration 84, loss = 0.46625848\n",
      "Iteration 85, loss = 0.46008238\n",
      "Iteration 86, loss = 0.45433023\n",
      "Iteration 87, loss = 0.44861223\n",
      "Iteration 88, loss = 0.44305682\n",
      "Iteration 89, loss = 0.43741263\n",
      "Iteration 90, loss = 0.43205266\n",
      "Iteration 91, loss = 0.42693635\n",
      "Iteration 92, loss = 0.42154574\n",
      "Iteration 93, loss = 0.41663370\n",
      "Iteration 94, loss = 0.41141742\n",
      "Iteration 95, loss = 0.40661645\n",
      "Iteration 96, loss = 0.40170946\n",
      "Iteration 97, loss = 0.39710217\n",
      "Iteration 98, loss = 0.39224063\n",
      "Iteration 99, loss = 0.38774520\n",
      "Iteration 100, loss = 0.38290409\n",
      "Iteration 101, loss = 0.37846032\n",
      "Iteration 102, loss = 0.37411727\n",
      "Iteration 103, loss = 0.36967872\n",
      "Iteration 104, loss = 0.36518631\n",
      "Iteration 105, loss = 0.36094147\n",
      "Iteration 106, loss = 0.35652014\n",
      "Iteration 107, loss = 0.35251198\n",
      "Iteration 108, loss = 0.34818935\n",
      "Iteration 109, loss = 0.34406873\n",
      "Iteration 110, loss = 0.33994101\n",
      "Iteration 111, loss = 0.33583256\n",
      "Iteration 112, loss = 0.33173177\n",
      "Iteration 113, loss = 0.32801758\n",
      "Iteration 114, loss = 0.32402564\n",
      "Iteration 115, loss = 0.32010939\n",
      "Iteration 116, loss = 0.31637311\n",
      "Iteration 117, loss = 0.31247985\n",
      "Iteration 118, loss = 0.30906307\n",
      "Iteration 119, loss = 0.30551724\n",
      "Iteration 120, loss = 0.30179325\n",
      "Iteration 121, loss = 0.29820883\n",
      "Iteration 122, loss = 0.29475600\n",
      "Iteration 123, loss = 0.29120297\n",
      "Iteration 124, loss = 0.28803869\n",
      "Iteration 125, loss = 0.28500122\n",
      "Iteration 126, loss = 0.28168168\n",
      "Iteration 127, loss = 0.27838560\n",
      "Iteration 128, loss = 0.27524826\n",
      "Iteration 129, loss = 0.27223118\n",
      "Iteration 130, loss = 0.26937646\n",
      "Iteration 131, loss = 0.26640124\n",
      "Iteration 132, loss = 0.26344766\n",
      "Iteration 133, loss = 0.26072091\n",
      "Iteration 134, loss = 0.25772345\n",
      "Iteration 135, loss = 0.25518931\n",
      "Iteration 136, loss = 0.25229360\n",
      "Iteration 137, loss = 0.24974591\n",
      "Iteration 138, loss = 0.24704726\n",
      "Iteration 139, loss = 0.24470258\n",
      "Iteration 140, loss = 0.24217150\n",
      "Iteration 141, loss = 0.23963127\n",
      "Iteration 142, loss = 0.23710103\n",
      "Iteration 143, loss = 0.23486516\n",
      "Iteration 144, loss = 0.23221735\n",
      "Iteration 145, loss = 0.23021186\n",
      "Iteration 146, loss = 0.22764212\n",
      "Iteration 147, loss = 0.22569966\n",
      "Iteration 148, loss = 0.22317874\n",
      "Iteration 149, loss = 0.22105491\n",
      "Iteration 150, loss = 0.21906880\n",
      "Iteration 151, loss = 0.21691754\n",
      "Iteration 152, loss = 0.21456506\n",
      "Iteration 153, loss = 0.21266556\n",
      "Iteration 154, loss = 0.21050911\n",
      "Iteration 155, loss = 0.20876052\n",
      "Iteration 156, loss = 0.20668078\n",
      "Iteration 157, loss = 0.20478342\n",
      "Iteration 158, loss = 0.20264496\n",
      "Iteration 159, loss = 0.20095388\n",
      "Iteration 160, loss = 0.19925537\n",
      "Iteration 161, loss = 0.19719757\n",
      "Iteration 162, loss = 0.19559896\n",
      "Iteration 163, loss = 0.19389916\n",
      "Iteration 164, loss = 0.19197996\n",
      "Iteration 165, loss = 0.19024255\n",
      "Iteration 166, loss = 0.18860786\n",
      "Iteration 167, loss = 0.18695548\n",
      "Iteration 168, loss = 0.18524351\n",
      "Iteration 169, loss = 0.18352606\n",
      "Iteration 170, loss = 0.18192110\n",
      "Iteration 171, loss = 0.18033166\n",
      "Iteration 172, loss = 0.17871115\n",
      "Iteration 173, loss = 0.17744896\n",
      "Iteration 174, loss = 0.17573598\n",
      "Iteration 175, loss = 0.17428067\n",
      "Iteration 176, loss = 0.17283174\n",
      "Iteration 177, loss = 0.17123508\n",
      "Iteration 178, loss = 0.16982238\n",
      "Iteration 179, loss = 0.16831311\n",
      "Iteration 180, loss = 0.16689149\n",
      "Iteration 181, loss = 0.16557623\n",
      "Iteration 182, loss = 0.16414524\n",
      "Iteration 183, loss = 0.16264876\n",
      "Iteration 184, loss = 0.16142284\n",
      "Iteration 185, loss = 0.15992959\n",
      "Iteration 186, loss = 0.15879452\n",
      "Iteration 187, loss = 0.15730144\n",
      "Iteration 188, loss = 0.15623047\n",
      "Iteration 189, loss = 0.15490168\n",
      "Iteration 190, loss = 0.15338277\n",
      "Iteration 191, loss = 0.15233678\n",
      "Iteration 192, loss = 0.15114506\n",
      "Iteration 193, loss = 0.14986546\n",
      "Iteration 194, loss = 0.14872301\n",
      "Iteration 195, loss = 0.14751472\n",
      "Iteration 196, loss = 0.14639716\n",
      "Iteration 197, loss = 0.14523958\n",
      "Iteration 198, loss = 0.14421470\n",
      "Iteration 199, loss = 0.14277559\n",
      "Iteration 200, loss = 0.14168691\n",
      "[CV] END activation=logistic, alpha=0.0001, batch_size=50, hidden_layer_sizes=(100, 100, 100), solver=sgd; total time= 5.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.30430549\n",
      "Iteration 2, loss = 2.30194575\n",
      "Iteration 3, loss = 2.30171831\n",
      "Iteration 4, loss = 2.30191307\n",
      "Iteration 5, loss = 2.30124882\n",
      "Iteration 6, loss = 2.30134706\n",
      "Iteration 7, loss = 2.30098741\n",
      "Iteration 8, loss = 2.30103843\n",
      "Iteration 9, loss = 2.30055112\n",
      "Iteration 10, loss = 2.29997775\n",
      "Iteration 11, loss = 2.30013435\n",
      "Iteration 12, loss = 2.29996318\n",
      "Iteration 13, loss = 2.29933271\n",
      "Iteration 14, loss = 2.29888712\n",
      "Iteration 15, loss = 2.29860023\n",
      "Iteration 16, loss = 2.29797094\n",
      "Iteration 17, loss = 2.29751558\n",
      "Iteration 18, loss = 2.29679260\n",
      "Iteration 19, loss = 2.29594705\n",
      "Iteration 20, loss = 2.29508149\n",
      "Iteration 21, loss = 2.29436831\n",
      "Iteration 22, loss = 2.29293034\n",
      "Iteration 23, loss = 2.29181669\n",
      "Iteration 24, loss = 2.28991352\n",
      "Iteration 25, loss = 2.28767948\n",
      "Iteration 26, loss = 2.28520314\n",
      "Iteration 27, loss = 2.28162809\n",
      "Iteration 28, loss = 2.27678847\n",
      "Iteration 29, loss = 2.27005014\n",
      "Iteration 30, loss = 2.26043343\n",
      "Iteration 31, loss = 2.24586132\n",
      "Iteration 32, loss = 2.22224616\n",
      "Iteration 33, loss = 2.18226677\n",
      "Iteration 34, loss = 2.11560073\n",
      "Iteration 35, loss = 2.01667996\n",
      "Iteration 36, loss = 1.90164984\n",
      "Iteration 37, loss = 1.78675770\n",
      "Iteration 38, loss = 1.66812656\n",
      "Iteration 39, loss = 1.54459944\n",
      "Iteration 40, loss = 1.43587985\n",
      "Iteration 41, loss = 1.35477824\n",
      "Iteration 42, loss = 1.29526198\n",
      "Iteration 43, loss = 1.24682003\n",
      "Iteration 44, loss = 1.20218492\n",
      "Iteration 45, loss = 1.15554576\n",
      "Iteration 46, loss = 1.10318035\n",
      "Iteration 47, loss = 1.04354049\n",
      "Iteration 48, loss = 0.97946749\n",
      "Iteration 49, loss = 0.91825885\n",
      "Iteration 50, loss = 0.86571152\n",
      "Iteration 51, loss = 0.82422127\n",
      "Iteration 52, loss = 0.79060839\n",
      "Iteration 53, loss = 0.76320429\n",
      "Iteration 54, loss = 0.73949262\n",
      "Iteration 55, loss = 0.71837736\n",
      "Iteration 56, loss = 0.69962217\n",
      "Iteration 57, loss = 0.68231518\n",
      "Iteration 58, loss = 0.66581931\n",
      "Iteration 59, loss = 0.65077818\n",
      "Iteration 60, loss = 0.63624937\n",
      "Iteration 61, loss = 0.62292698\n",
      "Iteration 62, loss = 0.60988912\n",
      "Iteration 63, loss = 0.59786328\n",
      "Iteration 64, loss = 0.58625326\n",
      "Iteration 65, loss = 0.57535390\n",
      "Iteration 66, loss = 0.56502319\n",
      "Iteration 67, loss = 0.55532687\n",
      "Iteration 68, loss = 0.54615547\n",
      "Iteration 69, loss = 0.53704727\n",
      "Iteration 70, loss = 0.52860087\n",
      "Iteration 71, loss = 0.52049853\n",
      "Iteration 72, loss = 0.51279147\n",
      "Iteration 73, loss = 0.50510313\n",
      "Iteration 74, loss = 0.49801250\n",
      "Iteration 75, loss = 0.49091690\n",
      "Iteration 76, loss = 0.48427664\n",
      "Iteration 77, loss = 0.47762816\n",
      "Iteration 78, loss = 0.47134451\n",
      "Iteration 79, loss = 0.46521447\n",
      "Iteration 80, loss = 0.45915989\n",
      "Iteration 81, loss = 0.45331504\n",
      "Iteration 82, loss = 0.44760434\n",
      "Iteration 83, loss = 0.44190313\n",
      "Iteration 84, loss = 0.43626890\n",
      "Iteration 85, loss = 0.43090976\n",
      "Iteration 86, loss = 0.42560511\n",
      "Iteration 87, loss = 0.42021828\n",
      "Iteration 88, loss = 0.41497893\n",
      "Iteration 89, loss = 0.40965867\n",
      "Iteration 90, loss = 0.40472748\n",
      "Iteration 91, loss = 0.39961540\n",
      "Iteration 92, loss = 0.39452043\n",
      "Iteration 93, loss = 0.38964905\n",
      "Iteration 94, loss = 0.38499735\n",
      "Iteration 95, loss = 0.38029557\n",
      "Iteration 96, loss = 0.37572184\n",
      "Iteration 97, loss = 0.37112369\n",
      "Iteration 98, loss = 0.36632103\n",
      "Iteration 99, loss = 0.36208161\n",
      "Iteration 100, loss = 0.35766128\n",
      "Iteration 101, loss = 0.35361405\n",
      "Iteration 102, loss = 0.34920641\n",
      "Iteration 103, loss = 0.34500014\n",
      "Iteration 104, loss = 0.34125008\n",
      "Iteration 105, loss = 0.33706354\n",
      "Iteration 106, loss = 0.33338873\n",
      "Iteration 107, loss = 0.32977553\n",
      "Iteration 108, loss = 0.32596331\n",
      "Iteration 109, loss = 0.32250705\n",
      "Iteration 110, loss = 0.31891075\n",
      "Iteration 111, loss = 0.31555365\n",
      "Iteration 112, loss = 0.31237797\n",
      "Iteration 113, loss = 0.30902733\n",
      "Iteration 114, loss = 0.30580291\n",
      "Iteration 115, loss = 0.30266413\n",
      "Iteration 116, loss = 0.29943364\n",
      "Iteration 117, loss = 0.29652712\n",
      "Iteration 118, loss = 0.29369021\n",
      "Iteration 119, loss = 0.29079804\n",
      "Iteration 120, loss = 0.28787752\n",
      "Iteration 121, loss = 0.28522417\n",
      "Iteration 122, loss = 0.28242727\n",
      "Iteration 123, loss = 0.27969329\n",
      "Iteration 124, loss = 0.27692369\n",
      "Iteration 125, loss = 0.27432479\n",
      "Iteration 126, loss = 0.27195209\n",
      "Iteration 127, loss = 0.26926196\n",
      "Iteration 128, loss = 0.26674933\n",
      "Iteration 129, loss = 0.26443025\n",
      "Iteration 130, loss = 0.26183005\n",
      "Iteration 131, loss = 0.25942812\n",
      "Iteration 132, loss = 0.25702188\n",
      "Iteration 133, loss = 0.25492048\n",
      "Iteration 134, loss = 0.25252010\n",
      "Iteration 135, loss = 0.25016815\n",
      "Iteration 136, loss = 0.24796248\n",
      "Iteration 137, loss = 0.24577505\n",
      "Iteration 138, loss = 0.24361628\n",
      "Iteration 139, loss = 0.24127500\n",
      "Iteration 140, loss = 0.23919299\n",
      "Iteration 141, loss = 0.23705833\n",
      "Iteration 142, loss = 0.23497182\n",
      "Iteration 143, loss = 0.23285492\n",
      "Iteration 144, loss = 0.23074167\n",
      "Iteration 145, loss = 0.22877806\n",
      "Iteration 146, loss = 0.22665834\n",
      "Iteration 147, loss = 0.22484262\n",
      "Iteration 148, loss = 0.22282963\n",
      "Iteration 149, loss = 0.22094667\n",
      "Iteration 150, loss = 0.21899211\n",
      "Iteration 151, loss = 0.21733040\n",
      "Iteration 152, loss = 0.21516191\n",
      "Iteration 153, loss = 0.21333284\n",
      "Iteration 154, loss = 0.21175497\n",
      "Iteration 155, loss = 0.20970494\n",
      "Iteration 156, loss = 0.20773156\n",
      "Iteration 157, loss = 0.20591508\n",
      "Iteration 158, loss = 0.20415067\n",
      "Iteration 159, loss = 0.20265786\n",
      "Iteration 160, loss = 0.20083768\n",
      "Iteration 161, loss = 0.19907747\n",
      "Iteration 162, loss = 0.19725026\n",
      "Iteration 163, loss = 0.19586127\n",
      "Iteration 164, loss = 0.19409719\n",
      "Iteration 165, loss = 0.19233143\n",
      "Iteration 166, loss = 0.19067252\n",
      "Iteration 167, loss = 0.18909878\n",
      "Iteration 168, loss = 0.18763896\n",
      "Iteration 169, loss = 0.18585583\n",
      "Iteration 170, loss = 0.18445273\n",
      "Iteration 171, loss = 0.18291947\n",
      "Iteration 172, loss = 0.18142774\n",
      "Iteration 173, loss = 0.17995413\n",
      "Iteration 174, loss = 0.17855837\n",
      "Iteration 175, loss = 0.17695676\n",
      "Iteration 176, loss = 0.17553976\n",
      "Iteration 177, loss = 0.17401558\n",
      "Iteration 178, loss = 0.17259999\n",
      "Iteration 179, loss = 0.17113881\n",
      "Iteration 180, loss = 0.16991491\n",
      "Iteration 181, loss = 0.16842833\n",
      "Iteration 182, loss = 0.16693459\n",
      "Iteration 183, loss = 0.16559395\n",
      "Iteration 184, loss = 0.16443292\n",
      "Iteration 185, loss = 0.16292460\n",
      "Iteration 186, loss = 0.16169921\n",
      "Iteration 187, loss = 0.16056486\n",
      "Iteration 188, loss = 0.15914593\n",
      "Iteration 189, loss = 0.15784999\n",
      "Iteration 190, loss = 0.15657024\n",
      "Iteration 191, loss = 0.15542292\n",
      "Iteration 192, loss = 0.15423160\n",
      "Iteration 193, loss = 0.15287014\n",
      "Iteration 194, loss = 0.15182123\n",
      "Iteration 195, loss = 0.15059283\n",
      "Iteration 196, loss = 0.14949039\n",
      "Iteration 197, loss = 0.14830380\n",
      "Iteration 198, loss = 0.14702716\n",
      "Iteration 199, loss = 0.14596328\n",
      "Iteration 200, loss = 0.14484222\n",
      "[CV] END activation=logistic, alpha=0.0001, batch_size=50, hidden_layer_sizes=(100, 100, 100), solver=sgd; total time= 5.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.30430773\n",
      "Iteration 2, loss = 2.30243172\n",
      "Iteration 3, loss = 2.30237804\n",
      "Iteration 4, loss = 2.30196927\n",
      "Iteration 5, loss = 2.30214365\n",
      "Iteration 6, loss = 2.30156981\n",
      "Iteration 7, loss = 2.30161455\n",
      "Iteration 8, loss = 2.30130302\n",
      "Iteration 9, loss = 2.30085320\n",
      "Iteration 10, loss = 2.30103315\n",
      "Iteration 11, loss = 2.30064948\n",
      "Iteration 12, loss = 2.30038294\n",
      "Iteration 13, loss = 2.29988629\n",
      "Iteration 14, loss = 2.29987375\n",
      "Iteration 15, loss = 2.29933888\n",
      "Iteration 16, loss = 2.29878899\n",
      "Iteration 17, loss = 2.29866216\n",
      "Iteration 18, loss = 2.29766242\n",
      "Iteration 19, loss = 2.29744181\n",
      "Iteration 20, loss = 2.29660417\n",
      "Iteration 21, loss = 2.29573328\n",
      "Iteration 22, loss = 2.29529520\n",
      "Iteration 23, loss = 2.29414474\n",
      "Iteration 24, loss = 2.29301840\n",
      "Iteration 25, loss = 2.29127883\n",
      "Iteration 26, loss = 2.28974189\n",
      "Iteration 27, loss = 2.28724930\n",
      "Iteration 28, loss = 2.28448801\n",
      "Iteration 29, loss = 2.28064170\n",
      "Iteration 30, loss = 2.27558584\n",
      "Iteration 31, loss = 2.26806333\n",
      "Iteration 32, loss = 2.25742725\n",
      "Iteration 33, loss = 2.24034416\n",
      "Iteration 34, loss = 2.21227948\n",
      "Iteration 35, loss = 2.16377394\n",
      "Iteration 36, loss = 2.08333985\n",
      "Iteration 37, loss = 1.97461715\n",
      "Iteration 38, loss = 1.86181773\n",
      "Iteration 39, loss = 1.75449301\n",
      "Iteration 40, loss = 1.64268044\n",
      "Iteration 41, loss = 1.52330110\n",
      "Iteration 42, loss = 1.41446451\n",
      "Iteration 43, loss = 1.32959414\n",
      "Iteration 44, loss = 1.26609861\n",
      "Iteration 45, loss = 1.21500547\n",
      "Iteration 46, loss = 1.17084611\n",
      "Iteration 47, loss = 1.12907663\n",
      "Iteration 48, loss = 1.08792722\n",
      "Iteration 49, loss = 1.04613048\n",
      "Iteration 50, loss = 1.00380446\n",
      "Iteration 51, loss = 0.96104510\n",
      "Iteration 52, loss = 0.91856402\n",
      "Iteration 53, loss = 0.87862057\n",
      "Iteration 54, loss = 0.84192127\n",
      "Iteration 55, loss = 0.80941421\n",
      "Iteration 56, loss = 0.78143419\n",
      "Iteration 57, loss = 0.75710203\n",
      "Iteration 58, loss = 0.73565118\n",
      "Iteration 59, loss = 0.71657607\n",
      "Iteration 60, loss = 0.69928429\n",
      "Iteration 61, loss = 0.68306420\n",
      "Iteration 62, loss = 0.66814311\n",
      "Iteration 63, loss = 0.65362824\n",
      "Iteration 64, loss = 0.63994088\n",
      "Iteration 65, loss = 0.62676204\n",
      "Iteration 66, loss = 0.61392767\n",
      "Iteration 67, loss = 0.60154730\n",
      "Iteration 68, loss = 0.58973340\n",
      "Iteration 69, loss = 0.57822767\n",
      "Iteration 70, loss = 0.56741488\n",
      "Iteration 71, loss = 0.55665910\n",
      "Iteration 72, loss = 0.54633707\n",
      "Iteration 73, loss = 0.53658415\n",
      "Iteration 74, loss = 0.52709466\n",
      "Iteration 75, loss = 0.51784482\n",
      "Iteration 76, loss = 0.50873638\n",
      "Iteration 77, loss = 0.50018575\n",
      "Iteration 78, loss = 0.49173573\n",
      "Iteration 79, loss = 0.48393801\n",
      "Iteration 80, loss = 0.47566905\n",
      "Iteration 81, loss = 0.46806559\n",
      "Iteration 82, loss = 0.46051256\n",
      "Iteration 83, loss = 0.45307222\n",
      "Iteration 84, loss = 0.44600462\n",
      "Iteration 85, loss = 0.43877267\n",
      "Iteration 86, loss = 0.43195371\n",
      "Iteration 87, loss = 0.42528533\n",
      "Iteration 88, loss = 0.41849272\n",
      "Iteration 89, loss = 0.41210595\n",
      "Iteration 90, loss = 0.40549701\n",
      "Iteration 91, loss = 0.39936074\n",
      "Iteration 92, loss = 0.39316872\n",
      "Iteration 93, loss = 0.38721138\n",
      "Iteration 94, loss = 0.38151326\n",
      "Iteration 95, loss = 0.37567129\n",
      "Iteration 96, loss = 0.37005575\n",
      "Iteration 97, loss = 0.36421842\n",
      "Iteration 98, loss = 0.35916066\n",
      "Iteration 99, loss = 0.35387197\n",
      "Iteration 100, loss = 0.34897245\n",
      "Iteration 101, loss = 0.34402180\n",
      "Iteration 102, loss = 0.33919600\n",
      "Iteration 103, loss = 0.33443662\n",
      "Iteration 104, loss = 0.32998983\n",
      "Iteration 105, loss = 0.32543082\n",
      "Iteration 106, loss = 0.32117638\n",
      "Iteration 107, loss = 0.31690521\n",
      "Iteration 108, loss = 0.31290165\n",
      "Iteration 109, loss = 0.30875317\n",
      "Iteration 110, loss = 0.30511456\n",
      "Iteration 111, loss = 0.30137656\n",
      "Iteration 112, loss = 0.29752513\n",
      "Iteration 113, loss = 0.29398234\n",
      "Iteration 114, loss = 0.29031780\n",
      "Iteration 115, loss = 0.28681977\n",
      "Iteration 116, loss = 0.28345126\n",
      "Iteration 117, loss = 0.27995649\n",
      "Iteration 118, loss = 0.27687642\n",
      "Iteration 119, loss = 0.27371156\n",
      "Iteration 120, loss = 0.27040800\n",
      "Iteration 121, loss = 0.26727080\n",
      "Iteration 122, loss = 0.26435382\n",
      "Iteration 123, loss = 0.26130710\n",
      "Iteration 124, loss = 0.25843454\n",
      "Iteration 125, loss = 0.25551968\n",
      "Iteration 126, loss = 0.25260011\n",
      "Iteration 127, loss = 0.24999046\n",
      "Iteration 128, loss = 0.24709766\n",
      "Iteration 129, loss = 0.24457770\n",
      "Iteration 130, loss = 0.24186881\n",
      "Iteration 131, loss = 0.23927978\n",
      "Iteration 132, loss = 0.23689159\n",
      "Iteration 133, loss = 0.23441562\n",
      "Iteration 134, loss = 0.23160138\n",
      "Iteration 135, loss = 0.22934477\n",
      "Iteration 136, loss = 0.22696111\n",
      "Iteration 137, loss = 0.22454150\n",
      "Iteration 138, loss = 0.22227902\n",
      "Iteration 139, loss = 0.22008876\n",
      "Iteration 140, loss = 0.21771929\n",
      "Iteration 141, loss = 0.21576128\n",
      "Iteration 142, loss = 0.21356401\n",
      "Iteration 143, loss = 0.21143768\n",
      "Iteration 144, loss = 0.20931991\n",
      "Iteration 145, loss = 0.20755603\n",
      "Iteration 146, loss = 0.20542857\n",
      "Iteration 147, loss = 0.20339858\n",
      "Iteration 148, loss = 0.20149798\n",
      "Iteration 149, loss = 0.19967530\n",
      "Iteration 150, loss = 0.19781354\n",
      "Iteration 151, loss = 0.19598496\n",
      "Iteration 152, loss = 0.19424371\n",
      "Iteration 153, loss = 0.19234048\n",
      "Iteration 154, loss = 0.19052845\n",
      "Iteration 155, loss = 0.18888115\n",
      "Iteration 156, loss = 0.18716965\n",
      "Iteration 157, loss = 0.18566688\n",
      "Iteration 158, loss = 0.18381188\n",
      "Iteration 159, loss = 0.18221907\n",
      "Iteration 160, loss = 0.18070491\n",
      "Iteration 161, loss = 0.17913348\n",
      "Iteration 162, loss = 0.17756996\n",
      "Iteration 163, loss = 0.17607361\n",
      "Iteration 164, loss = 0.17469886\n",
      "Iteration 165, loss = 0.17313085\n",
      "Iteration 166, loss = 0.17162374\n",
      "Iteration 167, loss = 0.17028330\n",
      "Iteration 168, loss = 0.16877006\n",
      "Iteration 169, loss = 0.16743108\n",
      "Iteration 170, loss = 0.16620698\n",
      "Iteration 171, loss = 0.16476914\n",
      "Iteration 172, loss = 0.16361531\n",
      "Iteration 173, loss = 0.16211891\n",
      "Iteration 174, loss = 0.16087583\n",
      "Iteration 175, loss = 0.15947549\n",
      "Iteration 176, loss = 0.15834134\n",
      "Iteration 177, loss = 0.15699250\n",
      "Iteration 178, loss = 0.15574871\n",
      "Iteration 179, loss = 0.15448766\n",
      "Iteration 180, loss = 0.15342940\n",
      "Iteration 181, loss = 0.15216586\n",
      "Iteration 182, loss = 0.15104038\n",
      "Iteration 183, loss = 0.14973865\n",
      "Iteration 184, loss = 0.14866459\n",
      "Iteration 185, loss = 0.14757361\n",
      "Iteration 186, loss = 0.14634452\n",
      "Iteration 187, loss = 0.14519077\n",
      "Iteration 188, loss = 0.14428460\n",
      "Iteration 189, loss = 0.14333213\n",
      "Iteration 190, loss = 0.14207592\n",
      "Iteration 191, loss = 0.14102030\n",
      "Iteration 192, loss = 0.13999682\n",
      "Iteration 193, loss = 0.13881955\n",
      "Iteration 194, loss = 0.13795685\n",
      "Iteration 195, loss = 0.13686898\n",
      "Iteration 196, loss = 0.13593068\n",
      "Iteration 197, loss = 0.13491826\n",
      "Iteration 198, loss = 0.13396780\n",
      "Iteration 199, loss = 0.13317425\n",
      "Iteration 200, loss = 0.13213978\n",
      "[CV] END activation=logistic, alpha=0.0001, batch_size=50, hidden_layer_sizes=(100, 100, 100), solver=sgd; total time= 5.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.86372219\n",
      "Iteration 2, loss = 0.33877387\n",
      "Iteration 3, loss = 0.26536197\n",
      "Iteration 4, loss = 0.22555592\n",
      "Iteration 5, loss = 0.19685892\n",
      "Iteration 6, loss = 0.17467885\n",
      "Iteration 7, loss = 0.15786668\n",
      "Iteration 8, loss = 0.14239912\n",
      "Iteration 9, loss = 0.13070620\n",
      "Iteration 10, loss = 0.11926802\n",
      "Iteration 11, loss = 0.11006918\n",
      "Iteration 12, loss = 0.10094421\n",
      "Iteration 13, loss = 0.09339281\n",
      "Iteration 14, loss = 0.08621060\n",
      "Iteration 15, loss = 0.07933284\n",
      "Iteration 16, loss = 0.07381189\n",
      "Iteration 17, loss = 0.06874444\n",
      "Iteration 18, loss = 0.06306228\n",
      "Iteration 19, loss = 0.05889365\n",
      "Iteration 20, loss = 0.05438817\n",
      "Iteration 21, loss = 0.05041900\n",
      "Iteration 22, loss = 0.04718957\n",
      "Iteration 23, loss = 0.04367428\n",
      "Iteration 24, loss = 0.04056048\n",
      "Iteration 25, loss = 0.03775880\n",
      "Iteration 26, loss = 0.03499827\n",
      "Iteration 27, loss = 0.03246508\n",
      "Iteration 28, loss = 0.02998456\n",
      "Iteration 29, loss = 0.02762997\n",
      "Iteration 30, loss = 0.02571346\n",
      "Iteration 31, loss = 0.02411959\n",
      "Iteration 32, loss = 0.02236838\n",
      "Iteration 33, loss = 0.02053651\n",
      "Iteration 34, loss = 0.01924554\n",
      "Iteration 35, loss = 0.01779088\n",
      "Iteration 36, loss = 0.01662190\n",
      "Iteration 37, loss = 0.01531692\n",
      "Iteration 38, loss = 0.01449564\n",
      "Iteration 39, loss = 0.01346771\n",
      "Iteration 40, loss = 0.01250044\n",
      "Iteration 41, loss = 0.01159483\n",
      "Iteration 42, loss = 0.01079789\n",
      "Iteration 43, loss = 0.01030068\n",
      "Iteration 44, loss = 0.00964159\n",
      "Iteration 45, loss = 0.00911646\n",
      "Iteration 46, loss = 0.00863881\n",
      "Iteration 47, loss = 0.00816089\n",
      "Iteration 48, loss = 0.00786787\n",
      "Iteration 49, loss = 0.00744977\n",
      "Iteration 50, loss = 0.00699970\n",
      "Iteration 51, loss = 0.00678392\n",
      "Iteration 52, loss = 0.00647041\n",
      "Iteration 53, loss = 0.00612034\n",
      "Iteration 54, loss = 0.00593434\n",
      "Iteration 55, loss = 0.00574700\n",
      "Iteration 56, loss = 0.00552103\n",
      "Iteration 57, loss = 0.00530681\n",
      "Iteration 58, loss = 0.00524635\n",
      "Iteration 59, loss = 0.00503672\n",
      "Iteration 60, loss = 0.00492383\n",
      "Iteration 61, loss = 0.00474636\n",
      "Iteration 62, loss = 0.00468871\n",
      "Iteration 63, loss = 0.00451537\n",
      "Iteration 64, loss = 0.00442395\n",
      "Iteration 65, loss = 0.00436133\n",
      "Iteration 66, loss = 0.00425593\n",
      "Iteration 67, loss = 0.00417938\n",
      "Iteration 68, loss = 0.00406213\n",
      "Iteration 69, loss = 0.00402702\n",
      "Iteration 70, loss = 0.00392992\n",
      "Iteration 71, loss = 0.00388372\n",
      "Iteration 72, loss = 0.00384179\n",
      "Iteration 73, loss = 0.00375464\n",
      "Iteration 74, loss = 0.00372013\n",
      "Iteration 75, loss = 0.00368392\n",
      "Iteration 76, loss = 0.00360445\n",
      "Iteration 77, loss = 0.00361716\n",
      "Iteration 78, loss = 0.00362955\n",
      "Iteration 79, loss = 0.00436946\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.0001, batch_size=100, hidden_layer_sizes=(100,), solver=adam; total time= 1.1min\n",
      "Iteration 1, loss = 0.84938261\n",
      "Iteration 2, loss = 0.33897802\n",
      "Iteration 3, loss = 0.26490511\n",
      "Iteration 4, loss = 0.22566973\n",
      "Iteration 5, loss = 0.19827398\n",
      "Iteration 6, loss = 0.17669999\n",
      "Iteration 7, loss = 0.15982426\n",
      "Iteration 8, loss = 0.14507245\n",
      "Iteration 9, loss = 0.13288636\n",
      "Iteration 10, loss = 0.12108331\n",
      "Iteration 11, loss = 0.11133147\n",
      "Iteration 12, loss = 0.10250749\n",
      "Iteration 13, loss = 0.09422404\n",
      "Iteration 14, loss = 0.08674495\n",
      "Iteration 15, loss = 0.08018794\n",
      "Iteration 16, loss = 0.07419161\n",
      "Iteration 17, loss = 0.06841486\n",
      "Iteration 18, loss = 0.06315388\n",
      "Iteration 19, loss = 0.05856608\n",
      "Iteration 20, loss = 0.05417359\n",
      "Iteration 21, loss = 0.05023315\n",
      "Iteration 22, loss = 0.04657052\n",
      "Iteration 23, loss = 0.04305273\n",
      "Iteration 24, loss = 0.03974483\n",
      "Iteration 25, loss = 0.03725232\n",
      "Iteration 26, loss = 0.03433012\n",
      "Iteration 27, loss = 0.03178614\n",
      "Iteration 28, loss = 0.02929446\n",
      "Iteration 29, loss = 0.02705994\n",
      "Iteration 30, loss = 0.02514149\n",
      "Iteration 31, loss = 0.02324982\n",
      "Iteration 32, loss = 0.02167145\n",
      "Iteration 33, loss = 0.02010086\n",
      "Iteration 34, loss = 0.01852341\n",
      "Iteration 35, loss = 0.01733336\n",
      "Iteration 36, loss = 0.01627156\n",
      "Iteration 37, loss = 0.01488572\n",
      "Iteration 38, loss = 0.01385171\n",
      "Iteration 39, loss = 0.01293969\n",
      "Iteration 40, loss = 0.01221364\n",
      "Iteration 41, loss = 0.01136817\n",
      "Iteration 42, loss = 0.01059703\n",
      "Iteration 43, loss = 0.00992359\n",
      "Iteration 44, loss = 0.00954876\n",
      "Iteration 45, loss = 0.00889500\n",
      "Iteration 46, loss = 0.00839782\n",
      "Iteration 47, loss = 0.00801169\n",
      "Iteration 48, loss = 0.00756092\n",
      "Iteration 49, loss = 0.00725073\n",
      "Iteration 50, loss = 0.00687837\n",
      "Iteration 51, loss = 0.00656682\n",
      "Iteration 52, loss = 0.00631085\n",
      "Iteration 53, loss = 0.00604213\n",
      "Iteration 54, loss = 0.00581413\n",
      "Iteration 55, loss = 0.00564007\n",
      "Iteration 56, loss = 0.00544463\n",
      "Iteration 57, loss = 0.00524204\n",
      "Iteration 58, loss = 0.00509327\n",
      "Iteration 59, loss = 0.00489675\n",
      "Iteration 60, loss = 0.00481076\n",
      "Iteration 61, loss = 0.00464729\n",
      "Iteration 62, loss = 0.00456476\n",
      "Iteration 63, loss = 0.00442828\n",
      "Iteration 64, loss = 0.00436700\n",
      "Iteration 65, loss = 0.00427362\n",
      "Iteration 66, loss = 0.00416392\n",
      "Iteration 67, loss = 0.00410472\n",
      "Iteration 68, loss = 0.00401755\n",
      "Iteration 69, loss = 0.00394235\n",
      "Iteration 70, loss = 0.00389855\n",
      "Iteration 71, loss = 0.00383927\n",
      "Iteration 72, loss = 0.00377765\n",
      "Iteration 73, loss = 0.00390179\n",
      "Iteration 74, loss = 0.00376990\n",
      "Iteration 75, loss = 0.00356319\n",
      "Iteration 76, loss = 0.00356387\n",
      "Iteration 77, loss = 0.00352093\n",
      "Iteration 78, loss = 0.00344985\n",
      "Iteration 79, loss = 0.00342311\n",
      "Iteration 80, loss = 0.00343292\n",
      "Iteration 81, loss = 0.00340608\n",
      "Iteration 82, loss = 0.00336150\n",
      "Iteration 83, loss = 0.00332428\n",
      "Iteration 84, loss = 0.00329945\n",
      "Iteration 85, loss = 0.00324037\n",
      "Iteration 86, loss = 0.00323766\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.0001, batch_size=100, hidden_layer_sizes=(100,), solver=adam; total time= 1.1min\n",
      "Iteration 1, loss = 0.85583485\n",
      "Iteration 2, loss = 0.33568763\n",
      "Iteration 3, loss = 0.26159699\n",
      "Iteration 4, loss = 0.22171881\n",
      "Iteration 5, loss = 0.19408738\n",
      "Iteration 6, loss = 0.17258808\n",
      "Iteration 7, loss = 0.15511283\n",
      "Iteration 8, loss = 0.14085848\n",
      "Iteration 9, loss = 0.12824605\n",
      "Iteration 10, loss = 0.11800673\n",
      "Iteration 11, loss = 0.10835083\n",
      "Iteration 12, loss = 0.09989423\n",
      "Iteration 13, loss = 0.09262550\n",
      "Iteration 14, loss = 0.08565031\n",
      "Iteration 15, loss = 0.07912208\n",
      "Iteration 16, loss = 0.07331214\n",
      "Iteration 17, loss = 0.06822566\n",
      "Iteration 18, loss = 0.06327681\n",
      "Iteration 19, loss = 0.05876246\n",
      "Iteration 20, loss = 0.05472879\n",
      "Iteration 21, loss = 0.05046667\n",
      "Iteration 22, loss = 0.04685247\n",
      "Iteration 23, loss = 0.04341801\n",
      "Iteration 24, loss = 0.04026341\n",
      "Iteration 25, loss = 0.03754326\n",
      "Iteration 26, loss = 0.03465766\n",
      "Iteration 27, loss = 0.03225020\n",
      "Iteration 28, loss = 0.02977640\n",
      "Iteration 29, loss = 0.02740877\n",
      "Iteration 30, loss = 0.02552832\n",
      "Iteration 31, loss = 0.02373980\n",
      "Iteration 32, loss = 0.02189630\n",
      "Iteration 33, loss = 0.02022581\n",
      "Iteration 34, loss = 0.01891511\n",
      "Iteration 35, loss = 0.01756416\n",
      "Iteration 36, loss = 0.01631009\n",
      "Iteration 37, loss = 0.01531355\n",
      "Iteration 38, loss = 0.01415321\n",
      "Iteration 39, loss = 0.01331413\n",
      "Iteration 40, loss = 0.01239906\n",
      "Iteration 41, loss = 0.01166260\n",
      "Iteration 42, loss = 0.01089723\n",
      "Iteration 43, loss = 0.01020854\n",
      "Iteration 44, loss = 0.00975232\n",
      "Iteration 45, loss = 0.00919528\n",
      "Iteration 46, loss = 0.00857908\n",
      "Iteration 47, loss = 0.00819048\n",
      "Iteration 48, loss = 0.00774638\n",
      "Iteration 49, loss = 0.00735236\n",
      "Iteration 50, loss = 0.00697111\n",
      "Iteration 51, loss = 0.00674764\n",
      "Iteration 52, loss = 0.00635498\n",
      "Iteration 53, loss = 0.00620820\n",
      "Iteration 54, loss = 0.00596923\n",
      "Iteration 55, loss = 0.00571018\n",
      "Iteration 56, loss = 0.00550584\n",
      "Iteration 57, loss = 0.00542207\n",
      "Iteration 58, loss = 0.00526351\n",
      "Iteration 59, loss = 0.00499907\n",
      "Iteration 60, loss = 0.00486223\n",
      "Iteration 61, loss = 0.00473074\n",
      "Iteration 62, loss = 0.00464760\n",
      "Iteration 63, loss = 0.00450917\n",
      "Iteration 64, loss = 0.00437088\n",
      "Iteration 65, loss = 0.00429207\n",
      "Iteration 66, loss = 0.00420866\n",
      "Iteration 67, loss = 0.00415157\n",
      "Iteration 68, loss = 0.00405041\n",
      "Iteration 69, loss = 0.00398319\n",
      "Iteration 70, loss = 0.00392150\n",
      "Iteration 71, loss = 0.00384248\n",
      "Iteration 72, loss = 0.00381193\n",
      "Iteration 73, loss = 0.00377796\n",
      "Iteration 74, loss = 0.00368702\n",
      "Iteration 75, loss = 0.00361901\n",
      "Iteration 76, loss = 0.00357917\n",
      "Iteration 77, loss = 0.00356332\n",
      "Iteration 78, loss = 0.00349441\n",
      "Iteration 79, loss = 0.00344927\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.0001, batch_size=100, hidden_layer_sizes=(100,), solver=adam; total time= 1.1min\n",
      "Iteration 1, loss = 0.87164509\n",
      "Iteration 2, loss = 0.33964286\n",
      "Iteration 3, loss = 0.26450718\n",
      "Iteration 4, loss = 0.22551373\n",
      "Iteration 5, loss = 0.19784510\n",
      "Iteration 6, loss = 0.17616556\n",
      "Iteration 7, loss = 0.15858027\n",
      "Iteration 8, loss = 0.14405397\n",
      "Iteration 9, loss = 0.13176568\n",
      "Iteration 10, loss = 0.12030497\n",
      "Iteration 11, loss = 0.11077050\n",
      "Iteration 12, loss = 0.10197838\n",
      "Iteration 13, loss = 0.09442675\n",
      "Iteration 14, loss = 0.08674401\n",
      "Iteration 15, loss = 0.08015076\n",
      "Iteration 16, loss = 0.07379290\n",
      "Iteration 17, loss = 0.06861622\n",
      "Iteration 18, loss = 0.06333851\n",
      "Iteration 19, loss = 0.05894173\n",
      "Iteration 20, loss = 0.05420123\n",
      "Iteration 21, loss = 0.05055278\n",
      "Iteration 22, loss = 0.04701611\n",
      "Iteration 23, loss = 0.04343754\n",
      "Iteration 24, loss = 0.04013138\n",
      "Iteration 25, loss = 0.03727578\n",
      "Iteration 26, loss = 0.03458391\n",
      "Iteration 27, loss = 0.03179382\n",
      "Iteration 28, loss = 0.02950966\n",
      "Iteration 29, loss = 0.02732840\n",
      "Iteration 30, loss = 0.02553695\n",
      "Iteration 31, loss = 0.02386757\n",
      "Iteration 32, loss = 0.02195162\n",
      "Iteration 33, loss = 0.02028242\n",
      "Iteration 34, loss = 0.01891448\n",
      "Iteration 35, loss = 0.01756453\n",
      "Iteration 36, loss = 0.01636981\n",
      "Iteration 37, loss = 0.01521792\n",
      "Iteration 38, loss = 0.01421390\n",
      "Iteration 39, loss = 0.01320536\n",
      "Iteration 40, loss = 0.01239504\n",
      "Iteration 41, loss = 0.01154257\n",
      "Iteration 42, loss = 0.01093918\n",
      "Iteration 43, loss = 0.01010072\n",
      "Iteration 44, loss = 0.00964617\n",
      "Iteration 45, loss = 0.00910872\n",
      "Iteration 46, loss = 0.00853370\n",
      "Iteration 47, loss = 0.00823350\n",
      "Iteration 48, loss = 0.00767833\n",
      "Iteration 49, loss = 0.00732055\n",
      "Iteration 50, loss = 0.00703525\n",
      "Iteration 51, loss = 0.00671719\n",
      "Iteration 52, loss = 0.00644090\n",
      "Iteration 53, loss = 0.00610406\n",
      "Iteration 54, loss = 0.00590771\n",
      "Iteration 55, loss = 0.00568944\n",
      "Iteration 56, loss = 0.00554310\n",
      "Iteration 57, loss = 0.00537940\n",
      "Iteration 58, loss = 0.00515467\n",
      "Iteration 59, loss = 0.00500869\n",
      "Iteration 60, loss = 0.00491230\n",
      "Iteration 61, loss = 0.00475591\n",
      "Iteration 62, loss = 0.00459395\n",
      "Iteration 63, loss = 0.00455047\n",
      "Iteration 64, loss = 0.00440705\n",
      "Iteration 65, loss = 0.00428666\n",
      "Iteration 66, loss = 0.00420708\n",
      "Iteration 67, loss = 0.00410965\n",
      "Iteration 68, loss = 0.00403126\n",
      "Iteration 69, loss = 0.00402710\n",
      "Iteration 70, loss = 0.00399367\n",
      "Iteration 71, loss = 0.00387221\n",
      "Iteration 72, loss = 0.00381825\n",
      "Iteration 73, loss = 0.00373403\n",
      "Iteration 74, loss = 0.00368268\n",
      "Iteration 75, loss = 0.00365492\n",
      "Iteration 76, loss = 0.00360350\n",
      "Iteration 77, loss = 0.00352279\n",
      "Iteration 78, loss = 0.00350915\n",
      "Iteration 79, loss = 0.00366877\n",
      "Iteration 80, loss = 0.00368253\n",
      "Iteration 81, loss = 0.00337013\n",
      "Iteration 82, loss = 0.00331370\n",
      "Iteration 83, loss = 0.00329481\n",
      "Iteration 84, loss = 0.00329252\n",
      "Iteration 85, loss = 0.00329235\n",
      "Iteration 86, loss = 0.00332319\n",
      "Iteration 87, loss = 0.00365088\n",
      "Iteration 88, loss = 0.00344073\n",
      "Iteration 89, loss = 0.00315473\n",
      "Iteration 90, loss = 0.00310786\n",
      "Iteration 91, loss = 0.00309799\n",
      "Iteration 92, loss = 0.00312081\n",
      "Iteration 93, loss = 0.00311573\n",
      "Iteration 94, loss = 0.00310468\n",
      "Iteration 95, loss = 0.00310722\n",
      "Iteration 96, loss = 0.00497717\n",
      "Iteration 97, loss = 0.00335770\n",
      "Iteration 98, loss = 0.00307051\n",
      "Iteration 99, loss = 0.00302909\n",
      "Iteration 100, loss = 0.00300287\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.0001, batch_size=100, hidden_layer_sizes=(100,), solver=adam; total time= 1.3min\n",
      "Iteration 1, loss = 0.86304164\n",
      "Iteration 2, loss = 0.34019416\n",
      "Iteration 3, loss = 0.26652878\n",
      "Iteration 4, loss = 0.22749377\n",
      "Iteration 5, loss = 0.19954474\n",
      "Iteration 6, loss = 0.17752187\n",
      "Iteration 7, loss = 0.16040373\n",
      "Iteration 8, loss = 0.14501806\n",
      "Iteration 9, loss = 0.13205017\n",
      "Iteration 10, loss = 0.12134022\n",
      "Iteration 11, loss = 0.11104164\n",
      "Iteration 12, loss = 0.10234057\n",
      "Iteration 13, loss = 0.09361937\n",
      "Iteration 14, loss = 0.08641629\n",
      "Iteration 15, loss = 0.07990928\n",
      "Iteration 16, loss = 0.07339646\n",
      "Iteration 17, loss = 0.06815384\n",
      "Iteration 18, loss = 0.06311953\n",
      "Iteration 19, loss = 0.05816692\n",
      "Iteration 20, loss = 0.05379973\n",
      "Iteration 21, loss = 0.04984710\n",
      "Iteration 22, loss = 0.04619942\n",
      "Iteration 23, loss = 0.04257454\n",
      "Iteration 24, loss = 0.03918186\n",
      "Iteration 25, loss = 0.03652769\n",
      "Iteration 26, loss = 0.03350085\n",
      "Iteration 27, loss = 0.03112120\n",
      "Iteration 28, loss = 0.02887234\n",
      "Iteration 29, loss = 0.02651647\n",
      "Iteration 30, loss = 0.02467285\n",
      "Iteration 31, loss = 0.02279961\n",
      "Iteration 32, loss = 0.02110334\n",
      "Iteration 33, loss = 0.01953902\n",
      "Iteration 34, loss = 0.01803919\n",
      "Iteration 35, loss = 0.01675723\n",
      "Iteration 36, loss = 0.01545266\n",
      "Iteration 37, loss = 0.01432661\n",
      "Iteration 38, loss = 0.01347667\n",
      "Iteration 39, loss = 0.01266414\n",
      "Iteration 40, loss = 0.01158931\n",
      "Iteration 41, loss = 0.01107190\n",
      "Iteration 42, loss = 0.01020771\n",
      "Iteration 43, loss = 0.00961634\n",
      "Iteration 44, loss = 0.00903754\n",
      "Iteration 45, loss = 0.00867770\n",
      "Iteration 46, loss = 0.00814202\n",
      "Iteration 47, loss = 0.00767659\n",
      "Iteration 48, loss = 0.00731293\n",
      "Iteration 49, loss = 0.00701231\n",
      "Iteration 50, loss = 0.00668759\n",
      "Iteration 51, loss = 0.00640672\n",
      "Iteration 52, loss = 0.00619593\n",
      "Iteration 53, loss = 0.00591367\n",
      "Iteration 54, loss = 0.00572771\n",
      "Iteration 55, loss = 0.00546996\n",
      "Iteration 56, loss = 0.00524007\n",
      "Iteration 57, loss = 0.00515739\n",
      "Iteration 58, loss = 0.00497911\n",
      "Iteration 59, loss = 0.00480624\n",
      "Iteration 60, loss = 0.00471407\n",
      "Iteration 61, loss = 0.00462818\n",
      "Iteration 62, loss = 0.00446198\n",
      "Iteration 63, loss = 0.00435521\n",
      "Iteration 64, loss = 0.00427778\n",
      "Iteration 65, loss = 0.00422281\n",
      "Iteration 66, loss = 0.00406801\n",
      "Iteration 67, loss = 0.00405187\n",
      "Iteration 68, loss = 0.00397720\n",
      "Iteration 69, loss = 0.00389533\n",
      "Iteration 70, loss = 0.00384910\n",
      "Iteration 71, loss = 0.00377323\n",
      "Iteration 72, loss = 0.00371964\n",
      "Iteration 73, loss = 0.00370744\n",
      "Iteration 74, loss = 0.00362125\n",
      "Iteration 75, loss = 0.00359701\n",
      "Iteration 76, loss = 0.00354284\n",
      "Iteration 77, loss = 0.00345727\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.0001, batch_size=100, hidden_layer_sizes=(100,), solver=adam; total time= 1.0min\n",
      "Iteration 1, loss = 2.22577065\n",
      "Iteration 2, loss = 2.05713746\n",
      "Iteration 3, loss = 1.84936436\n",
      "Iteration 4, loss = 1.61560760\n",
      "Iteration 5, loss = 1.39354251\n",
      "Iteration 6, loss = 1.20548241\n",
      "Iteration 7, loss = 1.05601263\n",
      "Iteration 8, loss = 0.93964245\n",
      "Iteration 9, loss = 0.84926318\n",
      "Iteration 10, loss = 0.77843398\n",
      "Iteration 11, loss = 0.72183893\n",
      "Iteration 12, loss = 0.67579279\n",
      "Iteration 13, loss = 0.63774106\n",
      "Iteration 14, loss = 0.60578332\n",
      "Iteration 15, loss = 0.57852000\n",
      "Iteration 16, loss = 0.55511665\n",
      "Iteration 17, loss = 0.53458025\n",
      "Iteration 18, loss = 0.51670072\n",
      "Iteration 19, loss = 0.50073548\n",
      "Iteration 20, loss = 0.48652352\n",
      "Iteration 21, loss = 0.47381728\n",
      "Iteration 22, loss = 0.46236934\n",
      "Iteration 23, loss = 0.45189652\n",
      "Iteration 24, loss = 0.44235725\n",
      "Iteration 25, loss = 0.43373895\n",
      "Iteration 26, loss = 0.42575229\n",
      "Iteration 27, loss = 0.41843091\n",
      "Iteration 28, loss = 0.41164015\n",
      "Iteration 29, loss = 0.40533288\n",
      "Iteration 30, loss = 0.39940573\n",
      "Iteration 31, loss = 0.39398705\n",
      "Iteration 32, loss = 0.38887393\n",
      "Iteration 33, loss = 0.38406379\n",
      "Iteration 34, loss = 0.37951658\n",
      "Iteration 35, loss = 0.37529639\n",
      "Iteration 36, loss = 0.37123349\n",
      "Iteration 37, loss = 0.36744244\n",
      "Iteration 38, loss = 0.36384365\n",
      "Iteration 39, loss = 0.36035185\n",
      "Iteration 40, loss = 0.35716503\n",
      "Iteration 41, loss = 0.35399255\n",
      "Iteration 42, loss = 0.35102585\n",
      "Iteration 43, loss = 0.34817916\n",
      "Iteration 44, loss = 0.34546392\n",
      "Iteration 45, loss = 0.34283087\n",
      "Iteration 46, loss = 0.34031071\n",
      "Iteration 47, loss = 0.33791080\n",
      "Iteration 48, loss = 0.33560645\n",
      "Iteration 49, loss = 0.33330729\n",
      "Iteration 50, loss = 0.33112260\n",
      "Iteration 51, loss = 0.32906675\n",
      "Iteration 52, loss = 0.32708356\n",
      "Iteration 53, loss = 0.32507903\n",
      "Iteration 54, loss = 0.32316701\n",
      "Iteration 55, loss = 0.32138895\n",
      "Iteration 56, loss = 0.31958840\n",
      "Iteration 57, loss = 0.31781806\n",
      "Iteration 58, loss = 0.31618736\n",
      "Iteration 59, loss = 0.31452032\n",
      "Iteration 60, loss = 0.31289792\n",
      "Iteration 61, loss = 0.31140141\n",
      "Iteration 62, loss = 0.30984446\n",
      "Iteration 63, loss = 0.30835647\n",
      "Iteration 64, loss = 0.30695590\n",
      "Iteration 65, loss = 0.30548841\n",
      "Iteration 66, loss = 0.30415500\n",
      "Iteration 67, loss = 0.30268596\n",
      "Iteration 68, loss = 0.30147009\n",
      "Iteration 69, loss = 0.30012190\n",
      "Iteration 70, loss = 0.29888578\n",
      "Iteration 71, loss = 0.29764217\n",
      "Iteration 72, loss = 0.29645567\n",
      "Iteration 73, loss = 0.29515335\n",
      "Iteration 74, loss = 0.29399585\n",
      "Iteration 75, loss = 0.29287492\n",
      "Iteration 76, loss = 0.29174161\n",
      "Iteration 77, loss = 0.29059590\n",
      "Iteration 78, loss = 0.28952418\n",
      "Iteration 79, loss = 0.28845535\n",
      "Iteration 80, loss = 0.28732384\n",
      "Iteration 81, loss = 0.28626382\n",
      "Iteration 82, loss = 0.28522929\n",
      "Iteration 83, loss = 0.28423630\n",
      "Iteration 84, loss = 0.28322597\n",
      "Iteration 85, loss = 0.28221988\n",
      "Iteration 86, loss = 0.28123089\n",
      "Iteration 87, loss = 0.28028145\n",
      "Iteration 88, loss = 0.27928204\n",
      "Iteration 89, loss = 0.27832207\n",
      "Iteration 90, loss = 0.27742765\n",
      "Iteration 91, loss = 0.27652061\n",
      "Iteration 92, loss = 0.27560649\n",
      "Iteration 93, loss = 0.27473498\n",
      "Iteration 94, loss = 0.27380299\n",
      "Iteration 95, loss = 0.27290347\n",
      "Iteration 96, loss = 0.27207765\n",
      "Iteration 97, loss = 0.27118972\n",
      "Iteration 98, loss = 0.27034077\n",
      "Iteration 99, loss = 0.26950125\n",
      "Iteration 100, loss = 0.26862378\n",
      "Iteration 101, loss = 0.26783858\n",
      "Iteration 102, loss = 0.26696694\n",
      "Iteration 103, loss = 0.26619136\n",
      "Iteration 104, loss = 0.26534123\n",
      "Iteration 105, loss = 0.26455530\n",
      "Iteration 106, loss = 0.26374784\n",
      "Iteration 107, loss = 0.26298444\n",
      "Iteration 108, loss = 0.26214607\n",
      "Iteration 109, loss = 0.26139230\n",
      "Iteration 110, loss = 0.26060326\n",
      "Iteration 111, loss = 0.25987084\n",
      "Iteration 112, loss = 0.25906436\n",
      "Iteration 113, loss = 0.25832372\n",
      "Iteration 114, loss = 0.25758661\n",
      "Iteration 115, loss = 0.25681630\n",
      "Iteration 116, loss = 0.25610022\n",
      "Iteration 117, loss = 0.25531804\n",
      "Iteration 118, loss = 0.25458272\n",
      "Iteration 119, loss = 0.25391586\n",
      "Iteration 120, loss = 0.25319963\n",
      "Iteration 121, loss = 0.25244204\n",
      "Iteration 122, loss = 0.25169990\n",
      "Iteration 123, loss = 0.25105601\n",
      "Iteration 124, loss = 0.25029858\n",
      "Iteration 125, loss = 0.24963647\n",
      "Iteration 126, loss = 0.24893193\n",
      "Iteration 127, loss = 0.24825892\n",
      "Iteration 128, loss = 0.24750339\n",
      "Iteration 129, loss = 0.24682900\n",
      "Iteration 130, loss = 0.24615347\n",
      "Iteration 131, loss = 0.24547638\n",
      "Iteration 132, loss = 0.24480214\n",
      "Iteration 133, loss = 0.24412027\n",
      "Iteration 134, loss = 0.24343997\n",
      "Iteration 135, loss = 0.24278741\n",
      "Iteration 136, loss = 0.24212657\n",
      "Iteration 137, loss = 0.24148263\n",
      "Iteration 138, loss = 0.24082324\n",
      "Iteration 139, loss = 0.24017207\n",
      "Iteration 140, loss = 0.23954014\n",
      "Iteration 141, loss = 0.23885709\n",
      "Iteration 142, loss = 0.23824060\n",
      "Iteration 143, loss = 0.23760765\n",
      "Iteration 144, loss = 0.23695970\n",
      "Iteration 145, loss = 0.23632723\n",
      "Iteration 146, loss = 0.23567472\n",
      "Iteration 147, loss = 0.23505208\n",
      "Iteration 148, loss = 0.23441472\n",
      "Iteration 149, loss = 0.23378744\n",
      "Iteration 150, loss = 0.23318266\n",
      "Iteration 151, loss = 0.23252297\n",
      "Iteration 152, loss = 0.23194992\n",
      "Iteration 153, loss = 0.23131742\n",
      "Iteration 154, loss = 0.23071061\n",
      "Iteration 155, loss = 0.23009286\n",
      "Iteration 156, loss = 0.22950035\n",
      "Iteration 157, loss = 0.22886789\n",
      "Iteration 158, loss = 0.22827749\n",
      "Iteration 159, loss = 0.22771030\n",
      "Iteration 160, loss = 0.22709658\n",
      "Iteration 161, loss = 0.22651195\n",
      "Iteration 162, loss = 0.22589218\n",
      "Iteration 163, loss = 0.22534212\n",
      "Iteration 164, loss = 0.22474336\n",
      "Iteration 165, loss = 0.22418128\n",
      "Iteration 166, loss = 0.22360313\n",
      "Iteration 167, loss = 0.22300293\n",
      "Iteration 168, loss = 0.22242852\n",
      "Iteration 169, loss = 0.22186970\n",
      "Iteration 170, loss = 0.22129859\n",
      "Iteration 171, loss = 0.22070601\n",
      "Iteration 172, loss = 0.22014682\n",
      "Iteration 173, loss = 0.21964335\n",
      "Iteration 174, loss = 0.21901108\n",
      "Iteration 175, loss = 0.21843433\n",
      "Iteration 176, loss = 0.21787347\n",
      "Iteration 177, loss = 0.21733072\n",
      "Iteration 178, loss = 0.21679685\n",
      "Iteration 179, loss = 0.21624489\n",
      "Iteration 180, loss = 0.21570748\n",
      "Iteration 181, loss = 0.21513514\n",
      "Iteration 182, loss = 0.21460839\n",
      "Iteration 183, loss = 0.21402272\n",
      "Iteration 184, loss = 0.21348817\n",
      "Iteration 185, loss = 0.21297763\n",
      "Iteration 186, loss = 0.21244200\n",
      "Iteration 187, loss = 0.21193100\n",
      "Iteration 188, loss = 0.21133169\n",
      "Iteration 189, loss = 0.21087445\n",
      "Iteration 190, loss = 0.21030254\n",
      "Iteration 191, loss = 0.20981577\n",
      "Iteration 192, loss = 0.20926886\n",
      "Iteration 193, loss = 0.20874129\n",
      "Iteration 194, loss = 0.20821986\n",
      "Iteration 195, loss = 0.20770475\n",
      "Iteration 196, loss = 0.20715577\n",
      "Iteration 197, loss = 0.20666014\n",
      "Iteration 198, loss = 0.20618473\n",
      "Iteration 199, loss = 0.20566168\n",
      "Iteration 200, loss = 0.20514685\n",
      "[CV] END activation=logistic, alpha=0.0001, batch_size=100, hidden_layer_sizes=(100,), solver=sgd; total time= 1.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.22490885\n",
      "Iteration 2, loss = 2.04942119\n",
      "Iteration 3, loss = 1.83473551\n",
      "Iteration 4, loss = 1.59380352\n",
      "Iteration 5, loss = 1.36783849\n",
      "Iteration 6, loss = 1.18174632\n",
      "Iteration 7, loss = 1.03764049\n",
      "Iteration 8, loss = 0.92673251\n",
      "Iteration 9, loss = 0.84037924\n",
      "Iteration 10, loss = 0.77214424\n",
      "Iteration 11, loss = 0.71723582\n",
      "Iteration 12, loss = 0.67219943\n",
      "Iteration 13, loss = 0.63486272\n",
      "Iteration 14, loss = 0.60334445\n",
      "Iteration 15, loss = 0.57642918\n",
      "Iteration 16, loss = 0.55318165\n",
      "Iteration 17, loss = 0.53296181\n",
      "Iteration 18, loss = 0.51511101\n",
      "Iteration 19, loss = 0.49930350\n",
      "Iteration 20, loss = 0.48527074\n",
      "Iteration 21, loss = 0.47264283\n",
      "Iteration 22, loss = 0.46128149\n",
      "Iteration 23, loss = 0.45092659\n",
      "Iteration 24, loss = 0.44154504\n",
      "Iteration 25, loss = 0.43299200\n",
      "Iteration 26, loss = 0.42509796\n",
      "Iteration 27, loss = 0.41778714\n",
      "Iteration 28, loss = 0.41108813\n",
      "Iteration 29, loss = 0.40477631\n",
      "Iteration 30, loss = 0.39903302\n",
      "Iteration 31, loss = 0.39365699\n",
      "Iteration 32, loss = 0.38855093\n",
      "Iteration 33, loss = 0.38375621\n",
      "Iteration 34, loss = 0.37922041\n",
      "Iteration 35, loss = 0.37508870\n",
      "Iteration 36, loss = 0.37104303\n",
      "Iteration 37, loss = 0.36730747\n",
      "Iteration 38, loss = 0.36373418\n",
      "Iteration 39, loss = 0.36033469\n",
      "Iteration 40, loss = 0.35709548\n",
      "Iteration 41, loss = 0.35399096\n",
      "Iteration 42, loss = 0.35108807\n",
      "Iteration 43, loss = 0.34823014\n",
      "Iteration 44, loss = 0.34544339\n",
      "Iteration 45, loss = 0.34292107\n",
      "Iteration 46, loss = 0.34042385\n",
      "Iteration 47, loss = 0.33797254\n",
      "Iteration 48, loss = 0.33570592\n",
      "Iteration 49, loss = 0.33345950\n",
      "Iteration 50, loss = 0.33133496\n",
      "Iteration 51, loss = 0.32926451\n",
      "Iteration 52, loss = 0.32720294\n",
      "Iteration 53, loss = 0.32520870\n",
      "Iteration 54, loss = 0.32340694\n",
      "Iteration 55, loss = 0.32152524\n",
      "Iteration 56, loss = 0.31973225\n",
      "Iteration 57, loss = 0.31802846\n",
      "Iteration 58, loss = 0.31636281\n",
      "Iteration 59, loss = 0.31468464\n",
      "Iteration 60, loss = 0.31310326\n",
      "Iteration 61, loss = 0.31158445\n",
      "Iteration 62, loss = 0.31003143\n",
      "Iteration 63, loss = 0.30853967\n",
      "Iteration 64, loss = 0.30715671\n",
      "Iteration 65, loss = 0.30569151\n",
      "Iteration 66, loss = 0.30429014\n",
      "Iteration 67, loss = 0.30292862\n",
      "Iteration 68, loss = 0.30162063\n",
      "Iteration 69, loss = 0.30031824\n",
      "Iteration 70, loss = 0.29904159\n",
      "Iteration 71, loss = 0.29777404\n",
      "Iteration 72, loss = 0.29650341\n",
      "Iteration 73, loss = 0.29535921\n",
      "Iteration 74, loss = 0.29414668\n",
      "Iteration 75, loss = 0.29302773\n",
      "Iteration 76, loss = 0.29184669\n",
      "Iteration 77, loss = 0.29071746\n",
      "Iteration 78, loss = 0.28962178\n",
      "Iteration 79, loss = 0.28847602\n",
      "Iteration 80, loss = 0.28744875\n",
      "Iteration 81, loss = 0.28638093\n",
      "Iteration 82, loss = 0.28534377\n",
      "Iteration 83, loss = 0.28431223\n",
      "Iteration 84, loss = 0.28328286\n",
      "Iteration 85, loss = 0.28229236\n",
      "Iteration 86, loss = 0.28126428\n",
      "Iteration 87, loss = 0.28029235\n",
      "Iteration 88, loss = 0.27935676\n",
      "Iteration 89, loss = 0.27839331\n",
      "Iteration 90, loss = 0.27742832\n",
      "Iteration 91, loss = 0.27652303\n",
      "Iteration 92, loss = 0.27556514\n",
      "Iteration 93, loss = 0.27469883\n",
      "Iteration 94, loss = 0.27369734\n",
      "Iteration 95, loss = 0.27283937\n",
      "Iteration 96, loss = 0.27201525\n",
      "Iteration 97, loss = 0.27114977\n",
      "Iteration 98, loss = 0.27022630\n",
      "Iteration 99, loss = 0.26939113\n",
      "Iteration 100, loss = 0.26854449\n",
      "Iteration 101, loss = 0.26765730\n",
      "Iteration 102, loss = 0.26686550\n",
      "Iteration 103, loss = 0.26600992\n",
      "Iteration 104, loss = 0.26521882\n",
      "Iteration 105, loss = 0.26436374\n",
      "Iteration 106, loss = 0.26358927\n",
      "Iteration 107, loss = 0.26278942\n",
      "Iteration 108, loss = 0.26198034\n",
      "Iteration 109, loss = 0.26115950\n",
      "Iteration 110, loss = 0.26038590\n",
      "Iteration 111, loss = 0.25959075\n",
      "Iteration 112, loss = 0.25881908\n",
      "Iteration 113, loss = 0.25808521\n",
      "Iteration 114, loss = 0.25728137\n",
      "Iteration 115, loss = 0.25653074\n",
      "Iteration 116, loss = 0.25583608\n",
      "Iteration 117, loss = 0.25502020\n",
      "Iteration 118, loss = 0.25429602\n",
      "Iteration 119, loss = 0.25354721\n",
      "Iteration 120, loss = 0.25280628\n",
      "Iteration 121, loss = 0.25206163\n",
      "Iteration 122, loss = 0.25140351\n",
      "Iteration 123, loss = 0.25058517\n",
      "Iteration 124, loss = 0.24998081\n",
      "Iteration 125, loss = 0.24920414\n",
      "Iteration 126, loss = 0.24855150\n",
      "Iteration 127, loss = 0.24782856\n",
      "Iteration 128, loss = 0.24709100\n",
      "Iteration 129, loss = 0.24637821\n",
      "Iteration 130, loss = 0.24571837\n",
      "Iteration 131, loss = 0.24501601\n",
      "Iteration 132, loss = 0.24433865\n",
      "Iteration 133, loss = 0.24369552\n",
      "Iteration 134, loss = 0.24297695\n",
      "Iteration 135, loss = 0.24233654\n",
      "Iteration 136, loss = 0.24162643\n",
      "Iteration 137, loss = 0.24096071\n",
      "Iteration 138, loss = 0.24030473\n",
      "Iteration 139, loss = 0.23961740\n",
      "Iteration 140, loss = 0.23899488\n",
      "Iteration 141, loss = 0.23828044\n",
      "Iteration 142, loss = 0.23766909\n",
      "Iteration 143, loss = 0.23703927\n",
      "Iteration 144, loss = 0.23642316\n",
      "Iteration 145, loss = 0.23575347\n",
      "Iteration 146, loss = 0.23512141\n",
      "Iteration 147, loss = 0.23449468\n",
      "Iteration 148, loss = 0.23385067\n",
      "Iteration 149, loss = 0.23322334\n",
      "Iteration 150, loss = 0.23262209\n",
      "Iteration 151, loss = 0.23198720\n",
      "Iteration 152, loss = 0.23136930\n",
      "Iteration 153, loss = 0.23073417\n",
      "Iteration 154, loss = 0.23017045\n",
      "Iteration 155, loss = 0.22955156\n",
      "Iteration 156, loss = 0.22892815\n",
      "Iteration 157, loss = 0.22837284\n",
      "Iteration 158, loss = 0.22772019\n",
      "Iteration 159, loss = 0.22713537\n",
      "Iteration 160, loss = 0.22655144\n",
      "Iteration 161, loss = 0.22595383\n",
      "Iteration 162, loss = 0.22536426\n",
      "Iteration 163, loss = 0.22477297\n",
      "Iteration 164, loss = 0.22418512\n",
      "Iteration 165, loss = 0.22363249\n",
      "Iteration 166, loss = 0.22298129\n",
      "Iteration 167, loss = 0.22246414\n",
      "Iteration 168, loss = 0.22192141\n",
      "Iteration 169, loss = 0.22131784\n",
      "Iteration 170, loss = 0.22077962\n",
      "Iteration 171, loss = 0.22020678\n",
      "Iteration 172, loss = 0.21954625\n",
      "Iteration 173, loss = 0.21910736\n",
      "Iteration 174, loss = 0.21850330\n",
      "Iteration 175, loss = 0.21796887\n",
      "Iteration 176, loss = 0.21744204\n",
      "Iteration 177, loss = 0.21689475\n",
      "Iteration 178, loss = 0.21632293\n",
      "Iteration 179, loss = 0.21579558\n",
      "Iteration 180, loss = 0.21525528\n",
      "Iteration 181, loss = 0.21469004\n",
      "Iteration 182, loss = 0.21417316\n",
      "Iteration 183, loss = 0.21363099\n",
      "Iteration 184, loss = 0.21312371\n",
      "Iteration 185, loss = 0.21257905\n",
      "Iteration 186, loss = 0.21207118\n",
      "Iteration 187, loss = 0.21157115\n",
      "Iteration 188, loss = 0.21100562\n",
      "Iteration 189, loss = 0.21050814\n",
      "Iteration 190, loss = 0.20996378\n",
      "Iteration 191, loss = 0.20947356\n",
      "Iteration 192, loss = 0.20897800\n",
      "Iteration 193, loss = 0.20842640\n",
      "Iteration 194, loss = 0.20795819\n",
      "Iteration 195, loss = 0.20744387\n",
      "Iteration 196, loss = 0.20692638\n",
      "Iteration 197, loss = 0.20645400\n",
      "Iteration 198, loss = 0.20592432\n",
      "Iteration 199, loss = 0.20546115\n",
      "Iteration 200, loss = 0.20497482\n",
      "[CV] END activation=logistic, alpha=0.0001, batch_size=100, hidden_layer_sizes=(100,), solver=sgd; total time= 1.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.24131530\n",
      "Iteration 2, loss = 2.07434221\n",
      "Iteration 3, loss = 1.87033593\n",
      "Iteration 4, loss = 1.63359585\n",
      "Iteration 5, loss = 1.40404193\n",
      "Iteration 6, loss = 1.21137745\n",
      "Iteration 7, loss = 1.06044414\n",
      "Iteration 8, loss = 0.94431080\n",
      "Iteration 9, loss = 0.85460099\n",
      "Iteration 10, loss = 0.78383174\n",
      "Iteration 11, loss = 0.72717890\n",
      "Iteration 12, loss = 0.68106620\n",
      "Iteration 13, loss = 0.64257724\n",
      "Iteration 14, loss = 0.61020513\n",
      "Iteration 15, loss = 0.58246223\n",
      "Iteration 16, loss = 0.55861826\n",
      "Iteration 17, loss = 0.53772994\n",
      "Iteration 18, loss = 0.51923971\n",
      "Iteration 19, loss = 0.50295295\n",
      "Iteration 20, loss = 0.48844526\n",
      "Iteration 21, loss = 0.47524384\n",
      "Iteration 22, loss = 0.46356053\n",
      "Iteration 23, loss = 0.45275687\n",
      "Iteration 24, loss = 0.44304113\n",
      "Iteration 25, loss = 0.43407547\n",
      "Iteration 26, loss = 0.42590687\n",
      "Iteration 27, loss = 0.41827907\n",
      "Iteration 28, loss = 0.41142885\n",
      "Iteration 29, loss = 0.40495735\n",
      "Iteration 30, loss = 0.39890660\n",
      "Iteration 31, loss = 0.39332885\n",
      "Iteration 32, loss = 0.38805008\n",
      "Iteration 33, loss = 0.38319450\n",
      "Iteration 34, loss = 0.37850005\n",
      "Iteration 35, loss = 0.37416020\n",
      "Iteration 36, loss = 0.37008185\n",
      "Iteration 37, loss = 0.36616186\n",
      "Iteration 38, loss = 0.36251326\n",
      "Iteration 39, loss = 0.35891849\n",
      "Iteration 40, loss = 0.35564595\n",
      "Iteration 41, loss = 0.35249203\n",
      "Iteration 42, loss = 0.34940136\n",
      "Iteration 43, loss = 0.34652627\n",
      "Iteration 44, loss = 0.34374126\n",
      "Iteration 45, loss = 0.34102603\n",
      "Iteration 46, loss = 0.33851989\n",
      "Iteration 47, loss = 0.33603031\n",
      "Iteration 48, loss = 0.33366152\n",
      "Iteration 49, loss = 0.33132491\n",
      "Iteration 50, loss = 0.32908189\n",
      "Iteration 51, loss = 0.32698650\n",
      "Iteration 52, loss = 0.32492902\n",
      "Iteration 53, loss = 0.32294339\n",
      "Iteration 54, loss = 0.32098501\n",
      "Iteration 55, loss = 0.31907671\n",
      "Iteration 56, loss = 0.31725696\n",
      "Iteration 57, loss = 0.31555708\n",
      "Iteration 58, loss = 0.31381898\n",
      "Iteration 59, loss = 0.31205383\n",
      "Iteration 60, loss = 0.31044315\n",
      "Iteration 61, loss = 0.30890393\n",
      "Iteration 62, loss = 0.30729955\n",
      "Iteration 63, loss = 0.30582165\n",
      "Iteration 64, loss = 0.30430148\n",
      "Iteration 65, loss = 0.30286097\n",
      "Iteration 66, loss = 0.30137793\n",
      "Iteration 67, loss = 0.30002336\n",
      "Iteration 68, loss = 0.29867648\n",
      "Iteration 69, loss = 0.29739328\n",
      "Iteration 70, loss = 0.29609618\n",
      "Iteration 71, loss = 0.29478420\n",
      "Iteration 72, loss = 0.29351616\n",
      "Iteration 73, loss = 0.29228551\n",
      "Iteration 74, loss = 0.29112501\n",
      "Iteration 75, loss = 0.28990176\n",
      "Iteration 76, loss = 0.28874784\n",
      "Iteration 77, loss = 0.28757576\n",
      "Iteration 78, loss = 0.28647959\n",
      "Iteration 79, loss = 0.28533216\n",
      "Iteration 80, loss = 0.28424182\n",
      "Iteration 81, loss = 0.28315986\n",
      "Iteration 82, loss = 0.28216291\n",
      "Iteration 83, loss = 0.28107767\n",
      "Iteration 84, loss = 0.27999900\n",
      "Iteration 85, loss = 0.27899108\n",
      "Iteration 86, loss = 0.27796833\n",
      "Iteration 87, loss = 0.27702819\n",
      "Iteration 88, loss = 0.27603849\n",
      "Iteration 89, loss = 0.27506725\n",
      "Iteration 90, loss = 0.27407839\n",
      "Iteration 91, loss = 0.27310554\n",
      "Iteration 92, loss = 0.27229616\n",
      "Iteration 93, loss = 0.27128026\n",
      "Iteration 94, loss = 0.27042598\n",
      "Iteration 95, loss = 0.26943850\n",
      "Iteration 96, loss = 0.26855940\n",
      "Iteration 97, loss = 0.26768073\n",
      "Iteration 98, loss = 0.26677846\n",
      "Iteration 99, loss = 0.26595497\n",
      "Iteration 100, loss = 0.26509699\n",
      "Iteration 101, loss = 0.26424963\n",
      "Iteration 102, loss = 0.26340608\n",
      "Iteration 103, loss = 0.26258469\n",
      "Iteration 104, loss = 0.26175072\n",
      "Iteration 105, loss = 0.26091703\n",
      "Iteration 106, loss = 0.26010595\n",
      "Iteration 107, loss = 0.25927283\n",
      "Iteration 108, loss = 0.25846082\n",
      "Iteration 109, loss = 0.25767356\n",
      "Iteration 110, loss = 0.25688281\n",
      "Iteration 111, loss = 0.25610966\n",
      "Iteration 112, loss = 0.25529735\n",
      "Iteration 113, loss = 0.25459221\n",
      "Iteration 114, loss = 0.25376685\n",
      "Iteration 115, loss = 0.25297256\n",
      "Iteration 116, loss = 0.25229065\n",
      "Iteration 117, loss = 0.25154798\n",
      "Iteration 118, loss = 0.25074547\n",
      "Iteration 119, loss = 0.25009800\n",
      "Iteration 120, loss = 0.24932138\n",
      "Iteration 121, loss = 0.24858901\n",
      "Iteration 122, loss = 0.24787252\n",
      "Iteration 123, loss = 0.24714450\n",
      "Iteration 124, loss = 0.24643655\n",
      "Iteration 125, loss = 0.24571642\n",
      "Iteration 126, loss = 0.24505476\n",
      "Iteration 127, loss = 0.24432682\n",
      "Iteration 128, loss = 0.24363411\n",
      "Iteration 129, loss = 0.24296240\n",
      "Iteration 130, loss = 0.24224248\n",
      "Iteration 131, loss = 0.24152539\n",
      "Iteration 132, loss = 0.24087989\n",
      "Iteration 133, loss = 0.24017384\n",
      "Iteration 134, loss = 0.23953679\n",
      "Iteration 135, loss = 0.23889230\n",
      "Iteration 136, loss = 0.23822601\n",
      "Iteration 137, loss = 0.23755679\n",
      "Iteration 138, loss = 0.23689125\n",
      "Iteration 139, loss = 0.23623942\n",
      "Iteration 140, loss = 0.23561330\n",
      "Iteration 141, loss = 0.23493341\n",
      "Iteration 142, loss = 0.23431464\n",
      "Iteration 143, loss = 0.23366040\n",
      "Iteration 144, loss = 0.23305670\n",
      "Iteration 145, loss = 0.23241436\n",
      "Iteration 146, loss = 0.23177727\n",
      "Iteration 147, loss = 0.23121491\n",
      "Iteration 148, loss = 0.23052323\n",
      "Iteration 149, loss = 0.22993888\n",
      "Iteration 150, loss = 0.22932119\n",
      "Iteration 151, loss = 0.22872182\n",
      "Iteration 152, loss = 0.22806851\n",
      "Iteration 153, loss = 0.22747120\n",
      "Iteration 154, loss = 0.22688723\n",
      "Iteration 155, loss = 0.22627122\n",
      "Iteration 156, loss = 0.22569837\n",
      "Iteration 157, loss = 0.22508476\n",
      "Iteration 158, loss = 0.22453292\n",
      "Iteration 159, loss = 0.22392673\n",
      "Iteration 160, loss = 0.22332666\n",
      "Iteration 161, loss = 0.22276291\n",
      "Iteration 162, loss = 0.22222714\n",
      "Iteration 163, loss = 0.22160128\n",
      "Iteration 164, loss = 0.22108244\n",
      "Iteration 165, loss = 0.22045840\n",
      "Iteration 166, loss = 0.21987946\n",
      "Iteration 167, loss = 0.21937148\n",
      "Iteration 168, loss = 0.21876517\n",
      "Iteration 169, loss = 0.21822599\n",
      "Iteration 170, loss = 0.21767317\n",
      "Iteration 171, loss = 0.21712195\n",
      "Iteration 172, loss = 0.21661289\n",
      "Iteration 173, loss = 0.21602962\n",
      "Iteration 174, loss = 0.21550437\n",
      "Iteration 175, loss = 0.21492560\n",
      "Iteration 176, loss = 0.21439527\n",
      "Iteration 177, loss = 0.21389784\n",
      "Iteration 178, loss = 0.21334221\n",
      "Iteration 179, loss = 0.21276040\n",
      "Iteration 180, loss = 0.21232468\n",
      "Iteration 181, loss = 0.21177720\n",
      "Iteration 182, loss = 0.21122745\n",
      "Iteration 183, loss = 0.21074308\n",
      "Iteration 184, loss = 0.21020030\n",
      "Iteration 185, loss = 0.20968945\n",
      "Iteration 186, loss = 0.20917268\n",
      "Iteration 187, loss = 0.20870147\n",
      "Iteration 188, loss = 0.20818442\n",
      "Iteration 189, loss = 0.20763257\n",
      "Iteration 190, loss = 0.20715556\n",
      "Iteration 191, loss = 0.20667471\n",
      "Iteration 192, loss = 0.20619541\n",
      "Iteration 193, loss = 0.20570173\n",
      "Iteration 194, loss = 0.20516231\n",
      "Iteration 195, loss = 0.20469681\n",
      "Iteration 196, loss = 0.20423106\n",
      "Iteration 197, loss = 0.20371956\n",
      "Iteration 198, loss = 0.20325619\n",
      "Iteration 199, loss = 0.20275024\n",
      "Iteration 200, loss = 0.20231047\n",
      "[CV] END activation=logistic, alpha=0.0001, batch_size=100, hidden_layer_sizes=(100,), solver=sgd; total time= 1.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.23618003\n",
      "Iteration 2, loss = 2.06650448\n",
      "Iteration 3, loss = 1.85838350\n",
      "Iteration 4, loss = 1.61921951\n",
      "Iteration 5, loss = 1.38989208\n",
      "Iteration 6, loss = 1.19829587\n",
      "Iteration 7, loss = 1.04869925\n",
      "Iteration 8, loss = 0.93392257\n",
      "Iteration 9, loss = 0.84479997\n",
      "Iteration 10, loss = 0.77491480\n",
      "Iteration 11, loss = 0.71857747\n",
      "Iteration 12, loss = 0.67269244\n",
      "Iteration 13, loss = 0.63460147\n",
      "Iteration 14, loss = 0.60258114\n",
      "Iteration 15, loss = 0.57532029\n",
      "Iteration 16, loss = 0.55178588\n",
      "Iteration 17, loss = 0.53129386\n",
      "Iteration 18, loss = 0.51332383\n",
      "Iteration 19, loss = 0.49746901\n",
      "Iteration 20, loss = 0.48327766\n",
      "Iteration 21, loss = 0.47067828\n",
      "Iteration 22, loss = 0.45924866\n",
      "Iteration 23, loss = 0.44889896\n",
      "Iteration 24, loss = 0.43952067\n",
      "Iteration 25, loss = 0.43102656\n",
      "Iteration 26, loss = 0.42311272\n",
      "Iteration 27, loss = 0.41593291\n",
      "Iteration 28, loss = 0.40921566\n",
      "Iteration 29, loss = 0.40299366\n",
      "Iteration 30, loss = 0.39720381\n",
      "Iteration 31, loss = 0.39185761\n",
      "Iteration 32, loss = 0.38683102\n",
      "Iteration 33, loss = 0.38208551\n",
      "Iteration 34, loss = 0.37762657\n",
      "Iteration 35, loss = 0.37342493\n",
      "Iteration 36, loss = 0.36949188\n",
      "Iteration 37, loss = 0.36573834\n",
      "Iteration 38, loss = 0.36215119\n",
      "Iteration 39, loss = 0.35866565\n",
      "Iteration 40, loss = 0.35554715\n",
      "Iteration 41, loss = 0.35244031\n",
      "Iteration 42, loss = 0.34953053\n",
      "Iteration 43, loss = 0.34668391\n",
      "Iteration 44, loss = 0.34398888\n",
      "Iteration 45, loss = 0.34135904\n",
      "Iteration 46, loss = 0.33886442\n",
      "Iteration 47, loss = 0.33647850\n",
      "Iteration 48, loss = 0.33420138\n",
      "Iteration 49, loss = 0.33197052\n",
      "Iteration 50, loss = 0.32981581\n",
      "Iteration 51, loss = 0.32772547\n",
      "Iteration 52, loss = 0.32567486\n",
      "Iteration 53, loss = 0.32366354\n",
      "Iteration 54, loss = 0.32183250\n",
      "Iteration 55, loss = 0.32000067\n",
      "Iteration 56, loss = 0.31818861\n",
      "Iteration 57, loss = 0.31649597\n",
      "Iteration 58, loss = 0.31473411\n",
      "Iteration 59, loss = 0.31310382\n",
      "Iteration 60, loss = 0.31134110\n",
      "Iteration 61, loss = 0.31001306\n",
      "Iteration 62, loss = 0.30839083\n",
      "Iteration 63, loss = 0.30689069\n",
      "Iteration 64, loss = 0.30542407\n",
      "Iteration 65, loss = 0.30401625\n",
      "Iteration 66, loss = 0.30259771\n",
      "Iteration 67, loss = 0.30125640\n",
      "Iteration 68, loss = 0.29993590\n",
      "Iteration 69, loss = 0.29859838\n",
      "Iteration 70, loss = 0.29733360\n",
      "Iteration 71, loss = 0.29600820\n",
      "Iteration 72, loss = 0.29482597\n",
      "Iteration 73, loss = 0.29360567\n",
      "Iteration 74, loss = 0.29235935\n",
      "Iteration 75, loss = 0.29122874\n",
      "Iteration 76, loss = 0.29002195\n",
      "Iteration 77, loss = 0.28890278\n",
      "Iteration 78, loss = 0.28778566\n",
      "Iteration 79, loss = 0.28667819\n",
      "Iteration 80, loss = 0.28557405\n",
      "Iteration 81, loss = 0.28450659\n",
      "Iteration 82, loss = 0.28348564\n",
      "Iteration 83, loss = 0.28242376\n",
      "Iteration 84, loss = 0.28137386\n",
      "Iteration 85, loss = 0.28037722\n",
      "Iteration 86, loss = 0.27940357\n",
      "Iteration 87, loss = 0.27840162\n",
      "Iteration 88, loss = 0.27741315\n",
      "Iteration 89, loss = 0.27647335\n",
      "Iteration 90, loss = 0.27553143\n",
      "Iteration 91, loss = 0.27456854\n",
      "Iteration 92, loss = 0.27362212\n",
      "Iteration 93, loss = 0.27273176\n",
      "Iteration 94, loss = 0.27185677\n",
      "Iteration 95, loss = 0.27092714\n",
      "Iteration 96, loss = 0.26998808\n",
      "Iteration 97, loss = 0.26913553\n",
      "Iteration 98, loss = 0.26826329\n",
      "Iteration 99, loss = 0.26741366\n",
      "Iteration 100, loss = 0.26656119\n",
      "Iteration 101, loss = 0.26569336\n",
      "Iteration 102, loss = 0.26489081\n",
      "Iteration 103, loss = 0.26403028\n",
      "Iteration 104, loss = 0.26324527\n",
      "Iteration 105, loss = 0.26238547\n",
      "Iteration 106, loss = 0.26160336\n",
      "Iteration 107, loss = 0.26076121\n",
      "Iteration 108, loss = 0.26000435\n",
      "Iteration 109, loss = 0.25919930\n",
      "Iteration 110, loss = 0.25843286\n",
      "Iteration 111, loss = 0.25764008\n",
      "Iteration 112, loss = 0.25684070\n",
      "Iteration 113, loss = 0.25611711\n",
      "Iteration 114, loss = 0.25532684\n",
      "Iteration 115, loss = 0.25458686\n",
      "Iteration 116, loss = 0.25384074\n",
      "Iteration 117, loss = 0.25312578\n",
      "Iteration 118, loss = 0.25234185\n",
      "Iteration 119, loss = 0.25163809\n",
      "Iteration 120, loss = 0.25091485\n",
      "Iteration 121, loss = 0.25016714\n",
      "Iteration 122, loss = 0.24951296\n",
      "Iteration 123, loss = 0.24876532\n",
      "Iteration 124, loss = 0.24808679\n",
      "Iteration 125, loss = 0.24734783\n",
      "Iteration 126, loss = 0.24665768\n",
      "Iteration 127, loss = 0.24599471\n",
      "Iteration 128, loss = 0.24531316\n",
      "Iteration 129, loss = 0.24462319\n",
      "Iteration 130, loss = 0.24390435\n",
      "Iteration 131, loss = 0.24323476\n",
      "Iteration 132, loss = 0.24259499\n",
      "Iteration 133, loss = 0.24194693\n",
      "Iteration 134, loss = 0.24123991\n",
      "Iteration 135, loss = 0.24058225\n",
      "Iteration 136, loss = 0.23997248\n",
      "Iteration 137, loss = 0.23933382\n",
      "Iteration 138, loss = 0.23868047\n",
      "Iteration 139, loss = 0.23797663\n",
      "Iteration 140, loss = 0.23738408\n",
      "Iteration 141, loss = 0.23672348\n",
      "Iteration 142, loss = 0.23609777\n",
      "Iteration 143, loss = 0.23550587\n",
      "Iteration 144, loss = 0.23485173\n",
      "Iteration 145, loss = 0.23424655\n",
      "Iteration 146, loss = 0.23363005\n",
      "Iteration 147, loss = 0.23301613\n",
      "Iteration 148, loss = 0.23239949\n",
      "Iteration 149, loss = 0.23178547\n",
      "Iteration 150, loss = 0.23119802\n",
      "Iteration 151, loss = 0.23062721\n",
      "Iteration 152, loss = 0.23000691\n",
      "Iteration 153, loss = 0.22938420\n",
      "Iteration 154, loss = 0.22887477\n",
      "Iteration 155, loss = 0.22823295\n",
      "Iteration 156, loss = 0.22764847\n",
      "Iteration 157, loss = 0.22706514\n",
      "Iteration 158, loss = 0.22651815\n",
      "Iteration 159, loss = 0.22595148\n",
      "Iteration 160, loss = 0.22536694\n",
      "Iteration 161, loss = 0.22477337\n",
      "Iteration 162, loss = 0.22421336\n",
      "Iteration 163, loss = 0.22362998\n",
      "Iteration 164, loss = 0.22310907\n",
      "Iteration 165, loss = 0.22252109\n",
      "Iteration 166, loss = 0.22197832\n",
      "Iteration 167, loss = 0.22144885\n",
      "Iteration 168, loss = 0.22089344\n",
      "Iteration 169, loss = 0.22035289\n",
      "Iteration 170, loss = 0.21976980\n",
      "Iteration 171, loss = 0.21927303\n",
      "Iteration 172, loss = 0.21869116\n",
      "Iteration 173, loss = 0.21819535\n",
      "Iteration 174, loss = 0.21765049\n",
      "Iteration 175, loss = 0.21710338\n",
      "Iteration 176, loss = 0.21659972\n",
      "Iteration 177, loss = 0.21604044\n",
      "Iteration 178, loss = 0.21551010\n",
      "Iteration 179, loss = 0.21502978\n",
      "Iteration 180, loss = 0.21448440\n",
      "Iteration 181, loss = 0.21396775\n",
      "Iteration 182, loss = 0.21346531\n",
      "Iteration 183, loss = 0.21300224\n",
      "Iteration 184, loss = 0.21246828\n",
      "Iteration 185, loss = 0.21194971\n",
      "Iteration 186, loss = 0.21144083\n",
      "Iteration 187, loss = 0.21095146\n",
      "Iteration 188, loss = 0.21039111\n",
      "Iteration 189, loss = 0.20988289\n",
      "Iteration 190, loss = 0.20942185\n",
      "Iteration 191, loss = 0.20896742\n",
      "Iteration 192, loss = 0.20845202\n",
      "Iteration 193, loss = 0.20795019\n",
      "Iteration 194, loss = 0.20747703\n",
      "Iteration 195, loss = 0.20696767\n",
      "Iteration 196, loss = 0.20646652\n",
      "Iteration 197, loss = 0.20600983\n",
      "Iteration 198, loss = 0.20558426\n",
      "Iteration 199, loss = 0.20503804\n",
      "Iteration 200, loss = 0.20458840\n",
      "[CV] END activation=logistic, alpha=0.0001, batch_size=100, hidden_layer_sizes=(100,), solver=sgd; total time= 1.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.24177557\n",
      "Iteration 2, loss = 2.05077606\n",
      "Iteration 3, loss = 1.83490068\n",
      "Iteration 4, loss = 1.59724425\n",
      "Iteration 5, loss = 1.37501161\n",
      "Iteration 6, loss = 1.19045110\n",
      "Iteration 7, loss = 1.04586882\n",
      "Iteration 8, loss = 0.93419300\n",
      "Iteration 9, loss = 0.84773027\n",
      "Iteration 10, loss = 0.77927195\n",
      "Iteration 11, loss = 0.72405015\n",
      "Iteration 12, loss = 0.67884783\n",
      "Iteration 13, loss = 0.64112091\n",
      "Iteration 14, loss = 0.60925487\n",
      "Iteration 15, loss = 0.58199768\n",
      "Iteration 16, loss = 0.55836458\n",
      "Iteration 17, loss = 0.53781652\n",
      "Iteration 18, loss = 0.51973458\n",
      "Iteration 19, loss = 0.50373133\n",
      "Iteration 20, loss = 0.48944380\n",
      "Iteration 21, loss = 0.47657190\n",
      "Iteration 22, loss = 0.46508090\n",
      "Iteration 23, loss = 0.45455929\n",
      "Iteration 24, loss = 0.44503157\n",
      "Iteration 25, loss = 0.43636071\n",
      "Iteration 26, loss = 0.42833509\n",
      "Iteration 27, loss = 0.42098176\n",
      "Iteration 28, loss = 0.41407387\n",
      "Iteration 29, loss = 0.40781819\n",
      "Iteration 30, loss = 0.40193261\n",
      "Iteration 31, loss = 0.39644310\n",
      "Iteration 32, loss = 0.39137216\n",
      "Iteration 33, loss = 0.38652571\n",
      "Iteration 34, loss = 0.38207865\n",
      "Iteration 35, loss = 0.37774143\n",
      "Iteration 36, loss = 0.37373231\n",
      "Iteration 37, loss = 0.36990678\n",
      "Iteration 38, loss = 0.36627092\n",
      "Iteration 39, loss = 0.36288821\n",
      "Iteration 40, loss = 0.35962007\n",
      "Iteration 41, loss = 0.35647458\n",
      "Iteration 42, loss = 0.35350580\n",
      "Iteration 43, loss = 0.35067706\n",
      "Iteration 44, loss = 0.34790804\n",
      "Iteration 45, loss = 0.34536236\n",
      "Iteration 46, loss = 0.34278276\n",
      "Iteration 47, loss = 0.34030406\n",
      "Iteration 48, loss = 0.33800525\n",
      "Iteration 49, loss = 0.33572639\n",
      "Iteration 50, loss = 0.33365646\n",
      "Iteration 51, loss = 0.33148433\n",
      "Iteration 52, loss = 0.32946952\n",
      "Iteration 53, loss = 0.32752228\n",
      "Iteration 54, loss = 0.32560369\n",
      "Iteration 55, loss = 0.32374865\n",
      "Iteration 56, loss = 0.32200467\n",
      "Iteration 57, loss = 0.32024204\n",
      "Iteration 58, loss = 0.31850536\n",
      "Iteration 59, loss = 0.31685318\n",
      "Iteration 60, loss = 0.31528171\n",
      "Iteration 61, loss = 0.31367920\n",
      "Iteration 62, loss = 0.31218753\n",
      "Iteration 63, loss = 0.31064408\n",
      "Iteration 64, loss = 0.30921908\n",
      "Iteration 65, loss = 0.30769157\n",
      "Iteration 66, loss = 0.30639068\n",
      "Iteration 67, loss = 0.30503099\n",
      "Iteration 68, loss = 0.30364356\n",
      "Iteration 69, loss = 0.30234646\n",
      "Iteration 70, loss = 0.30104905\n",
      "Iteration 71, loss = 0.29977074\n",
      "Iteration 72, loss = 0.29858960\n",
      "Iteration 73, loss = 0.29737600\n",
      "Iteration 74, loss = 0.29614254\n",
      "Iteration 75, loss = 0.29500853\n",
      "Iteration 76, loss = 0.29384079\n",
      "Iteration 77, loss = 0.29271204\n",
      "Iteration 78, loss = 0.29161025\n",
      "Iteration 79, loss = 0.29053053\n",
      "Iteration 80, loss = 0.28940127\n",
      "Iteration 81, loss = 0.28833706\n",
      "Iteration 82, loss = 0.28728290\n",
      "Iteration 83, loss = 0.28628778\n",
      "Iteration 84, loss = 0.28525160\n",
      "Iteration 85, loss = 0.28423481\n",
      "Iteration 86, loss = 0.28325774\n",
      "Iteration 87, loss = 0.28226660\n",
      "Iteration 88, loss = 0.28129054\n",
      "Iteration 89, loss = 0.28029868\n",
      "Iteration 90, loss = 0.27938996\n",
      "Iteration 91, loss = 0.27845680\n",
      "Iteration 92, loss = 0.27753379\n",
      "Iteration 93, loss = 0.27663470\n",
      "Iteration 94, loss = 0.27573363\n",
      "Iteration 95, loss = 0.27477891\n",
      "Iteration 96, loss = 0.27394348\n",
      "Iteration 97, loss = 0.27308097\n",
      "Iteration 98, loss = 0.27222047\n",
      "Iteration 99, loss = 0.27136523\n",
      "Iteration 100, loss = 0.27050046\n",
      "Iteration 101, loss = 0.26965552\n",
      "Iteration 102, loss = 0.26881670\n",
      "Iteration 103, loss = 0.26801094\n",
      "Iteration 104, loss = 0.26715725\n",
      "Iteration 105, loss = 0.26631421\n",
      "Iteration 106, loss = 0.26560000\n",
      "Iteration 107, loss = 0.26476911\n",
      "Iteration 108, loss = 0.26397790\n",
      "Iteration 109, loss = 0.26317531\n",
      "Iteration 110, loss = 0.26244151\n",
      "Iteration 111, loss = 0.26163358\n",
      "Iteration 112, loss = 0.26091379\n",
      "Iteration 113, loss = 0.26014936\n",
      "Iteration 114, loss = 0.25931843\n",
      "Iteration 115, loss = 0.25860416\n",
      "Iteration 116, loss = 0.25785452\n",
      "Iteration 117, loss = 0.25712374\n",
      "Iteration 118, loss = 0.25635160\n",
      "Iteration 119, loss = 0.25562067\n",
      "Iteration 120, loss = 0.25493053\n",
      "Iteration 121, loss = 0.25418865\n",
      "Iteration 122, loss = 0.25340682\n",
      "Iteration 123, loss = 0.25276469\n",
      "Iteration 124, loss = 0.25208989\n",
      "Iteration 125, loss = 0.25132848\n",
      "Iteration 126, loss = 0.25067870\n",
      "Iteration 127, loss = 0.24994919\n",
      "Iteration 128, loss = 0.24929605\n",
      "Iteration 129, loss = 0.24860293\n",
      "Iteration 130, loss = 0.24791819\n",
      "Iteration 131, loss = 0.24723423\n",
      "Iteration 132, loss = 0.24658388\n",
      "Iteration 133, loss = 0.24587593\n",
      "Iteration 134, loss = 0.24518597\n",
      "Iteration 135, loss = 0.24454930\n",
      "Iteration 136, loss = 0.24385314\n",
      "Iteration 137, loss = 0.24321949\n",
      "Iteration 138, loss = 0.24258940\n",
      "Iteration 139, loss = 0.24189683\n",
      "Iteration 140, loss = 0.24128821\n",
      "Iteration 141, loss = 0.24061951\n",
      "Iteration 142, loss = 0.24001310\n",
      "Iteration 143, loss = 0.23931523\n",
      "Iteration 144, loss = 0.23871064\n",
      "Iteration 145, loss = 0.23808008\n",
      "Iteration 146, loss = 0.23745406\n",
      "Iteration 147, loss = 0.23679183\n",
      "Iteration 148, loss = 0.23615621\n",
      "Iteration 149, loss = 0.23554181\n",
      "Iteration 150, loss = 0.23493659\n",
      "Iteration 151, loss = 0.23436999\n",
      "Iteration 152, loss = 0.23373588\n",
      "Iteration 153, loss = 0.23317031\n",
      "Iteration 154, loss = 0.23254490\n",
      "Iteration 155, loss = 0.23189732\n",
      "Iteration 156, loss = 0.23133747\n",
      "Iteration 157, loss = 0.23067727\n",
      "Iteration 158, loss = 0.23010366\n",
      "Iteration 159, loss = 0.22955954\n",
      "Iteration 160, loss = 0.22892959\n",
      "Iteration 161, loss = 0.22832735\n",
      "Iteration 162, loss = 0.22768528\n",
      "Iteration 163, loss = 0.22721383\n",
      "Iteration 164, loss = 0.22661531\n",
      "Iteration 165, loss = 0.22601910\n",
      "Iteration 166, loss = 0.22544376\n",
      "Iteration 167, loss = 0.22488618\n",
      "Iteration 168, loss = 0.22431141\n",
      "Iteration 169, loss = 0.22370720\n",
      "Iteration 170, loss = 0.22316412\n",
      "Iteration 171, loss = 0.22259303\n",
      "Iteration 172, loss = 0.22209745\n",
      "Iteration 173, loss = 0.22148570\n",
      "Iteration 174, loss = 0.22087985\n",
      "Iteration 175, loss = 0.22037114\n",
      "Iteration 176, loss = 0.21983884\n",
      "Iteration 177, loss = 0.21928707\n",
      "Iteration 178, loss = 0.21874681\n",
      "Iteration 179, loss = 0.21816764\n",
      "Iteration 180, loss = 0.21764614\n",
      "Iteration 181, loss = 0.21710156\n",
      "Iteration 182, loss = 0.21657599\n",
      "Iteration 183, loss = 0.21607441\n",
      "Iteration 184, loss = 0.21551449\n",
      "Iteration 185, loss = 0.21498422\n",
      "Iteration 186, loss = 0.21446599\n",
      "Iteration 187, loss = 0.21392886\n",
      "Iteration 188, loss = 0.21340396\n",
      "Iteration 189, loss = 0.21285844\n",
      "Iteration 190, loss = 0.21234934\n",
      "Iteration 191, loss = 0.21186279\n",
      "Iteration 192, loss = 0.21132321\n",
      "Iteration 193, loss = 0.21083692\n",
      "Iteration 194, loss = 0.21031356\n",
      "Iteration 195, loss = 0.20983128\n",
      "Iteration 196, loss = 0.20929716\n",
      "Iteration 197, loss = 0.20880597\n",
      "Iteration 198, loss = 0.20831295\n",
      "Iteration 199, loss = 0.20775753\n",
      "Iteration 200, loss = 0.20733199\n",
      "[CV] END activation=logistic, alpha=0.0001, batch_size=100, hidden_layer_sizes=(100,), solver=sgd; total time= 1.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.81705692\n",
      "Iteration 2, loss = 0.27222193\n",
      "Iteration 3, loss = 0.20183111\n",
      "Iteration 4, loss = 0.16044768\n",
      "Iteration 5, loss = 0.13020308\n",
      "Iteration 6, loss = 0.10791062\n",
      "Iteration 7, loss = 0.09028066\n",
      "Iteration 8, loss = 0.07559752\n",
      "Iteration 9, loss = 0.06412767\n",
      "Iteration 10, loss = 0.05281205\n",
      "Iteration 11, loss = 0.04522943\n",
      "Iteration 12, loss = 0.03764350\n",
      "Iteration 13, loss = 0.03226816\n",
      "Iteration 14, loss = 0.02602225\n",
      "Iteration 15, loss = 0.02181339\n",
      "Iteration 16, loss = 0.01870631\n",
      "Iteration 17, loss = 0.01590373\n",
      "Iteration 18, loss = 0.01322646\n",
      "Iteration 19, loss = 0.01136123\n",
      "Iteration 20, loss = 0.00955736\n",
      "Iteration 21, loss = 0.00802164\n",
      "Iteration 22, loss = 0.00722989\n",
      "Iteration 23, loss = 0.00695911\n",
      "Iteration 24, loss = 0.00644985\n",
      "Iteration 25, loss = 0.00592102\n",
      "Iteration 26, loss = 0.00594036\n",
      "Iteration 27, loss = 0.00632018\n",
      "Iteration 28, loss = 0.00954116\n",
      "Iteration 29, loss = 0.01344404\n",
      "Iteration 30, loss = 0.00500477\n",
      "Iteration 31, loss = 0.00435956\n",
      "Iteration 32, loss = 0.00417039\n",
      "Iteration 33, loss = 0.00406908\n",
      "Iteration 34, loss = 0.00396410\n",
      "Iteration 35, loss = 0.00386896\n",
      "Iteration 36, loss = 0.00376640\n",
      "Iteration 37, loss = 0.00367818\n",
      "Iteration 38, loss = 0.00364334\n",
      "Iteration 39, loss = 0.01767697\n",
      "Iteration 40, loss = 0.00982443\n",
      "Iteration 41, loss = 0.00434196\n",
      "Iteration 42, loss = 0.00401550\n",
      "Iteration 43, loss = 0.00384333\n",
      "Iteration 44, loss = 0.00377645\n",
      "Iteration 45, loss = 0.00370943\n",
      "Iteration 46, loss = 0.00364492\n",
      "Iteration 47, loss = 0.00357870\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.0001, batch_size=100, hidden_layer_sizes=(400, 100), solver=adam; total time= 2.4min\n",
      "Iteration 1, loss = 0.85197766\n",
      "Iteration 2, loss = 0.27513303\n",
      "Iteration 3, loss = 0.20306172\n",
      "Iteration 4, loss = 0.16228500\n",
      "Iteration 5, loss = 0.13243879\n",
      "Iteration 6, loss = 0.11052748\n",
      "Iteration 7, loss = 0.09254939\n",
      "Iteration 8, loss = 0.07780474\n",
      "Iteration 9, loss = 0.06714494\n",
      "Iteration 10, loss = 0.05448819\n",
      "Iteration 11, loss = 0.04654902\n",
      "Iteration 12, loss = 0.03867357\n",
      "Iteration 13, loss = 0.03212616\n",
      "Iteration 14, loss = 0.02689574\n",
      "Iteration 15, loss = 0.02233820\n",
      "Iteration 16, loss = 0.01848045\n",
      "Iteration 17, loss = 0.01520247\n",
      "Iteration 18, loss = 0.01376608\n",
      "Iteration 19, loss = 0.01283471\n",
      "Iteration 20, loss = 0.00964435\n",
      "Iteration 21, loss = 0.00846453\n",
      "Iteration 22, loss = 0.00799916\n",
      "Iteration 23, loss = 0.00746480\n",
      "Iteration 24, loss = 0.00596204\n",
      "Iteration 25, loss = 0.00525107\n",
      "Iteration 26, loss = 0.00500870\n",
      "Iteration 27, loss = 0.00494490\n",
      "Iteration 28, loss = 0.01627032\n",
      "Iteration 29, loss = 0.00610936\n",
      "Iteration 30, loss = 0.00458273\n",
      "Iteration 31, loss = 0.00431336\n",
      "Iteration 32, loss = 0.00413005\n",
      "Iteration 33, loss = 0.00403552\n",
      "Iteration 34, loss = 0.00392150\n",
      "Iteration 35, loss = 0.00381990\n",
      "Iteration 36, loss = 0.00373536\n",
      "Iteration 37, loss = 0.00709895\n",
      "Iteration 38, loss = 0.01842348\n",
      "Iteration 39, loss = 0.00633540\n",
      "Iteration 40, loss = 0.00426836\n",
      "Iteration 41, loss = 0.00397231\n",
      "Iteration 42, loss = 0.00388530\n",
      "Iteration 43, loss = 0.00381460\n",
      "Iteration 44, loss = 0.00374371\n",
      "Iteration 45, loss = 0.00367459\n",
      "Iteration 46, loss = 0.00360319\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.0001, batch_size=100, hidden_layer_sizes=(400, 100), solver=adam; total time= 2.2min\n",
      "Iteration 1, loss = 0.81296484\n",
      "Iteration 2, loss = 0.27309130\n",
      "Iteration 3, loss = 0.20350868\n",
      "Iteration 4, loss = 0.16214208\n",
      "Iteration 5, loss = 0.13263652\n",
      "Iteration 6, loss = 0.10970511\n",
      "Iteration 7, loss = 0.09111658\n",
      "Iteration 8, loss = 0.07676345\n",
      "Iteration 9, loss = 0.06416820\n",
      "Iteration 10, loss = 0.05437234\n",
      "Iteration 11, loss = 0.04528867\n",
      "Iteration 12, loss = 0.03795573\n",
      "Iteration 13, loss = 0.03177193\n",
      "Iteration 14, loss = 0.02605942\n",
      "Iteration 15, loss = 0.02221723\n",
      "Iteration 16, loss = 0.01825470\n",
      "Iteration 17, loss = 0.01576484\n",
      "Iteration 18, loss = 0.01316582\n",
      "Iteration 19, loss = 0.01134090\n",
      "Iteration 20, loss = 0.00967799\n",
      "Iteration 21, loss = 0.00801160\n",
      "Iteration 22, loss = 0.00804708\n",
      "Iteration 23, loss = 0.00844170\n",
      "Iteration 24, loss = 0.00642851\n",
      "Iteration 25, loss = 0.00612745\n",
      "Iteration 26, loss = 0.00485362\n",
      "Iteration 27, loss = 0.00454551\n",
      "Iteration 28, loss = 0.00440986\n",
      "Iteration 29, loss = 0.00421683\n",
      "Iteration 30, loss = 0.00401473\n",
      "Iteration 31, loss = 0.01867148\n",
      "Iteration 32, loss = 0.00687286\n",
      "Iteration 33, loss = 0.00418493\n",
      "Iteration 34, loss = 0.00398383\n",
      "Iteration 35, loss = 0.00388224\n",
      "Iteration 36, loss = 0.00379884\n",
      "Iteration 37, loss = 0.00371332\n",
      "Iteration 38, loss = 0.00363599\n",
      "Iteration 39, loss = 0.00355227\n",
      "Iteration 40, loss = 0.00347001\n",
      "Iteration 41, loss = 0.00339106\n",
      "Iteration 42, loss = 0.00404992\n",
      "Iteration 43, loss = 0.02527604\n",
      "Iteration 44, loss = 0.00481918\n",
      "Iteration 45, loss = 0.00379588\n",
      "Iteration 46, loss = 0.00368400\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.0001, batch_size=100, hidden_layer_sizes=(400, 100), solver=adam; total time= 2.3min\n",
      "Iteration 1, loss = 0.81088134\n",
      "Iteration 2, loss = 0.27215993\n",
      "Iteration 3, loss = 0.20416396\n",
      "Iteration 4, loss = 0.16331071\n",
      "Iteration 5, loss = 0.13701582\n",
      "Iteration 6, loss = 0.11471018\n",
      "Iteration 7, loss = 0.09531610\n",
      "Iteration 8, loss = 0.07932549\n",
      "Iteration 9, loss = 0.06793229\n",
      "Iteration 10, loss = 0.05674034\n",
      "Iteration 11, loss = 0.04843190\n",
      "Iteration 12, loss = 0.04055566\n",
      "Iteration 13, loss = 0.03470007\n",
      "Iteration 14, loss = 0.02921988\n",
      "Iteration 15, loss = 0.02476878\n",
      "Iteration 16, loss = 0.01982460\n",
      "Iteration 17, loss = 0.01729873\n",
      "Iteration 18, loss = 0.01486245\n",
      "Iteration 19, loss = 0.01220880\n",
      "Iteration 20, loss = 0.01039514\n",
      "Iteration 21, loss = 0.00921188\n",
      "Iteration 22, loss = 0.00759775\n",
      "Iteration 23, loss = 0.00719785\n",
      "Iteration 24, loss = 0.00742373\n",
      "Iteration 25, loss = 0.00694158\n",
      "Iteration 26, loss = 0.00589919\n",
      "Iteration 27, loss = 0.00660670\n",
      "Iteration 28, loss = 0.00516580\n",
      "Iteration 29, loss = 0.00448547\n",
      "Iteration 30, loss = 0.00416225\n",
      "Iteration 31, loss = 0.00397370\n",
      "Iteration 32, loss = 0.00382572\n",
      "Iteration 33, loss = 0.01419203\n",
      "Iteration 34, loss = 0.01000653\n",
      "Iteration 35, loss = 0.00420715\n",
      "Iteration 36, loss = 0.00394012\n",
      "Iteration 37, loss = 0.00384107\n",
      "Iteration 38, loss = 0.00376101\n",
      "Iteration 39, loss = 0.00368650\n",
      "Iteration 40, loss = 0.00360934\n",
      "Iteration 41, loss = 0.00354333\n",
      "Iteration 42, loss = 0.00346307\n",
      "Iteration 43, loss = 0.00526405\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.0001, batch_size=100, hidden_layer_sizes=(400, 100), solver=adam; total time= 2.1min\n",
      "Iteration 1, loss = 0.82176644\n",
      "Iteration 2, loss = 0.27768597\n",
      "Iteration 3, loss = 0.20763566\n",
      "Iteration 4, loss = 0.16675676\n",
      "Iteration 5, loss = 0.13864819\n",
      "Iteration 6, loss = 0.11521614\n",
      "Iteration 7, loss = 0.09531675\n",
      "Iteration 8, loss = 0.08015891\n",
      "Iteration 9, loss = 0.06872452\n",
      "Iteration 10, loss = 0.05748053\n",
      "Iteration 11, loss = 0.04828971\n",
      "Iteration 12, loss = 0.03976144\n",
      "Iteration 13, loss = 0.03367082\n",
      "Iteration 14, loss = 0.02913369\n",
      "Iteration 15, loss = 0.02306827\n",
      "Iteration 16, loss = 0.02003019\n",
      "Iteration 17, loss = 0.01647132\n",
      "Iteration 18, loss = 0.01501513\n",
      "Iteration 19, loss = 0.01147963\n",
      "Iteration 20, loss = 0.01093982\n",
      "Iteration 21, loss = 0.00861359\n",
      "Iteration 22, loss = 0.00735215\n",
      "Iteration 23, loss = 0.00667007\n",
      "Iteration 24, loss = 0.00609231\n",
      "Iteration 25, loss = 0.00547652\n",
      "Iteration 26, loss = 0.00619491\n",
      "Iteration 27, loss = 0.00708166\n",
      "Iteration 28, loss = 0.01334828\n",
      "Iteration 29, loss = 0.00522352\n",
      "Iteration 30, loss = 0.00438713\n",
      "Iteration 31, loss = 0.00420252\n",
      "Iteration 32, loss = 0.00410880\n",
      "Iteration 33, loss = 0.00397527\n",
      "Iteration 34, loss = 0.00385087\n",
      "Iteration 35, loss = 0.00375871\n",
      "Iteration 36, loss = 0.00365305\n",
      "Iteration 37, loss = 0.00403908\n",
      "Iteration 38, loss = 0.02231659\n",
      "Iteration 39, loss = 0.00503176\n",
      "Iteration 40, loss = 0.00406938\n",
      "Iteration 41, loss = 0.00384548\n",
      "Iteration 42, loss = 0.00376683\n",
      "Iteration 43, loss = 0.00369574\n",
      "Iteration 44, loss = 0.00363134\n",
      "Iteration 45, loss = 0.00356442\n",
      "Iteration 46, loss = 0.00349755\n",
      "Iteration 47, loss = 0.00342036\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.0001, batch_size=100, hidden_layer_sizes=(400, 100), solver=adam; total time= 2.3min\n",
      "Iteration 1, loss = 2.30289626\n",
      "Iteration 2, loss = 2.29316009\n",
      "Iteration 3, loss = 2.28638171\n",
      "Iteration 4, loss = 2.27901564\n",
      "Iteration 5, loss = 2.27041128\n",
      "Iteration 6, loss = 2.25981019\n",
      "Iteration 7, loss = 2.24696171\n",
      "Iteration 8, loss = 2.22980499\n",
      "Iteration 9, loss = 2.20724827\n",
      "Iteration 10, loss = 2.17635973\n",
      "Iteration 11, loss = 2.13360234\n",
      "Iteration 12, loss = 2.07430768\n",
      "Iteration 13, loss = 1.99496160\n",
      "Iteration 14, loss = 1.89481884\n",
      "Iteration 15, loss = 1.77814492\n",
      "Iteration 16, loss = 1.65149393\n",
      "Iteration 17, loss = 1.52279805\n",
      "Iteration 18, loss = 1.39975045\n",
      "Iteration 19, loss = 1.28845255\n",
      "Iteration 20, loss = 1.19156752\n",
      "Iteration 21, loss = 1.10937269\n",
      "Iteration 22, loss = 1.03959604\n",
      "Iteration 23, loss = 0.98005309\n",
      "Iteration 24, loss = 0.92816305\n",
      "Iteration 25, loss = 0.88238337\n",
      "Iteration 26, loss = 0.84159508\n",
      "Iteration 27, loss = 0.80515799\n",
      "Iteration 28, loss = 0.77234252\n",
      "Iteration 29, loss = 0.74270487\n",
      "Iteration 30, loss = 0.71609659\n",
      "Iteration 31, loss = 0.69200081\n",
      "Iteration 32, loss = 0.66996667\n",
      "Iteration 33, loss = 0.65012851\n",
      "Iteration 34, loss = 0.63197597\n",
      "Iteration 35, loss = 0.61529598\n",
      "Iteration 36, loss = 0.59993558\n",
      "Iteration 37, loss = 0.58576119\n",
      "Iteration 38, loss = 0.57249248\n",
      "Iteration 39, loss = 0.56015759\n",
      "Iteration 40, loss = 0.54877008\n",
      "Iteration 41, loss = 0.53777891\n",
      "Iteration 42, loss = 0.52747525\n",
      "Iteration 43, loss = 0.51760123\n",
      "Iteration 44, loss = 0.50834153\n",
      "Iteration 45, loss = 0.49959968\n",
      "Iteration 46, loss = 0.49109942\n",
      "Iteration 47, loss = 0.48320991\n",
      "Iteration 48, loss = 0.47547794\n",
      "Iteration 49, loss = 0.46821925\n",
      "Iteration 50, loss = 0.46123397\n",
      "Iteration 51, loss = 0.45467712\n",
      "Iteration 52, loss = 0.44838262\n",
      "Iteration 53, loss = 0.44237572\n",
      "Iteration 54, loss = 0.43674690\n",
      "Iteration 55, loss = 0.43129549\n",
      "Iteration 56, loss = 0.42617430\n",
      "Iteration 57, loss = 0.42118020\n",
      "Iteration 58, loss = 0.41645453\n",
      "Iteration 59, loss = 0.41210067\n",
      "Iteration 60, loss = 0.40786197\n",
      "Iteration 61, loss = 0.40389838\n",
      "Iteration 62, loss = 0.39993275\n",
      "Iteration 63, loss = 0.39627995\n",
      "Iteration 64, loss = 0.39283125\n",
      "Iteration 65, loss = 0.38953666\n",
      "Iteration 66, loss = 0.38638023\n",
      "Iteration 67, loss = 0.38330202\n",
      "Iteration 68, loss = 0.38047848\n",
      "Iteration 69, loss = 0.37755119\n",
      "Iteration 70, loss = 0.37498463\n",
      "Iteration 71, loss = 0.37237838\n",
      "Iteration 72, loss = 0.37000007\n",
      "Iteration 73, loss = 0.36769987\n",
      "Iteration 74, loss = 0.36527645\n",
      "Iteration 75, loss = 0.36290391\n",
      "Iteration 76, loss = 0.36094060\n",
      "Iteration 77, loss = 0.35885696\n",
      "Iteration 78, loss = 0.35678749\n",
      "Iteration 79, loss = 0.35484805\n",
      "Iteration 80, loss = 0.35302162\n",
      "Iteration 81, loss = 0.35124623\n",
      "Iteration 82, loss = 0.34919125\n",
      "Iteration 83, loss = 0.34766149\n",
      "Iteration 84, loss = 0.34588157\n",
      "Iteration 85, loss = 0.34434424\n",
      "Iteration 86, loss = 0.34268683\n",
      "Iteration 87, loss = 0.34104925\n",
      "Iteration 88, loss = 0.33953478\n",
      "Iteration 89, loss = 0.33812078\n",
      "Iteration 90, loss = 0.33658385\n",
      "Iteration 91, loss = 0.33524046\n",
      "Iteration 92, loss = 0.33378194\n",
      "Iteration 93, loss = 0.33246648\n",
      "Iteration 94, loss = 0.33105918\n",
      "Iteration 95, loss = 0.32979694\n",
      "Iteration 96, loss = 0.32847047\n",
      "Iteration 97, loss = 0.32708922\n",
      "Iteration 98, loss = 0.32594625\n",
      "Iteration 99, loss = 0.32473957\n",
      "Iteration 100, loss = 0.32357301\n",
      "Iteration 101, loss = 0.32238809\n",
      "Iteration 102, loss = 0.32119922\n",
      "Iteration 103, loss = 0.32023474\n",
      "Iteration 104, loss = 0.31895714\n",
      "Iteration 105, loss = 0.31791790\n",
      "Iteration 106, loss = 0.31678635\n",
      "Iteration 107, loss = 0.31570243\n",
      "Iteration 108, loss = 0.31470677\n",
      "Iteration 109, loss = 0.31370304\n",
      "Iteration 110, loss = 0.31251492\n",
      "Iteration 111, loss = 0.31161071\n",
      "Iteration 112, loss = 0.31071658\n",
      "Iteration 113, loss = 0.30955314\n",
      "Iteration 114, loss = 0.30862905\n",
      "Iteration 115, loss = 0.30769026\n",
      "Iteration 116, loss = 0.30671087\n",
      "Iteration 117, loss = 0.30581919\n",
      "Iteration 118, loss = 0.30499759\n",
      "Iteration 119, loss = 0.30406003\n",
      "Iteration 120, loss = 0.30300392\n",
      "Iteration 121, loss = 0.30231987\n",
      "Iteration 122, loss = 0.30153060\n",
      "Iteration 123, loss = 0.30047705\n",
      "Iteration 124, loss = 0.29971154\n",
      "Iteration 125, loss = 0.29884204\n",
      "Iteration 126, loss = 0.29806916\n",
      "Iteration 127, loss = 0.29725245\n",
      "Iteration 128, loss = 0.29632615\n",
      "Iteration 129, loss = 0.29551526\n",
      "Iteration 130, loss = 0.29484828\n",
      "Iteration 131, loss = 0.29405081\n",
      "Iteration 132, loss = 0.29320091\n",
      "Iteration 133, loss = 0.29234647\n",
      "Iteration 134, loss = 0.29169459\n",
      "Iteration 135, loss = 0.29087259\n",
      "Iteration 136, loss = 0.29021978\n",
      "Iteration 137, loss = 0.28944957\n",
      "Iteration 138, loss = 0.28866647\n",
      "Iteration 139, loss = 0.28790673\n",
      "Iteration 140, loss = 0.28720308\n",
      "Iteration 141, loss = 0.28657463\n",
      "Iteration 142, loss = 0.28579635\n",
      "Iteration 143, loss = 0.28512213\n",
      "Iteration 144, loss = 0.28443756\n",
      "Iteration 145, loss = 0.28370971\n",
      "Iteration 146, loss = 0.28303612\n",
      "Iteration 147, loss = 0.28240965\n",
      "Iteration 148, loss = 0.28178157\n",
      "Iteration 149, loss = 0.28098932\n",
      "Iteration 150, loss = 0.28034401\n",
      "Iteration 151, loss = 0.27975842\n",
      "Iteration 152, loss = 0.27911478\n",
      "Iteration 153, loss = 0.27848285\n",
      "Iteration 154, loss = 0.27778770\n",
      "Iteration 155, loss = 0.27706777\n",
      "Iteration 156, loss = 0.27647364\n",
      "Iteration 157, loss = 0.27584757\n",
      "Iteration 158, loss = 0.27525510\n",
      "Iteration 159, loss = 0.27453487\n",
      "Iteration 160, loss = 0.27396028\n",
      "Iteration 161, loss = 0.27332512\n",
      "Iteration 162, loss = 0.27264851\n",
      "Iteration 163, loss = 0.27210549\n",
      "Iteration 164, loss = 0.27154127\n",
      "Iteration 165, loss = 0.27094244\n",
      "Iteration 166, loss = 0.27037634\n",
      "Iteration 167, loss = 0.26967363\n",
      "Iteration 168, loss = 0.26908550\n",
      "Iteration 169, loss = 0.26861054\n",
      "Iteration 170, loss = 0.26795948\n",
      "Iteration 171, loss = 0.26736414\n",
      "Iteration 172, loss = 0.26681022\n",
      "Iteration 173, loss = 0.26624717\n",
      "Iteration 174, loss = 0.26558811\n",
      "Iteration 175, loss = 0.26504471\n",
      "Iteration 176, loss = 0.26454774\n",
      "Iteration 177, loss = 0.26385992\n",
      "Iteration 178, loss = 0.26331078\n",
      "Iteration 179, loss = 0.26280867\n",
      "Iteration 180, loss = 0.26228663\n",
      "Iteration 181, loss = 0.26166853\n",
      "Iteration 182, loss = 0.26118130\n",
      "Iteration 183, loss = 0.26062839\n",
      "Iteration 184, loss = 0.25998270\n",
      "Iteration 185, loss = 0.25949041\n",
      "Iteration 186, loss = 0.25898813\n",
      "Iteration 187, loss = 0.25839715\n",
      "Iteration 188, loss = 0.25794516\n",
      "Iteration 189, loss = 0.25733523\n",
      "Iteration 190, loss = 0.25674933\n",
      "Iteration 191, loss = 0.25624615\n",
      "Iteration 192, loss = 0.25573983\n",
      "Iteration 193, loss = 0.25525817\n",
      "Iteration 194, loss = 0.25467754\n",
      "Iteration 195, loss = 0.25407803\n",
      "Iteration 196, loss = 0.25359175\n",
      "Iteration 197, loss = 0.25303985\n",
      "Iteration 198, loss = 0.25258571\n",
      "Iteration 199, loss = 0.25208383\n",
      "Iteration 200, loss = 0.25153927\n",
      "[CV] END activation=logistic, alpha=0.0001, batch_size=100, hidden_layer_sizes=(400, 100), solver=sgd; total time= 6.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.30205284\n",
      "Iteration 2, loss = 2.29093955\n",
      "Iteration 3, loss = 2.28325904\n",
      "Iteration 4, loss = 2.27458992\n",
      "Iteration 5, loss = 2.26480318\n",
      "Iteration 6, loss = 2.25185020\n",
      "Iteration 7, loss = 2.23581025\n",
      "Iteration 8, loss = 2.21462721\n",
      "Iteration 9, loss = 2.18562997\n",
      "Iteration 10, loss = 2.14561714\n",
      "Iteration 11, loss = 2.08975349\n",
      "Iteration 12, loss = 2.01347600\n",
      "Iteration 13, loss = 1.91504449\n",
      "Iteration 14, loss = 1.79769624\n",
      "Iteration 15, loss = 1.67086221\n",
      "Iteration 16, loss = 1.54550679\n",
      "Iteration 17, loss = 1.42856857\n",
      "Iteration 18, loss = 1.32292562\n",
      "Iteration 19, loss = 1.22818413\n",
      "Iteration 20, loss = 1.14355319\n",
      "Iteration 21, loss = 1.06844773\n",
      "Iteration 22, loss = 1.00257179\n",
      "Iteration 23, loss = 0.94482973\n",
      "Iteration 24, loss = 0.89456081\n",
      "Iteration 25, loss = 0.85032373\n",
      "Iteration 26, loss = 0.81113394\n",
      "Iteration 27, loss = 0.77614994\n",
      "Iteration 28, loss = 0.74489709\n",
      "Iteration 29, loss = 0.71682911\n",
      "Iteration 30, loss = 0.69155365\n",
      "Iteration 31, loss = 0.66863482\n",
      "Iteration 32, loss = 0.64776687\n",
      "Iteration 33, loss = 0.62893188\n",
      "Iteration 34, loss = 0.61170928\n",
      "Iteration 35, loss = 0.59580823\n",
      "Iteration 36, loss = 0.58133672\n",
      "Iteration 37, loss = 0.56796461\n",
      "Iteration 38, loss = 0.55552622\n",
      "Iteration 39, loss = 0.54388637\n",
      "Iteration 40, loss = 0.53323207\n",
      "Iteration 41, loss = 0.52317355\n",
      "Iteration 42, loss = 0.51378116\n",
      "Iteration 43, loss = 0.50490859\n",
      "Iteration 44, loss = 0.49657621\n",
      "Iteration 45, loss = 0.48867769\n",
      "Iteration 46, loss = 0.48137278\n",
      "Iteration 47, loss = 0.47434285\n",
      "Iteration 48, loss = 0.46755969\n",
      "Iteration 49, loss = 0.46117863\n",
      "Iteration 50, loss = 0.45517131\n",
      "Iteration 51, loss = 0.44927135\n",
      "Iteration 52, loss = 0.44390049\n",
      "Iteration 53, loss = 0.43855897\n",
      "Iteration 54, loss = 0.43352009\n",
      "Iteration 55, loss = 0.42878630\n",
      "Iteration 56, loss = 0.42428237\n",
      "Iteration 57, loss = 0.42001006\n",
      "Iteration 58, loss = 0.41572228\n",
      "Iteration 59, loss = 0.41169948\n",
      "Iteration 60, loss = 0.40789802\n",
      "Iteration 61, loss = 0.40409224\n",
      "Iteration 62, loss = 0.40068887\n",
      "Iteration 63, loss = 0.39735622\n",
      "Iteration 64, loss = 0.39399755\n",
      "Iteration 65, loss = 0.39106764\n",
      "Iteration 66, loss = 0.38800613\n",
      "Iteration 67, loss = 0.38510654\n",
      "Iteration 68, loss = 0.38233777\n",
      "Iteration 69, loss = 0.37970186\n",
      "Iteration 70, loss = 0.37713257\n",
      "Iteration 71, loss = 0.37465203\n",
      "Iteration 72, loss = 0.37228439\n",
      "Iteration 73, loss = 0.36997803\n",
      "Iteration 74, loss = 0.36780090\n",
      "Iteration 75, loss = 0.36551864\n",
      "Iteration 76, loss = 0.36337179\n",
      "Iteration 77, loss = 0.36139080\n",
      "Iteration 78, loss = 0.35945280\n",
      "Iteration 79, loss = 0.35751408\n",
      "Iteration 80, loss = 0.35558506\n",
      "Iteration 81, loss = 0.35384562\n",
      "Iteration 82, loss = 0.35212134\n",
      "Iteration 83, loss = 0.35041639\n",
      "Iteration 84, loss = 0.34869928\n",
      "Iteration 85, loss = 0.34714176\n",
      "Iteration 86, loss = 0.34554424\n",
      "Iteration 87, loss = 0.34407421\n",
      "Iteration 88, loss = 0.34240775\n",
      "Iteration 89, loss = 0.34099124\n",
      "Iteration 90, loss = 0.33958647\n",
      "Iteration 91, loss = 0.33814021\n",
      "Iteration 92, loss = 0.33675573\n",
      "Iteration 93, loss = 0.33547873\n",
      "Iteration 94, loss = 0.33410281\n",
      "Iteration 95, loss = 0.33272752\n",
      "Iteration 96, loss = 0.33142413\n",
      "Iteration 97, loss = 0.33012468\n",
      "Iteration 98, loss = 0.32902944\n",
      "Iteration 99, loss = 0.32763496\n",
      "Iteration 100, loss = 0.32660166\n",
      "Iteration 101, loss = 0.32547099\n",
      "Iteration 102, loss = 0.32434214\n",
      "Iteration 103, loss = 0.32325016\n",
      "Iteration 104, loss = 0.32197761\n",
      "Iteration 105, loss = 0.32098429\n",
      "Iteration 106, loss = 0.31991356\n",
      "Iteration 107, loss = 0.31890966\n",
      "Iteration 108, loss = 0.31786325\n",
      "Iteration 109, loss = 0.31675784\n",
      "Iteration 110, loss = 0.31580967\n",
      "Iteration 111, loss = 0.31465247\n",
      "Iteration 112, loss = 0.31376433\n",
      "Iteration 113, loss = 0.31282841\n",
      "Iteration 114, loss = 0.31181642\n",
      "Iteration 115, loss = 0.31094144\n",
      "Iteration 116, loss = 0.30983309\n",
      "Iteration 117, loss = 0.30917425\n",
      "Iteration 118, loss = 0.30806319\n",
      "Iteration 119, loss = 0.30725586\n",
      "Iteration 120, loss = 0.30641653\n",
      "Iteration 121, loss = 0.30550148\n",
      "Iteration 122, loss = 0.30459742\n",
      "Iteration 123, loss = 0.30367048\n",
      "Iteration 124, loss = 0.30278654\n",
      "Iteration 125, loss = 0.30205429\n",
      "Iteration 126, loss = 0.30122572\n",
      "Iteration 127, loss = 0.30034679\n",
      "Iteration 128, loss = 0.29954642\n",
      "Iteration 129, loss = 0.29874054\n",
      "Iteration 130, loss = 0.29800221\n",
      "Iteration 131, loss = 0.29719147\n",
      "Iteration 132, loss = 0.29640634\n",
      "Iteration 133, loss = 0.29563750\n",
      "Iteration 134, loss = 0.29487429\n",
      "Iteration 135, loss = 0.29419355\n",
      "Iteration 136, loss = 0.29333503\n",
      "Iteration 137, loss = 0.29248501\n",
      "Iteration 138, loss = 0.29190142\n",
      "Iteration 139, loss = 0.29104409\n",
      "Iteration 140, loss = 0.29032882\n",
      "Iteration 141, loss = 0.28969820\n",
      "Iteration 142, loss = 0.28893135\n",
      "Iteration 143, loss = 0.28820824\n",
      "Iteration 144, loss = 0.28740155\n",
      "Iteration 145, loss = 0.28679079\n",
      "Iteration 146, loss = 0.28609196\n",
      "Iteration 147, loss = 0.28553224\n",
      "Iteration 148, loss = 0.28476609\n",
      "Iteration 149, loss = 0.28405785\n",
      "Iteration 150, loss = 0.28336068\n",
      "Iteration 151, loss = 0.28273911\n",
      "Iteration 152, loss = 0.28205817\n",
      "Iteration 153, loss = 0.28138993\n",
      "Iteration 154, loss = 0.28072213\n",
      "Iteration 155, loss = 0.28009565\n",
      "Iteration 156, loss = 0.27941199\n",
      "Iteration 157, loss = 0.27882095\n",
      "Iteration 158, loss = 0.27821998\n",
      "Iteration 159, loss = 0.27754036\n",
      "Iteration 160, loss = 0.27691054\n",
      "Iteration 161, loss = 0.27616671\n",
      "Iteration 162, loss = 0.27569291\n",
      "Iteration 163, loss = 0.27518997\n",
      "Iteration 164, loss = 0.27447562\n",
      "Iteration 165, loss = 0.27383067\n",
      "Iteration 166, loss = 0.27319643\n",
      "Iteration 167, loss = 0.27266199\n",
      "Iteration 168, loss = 0.27204396\n",
      "Iteration 169, loss = 0.27139783\n",
      "Iteration 170, loss = 0.27087356\n",
      "Iteration 171, loss = 0.27020333\n",
      "Iteration 172, loss = 0.26966599\n",
      "Iteration 173, loss = 0.26898519\n",
      "Iteration 174, loss = 0.26857161\n",
      "Iteration 175, loss = 0.26782950\n",
      "Iteration 176, loss = 0.26732082\n",
      "Iteration 177, loss = 0.26678237\n",
      "Iteration 178, loss = 0.26615621\n",
      "Iteration 179, loss = 0.26564736\n",
      "Iteration 180, loss = 0.26504619\n",
      "Iteration 181, loss = 0.26439860\n",
      "Iteration 182, loss = 0.26396372\n",
      "Iteration 183, loss = 0.26332892\n",
      "Iteration 184, loss = 0.26284641\n",
      "Iteration 185, loss = 0.26211037\n",
      "Iteration 186, loss = 0.26171917\n",
      "Iteration 187, loss = 0.26109629\n",
      "Iteration 188, loss = 0.26053232\n",
      "Iteration 189, loss = 0.25998404\n",
      "Iteration 190, loss = 0.25950626\n",
      "Iteration 191, loss = 0.25881717\n",
      "Iteration 192, loss = 0.25842546\n",
      "Iteration 193, loss = 0.25788936\n",
      "Iteration 194, loss = 0.25725887\n",
      "Iteration 195, loss = 0.25679497\n",
      "Iteration 196, loss = 0.25624616\n",
      "Iteration 197, loss = 0.25563419\n",
      "Iteration 198, loss = 0.25512383\n",
      "Iteration 199, loss = 0.25473460\n",
      "Iteration 200, loss = 0.25412605\n",
      "[CV] END activation=logistic, alpha=0.0001, batch_size=100, hidden_layer_sizes=(400, 100), solver=sgd; total time= 6.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.30561652\n",
      "Iteration 2, loss = 2.29327890\n",
      "Iteration 3, loss = 2.28616201\n",
      "Iteration 4, loss = 2.27798302\n",
      "Iteration 5, loss = 2.26826985\n",
      "Iteration 6, loss = 2.25613811\n",
      "Iteration 7, loss = 2.24067701\n",
      "Iteration 8, loss = 2.21976650\n",
      "Iteration 9, loss = 2.19133961\n",
      "Iteration 10, loss = 2.15143114\n",
      "Iteration 11, loss = 2.09568690\n",
      "Iteration 12, loss = 2.01968535\n",
      "Iteration 13, loss = 1.92231041\n",
      "Iteration 14, loss = 1.80795937\n",
      "Iteration 15, loss = 1.68661463\n",
      "Iteration 16, loss = 1.56910066\n",
      "Iteration 17, loss = 1.46203920\n",
      "Iteration 18, loss = 1.36673380\n",
      "Iteration 19, loss = 1.28063327\n",
      "Iteration 20, loss = 1.20154020\n",
      "Iteration 21, loss = 1.12833231\n",
      "Iteration 22, loss = 1.06207757\n",
      "Iteration 23, loss = 1.00240608\n",
      "Iteration 24, loss = 0.94958906\n",
      "Iteration 25, loss = 0.90279940\n",
      "Iteration 26, loss = 0.86113507\n",
      "Iteration 27, loss = 0.82418785\n",
      "Iteration 28, loss = 0.79091560\n",
      "Iteration 29, loss = 0.76051834\n",
      "Iteration 30, loss = 0.73301218\n",
      "Iteration 31, loss = 0.70763751\n",
      "Iteration 32, loss = 0.68403962\n",
      "Iteration 33, loss = 0.66257822\n",
      "Iteration 34, loss = 0.64267213\n",
      "Iteration 35, loss = 0.62447295\n",
      "Iteration 36, loss = 0.60743545\n",
      "Iteration 37, loss = 0.59178340\n",
      "Iteration 38, loss = 0.57725732\n",
      "Iteration 39, loss = 0.56392338\n",
      "Iteration 40, loss = 0.55146133\n",
      "Iteration 41, loss = 0.53977558\n",
      "Iteration 42, loss = 0.52904951\n",
      "Iteration 43, loss = 0.51903600\n",
      "Iteration 44, loss = 0.50961520\n",
      "Iteration 45, loss = 0.50070389\n",
      "Iteration 46, loss = 0.49238250\n",
      "Iteration 47, loss = 0.48464126\n",
      "Iteration 48, loss = 0.47725874\n",
      "Iteration 49, loss = 0.47044442\n",
      "Iteration 50, loss = 0.46382962\n",
      "Iteration 51, loss = 0.45763476\n",
      "Iteration 52, loss = 0.45179248\n",
      "Iteration 53, loss = 0.44639266\n",
      "Iteration 54, loss = 0.44108061\n",
      "Iteration 55, loss = 0.43596794\n",
      "Iteration 56, loss = 0.43153586\n",
      "Iteration 57, loss = 0.42691720\n",
      "Iteration 58, loss = 0.42252892\n",
      "Iteration 59, loss = 0.41847021\n",
      "Iteration 60, loss = 0.41462334\n",
      "Iteration 61, loss = 0.41089317\n",
      "Iteration 62, loss = 0.40717905\n",
      "Iteration 63, loss = 0.40390555\n",
      "Iteration 64, loss = 0.40044766\n",
      "Iteration 65, loss = 0.39728180\n",
      "Iteration 66, loss = 0.39407474\n",
      "Iteration 67, loss = 0.39120434\n",
      "Iteration 68, loss = 0.38827814\n",
      "Iteration 69, loss = 0.38560143\n",
      "Iteration 70, loss = 0.38293242\n",
      "Iteration 71, loss = 0.38019607\n",
      "Iteration 72, loss = 0.37769813\n",
      "Iteration 73, loss = 0.37541285\n",
      "Iteration 74, loss = 0.37300311\n",
      "Iteration 75, loss = 0.37075795\n",
      "Iteration 76, loss = 0.36831254\n",
      "Iteration 77, loss = 0.36642591\n",
      "Iteration 78, loss = 0.36433422\n",
      "Iteration 79, loss = 0.36228411\n",
      "Iteration 80, loss = 0.36015909\n",
      "Iteration 81, loss = 0.35829676\n",
      "Iteration 82, loss = 0.35645760\n",
      "Iteration 83, loss = 0.35462511\n",
      "Iteration 84, loss = 0.35283948\n",
      "Iteration 85, loss = 0.35102955\n",
      "Iteration 86, loss = 0.34933992\n",
      "Iteration 87, loss = 0.34767800\n",
      "Iteration 88, loss = 0.34612339\n",
      "Iteration 89, loss = 0.34445588\n",
      "Iteration 90, loss = 0.34288834\n",
      "Iteration 91, loss = 0.34128963\n",
      "Iteration 92, loss = 0.33986532\n",
      "Iteration 93, loss = 0.33837012\n",
      "Iteration 94, loss = 0.33688297\n",
      "Iteration 95, loss = 0.33549477\n",
      "Iteration 96, loss = 0.33414365\n",
      "Iteration 97, loss = 0.33274283\n",
      "Iteration 98, loss = 0.33139995\n",
      "Iteration 99, loss = 0.33009719\n",
      "Iteration 100, loss = 0.32875676\n",
      "Iteration 101, loss = 0.32758546\n",
      "Iteration 102, loss = 0.32618023\n",
      "Iteration 103, loss = 0.32492197\n",
      "Iteration 104, loss = 0.32370965\n",
      "Iteration 105, loss = 0.32247946\n",
      "Iteration 106, loss = 0.32134600\n",
      "Iteration 107, loss = 0.32019678\n",
      "Iteration 108, loss = 0.31905208\n",
      "Iteration 109, loss = 0.31789051\n",
      "Iteration 110, loss = 0.31678373\n",
      "Iteration 111, loss = 0.31563104\n",
      "Iteration 112, loss = 0.31455916\n",
      "Iteration 113, loss = 0.31346799\n",
      "Iteration 114, loss = 0.31237487\n",
      "Iteration 115, loss = 0.31130653\n",
      "Iteration 116, loss = 0.31023056\n",
      "Iteration 117, loss = 0.30910310\n",
      "Iteration 118, loss = 0.30832394\n",
      "Iteration 119, loss = 0.30718705\n",
      "Iteration 120, loss = 0.30622986\n",
      "Iteration 121, loss = 0.30517798\n",
      "Iteration 122, loss = 0.30422285\n",
      "Iteration 123, loss = 0.30323691\n",
      "Iteration 124, loss = 0.30229997\n",
      "Iteration 125, loss = 0.30133713\n",
      "Iteration 126, loss = 0.30049790\n",
      "Iteration 127, loss = 0.29942219\n",
      "Iteration 128, loss = 0.29854313\n",
      "Iteration 129, loss = 0.29770884\n",
      "Iteration 130, loss = 0.29684045\n",
      "Iteration 131, loss = 0.29593756\n",
      "Iteration 132, loss = 0.29504359\n",
      "Iteration 133, loss = 0.29402465\n",
      "Iteration 134, loss = 0.29326087\n",
      "Iteration 135, loss = 0.29243347\n",
      "Iteration 136, loss = 0.29143425\n",
      "Iteration 137, loss = 0.29072533\n",
      "Iteration 138, loss = 0.28982602\n",
      "Iteration 139, loss = 0.28889425\n",
      "Iteration 140, loss = 0.28814954\n",
      "Iteration 141, loss = 0.28730125\n",
      "Iteration 142, loss = 0.28657832\n",
      "Iteration 143, loss = 0.28563001\n",
      "Iteration 144, loss = 0.28472058\n",
      "Iteration 145, loss = 0.28407994\n",
      "Iteration 146, loss = 0.28325940\n",
      "Iteration 147, loss = 0.28251195\n",
      "Iteration 148, loss = 0.28171877\n",
      "Iteration 149, loss = 0.28104322\n",
      "Iteration 150, loss = 0.28019334\n",
      "Iteration 151, loss = 0.27949131\n",
      "Iteration 152, loss = 0.27883827\n",
      "Iteration 153, loss = 0.27797429\n",
      "Iteration 154, loss = 0.27728814\n",
      "Iteration 155, loss = 0.27650234\n",
      "Iteration 156, loss = 0.27581745\n",
      "Iteration 157, loss = 0.27505189\n",
      "Iteration 158, loss = 0.27439021\n",
      "Iteration 159, loss = 0.27370441\n",
      "Iteration 160, loss = 0.27293420\n",
      "Iteration 161, loss = 0.27222653\n",
      "Iteration 162, loss = 0.27154642\n",
      "Iteration 163, loss = 0.27076912\n",
      "Iteration 164, loss = 0.27022158\n",
      "Iteration 165, loss = 0.26938261\n",
      "Iteration 166, loss = 0.26872072\n",
      "Iteration 167, loss = 0.26820794\n",
      "Iteration 168, loss = 0.26752647\n",
      "Iteration 169, loss = 0.26685623\n",
      "Iteration 170, loss = 0.26603567\n",
      "Iteration 171, loss = 0.26539547\n",
      "Iteration 172, loss = 0.26476023\n",
      "Iteration 173, loss = 0.26421337\n",
      "Iteration 174, loss = 0.26358067\n",
      "Iteration 175, loss = 0.26289721\n",
      "Iteration 176, loss = 0.26228694\n",
      "Iteration 177, loss = 0.26158179\n",
      "Iteration 178, loss = 0.26097846\n",
      "Iteration 179, loss = 0.26040042\n",
      "Iteration 180, loss = 0.25978723\n",
      "Iteration 181, loss = 0.25908922\n",
      "Iteration 182, loss = 0.25850660\n",
      "Iteration 183, loss = 0.25781146\n",
      "Iteration 184, loss = 0.25737557\n",
      "Iteration 185, loss = 0.25672046\n",
      "Iteration 186, loss = 0.25601440\n",
      "Iteration 187, loss = 0.25556479\n",
      "Iteration 188, loss = 0.25490359\n",
      "Iteration 189, loss = 0.25439665\n",
      "Iteration 190, loss = 0.25376570\n",
      "Iteration 191, loss = 0.25310944\n",
      "Iteration 192, loss = 0.25260978\n",
      "Iteration 193, loss = 0.25192561\n",
      "Iteration 194, loss = 0.25141038\n",
      "Iteration 195, loss = 0.25077591\n",
      "Iteration 196, loss = 0.25032534\n",
      "Iteration 197, loss = 0.24956933\n",
      "Iteration 198, loss = 0.24919289\n",
      "Iteration 199, loss = 0.24866897\n",
      "Iteration 200, loss = 0.24803909\n",
      "[CV] END activation=logistic, alpha=0.0001, batch_size=100, hidden_layer_sizes=(400, 100), solver=sgd; total time= 6.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.30043893\n",
      "Iteration 2, loss = 2.29136292\n",
      "Iteration 3, loss = 2.28392766\n",
      "Iteration 4, loss = 2.27603224\n",
      "Iteration 5, loss = 2.26646432\n",
      "Iteration 6, loss = 2.25465286\n",
      "Iteration 7, loss = 2.23978885\n",
      "Iteration 8, loss = 2.22020970\n",
      "Iteration 9, loss = 2.19395612\n",
      "Iteration 10, loss = 2.15799980\n",
      "Iteration 11, loss = 2.10726978\n",
      "Iteration 12, loss = 2.03696966\n",
      "Iteration 13, loss = 1.94217889\n",
      "Iteration 14, loss = 1.82442527\n",
      "Iteration 15, loss = 1.69206745\n",
      "Iteration 16, loss = 1.55830940\n",
      "Iteration 17, loss = 1.43318012\n",
      "Iteration 18, loss = 1.32142237\n",
      "Iteration 19, loss = 1.22370873\n",
      "Iteration 20, loss = 1.13999065\n",
      "Iteration 21, loss = 1.06802111\n",
      "Iteration 22, loss = 1.00577381\n",
      "Iteration 23, loss = 0.95117417\n",
      "Iteration 24, loss = 0.90311276\n",
      "Iteration 25, loss = 0.85983367\n",
      "Iteration 26, loss = 0.82061654\n",
      "Iteration 27, loss = 0.78524774\n",
      "Iteration 28, loss = 0.75296642\n",
      "Iteration 29, loss = 0.72371408\n",
      "Iteration 30, loss = 0.69688161\n",
      "Iteration 31, loss = 0.67257885\n",
      "Iteration 32, loss = 0.65041258\n",
      "Iteration 33, loss = 0.63034259\n",
      "Iteration 34, loss = 0.61204488\n",
      "Iteration 35, loss = 0.59543218\n",
      "Iteration 36, loss = 0.58002230\n",
      "Iteration 37, loss = 0.56611416\n",
      "Iteration 38, loss = 0.55317501\n",
      "Iteration 39, loss = 0.54126299\n",
      "Iteration 40, loss = 0.53020060\n",
      "Iteration 41, loss = 0.52004808\n",
      "Iteration 42, loss = 0.51032813\n",
      "Iteration 43, loss = 0.50134214\n",
      "Iteration 44, loss = 0.49296251\n",
      "Iteration 45, loss = 0.48493437\n",
      "Iteration 46, loss = 0.47761751\n",
      "Iteration 47, loss = 0.47065493\n",
      "Iteration 48, loss = 0.46398806\n",
      "Iteration 49, loss = 0.45775352\n",
      "Iteration 50, loss = 0.45181164\n",
      "Iteration 51, loss = 0.44613983\n",
      "Iteration 52, loss = 0.44084854\n",
      "Iteration 53, loss = 0.43568210\n",
      "Iteration 54, loss = 0.43095203\n",
      "Iteration 55, loss = 0.42636599\n",
      "Iteration 56, loss = 0.42193910\n",
      "Iteration 57, loss = 0.41790877\n",
      "Iteration 58, loss = 0.41375890\n",
      "Iteration 59, loss = 0.40999700\n",
      "Iteration 60, loss = 0.40626508\n",
      "Iteration 61, loss = 0.40290854\n",
      "Iteration 62, loss = 0.39943252\n",
      "Iteration 63, loss = 0.39631118\n",
      "Iteration 64, loss = 0.39308214\n",
      "Iteration 65, loss = 0.39015301\n",
      "Iteration 66, loss = 0.38736297\n",
      "Iteration 67, loss = 0.38455209\n",
      "Iteration 68, loss = 0.38209476\n",
      "Iteration 69, loss = 0.37936522\n",
      "Iteration 70, loss = 0.37695741\n",
      "Iteration 71, loss = 0.37458747\n",
      "Iteration 72, loss = 0.37223000\n",
      "Iteration 73, loss = 0.36998953\n",
      "Iteration 74, loss = 0.36775721\n",
      "Iteration 75, loss = 0.36570194\n",
      "Iteration 76, loss = 0.36370616\n",
      "Iteration 77, loss = 0.36168665\n",
      "Iteration 78, loss = 0.35977432\n",
      "Iteration 79, loss = 0.35785222\n",
      "Iteration 80, loss = 0.35608389\n",
      "Iteration 81, loss = 0.35427740\n",
      "Iteration 82, loss = 0.35248880\n",
      "Iteration 83, loss = 0.35068545\n",
      "Iteration 84, loss = 0.34915544\n",
      "Iteration 85, loss = 0.34750762\n",
      "Iteration 86, loss = 0.34584765\n",
      "Iteration 87, loss = 0.34429626\n",
      "Iteration 88, loss = 0.34284505\n",
      "Iteration 89, loss = 0.34131280\n",
      "Iteration 90, loss = 0.33992378\n",
      "Iteration 91, loss = 0.33843001\n",
      "Iteration 92, loss = 0.33700370\n",
      "Iteration 93, loss = 0.33559002\n",
      "Iteration 94, loss = 0.33430725\n",
      "Iteration 95, loss = 0.33295712\n",
      "Iteration 96, loss = 0.33152932\n",
      "Iteration 97, loss = 0.33038807\n",
      "Iteration 98, loss = 0.32897604\n",
      "Iteration 99, loss = 0.32782368\n",
      "Iteration 100, loss = 0.32660062\n",
      "Iteration 101, loss = 0.32538467\n",
      "Iteration 102, loss = 0.32420282\n",
      "Iteration 103, loss = 0.32309352\n",
      "Iteration 104, loss = 0.32189196\n",
      "Iteration 105, loss = 0.32077788\n",
      "Iteration 106, loss = 0.31956628\n",
      "Iteration 107, loss = 0.31855823\n",
      "Iteration 108, loss = 0.31739020\n",
      "Iteration 109, loss = 0.31634136\n",
      "Iteration 110, loss = 0.31533357\n",
      "Iteration 111, loss = 0.31424753\n",
      "Iteration 112, loss = 0.31320756\n",
      "Iteration 113, loss = 0.31217806\n",
      "Iteration 114, loss = 0.31125889\n",
      "Iteration 115, loss = 0.31029512\n",
      "Iteration 116, loss = 0.30922331\n",
      "Iteration 117, loss = 0.30821248\n",
      "Iteration 118, loss = 0.30730722\n",
      "Iteration 119, loss = 0.30639198\n",
      "Iteration 120, loss = 0.30539133\n",
      "Iteration 121, loss = 0.30446432\n",
      "Iteration 122, loss = 0.30348685\n",
      "Iteration 123, loss = 0.30264080\n",
      "Iteration 124, loss = 0.30178458\n",
      "Iteration 125, loss = 0.30079676\n",
      "Iteration 126, loss = 0.29999856\n",
      "Iteration 127, loss = 0.29922394\n",
      "Iteration 128, loss = 0.29817637\n",
      "Iteration 129, loss = 0.29733212\n",
      "Iteration 130, loss = 0.29648814\n",
      "Iteration 131, loss = 0.29564631\n",
      "Iteration 132, loss = 0.29482356\n",
      "Iteration 133, loss = 0.29404540\n",
      "Iteration 134, loss = 0.29309543\n",
      "Iteration 135, loss = 0.29232629\n",
      "Iteration 136, loss = 0.29159918\n",
      "Iteration 137, loss = 0.29073023\n",
      "Iteration 138, loss = 0.29006786\n",
      "Iteration 139, loss = 0.28911386\n",
      "Iteration 140, loss = 0.28840967\n",
      "Iteration 141, loss = 0.28767394\n",
      "Iteration 142, loss = 0.28702216\n",
      "Iteration 143, loss = 0.28613073\n",
      "Iteration 144, loss = 0.28542014\n",
      "Iteration 145, loss = 0.28474745\n",
      "Iteration 146, loss = 0.28395043\n",
      "Iteration 147, loss = 0.28318377\n",
      "Iteration 148, loss = 0.28241622\n",
      "Iteration 149, loss = 0.28188720\n",
      "Iteration 150, loss = 0.28110689\n",
      "Iteration 151, loss = 0.28041803\n",
      "Iteration 152, loss = 0.27968473\n",
      "Iteration 153, loss = 0.27908944\n",
      "Iteration 154, loss = 0.27839354\n",
      "Iteration 155, loss = 0.27760178\n",
      "Iteration 156, loss = 0.27699176\n",
      "Iteration 157, loss = 0.27618031\n",
      "Iteration 158, loss = 0.27553506\n",
      "Iteration 159, loss = 0.27493617\n",
      "Iteration 160, loss = 0.27428015\n",
      "Iteration 161, loss = 0.27358856\n",
      "Iteration 162, loss = 0.27298771\n",
      "Iteration 163, loss = 0.27234844\n",
      "Iteration 164, loss = 0.27161959\n",
      "Iteration 165, loss = 0.27102531\n",
      "Iteration 166, loss = 0.27037131\n",
      "Iteration 167, loss = 0.26981860\n",
      "Iteration 168, loss = 0.26921115\n",
      "Iteration 169, loss = 0.26856909\n",
      "Iteration 170, loss = 0.26796071\n",
      "Iteration 171, loss = 0.26735096\n",
      "Iteration 172, loss = 0.26668734\n",
      "Iteration 173, loss = 0.26622314\n",
      "Iteration 174, loss = 0.26552373\n",
      "Iteration 175, loss = 0.26502913\n",
      "Iteration 176, loss = 0.26434919\n",
      "Iteration 177, loss = 0.26374311\n",
      "Iteration 178, loss = 0.26310012\n",
      "Iteration 179, loss = 0.26257691\n",
      "Iteration 180, loss = 0.26198380\n",
      "Iteration 181, loss = 0.26137086\n",
      "Iteration 182, loss = 0.26089890\n",
      "Iteration 183, loss = 0.26023322\n",
      "Iteration 184, loss = 0.25969532\n",
      "Iteration 185, loss = 0.25899675\n",
      "Iteration 186, loss = 0.25857658\n",
      "Iteration 187, loss = 0.25803201\n",
      "Iteration 188, loss = 0.25752090\n",
      "Iteration 189, loss = 0.25691394\n",
      "Iteration 190, loss = 0.25633441\n",
      "Iteration 191, loss = 0.25572869\n",
      "Iteration 192, loss = 0.25514501\n",
      "Iteration 193, loss = 0.25463869\n",
      "Iteration 194, loss = 0.25406933\n",
      "Iteration 195, loss = 0.25356892\n",
      "Iteration 196, loss = 0.25303205\n",
      "Iteration 197, loss = 0.25253770\n",
      "Iteration 198, loss = 0.25192799\n",
      "Iteration 199, loss = 0.25137428\n",
      "Iteration 200, loss = 0.25088976\n",
      "[CV] END activation=logistic, alpha=0.0001, batch_size=100, hidden_layer_sizes=(400, 100), solver=sgd; total time= 6.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.30988007\n",
      "Iteration 2, loss = 2.29496657\n",
      "Iteration 3, loss = 2.28869586\n",
      "Iteration 4, loss = 2.28156576\n",
      "Iteration 5, loss = 2.27326922\n",
      "Iteration 6, loss = 2.26358615\n",
      "Iteration 7, loss = 2.25145163\n",
      "Iteration 8, loss = 2.23541424\n",
      "Iteration 9, loss = 2.21454578\n",
      "Iteration 10, loss = 2.18527524\n",
      "Iteration 11, loss = 2.14495007\n",
      "Iteration 12, loss = 2.08949212\n",
      "Iteration 13, loss = 2.01526055\n",
      "Iteration 14, loss = 1.92225484\n",
      "Iteration 15, loss = 1.81392956\n",
      "Iteration 16, loss = 1.69761249\n",
      "Iteration 17, loss = 1.57943471\n",
      "Iteration 18, loss = 1.46612462\n",
      "Iteration 19, loss = 1.36147372\n",
      "Iteration 20, loss = 1.26739320\n",
      "Iteration 21, loss = 1.18347728\n",
      "Iteration 22, loss = 1.10908786\n",
      "Iteration 23, loss = 1.04363021\n",
      "Iteration 24, loss = 0.98599666\n",
      "Iteration 25, loss = 0.93491542\n",
      "Iteration 26, loss = 0.88881475\n",
      "Iteration 27, loss = 0.84707594\n",
      "Iteration 28, loss = 0.80906344\n",
      "Iteration 29, loss = 0.77397237\n",
      "Iteration 30, loss = 0.74209585\n",
      "Iteration 31, loss = 0.71271006\n",
      "Iteration 32, loss = 0.68623808\n",
      "Iteration 33, loss = 0.66190943\n",
      "Iteration 34, loss = 0.64006861\n",
      "Iteration 35, loss = 0.62027065\n",
      "Iteration 36, loss = 0.60217473\n",
      "Iteration 37, loss = 0.58592443\n",
      "Iteration 38, loss = 0.57112877\n",
      "Iteration 39, loss = 0.55763323\n",
      "Iteration 40, loss = 0.54523214\n",
      "Iteration 41, loss = 0.53394227\n",
      "Iteration 42, loss = 0.52344128\n",
      "Iteration 43, loss = 0.51374827\n",
      "Iteration 44, loss = 0.50475355\n",
      "Iteration 45, loss = 0.49645279\n",
      "Iteration 46, loss = 0.48870218\n",
      "Iteration 47, loss = 0.48132618\n",
      "Iteration 48, loss = 0.47449422\n",
      "Iteration 49, loss = 0.46796822\n",
      "Iteration 50, loss = 0.46194153\n",
      "Iteration 51, loss = 0.45621758\n",
      "Iteration 52, loss = 0.45073936\n",
      "Iteration 53, loss = 0.44558884\n",
      "Iteration 54, loss = 0.44069286\n",
      "Iteration 55, loss = 0.43615561\n",
      "Iteration 56, loss = 0.43176079\n",
      "Iteration 57, loss = 0.42750829\n",
      "Iteration 58, loss = 0.42343989\n",
      "Iteration 59, loss = 0.41970493\n",
      "Iteration 60, loss = 0.41589883\n",
      "Iteration 61, loss = 0.41251594\n",
      "Iteration 62, loss = 0.40902907\n",
      "Iteration 63, loss = 0.40588961\n",
      "Iteration 64, loss = 0.40273498\n",
      "Iteration 65, loss = 0.39968135\n",
      "Iteration 66, loss = 0.39682632\n",
      "Iteration 67, loss = 0.39387828\n",
      "Iteration 68, loss = 0.39132177\n",
      "Iteration 69, loss = 0.38868471\n",
      "Iteration 70, loss = 0.38626014\n",
      "Iteration 71, loss = 0.38377678\n",
      "Iteration 72, loss = 0.38147534\n",
      "Iteration 73, loss = 0.37908464\n",
      "Iteration 74, loss = 0.37679361\n",
      "Iteration 75, loss = 0.37471766\n",
      "Iteration 76, loss = 0.37262122\n",
      "Iteration 77, loss = 0.37055564\n",
      "Iteration 78, loss = 0.36846423\n",
      "Iteration 79, loss = 0.36666900\n",
      "Iteration 80, loss = 0.36471579\n",
      "Iteration 81, loss = 0.36298585\n",
      "Iteration 82, loss = 0.36113183\n",
      "Iteration 83, loss = 0.35937577\n",
      "Iteration 84, loss = 0.35768604\n",
      "Iteration 85, loss = 0.35594338\n",
      "Iteration 86, loss = 0.35432155\n",
      "Iteration 87, loss = 0.35275117\n",
      "Iteration 88, loss = 0.35101417\n",
      "Iteration 89, loss = 0.34962274\n",
      "Iteration 90, loss = 0.34808037\n",
      "Iteration 91, loss = 0.34670289\n",
      "Iteration 92, loss = 0.34518104\n",
      "Iteration 93, loss = 0.34364097\n",
      "Iteration 94, loss = 0.34232947\n",
      "Iteration 95, loss = 0.34101259\n",
      "Iteration 96, loss = 0.33965600\n",
      "Iteration 97, loss = 0.33828186\n",
      "Iteration 98, loss = 0.33695177\n",
      "Iteration 99, loss = 0.33560293\n",
      "Iteration 100, loss = 0.33445010\n",
      "Iteration 101, loss = 0.33310744\n",
      "Iteration 102, loss = 0.33201835\n",
      "Iteration 103, loss = 0.33068721\n",
      "Iteration 104, loss = 0.32953594\n",
      "Iteration 105, loss = 0.32839705\n",
      "Iteration 106, loss = 0.32728217\n",
      "Iteration 107, loss = 0.32608413\n",
      "Iteration 108, loss = 0.32482806\n",
      "Iteration 109, loss = 0.32380230\n",
      "Iteration 110, loss = 0.32289057\n",
      "Iteration 111, loss = 0.32160889\n",
      "Iteration 112, loss = 0.32068241\n",
      "Iteration 113, loss = 0.31943676\n",
      "Iteration 114, loss = 0.31856010\n",
      "Iteration 115, loss = 0.31753149\n",
      "Iteration 116, loss = 0.31657737\n",
      "Iteration 117, loss = 0.31545959\n",
      "Iteration 118, loss = 0.31460313\n",
      "Iteration 119, loss = 0.31337754\n",
      "Iteration 120, loss = 0.31266623\n",
      "Iteration 121, loss = 0.31168412\n",
      "Iteration 122, loss = 0.31057446\n",
      "Iteration 123, loss = 0.30969761\n",
      "Iteration 124, loss = 0.30880428\n",
      "Iteration 125, loss = 0.30791015\n",
      "Iteration 126, loss = 0.30700255\n",
      "Iteration 127, loss = 0.30609659\n",
      "Iteration 128, loss = 0.30516964\n",
      "Iteration 129, loss = 0.30428573\n",
      "Iteration 130, loss = 0.30343746\n",
      "Iteration 131, loss = 0.30256606\n",
      "Iteration 132, loss = 0.30180354\n",
      "Iteration 133, loss = 0.30085540\n",
      "Iteration 134, loss = 0.30001728\n",
      "Iteration 135, loss = 0.29924414\n",
      "Iteration 136, loss = 0.29838843\n",
      "Iteration 137, loss = 0.29762688\n",
      "Iteration 138, loss = 0.29677082\n",
      "Iteration 139, loss = 0.29594986\n",
      "Iteration 140, loss = 0.29517384\n",
      "Iteration 141, loss = 0.29437050\n",
      "Iteration 142, loss = 0.29351192\n",
      "Iteration 143, loss = 0.29288197\n",
      "Iteration 144, loss = 0.29207494\n",
      "Iteration 145, loss = 0.29115748\n",
      "Iteration 146, loss = 0.29053274\n",
      "Iteration 147, loss = 0.28979207\n",
      "Iteration 148, loss = 0.28900879\n",
      "Iteration 149, loss = 0.28834496\n",
      "Iteration 150, loss = 0.28757512\n",
      "Iteration 151, loss = 0.28679683\n",
      "Iteration 152, loss = 0.28605375\n",
      "Iteration 153, loss = 0.28527986\n",
      "Iteration 154, loss = 0.28451677\n",
      "Iteration 155, loss = 0.28395390\n",
      "Iteration 156, loss = 0.28328555\n",
      "Iteration 157, loss = 0.28254866\n",
      "Iteration 158, loss = 0.28194380\n",
      "Iteration 159, loss = 0.28121884\n",
      "Iteration 160, loss = 0.28058186\n",
      "Iteration 161, loss = 0.27981017\n",
      "Iteration 162, loss = 0.27915653\n",
      "Iteration 163, loss = 0.27855041\n",
      "Iteration 164, loss = 0.27775662\n",
      "Iteration 165, loss = 0.27722473\n",
      "Iteration 166, loss = 0.27653349\n",
      "Iteration 167, loss = 0.27580988\n",
      "Iteration 168, loss = 0.27528218\n",
      "Iteration 169, loss = 0.27457785\n",
      "Iteration 170, loss = 0.27396770\n",
      "Iteration 171, loss = 0.27326604\n",
      "Iteration 172, loss = 0.27273385\n",
      "Iteration 173, loss = 0.27207147\n",
      "Iteration 174, loss = 0.27142721\n",
      "Iteration 175, loss = 0.27088967\n",
      "Iteration 176, loss = 0.27014621\n",
      "Iteration 177, loss = 0.26960004\n",
      "Iteration 178, loss = 0.26902172\n",
      "Iteration 179, loss = 0.26851020\n",
      "Iteration 180, loss = 0.26783587\n",
      "Iteration 181, loss = 0.26713717\n",
      "Iteration 182, loss = 0.26663209\n",
      "Iteration 183, loss = 0.26606151\n",
      "Iteration 184, loss = 0.26543830\n",
      "Iteration 185, loss = 0.26473727\n",
      "Iteration 186, loss = 0.26419000\n",
      "Iteration 187, loss = 0.26367841\n",
      "Iteration 188, loss = 0.26311006\n",
      "Iteration 189, loss = 0.26245735\n",
      "Iteration 190, loss = 0.26199391\n",
      "Iteration 191, loss = 0.26141520\n",
      "Iteration 192, loss = 0.26082974\n",
      "Iteration 193, loss = 0.26029710\n",
      "Iteration 194, loss = 0.25972100\n",
      "Iteration 195, loss = 0.25919452\n",
      "Iteration 196, loss = 0.25870772\n",
      "Iteration 197, loss = 0.25814799\n",
      "Iteration 198, loss = 0.25757617\n",
      "Iteration 199, loss = 0.25709832\n",
      "Iteration 200, loss = 0.25650112\n",
      "[CV] END activation=logistic, alpha=0.0001, batch_size=100, hidden_layer_sizes=(400, 100), solver=sgd; total time= 6.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.31207007\n",
      "Iteration 2, loss = 0.39746121\n",
      "Iteration 3, loss = 0.27358791\n",
      "Iteration 4, loss = 0.21070421\n",
      "Iteration 5, loss = 0.16955749\n",
      "Iteration 6, loss = 0.14144741\n",
      "Iteration 7, loss = 0.11903654\n",
      "Iteration 8, loss = 0.10241108\n",
      "Iteration 9, loss = 0.08763824\n",
      "Iteration 10, loss = 0.07465175\n",
      "Iteration 11, loss = 0.06517558\n",
      "Iteration 12, loss = 0.05745217\n",
      "Iteration 13, loss = 0.04840251\n",
      "Iteration 14, loss = 0.04325619\n",
      "Iteration 15, loss = 0.03583658\n",
      "Iteration 16, loss = 0.03125761\n",
      "Iteration 17, loss = 0.02741951\n",
      "Iteration 18, loss = 0.02420884\n",
      "Iteration 19, loss = 0.02080682\n",
      "Iteration 20, loss = 0.01742297\n",
      "Iteration 21, loss = 0.01533067\n",
      "Iteration 22, loss = 0.01247126\n",
      "Iteration 23, loss = 0.01121703\n",
      "Iteration 24, loss = 0.01001690\n",
      "Iteration 25, loss = 0.00891863\n",
      "Iteration 26, loss = 0.00705455\n",
      "Iteration 27, loss = 0.00774075\n",
      "Iteration 28, loss = 0.01321844\n",
      "Iteration 29, loss = 0.00672226\n",
      "Iteration 30, loss = 0.00479685\n",
      "Iteration 31, loss = 0.00366567\n",
      "Iteration 32, loss = 0.00374425\n",
      "Iteration 33, loss = 0.01114141\n",
      "Iteration 34, loss = 0.00597522\n",
      "Iteration 35, loss = 0.00409323\n",
      "Iteration 36, loss = 0.00338733\n",
      "Iteration 37, loss = 0.00275669\n",
      "Iteration 38, loss = 0.00265802\n",
      "Iteration 39, loss = 0.00260916\n",
      "Iteration 40, loss = 0.00255365\n",
      "Iteration 41, loss = 0.00250875\n",
      "Iteration 42, loss = 0.00246104\n",
      "Iteration 43, loss = 0.00246332\n",
      "Iteration 44, loss = 0.00237901\n",
      "Iteration 45, loss = 0.00234207\n",
      "Iteration 46, loss = 0.00230460\n",
      "Iteration 47, loss = 0.00225804\n",
      "Iteration 48, loss = 0.03365208\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.0001, batch_size=100, hidden_layer_sizes=(100, 100, 100), solver=adam; total time= 1.0min\n",
      "Iteration 1, loss = 1.37506792\n",
      "Iteration 2, loss = 0.47235984\n",
      "Iteration 3, loss = 0.29666202\n",
      "Iteration 4, loss = 0.22242747\n",
      "Iteration 5, loss = 0.18099394\n",
      "Iteration 6, loss = 0.15024728\n",
      "Iteration 7, loss = 0.12651731\n",
      "Iteration 8, loss = 0.10963062\n",
      "Iteration 9, loss = 0.09381042\n",
      "Iteration 10, loss = 0.08173606\n",
      "Iteration 11, loss = 0.07261970\n",
      "Iteration 12, loss = 0.06253018\n",
      "Iteration 13, loss = 0.05319512\n",
      "Iteration 14, loss = 0.04782265\n",
      "Iteration 15, loss = 0.04049413\n",
      "Iteration 16, loss = 0.03539625\n",
      "Iteration 17, loss = 0.03078263\n",
      "Iteration 18, loss = 0.02731228\n",
      "Iteration 19, loss = 0.02308977\n",
      "Iteration 20, loss = 0.02135717\n",
      "Iteration 21, loss = 0.01657674\n",
      "Iteration 22, loss = 0.01493146\n",
      "Iteration 23, loss = 0.01336834\n",
      "Iteration 24, loss = 0.01217924\n",
      "Iteration 25, loss = 0.01208185\n",
      "Iteration 26, loss = 0.01122346\n",
      "Iteration 27, loss = 0.00716789\n",
      "Iteration 28, loss = 0.00663835\n",
      "Iteration 29, loss = 0.00753323\n",
      "Iteration 30, loss = 0.00815249\n",
      "Iteration 31, loss = 0.00526464\n",
      "Iteration 32, loss = 0.00424096\n",
      "Iteration 33, loss = 0.00805784\n",
      "Iteration 34, loss = 0.01589190\n",
      "Iteration 35, loss = 0.00564803\n",
      "Iteration 36, loss = 0.00365570\n",
      "Iteration 37, loss = 0.00337337\n",
      "Iteration 38, loss = 0.00325504\n",
      "Iteration 39, loss = 0.00321540\n",
      "Iteration 40, loss = 0.00308942\n",
      "Iteration 41, loss = 0.00299899\n",
      "Iteration 42, loss = 0.00295966\n",
      "Iteration 43, loss = 0.00292449\n",
      "Iteration 44, loss = 0.00938792\n",
      "Iteration 45, loss = 0.02244091\n",
      "Iteration 46, loss = 0.00455254\n",
      "Iteration 47, loss = 0.00329377\n",
      "Iteration 48, loss = 0.00308655\n",
      "Iteration 49, loss = 0.00291847\n",
      "Iteration 50, loss = 0.00285921\n",
      "Iteration 51, loss = 0.00281028\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.0001, batch_size=100, hidden_layer_sizes=(100, 100, 100), solver=adam; total time= 1.1min\n",
      "Iteration 1, loss = 1.34191200\n",
      "Iteration 2, loss = 0.40378947\n",
      "Iteration 3, loss = 0.27441631\n",
      "Iteration 4, loss = 0.21243058\n",
      "Iteration 5, loss = 0.17342187\n",
      "Iteration 6, loss = 0.14396991\n",
      "Iteration 7, loss = 0.12304198\n",
      "Iteration 8, loss = 0.10681432\n",
      "Iteration 9, loss = 0.09194047\n",
      "Iteration 10, loss = 0.08013668\n",
      "Iteration 11, loss = 0.07014693\n",
      "Iteration 12, loss = 0.06083155\n",
      "Iteration 13, loss = 0.05322170\n",
      "Iteration 14, loss = 0.04579129\n",
      "Iteration 15, loss = 0.04028747\n",
      "Iteration 16, loss = 0.03410604\n",
      "Iteration 17, loss = 0.03058960\n",
      "Iteration 18, loss = 0.02614580\n",
      "Iteration 19, loss = 0.02234178\n",
      "Iteration 20, loss = 0.01861637\n",
      "Iteration 21, loss = 0.01767906\n",
      "Iteration 22, loss = 0.01453492\n",
      "Iteration 23, loss = 0.01268333\n",
      "Iteration 24, loss = 0.01032963\n",
      "Iteration 25, loss = 0.01102958\n",
      "Iteration 26, loss = 0.00967643\n",
      "Iteration 27, loss = 0.00682807\n",
      "Iteration 28, loss = 0.00711161\n",
      "Iteration 29, loss = 0.01084290\n",
      "Iteration 30, loss = 0.00604295\n",
      "Iteration 31, loss = 0.00464793\n",
      "Iteration 32, loss = 0.00382043\n",
      "Iteration 33, loss = 0.00388816\n",
      "Iteration 34, loss = 0.00331306\n",
      "Iteration 35, loss = 0.00315457\n",
      "Iteration 36, loss = 0.00295165\n",
      "Iteration 37, loss = 0.00283150\n",
      "Iteration 38, loss = 0.00276047\n",
      "Iteration 39, loss = 0.00267893\n",
      "Iteration 40, loss = 0.00260530\n",
      "Iteration 41, loss = 0.00255901\n",
      "Iteration 42, loss = 0.00252386\n",
      "Iteration 43, loss = 0.03111622\n",
      "Iteration 44, loss = 0.00602476\n",
      "Iteration 45, loss = 0.00318803\n",
      "Iteration 46, loss = 0.00273889\n",
      "Iteration 47, loss = 0.00264921\n",
      "Iteration 48, loss = 0.00260114\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.0001, batch_size=100, hidden_layer_sizes=(100, 100, 100), solver=adam; total time= 1.0min\n",
      "Iteration 1, loss = 1.33560604\n",
      "Iteration 2, loss = 0.45232050\n",
      "Iteration 3, loss = 0.30011708\n",
      "Iteration 4, loss = 0.22779684\n",
      "Iteration 5, loss = 0.18499744\n",
      "Iteration 6, loss = 0.15732404\n",
      "Iteration 7, loss = 0.13357560\n",
      "Iteration 8, loss = 0.11619929\n",
      "Iteration 9, loss = 0.10089381\n",
      "Iteration 10, loss = 0.08901007\n",
      "Iteration 11, loss = 0.07697203\n",
      "Iteration 12, loss = 0.06804077\n",
      "Iteration 13, loss = 0.05968430\n",
      "Iteration 14, loss = 0.05251490\n",
      "Iteration 15, loss = 0.04542141\n",
      "Iteration 16, loss = 0.03949430\n",
      "Iteration 17, loss = 0.03606258\n",
      "Iteration 18, loss = 0.03032951\n",
      "Iteration 19, loss = 0.02670339\n",
      "Iteration 20, loss = 0.02332257\n",
      "Iteration 21, loss = 0.02033631\n",
      "Iteration 22, loss = 0.01841797\n",
      "Iteration 23, loss = 0.01597727\n",
      "Iteration 24, loss = 0.01362824\n",
      "Iteration 25, loss = 0.01212144\n",
      "Iteration 26, loss = 0.01086774\n",
      "Iteration 27, loss = 0.00912418\n",
      "Iteration 28, loss = 0.00831414\n",
      "Iteration 29, loss = 0.01213103\n",
      "Iteration 30, loss = 0.00785045\n",
      "Iteration 31, loss = 0.00609714\n",
      "Iteration 32, loss = 0.00519645\n",
      "Iteration 33, loss = 0.00532973\n",
      "Iteration 34, loss = 0.01369409\n",
      "Iteration 35, loss = 0.00600332\n",
      "Iteration 36, loss = 0.00450100\n",
      "Iteration 37, loss = 0.00372323\n",
      "Iteration 38, loss = 0.00345315\n",
      "Iteration 39, loss = 0.00322926\n",
      "Iteration 40, loss = 0.00298440\n",
      "Iteration 41, loss = 0.00289568\n",
      "Iteration 42, loss = 0.00281676\n",
      "Iteration 43, loss = 0.00274410\n",
      "Iteration 44, loss = 0.00267718\n",
      "Iteration 45, loss = 0.00757874\n",
      "Iteration 46, loss = 0.02583782\n",
      "Iteration 47, loss = 0.00369645\n",
      "Iteration 48, loss = 0.00306943\n",
      "Iteration 49, loss = 0.00286777\n",
      "Iteration 50, loss = 0.00279237\n",
      "Iteration 51, loss = 0.00274869\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.0001, batch_size=100, hidden_layer_sizes=(100, 100, 100), solver=adam; total time= 1.1min\n",
      "Iteration 1, loss = 1.34301748\n",
      "Iteration 2, loss = 0.42452670\n",
      "Iteration 3, loss = 0.27037593\n",
      "Iteration 4, loss = 0.20385854\n",
      "Iteration 5, loss = 0.16318140\n",
      "Iteration 6, loss = 0.13534453\n",
      "Iteration 7, loss = 0.11380335\n",
      "Iteration 8, loss = 0.09801491\n",
      "Iteration 9, loss = 0.08358621\n",
      "Iteration 10, loss = 0.07354878\n",
      "Iteration 11, loss = 0.06237259\n",
      "Iteration 12, loss = 0.05425580\n",
      "Iteration 13, loss = 0.04670410\n",
      "Iteration 14, loss = 0.04121457\n",
      "Iteration 15, loss = 0.03457473\n",
      "Iteration 16, loss = 0.02940034\n",
      "Iteration 17, loss = 0.02449547\n",
      "Iteration 18, loss = 0.02188929\n",
      "Iteration 19, loss = 0.01821441\n",
      "Iteration 20, loss = 0.01626658\n",
      "Iteration 21, loss = 0.01333086\n",
      "Iteration 22, loss = 0.01266461\n",
      "Iteration 23, loss = 0.01048318\n",
      "Iteration 24, loss = 0.00959139\n",
      "Iteration 25, loss = 0.00703115\n",
      "Iteration 26, loss = 0.00636747\n",
      "Iteration 27, loss = 0.00567353\n",
      "Iteration 28, loss = 0.00696427\n",
      "Iteration 29, loss = 0.01347741\n",
      "Iteration 30, loss = 0.00599917\n",
      "Iteration 31, loss = 0.00377179\n",
      "Iteration 32, loss = 0.00336321\n",
      "Iteration 33, loss = 0.00309348\n",
      "Iteration 34, loss = 0.00296315\n",
      "Iteration 35, loss = 0.00284598\n",
      "Iteration 36, loss = 0.00276512\n",
      "Iteration 37, loss = 0.00269163\n",
      "Iteration 38, loss = 0.00261278\n",
      "Iteration 39, loss = 0.00255186\n",
      "Iteration 40, loss = 0.00250193\n",
      "Iteration 41, loss = 0.00244832\n",
      "Iteration 42, loss = 0.00238943\n",
      "Iteration 43, loss = 0.01320301\n",
      "Iteration 44, loss = 0.01679762\n",
      "Iteration 45, loss = 0.00384084\n",
      "Iteration 46, loss = 0.00265249\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.0001, batch_size=100, hidden_layer_sizes=(100, 100, 100), solver=adam; total time=  57.5s\n",
      "Iteration 1, loss = 2.30386006\n",
      "Iteration 2, loss = 2.30166981\n",
      "Iteration 3, loss = 2.30173722\n",
      "Iteration 4, loss = 2.30160820\n",
      "Iteration 5, loss = 2.30173620\n",
      "Iteration 6, loss = 2.30142586\n",
      "Iteration 7, loss = 2.30136405\n",
      "Iteration 8, loss = 2.30122749\n",
      "Iteration 9, loss = 2.30116370\n",
      "Iteration 10, loss = 2.30108948\n",
      "Iteration 11, loss = 2.30084010\n",
      "Iteration 12, loss = 2.30078229\n",
      "Iteration 13, loss = 2.30068008\n",
      "Iteration 14, loss = 2.30064352\n",
      "Iteration 15, loss = 2.30059154\n",
      "Iteration 16, loss = 2.30034171\n",
      "Iteration 17, loss = 2.30026741\n",
      "Iteration 18, loss = 2.30023148\n",
      "Iteration 19, loss = 2.29995527\n",
      "Iteration 20, loss = 2.29988072\n",
      "Iteration 21, loss = 2.29962442\n",
      "Iteration 22, loss = 2.29948933\n",
      "Iteration 23, loss = 2.29922201\n",
      "Iteration 24, loss = 2.29919178\n",
      "Iteration 25, loss = 2.29904395\n",
      "Iteration 26, loss = 2.29897701\n",
      "Iteration 27, loss = 2.29864862\n",
      "Iteration 28, loss = 2.29844564\n",
      "Iteration 29, loss = 2.29840861\n",
      "Iteration 30, loss = 2.29807965\n",
      "Iteration 31, loss = 2.29780310\n",
      "Iteration 32, loss = 2.29762595\n",
      "Iteration 33, loss = 2.29732759\n",
      "Iteration 34, loss = 2.29697639\n",
      "Iteration 35, loss = 2.29668768\n",
      "Iteration 36, loss = 2.29620843\n",
      "Iteration 37, loss = 2.29587476\n",
      "Iteration 38, loss = 2.29550647\n",
      "Iteration 39, loss = 2.29498348\n",
      "Iteration 40, loss = 2.29429172\n",
      "Iteration 41, loss = 2.29397752\n",
      "Iteration 42, loss = 2.29350458\n",
      "Iteration 43, loss = 2.29278213\n",
      "Iteration 44, loss = 2.29219927\n",
      "Iteration 45, loss = 2.29117887\n",
      "Iteration 46, loss = 2.29055023\n",
      "Iteration 47, loss = 2.28934499\n",
      "Iteration 48, loss = 2.28831230\n",
      "Iteration 49, loss = 2.28720746\n",
      "Iteration 50, loss = 2.28583101\n",
      "Iteration 51, loss = 2.28430130\n",
      "Iteration 52, loss = 2.28237490\n",
      "Iteration 53, loss = 2.28059250\n",
      "Iteration 54, loss = 2.27784379\n",
      "Iteration 55, loss = 2.27475797\n",
      "Iteration 56, loss = 2.27130391\n",
      "Iteration 57, loss = 2.26676969\n",
      "Iteration 58, loss = 2.26135741\n",
      "Iteration 59, loss = 2.25470736\n",
      "Iteration 60, loss = 2.24604061\n",
      "Iteration 61, loss = 2.23485567\n",
      "Iteration 62, loss = 2.21992964\n",
      "Iteration 63, loss = 2.20062106\n",
      "Iteration 64, loss = 2.17469160\n",
      "Iteration 65, loss = 2.13992919\n",
      "Iteration 66, loss = 2.09604921\n",
      "Iteration 67, loss = 2.04345806\n",
      "Iteration 68, loss = 1.98628286\n",
      "Iteration 69, loss = 1.92944316\n",
      "Iteration 70, loss = 1.87673662\n",
      "Iteration 71, loss = 1.83005298\n",
      "Iteration 72, loss = 1.78898740\n",
      "Iteration 73, loss = 1.75277358\n",
      "Iteration 74, loss = 1.71993766\n",
      "Iteration 75, loss = 1.68928392\n",
      "Iteration 76, loss = 1.65917454\n",
      "Iteration 77, loss = 1.62813466\n",
      "Iteration 78, loss = 1.59486770\n",
      "Iteration 79, loss = 1.55793359\n",
      "Iteration 80, loss = 1.51599986\n",
      "Iteration 81, loss = 1.46887792\n",
      "Iteration 82, loss = 1.41729843\n",
      "Iteration 83, loss = 1.36308828\n",
      "Iteration 84, loss = 1.30970013\n",
      "Iteration 85, loss = 1.25923955\n",
      "Iteration 86, loss = 1.21339131\n",
      "Iteration 87, loss = 1.17218885\n",
      "Iteration 88, loss = 1.13545809\n",
      "Iteration 89, loss = 1.10232215\n",
      "Iteration 90, loss = 1.07208647\n",
      "Iteration 91, loss = 1.04437242\n",
      "Iteration 92, loss = 1.01896292\n",
      "Iteration 93, loss = 0.99524823\n",
      "Iteration 94, loss = 0.97344925\n",
      "Iteration 95, loss = 0.95266005\n",
      "Iteration 96, loss = 0.93323715\n",
      "Iteration 97, loss = 0.91443720\n",
      "Iteration 98, loss = 0.89677272\n",
      "Iteration 99, loss = 0.87950551\n",
      "Iteration 100, loss = 0.86268488\n",
      "Iteration 101, loss = 0.84662828\n",
      "Iteration 102, loss = 0.83073775\n",
      "Iteration 103, loss = 0.81553052\n",
      "Iteration 104, loss = 0.80083532\n",
      "Iteration 105, loss = 0.78623547\n",
      "Iteration 106, loss = 0.77256011\n",
      "Iteration 107, loss = 0.75927514\n",
      "Iteration 108, loss = 0.74649990\n",
      "Iteration 109, loss = 0.73420723\n",
      "Iteration 110, loss = 0.72251472\n",
      "Iteration 111, loss = 0.71139386\n",
      "Iteration 112, loss = 0.70053491\n",
      "Iteration 113, loss = 0.69022298\n",
      "Iteration 114, loss = 0.68044864\n",
      "Iteration 115, loss = 0.67111122\n",
      "Iteration 116, loss = 0.66214571\n",
      "Iteration 117, loss = 0.65348535\n",
      "Iteration 118, loss = 0.64526501\n",
      "Iteration 119, loss = 0.63731374\n",
      "Iteration 120, loss = 0.62972762\n",
      "Iteration 121, loss = 0.62234921\n",
      "Iteration 122, loss = 0.61522934\n",
      "Iteration 123, loss = 0.60840150\n",
      "Iteration 124, loss = 0.60207808\n",
      "Iteration 125, loss = 0.59552622\n",
      "Iteration 126, loss = 0.58953059\n",
      "Iteration 127, loss = 0.58356493\n",
      "Iteration 128, loss = 0.57783103\n",
      "Iteration 129, loss = 0.57238396\n",
      "Iteration 130, loss = 0.56690024\n",
      "Iteration 131, loss = 0.56179137\n",
      "Iteration 132, loss = 0.55671016\n",
      "Iteration 133, loss = 0.55183869\n",
      "Iteration 134, loss = 0.54720164\n",
      "Iteration 135, loss = 0.54251331\n",
      "Iteration 136, loss = 0.53809254\n",
      "Iteration 137, loss = 0.53372742\n",
      "Iteration 138, loss = 0.52964095\n",
      "Iteration 139, loss = 0.52535855\n",
      "Iteration 140, loss = 0.52139239\n",
      "Iteration 141, loss = 0.51740673\n",
      "Iteration 142, loss = 0.51349413\n",
      "Iteration 143, loss = 0.50990888\n",
      "Iteration 144, loss = 0.50635290\n",
      "Iteration 145, loss = 0.50258837\n",
      "Iteration 146, loss = 0.49919311\n",
      "Iteration 147, loss = 0.49560827\n",
      "Iteration 148, loss = 0.49236684\n",
      "Iteration 149, loss = 0.48912111\n",
      "Iteration 150, loss = 0.48582648\n",
      "Iteration 151, loss = 0.48251789\n",
      "Iteration 152, loss = 0.47964439\n",
      "Iteration 153, loss = 0.47632160\n",
      "Iteration 154, loss = 0.47335796\n",
      "Iteration 155, loss = 0.47054133\n",
      "Iteration 156, loss = 0.46749628\n",
      "Iteration 157, loss = 0.46450886\n",
      "Iteration 158, loss = 0.46162639\n",
      "Iteration 159, loss = 0.45892315\n",
      "Iteration 160, loss = 0.45607888\n",
      "Iteration 161, loss = 0.45341471\n",
      "Iteration 162, loss = 0.45067387\n",
      "Iteration 163, loss = 0.44789560\n",
      "Iteration 164, loss = 0.44520961\n",
      "Iteration 165, loss = 0.44263363\n",
      "Iteration 166, loss = 0.44020880\n",
      "Iteration 167, loss = 0.43768423\n",
      "Iteration 168, loss = 0.43481493\n",
      "Iteration 169, loss = 0.43243185\n",
      "Iteration 170, loss = 0.42981142\n",
      "Iteration 171, loss = 0.42749109\n",
      "Iteration 172, loss = 0.42490574\n",
      "Iteration 173, loss = 0.42248284\n",
      "Iteration 174, loss = 0.42000070\n",
      "Iteration 175, loss = 0.41762394\n",
      "Iteration 176, loss = 0.41527885\n",
      "Iteration 177, loss = 0.41281108\n",
      "Iteration 178, loss = 0.41050422\n",
      "Iteration 179, loss = 0.40823100\n",
      "Iteration 180, loss = 0.40575986\n",
      "Iteration 181, loss = 0.40343037\n",
      "Iteration 182, loss = 0.40124310\n",
      "Iteration 183, loss = 0.39879941\n",
      "Iteration 184, loss = 0.39659047\n",
      "Iteration 185, loss = 0.39437130\n",
      "Iteration 186, loss = 0.39205095\n",
      "Iteration 187, loss = 0.38970910\n",
      "Iteration 188, loss = 0.38773170\n",
      "Iteration 189, loss = 0.38522097\n",
      "Iteration 190, loss = 0.38317952\n",
      "Iteration 191, loss = 0.38097678\n",
      "Iteration 192, loss = 0.37880489\n",
      "Iteration 193, loss = 0.37676822\n",
      "Iteration 194, loss = 0.37459219\n",
      "Iteration 195, loss = 0.37250619\n",
      "Iteration 196, loss = 0.37033407\n",
      "Iteration 197, loss = 0.36825657\n",
      "Iteration 198, loss = 0.36622410\n",
      "Iteration 199, loss = 0.36420443\n",
      "Iteration 200, loss = 0.36202796\n",
      "[CV] END activation=logistic, alpha=0.0001, batch_size=100, hidden_layer_sizes=(100, 100, 100), solver=sgd; total time= 3.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.30804422\n",
      "Iteration 2, loss = 2.30196534\n",
      "Iteration 3, loss = 2.30188682\n",
      "Iteration 4, loss = 2.30181051\n",
      "Iteration 5, loss = 2.30182576\n",
      "Iteration 6, loss = 2.30149043\n",
      "Iteration 7, loss = 2.30156381\n",
      "Iteration 8, loss = 2.30160070\n",
      "Iteration 9, loss = 2.30143025\n",
      "Iteration 10, loss = 2.30129244\n",
      "Iteration 11, loss = 2.30124969\n",
      "Iteration 12, loss = 2.30108850\n",
      "Iteration 13, loss = 2.30103503\n",
      "Iteration 14, loss = 2.30096997\n",
      "Iteration 15, loss = 2.30087236\n",
      "Iteration 16, loss = 2.30068382\n",
      "Iteration 17, loss = 2.30054644\n",
      "Iteration 18, loss = 2.30055891\n",
      "Iteration 19, loss = 2.30042646\n",
      "Iteration 20, loss = 2.30030941\n",
      "Iteration 21, loss = 2.30020270\n",
      "Iteration 22, loss = 2.30016268\n",
      "Iteration 23, loss = 2.30006510\n",
      "Iteration 24, loss = 2.29974282\n",
      "Iteration 25, loss = 2.29963291\n",
      "Iteration 26, loss = 2.29943437\n",
      "Iteration 27, loss = 2.29944366\n",
      "Iteration 28, loss = 2.29927843\n",
      "Iteration 29, loss = 2.29923734\n",
      "Iteration 30, loss = 2.29891567\n",
      "Iteration 31, loss = 2.29886918\n",
      "Iteration 32, loss = 2.29862670\n",
      "Iteration 33, loss = 2.29826957\n",
      "Iteration 34, loss = 2.29820369\n",
      "Iteration 35, loss = 2.29793785\n",
      "Iteration 36, loss = 2.29763613\n",
      "Iteration 37, loss = 2.29740278\n",
      "Iteration 38, loss = 2.29715968\n",
      "Iteration 39, loss = 2.29691597\n",
      "Iteration 40, loss = 2.29651085\n",
      "Iteration 41, loss = 2.29610722\n",
      "Iteration 42, loss = 2.29573692\n",
      "Iteration 43, loss = 2.29531661\n",
      "Iteration 44, loss = 2.29499538\n",
      "Iteration 45, loss = 2.29445969\n",
      "Iteration 46, loss = 2.29387496\n",
      "Iteration 47, loss = 2.29330655\n",
      "Iteration 48, loss = 2.29268236\n",
      "Iteration 49, loss = 2.29200005\n",
      "Iteration 50, loss = 2.29113591\n",
      "Iteration 51, loss = 2.29037111\n",
      "Iteration 52, loss = 2.28924607\n",
      "Iteration 53, loss = 2.28814661\n",
      "Iteration 54, loss = 2.28686339\n",
      "Iteration 55, loss = 2.28541145\n",
      "Iteration 56, loss = 2.28375248\n",
      "Iteration 57, loss = 2.28194862\n",
      "Iteration 58, loss = 2.27951813\n",
      "Iteration 59, loss = 2.27687498\n",
      "Iteration 60, loss = 2.27337764\n",
      "Iteration 61, loss = 2.26989240\n",
      "Iteration 62, loss = 2.26521746\n",
      "Iteration 63, loss = 2.25911297\n",
      "Iteration 64, loss = 2.25218656\n",
      "Iteration 65, loss = 2.24292368\n",
      "Iteration 66, loss = 2.23109329\n",
      "Iteration 67, loss = 2.21538665\n",
      "Iteration 68, loss = 2.19520509\n",
      "Iteration 69, loss = 2.16832576\n",
      "Iteration 70, loss = 2.13411056\n",
      "Iteration 71, loss = 2.09191359\n",
      "Iteration 72, loss = 2.04352670\n",
      "Iteration 73, loss = 1.99211548\n",
      "Iteration 74, loss = 1.94156236\n",
      "Iteration 75, loss = 1.89492844\n",
      "Iteration 76, loss = 1.85223921\n",
      "Iteration 77, loss = 1.81250706\n",
      "Iteration 78, loss = 1.77417797\n",
      "Iteration 79, loss = 1.73575779\n",
      "Iteration 80, loss = 1.69489794\n",
      "Iteration 81, loss = 1.64990631\n",
      "Iteration 82, loss = 1.60011250\n",
      "Iteration 83, loss = 1.54565378\n",
      "Iteration 84, loss = 1.48880205\n",
      "Iteration 85, loss = 1.43319637\n",
      "Iteration 86, loss = 1.38208888\n",
      "Iteration 87, loss = 1.33705259\n",
      "Iteration 88, loss = 1.29725747\n",
      "Iteration 89, loss = 1.26195107\n",
      "Iteration 90, loss = 1.22897090\n",
      "Iteration 91, loss = 1.19693571\n",
      "Iteration 92, loss = 1.16453508\n",
      "Iteration 93, loss = 1.13138973\n",
      "Iteration 94, loss = 1.09669631\n",
      "Iteration 95, loss = 1.06168320\n",
      "Iteration 96, loss = 1.02643811\n",
      "Iteration 97, loss = 0.99256689\n",
      "Iteration 98, loss = 0.96102551\n",
      "Iteration 99, loss = 0.93239764\n",
      "Iteration 100, loss = 0.90689703\n",
      "Iteration 101, loss = 0.88426255\n",
      "Iteration 102, loss = 0.86426152\n",
      "Iteration 103, loss = 0.84655534\n",
      "Iteration 104, loss = 0.83084622\n",
      "Iteration 105, loss = 0.81666873\n",
      "Iteration 106, loss = 0.80365914\n",
      "Iteration 107, loss = 0.79185635\n",
      "Iteration 108, loss = 0.78092749\n",
      "Iteration 109, loss = 0.77082076\n",
      "Iteration 110, loss = 0.76119734\n",
      "Iteration 111, loss = 0.75218002\n",
      "Iteration 112, loss = 0.74373649\n",
      "Iteration 113, loss = 0.73549078\n",
      "Iteration 114, loss = 0.72773063\n",
      "Iteration 115, loss = 0.72027668\n",
      "Iteration 116, loss = 0.71307372\n",
      "Iteration 117, loss = 0.70596831\n",
      "Iteration 118, loss = 0.69917407\n",
      "Iteration 119, loss = 0.69241937\n",
      "Iteration 120, loss = 0.68603652\n",
      "Iteration 121, loss = 0.67963529\n",
      "Iteration 122, loss = 0.67339523\n",
      "Iteration 123, loss = 0.66739275\n",
      "Iteration 124, loss = 0.66128673\n",
      "Iteration 125, loss = 0.65530769\n",
      "Iteration 126, loss = 0.64950487\n",
      "Iteration 127, loss = 0.64380364\n",
      "Iteration 128, loss = 0.63821355\n",
      "Iteration 129, loss = 0.63253639\n",
      "Iteration 130, loss = 0.62717808\n",
      "Iteration 131, loss = 0.62187813\n",
      "Iteration 132, loss = 0.61648644\n",
      "Iteration 133, loss = 0.61138743\n",
      "Iteration 134, loss = 0.60614727\n",
      "Iteration 135, loss = 0.60123874\n",
      "Iteration 136, loss = 0.59629019\n",
      "Iteration 137, loss = 0.59134875\n",
      "Iteration 138, loss = 0.58690254\n",
      "Iteration 139, loss = 0.58212071\n",
      "Iteration 140, loss = 0.57757074\n",
      "Iteration 141, loss = 0.57310793\n",
      "Iteration 142, loss = 0.56875510\n",
      "Iteration 143, loss = 0.56450173\n",
      "Iteration 144, loss = 0.56026641\n",
      "Iteration 145, loss = 0.55619477\n",
      "Iteration 146, loss = 0.55203798\n",
      "Iteration 147, loss = 0.54799859\n",
      "Iteration 148, loss = 0.54410325\n",
      "Iteration 149, loss = 0.54030571\n",
      "Iteration 150, loss = 0.53662545\n",
      "Iteration 151, loss = 0.53286745\n",
      "Iteration 152, loss = 0.52912743\n",
      "Iteration 153, loss = 0.52572526\n",
      "Iteration 154, loss = 0.52212939\n",
      "Iteration 155, loss = 0.51882965\n",
      "Iteration 156, loss = 0.51535075\n",
      "Iteration 157, loss = 0.51196015\n",
      "Iteration 158, loss = 0.50862578\n",
      "Iteration 159, loss = 0.50541202\n",
      "Iteration 160, loss = 0.50230299\n",
      "Iteration 161, loss = 0.49886574\n",
      "Iteration 162, loss = 0.49576665\n",
      "Iteration 163, loss = 0.49274237\n",
      "Iteration 164, loss = 0.48983795\n",
      "Iteration 165, loss = 0.48679633\n",
      "Iteration 166, loss = 0.48384419\n",
      "Iteration 167, loss = 0.48093899\n",
      "Iteration 168, loss = 0.47786060\n",
      "Iteration 169, loss = 0.47507230\n",
      "Iteration 170, loss = 0.47214544\n",
      "Iteration 171, loss = 0.46937011\n",
      "Iteration 172, loss = 0.46661372\n",
      "Iteration 173, loss = 0.46372388\n",
      "Iteration 174, loss = 0.46099450\n",
      "Iteration 175, loss = 0.45823533\n",
      "Iteration 176, loss = 0.45544346\n",
      "Iteration 177, loss = 0.45275859\n",
      "Iteration 178, loss = 0.45002103\n",
      "Iteration 179, loss = 0.44742937\n",
      "Iteration 180, loss = 0.44482593\n",
      "Iteration 181, loss = 0.44210716\n",
      "Iteration 182, loss = 0.43955755\n",
      "Iteration 183, loss = 0.43695458\n",
      "Iteration 184, loss = 0.43424749\n",
      "Iteration 185, loss = 0.43173118\n",
      "Iteration 186, loss = 0.42930503\n",
      "Iteration 187, loss = 0.42657381\n",
      "Iteration 188, loss = 0.42410879\n",
      "Iteration 189, loss = 0.42167073\n",
      "Iteration 190, loss = 0.41908154\n",
      "Iteration 191, loss = 0.41656528\n",
      "Iteration 192, loss = 0.41407261\n",
      "Iteration 193, loss = 0.41161345\n",
      "Iteration 194, loss = 0.40919609\n",
      "Iteration 195, loss = 0.40672901\n",
      "Iteration 196, loss = 0.40423676\n",
      "Iteration 197, loss = 0.40196179\n",
      "Iteration 198, loss = 0.39944346\n",
      "Iteration 199, loss = 0.39710074\n",
      "Iteration 200, loss = 0.39476881\n",
      "[CV] END activation=logistic, alpha=0.0001, batch_size=100, hidden_layer_sizes=(100, 100, 100), solver=sgd; total time= 3.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.30339859\n",
      "Iteration 2, loss = 2.30188866\n",
      "Iteration 3, loss = 2.30171869\n",
      "Iteration 4, loss = 2.30149263\n",
      "Iteration 5, loss = 2.30137956\n",
      "Iteration 6, loss = 2.30137729\n",
      "Iteration 7, loss = 2.30109775\n",
      "Iteration 8, loss = 2.30133025\n",
      "Iteration 9, loss = 2.30109415\n",
      "Iteration 10, loss = 2.30086109\n",
      "Iteration 11, loss = 2.30084533\n",
      "Iteration 12, loss = 2.30084145\n",
      "Iteration 13, loss = 2.30067922\n",
      "Iteration 14, loss = 2.30064550\n",
      "Iteration 15, loss = 2.30065976\n",
      "Iteration 16, loss = 2.30026311\n",
      "Iteration 17, loss = 2.30031037\n",
      "Iteration 18, loss = 2.30012386\n",
      "Iteration 19, loss = 2.30007305\n",
      "Iteration 20, loss = 2.29987927\n",
      "Iteration 21, loss = 2.29982767\n",
      "Iteration 22, loss = 2.29978509\n",
      "Iteration 23, loss = 2.29947951\n",
      "Iteration 24, loss = 2.29923351\n",
      "Iteration 25, loss = 2.29902984\n",
      "Iteration 26, loss = 2.29900238\n",
      "Iteration 27, loss = 2.29885953\n",
      "Iteration 28, loss = 2.29867281\n",
      "Iteration 29, loss = 2.29834033\n",
      "Iteration 30, loss = 2.29811747\n",
      "Iteration 31, loss = 2.29799834\n",
      "Iteration 32, loss = 2.29781453\n",
      "Iteration 33, loss = 2.29729758\n",
      "Iteration 34, loss = 2.29715719\n",
      "Iteration 35, loss = 2.29702706\n",
      "Iteration 36, loss = 2.29648449\n",
      "Iteration 37, loss = 2.29610400\n",
      "Iteration 38, loss = 2.29575996\n",
      "Iteration 39, loss = 2.29524070\n",
      "Iteration 40, loss = 2.29500206\n",
      "Iteration 41, loss = 2.29422853\n",
      "Iteration 42, loss = 2.29380072\n",
      "Iteration 43, loss = 2.29316495\n",
      "Iteration 44, loss = 2.29262134\n",
      "Iteration 45, loss = 2.29168501\n",
      "Iteration 46, loss = 2.29092892\n",
      "Iteration 47, loss = 2.28995597\n",
      "Iteration 48, loss = 2.28893639\n",
      "Iteration 49, loss = 2.28752009\n",
      "Iteration 50, loss = 2.28625290\n",
      "Iteration 51, loss = 2.28464635\n",
      "Iteration 52, loss = 2.28246493\n",
      "Iteration 53, loss = 2.28026451\n",
      "Iteration 54, loss = 2.27767566\n",
      "Iteration 55, loss = 2.27442447\n",
      "Iteration 56, loss = 2.27030886\n",
      "Iteration 57, loss = 2.26537356\n",
      "Iteration 58, loss = 2.25940203\n",
      "Iteration 59, loss = 2.25158434\n",
      "Iteration 60, loss = 2.24152753\n",
      "Iteration 61, loss = 2.22807516\n",
      "Iteration 62, loss = 2.21017891\n",
      "Iteration 63, loss = 2.18626230\n",
      "Iteration 64, loss = 2.15417318\n",
      "Iteration 65, loss = 2.11203128\n",
      "Iteration 66, loss = 2.05987983\n",
      "Iteration 67, loss = 2.00116084\n",
      "Iteration 68, loss = 1.94150211\n",
      "Iteration 69, loss = 1.88658830\n",
      "Iteration 70, loss = 1.83948723\n",
      "Iteration 71, loss = 1.79957148\n",
      "Iteration 72, loss = 1.76510086\n",
      "Iteration 73, loss = 1.73407817\n",
      "Iteration 74, loss = 1.70454607\n",
      "Iteration 75, loss = 1.67477556\n",
      "Iteration 76, loss = 1.64348966\n",
      "Iteration 77, loss = 1.60968175\n",
      "Iteration 78, loss = 1.57270196\n",
      "Iteration 79, loss = 1.53295130\n",
      "Iteration 80, loss = 1.49135778\n",
      "Iteration 81, loss = 1.44969792\n",
      "Iteration 82, loss = 1.40903469\n",
      "Iteration 83, loss = 1.37111687\n",
      "Iteration 84, loss = 1.33645097\n",
      "Iteration 85, loss = 1.30553147\n",
      "Iteration 86, loss = 1.27755593\n",
      "Iteration 87, loss = 1.25290829\n",
      "Iteration 88, loss = 1.23046653\n",
      "Iteration 89, loss = 1.21004283\n",
      "Iteration 90, loss = 1.19111188\n",
      "Iteration 91, loss = 1.17327344\n",
      "Iteration 92, loss = 1.15640278\n",
      "Iteration 93, loss = 1.14053595\n",
      "Iteration 94, loss = 1.12504335\n",
      "Iteration 95, loss = 1.11012884\n",
      "Iteration 96, loss = 1.09580663\n",
      "Iteration 97, loss = 1.08197690\n",
      "Iteration 98, loss = 1.06852869\n",
      "Iteration 99, loss = 1.05546120\n",
      "Iteration 100, loss = 1.04250586\n",
      "Iteration 101, loss = 1.02991222\n",
      "Iteration 102, loss = 1.01752412\n",
      "Iteration 103, loss = 1.00542709\n",
      "Iteration 104, loss = 0.99335883\n",
      "Iteration 105, loss = 0.98156405\n",
      "Iteration 106, loss = 0.96955116\n",
      "Iteration 107, loss = 0.95786607\n",
      "Iteration 108, loss = 0.94603293\n",
      "Iteration 109, loss = 0.93411160\n",
      "Iteration 110, loss = 0.92233078\n",
      "Iteration 111, loss = 0.91020461\n",
      "Iteration 112, loss = 0.89803947\n",
      "Iteration 113, loss = 0.88567177\n",
      "Iteration 114, loss = 0.87334776\n",
      "Iteration 115, loss = 0.86089310\n",
      "Iteration 116, loss = 0.84846562\n",
      "Iteration 117, loss = 0.83612704\n",
      "Iteration 118, loss = 0.82373175\n",
      "Iteration 119, loss = 0.81156027\n",
      "Iteration 120, loss = 0.79971779\n",
      "Iteration 121, loss = 0.78813782\n",
      "Iteration 122, loss = 0.77693453\n",
      "Iteration 123, loss = 0.76605854\n",
      "Iteration 124, loss = 0.75525362\n",
      "Iteration 125, loss = 0.74473755\n",
      "Iteration 126, loss = 0.73475718\n",
      "Iteration 127, loss = 0.72497330\n",
      "Iteration 128, loss = 0.71527766\n",
      "Iteration 129, loss = 0.70585782\n",
      "Iteration 130, loss = 0.69674762\n",
      "Iteration 131, loss = 0.68770285\n",
      "Iteration 132, loss = 0.67881919\n",
      "Iteration 133, loss = 0.67028865\n",
      "Iteration 134, loss = 0.66169775\n",
      "Iteration 135, loss = 0.65347850\n",
      "Iteration 136, loss = 0.64539259\n",
      "Iteration 137, loss = 0.63732132\n",
      "Iteration 138, loss = 0.62966892\n",
      "Iteration 139, loss = 0.62191258\n",
      "Iteration 140, loss = 0.61461780\n",
      "Iteration 141, loss = 0.60738507\n",
      "Iteration 142, loss = 0.60031613\n",
      "Iteration 143, loss = 0.59357762\n",
      "Iteration 144, loss = 0.58683706\n",
      "Iteration 145, loss = 0.58032598\n",
      "Iteration 146, loss = 0.57402283\n",
      "Iteration 147, loss = 0.56799516\n",
      "Iteration 148, loss = 0.56210586\n",
      "Iteration 149, loss = 0.55634058\n",
      "Iteration 150, loss = 0.55077630\n",
      "Iteration 151, loss = 0.54533311\n",
      "Iteration 152, loss = 0.54012798\n",
      "Iteration 153, loss = 0.53510534\n",
      "Iteration 154, loss = 0.53006362\n",
      "Iteration 155, loss = 0.52515862\n",
      "Iteration 156, loss = 0.52049584\n",
      "Iteration 157, loss = 0.51595748\n",
      "Iteration 158, loss = 0.51149867\n",
      "Iteration 159, loss = 0.50712090\n",
      "Iteration 160, loss = 0.50297650\n",
      "Iteration 161, loss = 0.49877864\n",
      "Iteration 162, loss = 0.49470013\n",
      "Iteration 163, loss = 0.49093595\n",
      "Iteration 164, loss = 0.48688941\n",
      "Iteration 165, loss = 0.48320327\n",
      "Iteration 166, loss = 0.47949739\n",
      "Iteration 167, loss = 0.47580925\n",
      "Iteration 168, loss = 0.47228101\n",
      "Iteration 169, loss = 0.46883322\n",
      "Iteration 170, loss = 0.46518777\n",
      "Iteration 171, loss = 0.46197416\n",
      "Iteration 172, loss = 0.45872490\n",
      "Iteration 173, loss = 0.45534597\n",
      "Iteration 174, loss = 0.45206719\n",
      "Iteration 175, loss = 0.44896597\n",
      "Iteration 176, loss = 0.44579893\n",
      "Iteration 177, loss = 0.44271846\n",
      "Iteration 178, loss = 0.43954849\n",
      "Iteration 179, loss = 0.43681960\n",
      "Iteration 180, loss = 0.43362747\n",
      "Iteration 181, loss = 0.43056095\n",
      "Iteration 182, loss = 0.42784391\n",
      "Iteration 183, loss = 0.42494280\n",
      "Iteration 184, loss = 0.42195030\n",
      "Iteration 185, loss = 0.41920906\n",
      "Iteration 186, loss = 0.41644091\n",
      "Iteration 187, loss = 0.41377016\n",
      "Iteration 188, loss = 0.41092020\n",
      "Iteration 189, loss = 0.40818664\n",
      "Iteration 190, loss = 0.40539717\n",
      "Iteration 191, loss = 0.40272130\n",
      "Iteration 192, loss = 0.40009678\n",
      "Iteration 193, loss = 0.39755221\n",
      "Iteration 194, loss = 0.39480075\n",
      "Iteration 195, loss = 0.39228387\n",
      "Iteration 196, loss = 0.38980399\n",
      "Iteration 197, loss = 0.38728521\n",
      "Iteration 198, loss = 0.38470911\n",
      "Iteration 199, loss = 0.38218592\n",
      "Iteration 200, loss = 0.37974614\n",
      "[CV] END activation=logistic, alpha=0.0001, batch_size=100, hidden_layer_sizes=(100, 100, 100), solver=sgd; total time= 3.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.30487053\n",
      "Iteration 2, loss = 2.30164421\n",
      "Iteration 3, loss = 2.30174939\n",
      "Iteration 4, loss = 2.30153773\n",
      "Iteration 5, loss = 2.30140869\n",
      "Iteration 6, loss = 2.30138098\n",
      "Iteration 7, loss = 2.30116972\n",
      "Iteration 8, loss = 2.30113781\n",
      "Iteration 9, loss = 2.30103092\n",
      "Iteration 10, loss = 2.30107751\n",
      "Iteration 11, loss = 2.30090587\n",
      "Iteration 12, loss = 2.30075977\n",
      "Iteration 13, loss = 2.30075951\n",
      "Iteration 14, loss = 2.30083149\n",
      "Iteration 15, loss = 2.30041199\n",
      "Iteration 16, loss = 2.30052574\n",
      "Iteration 17, loss = 2.30014697\n",
      "Iteration 18, loss = 2.30030509\n",
      "Iteration 19, loss = 2.30007871\n",
      "Iteration 20, loss = 2.29977087\n",
      "Iteration 21, loss = 2.29979197\n",
      "Iteration 22, loss = 2.29978848\n",
      "Iteration 23, loss = 2.29923989\n",
      "Iteration 24, loss = 2.29937147\n",
      "Iteration 25, loss = 2.29908511\n",
      "Iteration 26, loss = 2.29901121\n",
      "Iteration 27, loss = 2.29873803\n",
      "Iteration 28, loss = 2.29866390\n",
      "Iteration 29, loss = 2.29840996\n",
      "Iteration 30, loss = 2.29828173\n",
      "Iteration 31, loss = 2.29802594\n",
      "Iteration 32, loss = 2.29771187\n",
      "Iteration 33, loss = 2.29749445\n",
      "Iteration 34, loss = 2.29701725\n",
      "Iteration 35, loss = 2.29695899\n",
      "Iteration 36, loss = 2.29646565\n",
      "Iteration 37, loss = 2.29610644\n",
      "Iteration 38, loss = 2.29573117\n",
      "Iteration 39, loss = 2.29566538\n",
      "Iteration 40, loss = 2.29493349\n",
      "Iteration 41, loss = 2.29433832\n",
      "Iteration 42, loss = 2.29396269\n",
      "Iteration 43, loss = 2.29345606\n",
      "Iteration 44, loss = 2.29285074\n",
      "Iteration 45, loss = 2.29215192\n",
      "Iteration 46, loss = 2.29130051\n",
      "Iteration 47, loss = 2.29067220\n",
      "Iteration 48, loss = 2.28966496\n",
      "Iteration 49, loss = 2.28845575\n",
      "Iteration 50, loss = 2.28729304\n",
      "Iteration 51, loss = 2.28573266\n",
      "Iteration 52, loss = 2.28424939\n",
      "Iteration 53, loss = 2.28238421\n",
      "Iteration 54, loss = 2.28023041\n",
      "Iteration 55, loss = 2.27766207\n",
      "Iteration 56, loss = 2.27486894\n",
      "Iteration 57, loss = 2.27108453\n",
      "Iteration 58, loss = 2.26689095\n",
      "Iteration 59, loss = 2.26142490\n",
      "Iteration 60, loss = 2.25501538\n",
      "Iteration 61, loss = 2.24665831\n",
      "Iteration 62, loss = 2.23594391\n",
      "Iteration 63, loss = 2.22233444\n",
      "Iteration 64, loss = 2.20428774\n",
      "Iteration 65, loss = 2.18014271\n",
      "Iteration 66, loss = 2.14900671\n",
      "Iteration 67, loss = 2.10913345\n",
      "Iteration 68, loss = 2.06065786\n",
      "Iteration 69, loss = 2.00561935\n",
      "Iteration 70, loss = 1.94820532\n",
      "Iteration 71, loss = 1.89139028\n",
      "Iteration 72, loss = 1.83673117\n",
      "Iteration 73, loss = 1.78423355\n",
      "Iteration 74, loss = 1.73242370\n",
      "Iteration 75, loss = 1.68056487\n",
      "Iteration 76, loss = 1.62735638\n",
      "Iteration 77, loss = 1.57294419\n",
      "Iteration 78, loss = 1.51906060\n",
      "Iteration 79, loss = 1.46733578\n",
      "Iteration 80, loss = 1.42009162\n",
      "Iteration 81, loss = 1.37791857\n",
      "Iteration 82, loss = 1.34153386\n",
      "Iteration 83, loss = 1.31020702\n",
      "Iteration 84, loss = 1.28311112\n",
      "Iteration 85, loss = 1.25932836\n",
      "Iteration 86, loss = 1.23812911\n",
      "Iteration 87, loss = 1.21876219\n",
      "Iteration 88, loss = 1.20075288\n",
      "Iteration 89, loss = 1.18361296\n",
      "Iteration 90, loss = 1.16750037\n",
      "Iteration 91, loss = 1.15133938\n",
      "Iteration 92, loss = 1.13529243\n",
      "Iteration 93, loss = 1.11921042\n",
      "Iteration 94, loss = 1.10266880\n",
      "Iteration 95, loss = 1.08548734\n",
      "Iteration 96, loss = 1.06690293\n",
      "Iteration 97, loss = 1.04747886\n",
      "Iteration 98, loss = 1.02678260\n",
      "Iteration 99, loss = 1.00473556\n",
      "Iteration 100, loss = 0.98105865\n",
      "Iteration 101, loss = 0.95664296\n",
      "Iteration 102, loss = 0.93164917\n",
      "Iteration 103, loss = 0.90674474\n",
      "Iteration 104, loss = 0.88245945\n",
      "Iteration 105, loss = 0.85940695\n",
      "Iteration 106, loss = 0.83815025\n",
      "Iteration 107, loss = 0.81824526\n",
      "Iteration 108, loss = 0.80043192\n",
      "Iteration 109, loss = 0.78388283\n",
      "Iteration 110, loss = 0.76872061\n",
      "Iteration 111, loss = 0.75480094\n",
      "Iteration 112, loss = 0.74179015\n",
      "Iteration 113, loss = 0.72992634\n",
      "Iteration 114, loss = 0.71860877\n",
      "Iteration 115, loss = 0.70795752\n",
      "Iteration 116, loss = 0.69775978\n",
      "Iteration 117, loss = 0.68805133\n",
      "Iteration 118, loss = 0.67881448\n",
      "Iteration 119, loss = 0.67012786\n",
      "Iteration 120, loss = 0.66170317\n",
      "Iteration 121, loss = 0.65345540\n",
      "Iteration 122, loss = 0.64558856\n",
      "Iteration 123, loss = 0.63802886\n",
      "Iteration 124, loss = 0.63065762\n",
      "Iteration 125, loss = 0.62367464\n",
      "Iteration 126, loss = 0.61661355\n",
      "Iteration 127, loss = 0.61012080\n",
      "Iteration 128, loss = 0.60365403\n",
      "Iteration 129, loss = 0.59737021\n",
      "Iteration 130, loss = 0.59143827\n",
      "Iteration 131, loss = 0.58554325\n",
      "Iteration 132, loss = 0.57976127\n",
      "Iteration 133, loss = 0.57414599\n",
      "Iteration 134, loss = 0.56883350\n",
      "Iteration 135, loss = 0.56352674\n",
      "Iteration 136, loss = 0.55835849\n",
      "Iteration 137, loss = 0.55352176\n",
      "Iteration 138, loss = 0.54868693\n",
      "Iteration 139, loss = 0.54400411\n",
      "Iteration 140, loss = 0.53935425\n",
      "Iteration 141, loss = 0.53482431\n",
      "Iteration 142, loss = 0.53038561\n",
      "Iteration 143, loss = 0.52624071\n",
      "Iteration 144, loss = 0.52209530\n",
      "Iteration 145, loss = 0.51805799\n",
      "Iteration 146, loss = 0.51400309\n",
      "Iteration 147, loss = 0.51019194\n",
      "Iteration 148, loss = 0.50638127\n",
      "Iteration 149, loss = 0.50257780\n",
      "Iteration 150, loss = 0.49887176\n",
      "Iteration 151, loss = 0.49545942\n",
      "Iteration 152, loss = 0.49176495\n",
      "Iteration 153, loss = 0.48837720\n",
      "Iteration 154, loss = 0.48480502\n",
      "Iteration 155, loss = 0.48162838\n",
      "Iteration 156, loss = 0.47813635\n",
      "Iteration 157, loss = 0.47507243\n",
      "Iteration 158, loss = 0.47180278\n",
      "Iteration 159, loss = 0.46852550\n",
      "Iteration 160, loss = 0.46550810\n",
      "Iteration 161, loss = 0.46238180\n",
      "Iteration 162, loss = 0.45929781\n",
      "Iteration 163, loss = 0.45612906\n",
      "Iteration 164, loss = 0.45329099\n",
      "Iteration 165, loss = 0.45031061\n",
      "Iteration 166, loss = 0.44751068\n",
      "Iteration 167, loss = 0.44454008\n",
      "Iteration 168, loss = 0.44154666\n",
      "Iteration 169, loss = 0.43882970\n",
      "Iteration 170, loss = 0.43592808\n",
      "Iteration 171, loss = 0.43318502\n",
      "Iteration 172, loss = 0.43028870\n",
      "Iteration 173, loss = 0.42756339\n",
      "Iteration 174, loss = 0.42478493\n",
      "Iteration 175, loss = 0.42219456\n",
      "Iteration 176, loss = 0.41925959\n",
      "Iteration 177, loss = 0.41685993\n",
      "Iteration 178, loss = 0.41397836\n",
      "Iteration 179, loss = 0.41126044\n",
      "Iteration 180, loss = 0.40873625\n",
      "Iteration 181, loss = 0.40604933\n",
      "Iteration 182, loss = 0.40347008\n",
      "Iteration 183, loss = 0.40088481\n",
      "Iteration 184, loss = 0.39824158\n",
      "Iteration 185, loss = 0.39583302\n",
      "Iteration 186, loss = 0.39319454\n",
      "Iteration 187, loss = 0.39049722\n",
      "Iteration 188, loss = 0.38812259\n",
      "Iteration 189, loss = 0.38561599\n",
      "Iteration 190, loss = 0.38310512\n",
      "Iteration 191, loss = 0.38056232\n",
      "Iteration 192, loss = 0.37799295\n",
      "Iteration 193, loss = 0.37579672\n",
      "Iteration 194, loss = 0.37325788\n",
      "Iteration 195, loss = 0.37090966\n",
      "Iteration 196, loss = 0.36853824\n",
      "Iteration 197, loss = 0.36609053\n",
      "Iteration 198, loss = 0.36373012\n",
      "Iteration 199, loss = 0.36132921\n",
      "Iteration 200, loss = 0.35910458\n",
      "[CV] END activation=logistic, alpha=0.0001, batch_size=100, hidden_layer_sizes=(100, 100, 100), solver=sgd; total time= 3.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.30902356\n",
      "Iteration 2, loss = 2.30149900\n",
      "Iteration 3, loss = 2.30147284\n",
      "Iteration 4, loss = 2.30155743\n",
      "Iteration 5, loss = 2.30131814\n",
      "Iteration 6, loss = 2.30126296\n",
      "Iteration 7, loss = 2.30102429\n",
      "Iteration 8, loss = 2.30113279\n",
      "Iteration 9, loss = 2.30104261\n",
      "Iteration 10, loss = 2.30081477\n",
      "Iteration 11, loss = 2.30081731\n",
      "Iteration 12, loss = 2.30094160\n",
      "Iteration 13, loss = 2.30077394\n",
      "Iteration 14, loss = 2.30070149\n",
      "Iteration 15, loss = 2.30045840\n",
      "Iteration 16, loss = 2.30044700\n",
      "Iteration 17, loss = 2.30043860\n",
      "Iteration 18, loss = 2.30019884\n",
      "Iteration 19, loss = 2.30021235\n",
      "Iteration 20, loss = 2.30010343\n",
      "Iteration 21, loss = 2.30001274\n",
      "Iteration 22, loss = 2.29961226\n",
      "Iteration 23, loss = 2.29966720\n",
      "Iteration 24, loss = 2.29935156\n",
      "Iteration 25, loss = 2.29941874\n",
      "Iteration 26, loss = 2.29917137\n",
      "Iteration 27, loss = 2.29901077\n",
      "Iteration 28, loss = 2.29898103\n",
      "Iteration 29, loss = 2.29867170\n",
      "Iteration 30, loss = 2.29857770\n",
      "Iteration 31, loss = 2.29836122\n",
      "Iteration 32, loss = 2.29800774\n",
      "Iteration 33, loss = 2.29788520\n",
      "Iteration 34, loss = 2.29746765\n",
      "Iteration 35, loss = 2.29739553\n",
      "Iteration 36, loss = 2.29695233\n",
      "Iteration 37, loss = 2.29683353\n",
      "Iteration 38, loss = 2.29629299\n",
      "Iteration 39, loss = 2.29598468\n",
      "Iteration 40, loss = 2.29556194\n",
      "Iteration 41, loss = 2.29508286\n",
      "Iteration 42, loss = 2.29457825\n",
      "Iteration 43, loss = 2.29414382\n",
      "Iteration 44, loss = 2.29352845\n",
      "Iteration 45, loss = 2.29273762\n",
      "Iteration 46, loss = 2.29213465\n",
      "Iteration 47, loss = 2.29142822\n",
      "Iteration 48, loss = 2.29036885\n",
      "Iteration 49, loss = 2.28926098\n",
      "Iteration 50, loss = 2.28814250\n",
      "Iteration 51, loss = 2.28690978\n",
      "Iteration 52, loss = 2.28526409\n",
      "Iteration 53, loss = 2.28344525\n",
      "Iteration 54, loss = 2.28113523\n",
      "Iteration 55, loss = 2.27840422\n",
      "Iteration 56, loss = 2.27550833\n",
      "Iteration 57, loss = 2.27166699\n",
      "Iteration 58, loss = 2.26714806\n",
      "Iteration 59, loss = 2.26137194\n",
      "Iteration 60, loss = 2.25407567\n",
      "Iteration 61, loss = 2.24446957\n",
      "Iteration 62, loss = 2.23158253\n",
      "Iteration 63, loss = 2.21469184\n",
      "Iteration 64, loss = 2.19166039\n",
      "Iteration 65, loss = 2.16006867\n",
      "Iteration 66, loss = 2.11757019\n",
      "Iteration 67, loss = 2.06364729\n",
      "Iteration 68, loss = 2.00042414\n",
      "Iteration 69, loss = 1.93487266\n",
      "Iteration 70, loss = 1.87404396\n",
      "Iteration 71, loss = 1.82185102\n",
      "Iteration 72, loss = 1.77825327\n",
      "Iteration 73, loss = 1.74128273\n",
      "Iteration 74, loss = 1.70827905\n",
      "Iteration 75, loss = 1.67797221\n",
      "Iteration 76, loss = 1.64855440\n",
      "Iteration 77, loss = 1.61871805\n",
      "Iteration 78, loss = 1.58874181\n",
      "Iteration 79, loss = 1.55799383\n",
      "Iteration 80, loss = 1.52693135\n",
      "Iteration 81, loss = 1.49583369\n",
      "Iteration 82, loss = 1.46552800\n",
      "Iteration 83, loss = 1.43598495\n",
      "Iteration 84, loss = 1.40775347\n",
      "Iteration 85, loss = 1.38041343\n",
      "Iteration 86, loss = 1.35400450\n",
      "Iteration 87, loss = 1.32852383\n",
      "Iteration 88, loss = 1.30377557\n",
      "Iteration 89, loss = 1.27947390\n",
      "Iteration 90, loss = 1.25637483\n",
      "Iteration 91, loss = 1.23386202\n",
      "Iteration 92, loss = 1.21254436\n",
      "Iteration 93, loss = 1.19218891\n",
      "Iteration 94, loss = 1.17277546\n",
      "Iteration 95, loss = 1.15476671\n",
      "Iteration 96, loss = 1.13751907\n",
      "Iteration 97, loss = 1.12109472\n",
      "Iteration 98, loss = 1.10581804\n",
      "Iteration 99, loss = 1.09120067\n",
      "Iteration 100, loss = 1.07713197\n",
      "Iteration 101, loss = 1.06375092\n",
      "Iteration 102, loss = 1.05074392\n",
      "Iteration 103, loss = 1.03835654\n",
      "Iteration 104, loss = 1.02599522\n",
      "Iteration 105, loss = 1.01405237\n",
      "Iteration 106, loss = 1.00253189\n",
      "Iteration 107, loss = 0.99117111\n",
      "Iteration 108, loss = 0.97998251\n",
      "Iteration 109, loss = 0.96895788\n",
      "Iteration 110, loss = 0.95821783\n",
      "Iteration 111, loss = 0.94775547\n",
      "Iteration 112, loss = 0.93745901\n",
      "Iteration 113, loss = 0.92695373\n",
      "Iteration 114, loss = 0.91737263\n",
      "Iteration 115, loss = 0.90783813\n",
      "Iteration 116, loss = 0.89844383\n",
      "Iteration 117, loss = 0.88912486\n",
      "Iteration 118, loss = 0.88037254\n",
      "Iteration 119, loss = 0.87139622\n",
      "Iteration 120, loss = 0.86291269\n",
      "Iteration 121, loss = 0.85459542\n",
      "Iteration 122, loss = 0.84628721\n",
      "Iteration 123, loss = 0.83830748\n",
      "Iteration 124, loss = 0.83034215\n",
      "Iteration 125, loss = 0.82260797\n",
      "Iteration 126, loss = 0.81473011\n",
      "Iteration 127, loss = 0.80717870\n",
      "Iteration 128, loss = 0.79954292\n",
      "Iteration 129, loss = 0.79198906\n",
      "Iteration 130, loss = 0.78453403\n",
      "Iteration 131, loss = 0.77701797\n",
      "Iteration 132, loss = 0.76950517\n",
      "Iteration 133, loss = 0.76200075\n",
      "Iteration 134, loss = 0.75464524\n",
      "Iteration 135, loss = 0.74716909\n",
      "Iteration 136, loss = 0.73976803\n",
      "Iteration 137, loss = 0.73223241\n",
      "Iteration 138, loss = 0.72461211\n",
      "Iteration 139, loss = 0.71706111\n",
      "Iteration 140, loss = 0.70936410\n",
      "Iteration 141, loss = 0.70170036\n",
      "Iteration 142, loss = 0.69390182\n",
      "Iteration 143, loss = 0.68628150\n",
      "Iteration 144, loss = 0.67829010\n",
      "Iteration 145, loss = 0.67048077\n",
      "Iteration 146, loss = 0.66254809\n",
      "Iteration 147, loss = 0.65477103\n",
      "Iteration 148, loss = 0.64671227\n",
      "Iteration 149, loss = 0.63864105\n",
      "Iteration 150, loss = 0.63059510\n",
      "Iteration 151, loss = 0.62291995\n",
      "Iteration 152, loss = 0.61486876\n",
      "Iteration 153, loss = 0.60683795\n",
      "Iteration 154, loss = 0.59947129\n",
      "Iteration 155, loss = 0.59179394\n",
      "Iteration 156, loss = 0.58425366\n",
      "Iteration 157, loss = 0.57688551\n",
      "Iteration 158, loss = 0.56966599\n",
      "Iteration 159, loss = 0.56266522\n",
      "Iteration 160, loss = 0.55575883\n",
      "Iteration 161, loss = 0.54928552\n",
      "Iteration 162, loss = 0.54301452\n",
      "Iteration 163, loss = 0.53679585\n",
      "Iteration 164, loss = 0.53098133\n",
      "Iteration 165, loss = 0.52490859\n",
      "Iteration 166, loss = 0.51930095\n",
      "Iteration 167, loss = 0.51396873\n",
      "Iteration 168, loss = 0.50860102\n",
      "Iteration 169, loss = 0.50328998\n",
      "Iteration 170, loss = 0.49846290\n",
      "Iteration 171, loss = 0.49364816\n",
      "Iteration 172, loss = 0.48916424\n",
      "Iteration 173, loss = 0.48442645\n",
      "Iteration 174, loss = 0.47998586\n",
      "Iteration 175, loss = 0.47566430\n",
      "Iteration 176, loss = 0.47147183\n",
      "Iteration 177, loss = 0.46733770\n",
      "Iteration 178, loss = 0.46352324\n",
      "Iteration 179, loss = 0.45945965\n",
      "Iteration 180, loss = 0.45565543\n",
      "Iteration 181, loss = 0.45205495\n",
      "Iteration 182, loss = 0.44843111\n",
      "Iteration 183, loss = 0.44487260\n",
      "Iteration 184, loss = 0.44127696\n",
      "Iteration 185, loss = 0.43788311\n",
      "Iteration 186, loss = 0.43469289\n",
      "Iteration 187, loss = 0.43134267\n",
      "Iteration 188, loss = 0.42816008\n",
      "Iteration 189, loss = 0.42505788\n",
      "Iteration 190, loss = 0.42199266\n",
      "Iteration 191, loss = 0.41886731\n",
      "Iteration 192, loss = 0.41600809\n",
      "Iteration 193, loss = 0.41289049\n",
      "Iteration 194, loss = 0.40991961\n",
      "Iteration 195, loss = 0.40714581\n",
      "Iteration 196, loss = 0.40419693\n",
      "Iteration 197, loss = 0.40155693\n",
      "Iteration 198, loss = 0.39858399\n",
      "Iteration 199, loss = 0.39599257\n",
      "Iteration 200, loss = 0.39337098\n",
      "[CV] END activation=logistic, alpha=0.0001, batch_size=100, hidden_layer_sizes=(100, 100, 100), solver=sgd; total time= 3.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.14949665\n",
      "Iteration 2, loss = 0.45023357\n",
      "Iteration 3, loss = 0.32831091\n",
      "Iteration 4, loss = 0.27703839\n",
      "Iteration 5, loss = 0.24549278\n",
      "Iteration 6, loss = 0.22176908\n",
      "Iteration 7, loss = 0.20250749\n",
      "Iteration 8, loss = 0.18706625\n",
      "Iteration 9, loss = 0.17275337\n",
      "Iteration 10, loss = 0.16087733\n",
      "Iteration 11, loss = 0.15076461\n",
      "Iteration 12, loss = 0.14087912\n",
      "Iteration 13, loss = 0.13206165\n",
      "Iteration 14, loss = 0.12449991\n",
      "Iteration 15, loss = 0.11754829\n",
      "Iteration 16, loss = 0.11019224\n",
      "Iteration 17, loss = 0.10407276\n",
      "Iteration 18, loss = 0.09829878\n",
      "Iteration 19, loss = 0.09306557\n",
      "Iteration 20, loss = 0.08765382\n",
      "Iteration 21, loss = 0.08318854\n",
      "Iteration 22, loss = 0.07861550\n",
      "Iteration 23, loss = 0.07434591\n",
      "Iteration 24, loss = 0.07056235\n",
      "Iteration 25, loss = 0.06674516\n",
      "Iteration 26, loss = 0.06327146\n",
      "Iteration 27, loss = 0.05984104\n",
      "Iteration 28, loss = 0.05673779\n",
      "Iteration 29, loss = 0.05389881\n",
      "Iteration 30, loss = 0.05095673\n",
      "Iteration 31, loss = 0.04824641\n",
      "Iteration 32, loss = 0.04574017\n",
      "Iteration 33, loss = 0.04328163\n",
      "Iteration 34, loss = 0.04108462\n",
      "Iteration 35, loss = 0.03896249\n",
      "Iteration 36, loss = 0.03680876\n",
      "Iteration 37, loss = 0.03476419\n",
      "Iteration 38, loss = 0.03294714\n",
      "Iteration 39, loss = 0.03098597\n",
      "Iteration 40, loss = 0.02929657\n",
      "Iteration 41, loss = 0.02773563\n",
      "Iteration 42, loss = 0.02627817\n",
      "Iteration 43, loss = 0.02484797\n",
      "Iteration 44, loss = 0.02348517\n",
      "Iteration 45, loss = 0.02225124\n",
      "Iteration 46, loss = 0.02089711\n",
      "Iteration 47, loss = 0.01993299\n",
      "Iteration 48, loss = 0.01874172\n",
      "Iteration 49, loss = 0.01776224\n",
      "Iteration 50, loss = 0.01694711\n",
      "Iteration 51, loss = 0.01586361\n",
      "Iteration 52, loss = 0.01499182\n",
      "Iteration 53, loss = 0.01419802\n",
      "Iteration 54, loss = 0.01347551\n",
      "Iteration 55, loss = 0.01275910\n",
      "Iteration 56, loss = 0.01216158\n",
      "Iteration 57, loss = 0.01133425\n",
      "Iteration 58, loss = 0.01080066\n",
      "Iteration 59, loss = 0.01030638\n",
      "Iteration 60, loss = 0.00986131\n",
      "Iteration 61, loss = 0.00929233\n",
      "Iteration 62, loss = 0.00882875\n",
      "Iteration 63, loss = 0.00837429\n",
      "Iteration 64, loss = 0.00802815\n",
      "Iteration 65, loss = 0.00765668\n",
      "Iteration 66, loss = 0.00733252\n",
      "Iteration 67, loss = 0.00691800\n",
      "Iteration 68, loss = 0.00668474\n",
      "Iteration 69, loss = 0.00636353\n",
      "Iteration 70, loss = 0.00616051\n",
      "Iteration 71, loss = 0.00591352\n",
      "Iteration 72, loss = 0.00564509\n",
      "Iteration 73, loss = 0.00542090\n",
      "Iteration 74, loss = 0.00518339\n",
      "Iteration 75, loss = 0.00498793\n",
      "Iteration 76, loss = 0.00481470\n",
      "Iteration 77, loss = 0.00466641\n",
      "Iteration 78, loss = 0.00448476\n",
      "Iteration 79, loss = 0.00432044\n",
      "Iteration 80, loss = 0.00417290\n",
      "Iteration 81, loss = 0.00404553\n",
      "Iteration 82, loss = 0.00388980\n",
      "Iteration 83, loss = 0.00381225\n",
      "Iteration 84, loss = 0.00370446\n",
      "Iteration 85, loss = 0.00359082\n",
      "Iteration 86, loss = 0.00348141\n",
      "Iteration 87, loss = 0.00339125\n",
      "Iteration 88, loss = 0.00330056\n",
      "Iteration 89, loss = 0.00323219\n",
      "Iteration 90, loss = 0.00313146\n",
      "Iteration 91, loss = 0.00306822\n",
      "Iteration 92, loss = 0.00300405\n",
      "Iteration 93, loss = 0.00293919\n",
      "Iteration 94, loss = 0.00287301\n",
      "Iteration 95, loss = 0.00280713\n",
      "Iteration 96, loss = 0.00275209\n",
      "Iteration 97, loss = 0.00269163\n",
      "Iteration 98, loss = 0.00264501\n",
      "Iteration 99, loss = 0.00259479\n",
      "Iteration 100, loss = 0.00255025\n",
      "Iteration 101, loss = 0.00249988\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.0001, batch_size=200, hidden_layer_sizes=(100,), solver=adam; total time=  50.4s\n",
      "Iteration 1, loss = 1.14287464\n",
      "Iteration 2, loss = 0.44367693\n",
      "Iteration 3, loss = 0.32741078\n",
      "Iteration 4, loss = 0.27735908\n",
      "Iteration 5, loss = 0.24598103\n",
      "Iteration 6, loss = 0.22300698\n",
      "Iteration 7, loss = 0.20365502\n",
      "Iteration 8, loss = 0.18796151\n",
      "Iteration 9, loss = 0.17431435\n",
      "Iteration 10, loss = 0.16221527\n",
      "Iteration 11, loss = 0.15144123\n",
      "Iteration 12, loss = 0.14185349\n",
      "Iteration 13, loss = 0.13329910\n",
      "Iteration 14, loss = 0.12532841\n",
      "Iteration 15, loss = 0.11810677\n",
      "Iteration 16, loss = 0.11090623\n",
      "Iteration 17, loss = 0.10476320\n",
      "Iteration 18, loss = 0.09847455\n",
      "Iteration 19, loss = 0.09332872\n",
      "Iteration 20, loss = 0.08825599\n",
      "Iteration 21, loss = 0.08302234\n",
      "Iteration 22, loss = 0.07869620\n",
      "Iteration 23, loss = 0.07462985\n",
      "Iteration 24, loss = 0.07033925\n",
      "Iteration 25, loss = 0.06631165\n",
      "Iteration 26, loss = 0.06293392\n",
      "Iteration 27, loss = 0.05963079\n",
      "Iteration 28, loss = 0.05628928\n",
      "Iteration 29, loss = 0.05330807\n",
      "Iteration 30, loss = 0.05045522\n",
      "Iteration 31, loss = 0.04761193\n",
      "Iteration 32, loss = 0.04507169\n",
      "Iteration 33, loss = 0.04276125\n",
      "Iteration 34, loss = 0.04031294\n",
      "Iteration 35, loss = 0.03831707\n",
      "Iteration 36, loss = 0.03600207\n",
      "Iteration 37, loss = 0.03412577\n",
      "Iteration 38, loss = 0.03215805\n",
      "Iteration 39, loss = 0.03032357\n",
      "Iteration 40, loss = 0.02869836\n",
      "Iteration 41, loss = 0.02747090\n",
      "Iteration 42, loss = 0.02548230\n",
      "Iteration 43, loss = 0.02403907\n",
      "Iteration 44, loss = 0.02287306\n",
      "Iteration 45, loss = 0.02157477\n",
      "Iteration 46, loss = 0.02030873\n",
      "Iteration 47, loss = 0.01932765\n",
      "Iteration 48, loss = 0.01826213\n",
      "Iteration 49, loss = 0.01713682\n",
      "Iteration 50, loss = 0.01626201\n",
      "Iteration 51, loss = 0.01532143\n",
      "Iteration 52, loss = 0.01457727\n",
      "Iteration 53, loss = 0.01373811\n",
      "Iteration 54, loss = 0.01296881\n",
      "Iteration 55, loss = 0.01234535\n",
      "Iteration 56, loss = 0.01159587\n",
      "Iteration 57, loss = 0.01103678\n",
      "Iteration 58, loss = 0.01044738\n",
      "Iteration 59, loss = 0.00990299\n",
      "Iteration 60, loss = 0.00945810\n",
      "Iteration 61, loss = 0.00901120\n",
      "Iteration 62, loss = 0.00856386\n",
      "Iteration 63, loss = 0.00815997\n",
      "Iteration 64, loss = 0.00771651\n",
      "Iteration 65, loss = 0.00741170\n",
      "Iteration 66, loss = 0.00700820\n",
      "Iteration 67, loss = 0.00669721\n",
      "Iteration 68, loss = 0.00646805\n",
      "Iteration 69, loss = 0.00614056\n",
      "Iteration 70, loss = 0.00589895\n",
      "Iteration 71, loss = 0.00564530\n",
      "Iteration 72, loss = 0.00541012\n",
      "Iteration 73, loss = 0.00523549\n",
      "Iteration 74, loss = 0.00501084\n",
      "Iteration 75, loss = 0.00482749\n",
      "Iteration 76, loss = 0.00465422\n",
      "Iteration 77, loss = 0.00447395\n",
      "Iteration 78, loss = 0.00434055\n",
      "Iteration 79, loss = 0.00420714\n",
      "Iteration 80, loss = 0.00404574\n",
      "Iteration 81, loss = 0.00390758\n",
      "Iteration 82, loss = 0.00381475\n",
      "Iteration 83, loss = 0.00367579\n",
      "Iteration 84, loss = 0.00360422\n",
      "Iteration 85, loss = 0.00348541\n",
      "Iteration 86, loss = 0.00340668\n",
      "Iteration 87, loss = 0.00331017\n",
      "Iteration 88, loss = 0.00322404\n",
      "Iteration 89, loss = 0.00312720\n",
      "Iteration 90, loss = 0.00307298\n",
      "Iteration 91, loss = 0.00298698\n",
      "Iteration 92, loss = 0.00293316\n",
      "Iteration 93, loss = 0.00285870\n",
      "Iteration 94, loss = 0.00280924\n",
      "Iteration 95, loss = 0.00274834\n",
      "Iteration 96, loss = 0.00270394\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.0001, batch_size=200, hidden_layer_sizes=(100,), solver=adam; total time=  48.1s\n",
      "Iteration 1, loss = 1.13239571\n",
      "Iteration 2, loss = 0.44866842\n",
      "Iteration 3, loss = 0.33078257\n",
      "Iteration 4, loss = 0.27960451\n",
      "Iteration 5, loss = 0.24652183\n",
      "Iteration 6, loss = 0.22214268\n",
      "Iteration 7, loss = 0.20255483\n",
      "Iteration 8, loss = 0.18581767\n",
      "Iteration 9, loss = 0.17182578\n",
      "Iteration 10, loss = 0.15999174\n",
      "Iteration 11, loss = 0.14926234\n",
      "Iteration 12, loss = 0.13928025\n",
      "Iteration 13, loss = 0.13111495\n",
      "Iteration 14, loss = 0.12309588\n",
      "Iteration 15, loss = 0.11592115\n",
      "Iteration 16, loss = 0.10969291\n",
      "Iteration 17, loss = 0.10302626\n",
      "Iteration 18, loss = 0.09776757\n",
      "Iteration 19, loss = 0.09219889\n",
      "Iteration 20, loss = 0.08715342\n",
      "Iteration 21, loss = 0.08296167\n",
      "Iteration 22, loss = 0.07864102\n",
      "Iteration 23, loss = 0.07456115\n",
      "Iteration 24, loss = 0.07046605\n",
      "Iteration 25, loss = 0.06703395\n",
      "Iteration 26, loss = 0.06344770\n",
      "Iteration 27, loss = 0.05981255\n",
      "Iteration 28, loss = 0.05668114\n",
      "Iteration 29, loss = 0.05400721\n",
      "Iteration 30, loss = 0.05095228\n",
      "Iteration 31, loss = 0.04832120\n",
      "Iteration 32, loss = 0.04586454\n",
      "Iteration 33, loss = 0.04358625\n",
      "Iteration 34, loss = 0.04111667\n",
      "Iteration 35, loss = 0.03905623\n",
      "Iteration 36, loss = 0.03678062\n",
      "Iteration 37, loss = 0.03483247\n",
      "Iteration 38, loss = 0.03296322\n",
      "Iteration 39, loss = 0.03132080\n",
      "Iteration 40, loss = 0.02970748\n",
      "Iteration 41, loss = 0.02789250\n",
      "Iteration 42, loss = 0.02656967\n",
      "Iteration 43, loss = 0.02487823\n",
      "Iteration 44, loss = 0.02365100\n",
      "Iteration 45, loss = 0.02222789\n",
      "Iteration 46, loss = 0.02111004\n",
      "Iteration 47, loss = 0.01998283\n",
      "Iteration 48, loss = 0.01877582\n",
      "Iteration 49, loss = 0.01786804\n",
      "Iteration 50, loss = 0.01677124\n",
      "Iteration 51, loss = 0.01590447\n",
      "Iteration 52, loss = 0.01496353\n",
      "Iteration 53, loss = 0.01411536\n",
      "Iteration 54, loss = 0.01340934\n",
      "Iteration 55, loss = 0.01264146\n",
      "Iteration 56, loss = 0.01194951\n",
      "Iteration 57, loss = 0.01132972\n",
      "Iteration 58, loss = 0.01072155\n",
      "Iteration 59, loss = 0.01014721\n",
      "Iteration 60, loss = 0.00968600\n",
      "Iteration 61, loss = 0.00921698\n",
      "Iteration 62, loss = 0.00870869\n",
      "Iteration 63, loss = 0.00835862\n",
      "Iteration 64, loss = 0.00790288\n",
      "Iteration 65, loss = 0.00761885\n",
      "Iteration 66, loss = 0.00716445\n",
      "Iteration 67, loss = 0.00689232\n",
      "Iteration 68, loss = 0.00658827\n",
      "Iteration 69, loss = 0.00629259\n",
      "Iteration 70, loss = 0.00604664\n",
      "Iteration 71, loss = 0.00581311\n",
      "Iteration 72, loss = 0.00554271\n",
      "Iteration 73, loss = 0.00531945\n",
      "Iteration 74, loss = 0.00510545\n",
      "Iteration 75, loss = 0.00495728\n",
      "Iteration 76, loss = 0.00475802\n",
      "Iteration 77, loss = 0.00458655\n",
      "Iteration 78, loss = 0.00441746\n",
      "Iteration 79, loss = 0.00425732\n",
      "Iteration 80, loss = 0.00411129\n",
      "Iteration 81, loss = 0.00401331\n",
      "Iteration 82, loss = 0.00387965\n",
      "Iteration 83, loss = 0.00375763\n",
      "Iteration 84, loss = 0.00364673\n",
      "Iteration 85, loss = 0.00355197\n",
      "Iteration 86, loss = 0.00342704\n",
      "Iteration 87, loss = 0.00335838\n",
      "Iteration 88, loss = 0.00326839\n",
      "Iteration 89, loss = 0.00316978\n",
      "Iteration 90, loss = 0.00310155\n",
      "Iteration 91, loss = 0.00301970\n",
      "Iteration 92, loss = 0.00296328\n",
      "Iteration 93, loss = 0.00291527\n",
      "Iteration 94, loss = 0.00283762\n",
      "Iteration 95, loss = 0.00277802\n",
      "Iteration 96, loss = 0.00272365\n",
      "Iteration 97, loss = 0.00265934\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.0001, batch_size=200, hidden_layer_sizes=(100,), solver=adam; total time=  49.0s\n",
      "Iteration 1, loss = 1.10793131\n",
      "Iteration 2, loss = 0.43757230\n",
      "Iteration 3, loss = 0.32556768\n",
      "Iteration 4, loss = 0.27754929\n",
      "Iteration 5, loss = 0.24623677\n",
      "Iteration 6, loss = 0.22230960\n",
      "Iteration 7, loss = 0.20335347\n",
      "Iteration 8, loss = 0.18705524\n",
      "Iteration 9, loss = 0.17327586\n",
      "Iteration 10, loss = 0.16107736\n",
      "Iteration 11, loss = 0.15061128\n",
      "Iteration 12, loss = 0.14102288\n",
      "Iteration 13, loss = 0.13249697\n",
      "Iteration 14, loss = 0.12471905\n",
      "Iteration 15, loss = 0.11765779\n",
      "Iteration 16, loss = 0.11083330\n",
      "Iteration 17, loss = 0.10473122\n",
      "Iteration 18, loss = 0.09883925\n",
      "Iteration 19, loss = 0.09355666\n",
      "Iteration 20, loss = 0.08795744\n",
      "Iteration 21, loss = 0.08398075\n",
      "Iteration 22, loss = 0.07943378\n",
      "Iteration 23, loss = 0.07533438\n",
      "Iteration 24, loss = 0.07107036\n",
      "Iteration 25, loss = 0.06717889\n",
      "Iteration 26, loss = 0.06391171\n",
      "Iteration 27, loss = 0.06054547\n",
      "Iteration 28, loss = 0.05754564\n",
      "Iteration 29, loss = 0.05451786\n",
      "Iteration 30, loss = 0.05163961\n",
      "Iteration 31, loss = 0.04918787\n",
      "Iteration 32, loss = 0.04648766\n",
      "Iteration 33, loss = 0.04407380\n",
      "Iteration 34, loss = 0.04207563\n",
      "Iteration 35, loss = 0.03950162\n",
      "Iteration 36, loss = 0.03753577\n",
      "Iteration 37, loss = 0.03572818\n",
      "Iteration 38, loss = 0.03371056\n",
      "Iteration 39, loss = 0.03202652\n",
      "Iteration 40, loss = 0.03045886\n",
      "Iteration 41, loss = 0.02866254\n",
      "Iteration 42, loss = 0.02728889\n",
      "Iteration 43, loss = 0.02566782\n",
      "Iteration 44, loss = 0.02434677\n",
      "Iteration 45, loss = 0.02299649\n",
      "Iteration 46, loss = 0.02169240\n",
      "Iteration 47, loss = 0.02054406\n",
      "Iteration 48, loss = 0.01943735\n",
      "Iteration 49, loss = 0.01830888\n",
      "Iteration 50, loss = 0.01751908\n",
      "Iteration 51, loss = 0.01653314\n",
      "Iteration 52, loss = 0.01563116\n",
      "Iteration 53, loss = 0.01477410\n",
      "Iteration 54, loss = 0.01398023\n",
      "Iteration 55, loss = 0.01323760\n",
      "Iteration 56, loss = 0.01251892\n",
      "Iteration 57, loss = 0.01171189\n",
      "Iteration 58, loss = 0.01120989\n",
      "Iteration 59, loss = 0.01063971\n",
      "Iteration 60, loss = 0.01010867\n",
      "Iteration 61, loss = 0.00956859\n",
      "Iteration 62, loss = 0.00907930\n",
      "Iteration 63, loss = 0.00868979\n",
      "Iteration 64, loss = 0.00828116\n",
      "Iteration 65, loss = 0.00792840\n",
      "Iteration 66, loss = 0.00745617\n",
      "Iteration 67, loss = 0.00713727\n",
      "Iteration 68, loss = 0.00690127\n",
      "Iteration 69, loss = 0.00658800\n",
      "Iteration 70, loss = 0.00626739\n",
      "Iteration 71, loss = 0.00599225\n",
      "Iteration 72, loss = 0.00578671\n",
      "Iteration 73, loss = 0.00553706\n",
      "Iteration 74, loss = 0.00529047\n",
      "Iteration 75, loss = 0.00507930\n",
      "Iteration 76, loss = 0.00490588\n",
      "Iteration 77, loss = 0.00472863\n",
      "Iteration 78, loss = 0.00453761\n",
      "Iteration 79, loss = 0.00442586\n",
      "Iteration 80, loss = 0.00427485\n",
      "Iteration 81, loss = 0.00412973\n",
      "Iteration 82, loss = 0.00401994\n",
      "Iteration 83, loss = 0.00387070\n",
      "Iteration 84, loss = 0.00375949\n",
      "Iteration 85, loss = 0.00364800\n",
      "Iteration 86, loss = 0.00357498\n",
      "Iteration 87, loss = 0.00346128\n",
      "Iteration 88, loss = 0.00336269\n",
      "Iteration 89, loss = 0.00328742\n",
      "Iteration 90, loss = 0.00319253\n",
      "Iteration 91, loss = 0.00312914\n",
      "Iteration 92, loss = 0.00305272\n",
      "Iteration 93, loss = 0.00299279\n",
      "Iteration 94, loss = 0.00292224\n",
      "Iteration 95, loss = 0.00286611\n",
      "Iteration 96, loss = 0.00278691\n",
      "Iteration 97, loss = 0.00274224\n",
      "Iteration 98, loss = 0.00268983\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.0001, batch_size=200, hidden_layer_sizes=(100,), solver=adam; total time=  50.2s\n",
      "Iteration 1, loss = 1.13601295\n",
      "Iteration 2, loss = 0.44893436\n",
      "Iteration 3, loss = 0.33228085\n",
      "Iteration 4, loss = 0.28181038\n",
      "Iteration 5, loss = 0.24921873\n",
      "Iteration 6, loss = 0.22497266\n",
      "Iteration 7, loss = 0.20538218\n",
      "Iteration 8, loss = 0.18929712\n",
      "Iteration 9, loss = 0.17550144\n",
      "Iteration 10, loss = 0.16300424\n",
      "Iteration 11, loss = 0.15236220\n",
      "Iteration 12, loss = 0.14244982\n",
      "Iteration 13, loss = 0.13379328\n",
      "Iteration 14, loss = 0.12589745\n",
      "Iteration 15, loss = 0.11850627\n",
      "Iteration 16, loss = 0.11195352\n",
      "Iteration 17, loss = 0.10536156\n",
      "Iteration 18, loss = 0.09968377\n",
      "Iteration 19, loss = 0.09411312\n",
      "Iteration 20, loss = 0.08938193\n",
      "Iteration 21, loss = 0.08454712\n",
      "Iteration 22, loss = 0.07988103\n",
      "Iteration 23, loss = 0.07557547\n",
      "Iteration 24, loss = 0.07147300\n",
      "Iteration 25, loss = 0.06785823\n",
      "Iteration 26, loss = 0.06406186\n",
      "Iteration 27, loss = 0.06083228\n",
      "Iteration 28, loss = 0.05742357\n",
      "Iteration 29, loss = 0.05433487\n",
      "Iteration 30, loss = 0.05169970\n",
      "Iteration 31, loss = 0.04853237\n",
      "Iteration 32, loss = 0.04612325\n",
      "Iteration 33, loss = 0.04361651\n",
      "Iteration 34, loss = 0.04120826\n",
      "Iteration 35, loss = 0.03896543\n",
      "Iteration 36, loss = 0.03693612\n",
      "Iteration 37, loss = 0.03508303\n",
      "Iteration 38, loss = 0.03307734\n",
      "Iteration 39, loss = 0.03104707\n",
      "Iteration 40, loss = 0.02933464\n",
      "Iteration 41, loss = 0.02787962\n",
      "Iteration 42, loss = 0.02618959\n",
      "Iteration 43, loss = 0.02467082\n",
      "Iteration 44, loss = 0.02343055\n",
      "Iteration 45, loss = 0.02200189\n",
      "Iteration 46, loss = 0.02093964\n",
      "Iteration 47, loss = 0.01963476\n",
      "Iteration 48, loss = 0.01853794\n",
      "Iteration 49, loss = 0.01761571\n",
      "Iteration 50, loss = 0.01669883\n",
      "Iteration 51, loss = 0.01563768\n",
      "Iteration 52, loss = 0.01479010\n",
      "Iteration 53, loss = 0.01403087\n",
      "Iteration 54, loss = 0.01332354\n",
      "Iteration 55, loss = 0.01252432\n",
      "Iteration 56, loss = 0.01182100\n",
      "Iteration 57, loss = 0.01113551\n",
      "Iteration 58, loss = 0.01075669\n",
      "Iteration 59, loss = 0.01006935\n",
      "Iteration 60, loss = 0.00958153\n",
      "Iteration 61, loss = 0.00911141\n",
      "Iteration 62, loss = 0.00871989\n",
      "Iteration 63, loss = 0.00825033\n",
      "Iteration 64, loss = 0.00785907\n",
      "Iteration 65, loss = 0.00745691\n",
      "Iteration 66, loss = 0.00717203\n",
      "Iteration 67, loss = 0.00679613\n",
      "Iteration 68, loss = 0.00657609\n",
      "Iteration 69, loss = 0.00627180\n",
      "Iteration 70, loss = 0.00598728\n",
      "Iteration 71, loss = 0.00576530\n",
      "Iteration 72, loss = 0.00549920\n",
      "Iteration 73, loss = 0.00530997\n",
      "Iteration 74, loss = 0.00512001\n",
      "Iteration 75, loss = 0.00491528\n",
      "Iteration 76, loss = 0.00475394\n",
      "Iteration 77, loss = 0.00454127\n",
      "Iteration 78, loss = 0.00437779\n",
      "Iteration 79, loss = 0.00425222\n",
      "Iteration 80, loss = 0.00414214\n",
      "Iteration 81, loss = 0.00399672\n",
      "Iteration 82, loss = 0.00386898\n",
      "Iteration 83, loss = 0.00376632\n",
      "Iteration 84, loss = 0.00363878\n",
      "Iteration 85, loss = 0.00355453\n",
      "Iteration 86, loss = 0.00345908\n",
      "Iteration 87, loss = 0.00336200\n",
      "Iteration 88, loss = 0.00327524\n",
      "Iteration 89, loss = 0.00320921\n",
      "Iteration 90, loss = 0.00311642\n",
      "Iteration 91, loss = 0.00304605\n",
      "Iteration 92, loss = 0.00297944\n",
      "Iteration 93, loss = 0.00292833\n",
      "Iteration 94, loss = 0.00285307\n",
      "Iteration 95, loss = 0.00280089\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.0001, batch_size=200, hidden_layer_sizes=(100,), solver=adam; total time=  47.2s\n",
      "Iteration 1, loss = 2.27758653\n",
      "Iteration 2, loss = 2.19063458\n",
      "Iteration 3, loss = 2.10816254\n",
      "Iteration 4, loss = 2.01499031\n",
      "Iteration 5, loss = 1.90984578\n",
      "Iteration 6, loss = 1.79449939\n",
      "Iteration 7, loss = 1.67335924\n",
      "Iteration 8, loss = 1.55247890\n",
      "Iteration 9, loss = 1.43714526\n",
      "Iteration 10, loss = 1.33100034\n",
      "Iteration 11, loss = 1.23567877\n",
      "Iteration 12, loss = 1.15155766\n",
      "Iteration 13, loss = 1.07779789\n",
      "Iteration 14, loss = 1.01339836\n",
      "Iteration 15, loss = 0.95702211\n",
      "Iteration 16, loss = 0.90756153\n",
      "Iteration 17, loss = 0.86389927\n",
      "Iteration 18, loss = 0.82541008\n",
      "Iteration 19, loss = 0.79115544\n",
      "Iteration 20, loss = 0.76053572\n",
      "Iteration 21, loss = 0.73303320\n",
      "Iteration 22, loss = 0.70822572\n",
      "Iteration 23, loss = 0.68580581\n",
      "Iteration 24, loss = 0.66536412\n",
      "Iteration 25, loss = 0.64668875\n",
      "Iteration 26, loss = 0.62951225\n",
      "Iteration 27, loss = 0.61382432\n",
      "Iteration 28, loss = 0.59925723\n",
      "Iteration 29, loss = 0.58581482\n",
      "Iteration 30, loss = 0.57335033\n",
      "Iteration 31, loss = 0.56168679\n",
      "Iteration 32, loss = 0.55083394\n",
      "Iteration 33, loss = 0.54071691\n",
      "Iteration 34, loss = 0.53118648\n",
      "Iteration 35, loss = 0.52218254\n",
      "Iteration 36, loss = 0.51377202\n",
      "Iteration 37, loss = 0.50585334\n",
      "Iteration 38, loss = 0.49834738\n",
      "Iteration 39, loss = 0.49129846\n",
      "Iteration 40, loss = 0.48455291\n",
      "Iteration 41, loss = 0.47821017\n",
      "Iteration 42, loss = 0.47209975\n",
      "Iteration 43, loss = 0.46637517\n",
      "Iteration 44, loss = 0.46087800\n",
      "Iteration 45, loss = 0.45566080\n",
      "Iteration 46, loss = 0.45069376\n",
      "Iteration 47, loss = 0.44594740\n",
      "Iteration 48, loss = 0.44137574\n",
      "Iteration 49, loss = 0.43698495\n",
      "Iteration 50, loss = 0.43282062\n",
      "Iteration 51, loss = 0.42882837\n",
      "Iteration 52, loss = 0.42495325\n",
      "Iteration 53, loss = 0.42127938\n",
      "Iteration 54, loss = 0.41772386\n",
      "Iteration 55, loss = 0.41431418\n",
      "Iteration 56, loss = 0.41105697\n",
      "Iteration 57, loss = 0.40785197\n",
      "Iteration 58, loss = 0.40484115\n",
      "Iteration 59, loss = 0.40185100\n",
      "Iteration 60, loss = 0.39903257\n",
      "Iteration 61, loss = 0.39627180\n",
      "Iteration 62, loss = 0.39363252\n",
      "Iteration 63, loss = 0.39105359\n",
      "Iteration 64, loss = 0.38853053\n",
      "Iteration 65, loss = 0.38615140\n",
      "Iteration 66, loss = 0.38382916\n",
      "Iteration 67, loss = 0.38154098\n",
      "Iteration 68, loss = 0.37932896\n",
      "Iteration 69, loss = 0.37718043\n",
      "Iteration 70, loss = 0.37511798\n",
      "Iteration 71, loss = 0.37312741\n",
      "Iteration 72, loss = 0.37113095\n",
      "Iteration 73, loss = 0.36925889\n",
      "Iteration 74, loss = 0.36736260\n",
      "Iteration 75, loss = 0.36553003\n",
      "Iteration 76, loss = 0.36382281\n",
      "Iteration 77, loss = 0.36208437\n",
      "Iteration 78, loss = 0.36040609\n",
      "Iteration 79, loss = 0.35878196\n",
      "Iteration 80, loss = 0.35715711\n",
      "Iteration 81, loss = 0.35562670\n",
      "Iteration 82, loss = 0.35409918\n",
      "Iteration 83, loss = 0.35258342\n",
      "Iteration 84, loss = 0.35116524\n",
      "Iteration 85, loss = 0.34974538\n",
      "Iteration 86, loss = 0.34833255\n",
      "Iteration 87, loss = 0.34698070\n",
      "Iteration 88, loss = 0.34565130\n",
      "Iteration 89, loss = 0.34434378\n",
      "Iteration 90, loss = 0.34305531\n",
      "Iteration 91, loss = 0.34181612\n",
      "Iteration 92, loss = 0.34061009\n",
      "Iteration 93, loss = 0.33935843\n",
      "Iteration 94, loss = 0.33818511\n",
      "Iteration 95, loss = 0.33702888\n",
      "Iteration 96, loss = 0.33587815\n",
      "Iteration 97, loss = 0.33478778\n",
      "Iteration 98, loss = 0.33365327\n",
      "Iteration 99, loss = 0.33258330\n",
      "Iteration 100, loss = 0.33152288\n",
      "Iteration 101, loss = 0.33045706\n",
      "Iteration 102, loss = 0.32947505\n",
      "Iteration 103, loss = 0.32848883\n",
      "Iteration 104, loss = 0.32744124\n",
      "Iteration 105, loss = 0.32650296\n",
      "Iteration 106, loss = 0.32553760\n",
      "Iteration 107, loss = 0.32457679\n",
      "Iteration 108, loss = 0.32367465\n",
      "Iteration 109, loss = 0.32273938\n",
      "Iteration 110, loss = 0.32183716\n",
      "Iteration 111, loss = 0.32093672\n",
      "Iteration 112, loss = 0.32005601\n",
      "Iteration 113, loss = 0.31919359\n",
      "Iteration 114, loss = 0.31835968\n",
      "Iteration 115, loss = 0.31751254\n",
      "Iteration 116, loss = 0.31668081\n",
      "Iteration 117, loss = 0.31584709\n",
      "Iteration 118, loss = 0.31506231\n",
      "Iteration 119, loss = 0.31423878\n",
      "Iteration 120, loss = 0.31348158\n",
      "Iteration 121, loss = 0.31267784\n",
      "Iteration 122, loss = 0.31192611\n",
      "Iteration 123, loss = 0.31117105\n",
      "Iteration 124, loss = 0.31043708\n",
      "Iteration 125, loss = 0.30968244\n",
      "Iteration 126, loss = 0.30895554\n",
      "Iteration 127, loss = 0.30824252\n",
      "Iteration 128, loss = 0.30751572\n",
      "Iteration 129, loss = 0.30682992\n",
      "Iteration 130, loss = 0.30613690\n",
      "Iteration 131, loss = 0.30543631\n",
      "Iteration 132, loss = 0.30475190\n",
      "Iteration 133, loss = 0.30408428\n",
      "Iteration 134, loss = 0.30341924\n",
      "Iteration 135, loss = 0.30275706\n",
      "Iteration 136, loss = 0.30212063\n",
      "Iteration 137, loss = 0.30146503\n",
      "Iteration 138, loss = 0.30082174\n",
      "Iteration 139, loss = 0.30020582\n",
      "Iteration 140, loss = 0.29957558\n",
      "Iteration 141, loss = 0.29891560\n",
      "Iteration 142, loss = 0.29831136\n",
      "Iteration 143, loss = 0.29768832\n",
      "Iteration 144, loss = 0.29711106\n",
      "Iteration 145, loss = 0.29652012\n",
      "Iteration 146, loss = 0.29594157\n",
      "Iteration 147, loss = 0.29534571\n",
      "Iteration 148, loss = 0.29476034\n",
      "Iteration 149, loss = 0.29416589\n",
      "Iteration 150, loss = 0.29362153\n",
      "Iteration 151, loss = 0.29305030\n",
      "Iteration 152, loss = 0.29249868\n",
      "Iteration 153, loss = 0.29191296\n",
      "Iteration 154, loss = 0.29136643\n",
      "Iteration 155, loss = 0.29082795\n",
      "Iteration 156, loss = 0.29027069\n",
      "Iteration 157, loss = 0.28972231\n",
      "Iteration 158, loss = 0.28919607\n",
      "Iteration 159, loss = 0.28863204\n",
      "Iteration 160, loss = 0.28814593\n",
      "Iteration 161, loss = 0.28760658\n",
      "Iteration 162, loss = 0.28708550\n",
      "Iteration 163, loss = 0.28657202\n",
      "Iteration 164, loss = 0.28604834\n",
      "Iteration 165, loss = 0.28552965\n",
      "Iteration 166, loss = 0.28503604\n",
      "Iteration 167, loss = 0.28455939\n",
      "Iteration 168, loss = 0.28404216\n",
      "Iteration 169, loss = 0.28353094\n",
      "Iteration 170, loss = 0.28304004\n",
      "Iteration 171, loss = 0.28256000\n",
      "Iteration 172, loss = 0.28206651\n",
      "Iteration 173, loss = 0.28156468\n",
      "Iteration 174, loss = 0.28111107\n",
      "Iteration 175, loss = 0.28064292\n",
      "Iteration 176, loss = 0.28014739\n",
      "Iteration 177, loss = 0.27968769\n",
      "Iteration 178, loss = 0.27924241\n",
      "Iteration 179, loss = 0.27875488\n",
      "Iteration 180, loss = 0.27825250\n",
      "Iteration 181, loss = 0.27780632\n",
      "Iteration 182, loss = 0.27736737\n",
      "Iteration 183, loss = 0.27690893\n",
      "Iteration 184, loss = 0.27647835\n",
      "Iteration 185, loss = 0.27599751\n",
      "Iteration 186, loss = 0.27557992\n",
      "Iteration 187, loss = 0.27510389\n",
      "Iteration 188, loss = 0.27466905\n",
      "Iteration 189, loss = 0.27422347\n",
      "Iteration 190, loss = 0.27377722\n",
      "Iteration 191, loss = 0.27337280\n",
      "Iteration 192, loss = 0.27290521\n",
      "Iteration 193, loss = 0.27248380\n",
      "Iteration 194, loss = 0.27206176\n",
      "Iteration 195, loss = 0.27164570\n",
      "Iteration 196, loss = 0.27120203\n",
      "Iteration 197, loss = 0.27077927\n",
      "Iteration 198, loss = 0.27033144\n",
      "Iteration 199, loss = 0.26991217\n",
      "Iteration 200, loss = 0.26952619\n",
      "[CV] END activation=logistic, alpha=0.0001, batch_size=200, hidden_layer_sizes=(100,), solver=sgd; total time= 1.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.27235529\n",
      "Iteration 2, loss = 2.19119431\n",
      "Iteration 3, loss = 2.10998561\n",
      "Iteration 4, loss = 2.01763521\n",
      "Iteration 5, loss = 1.91283495\n",
      "Iteration 6, loss = 1.79714987\n",
      "Iteration 7, loss = 1.67483691\n",
      "Iteration 8, loss = 1.55225849\n",
      "Iteration 9, loss = 1.43474038\n",
      "Iteration 10, loss = 1.32634472\n",
      "Iteration 11, loss = 1.22883975\n",
      "Iteration 12, loss = 1.14298202\n",
      "Iteration 13, loss = 1.06782270\n",
      "Iteration 14, loss = 1.00244002\n",
      "Iteration 15, loss = 0.94549716\n",
      "Iteration 16, loss = 0.89580223\n",
      "Iteration 17, loss = 0.85243518\n",
      "Iteration 18, loss = 0.81406366\n",
      "Iteration 19, loss = 0.78016246\n",
      "Iteration 20, loss = 0.74995245\n",
      "Iteration 21, loss = 0.72295469\n",
      "Iteration 22, loss = 0.69862840\n",
      "Iteration 23, loss = 0.67664050\n",
      "Iteration 24, loss = 0.65660270\n",
      "Iteration 25, loss = 0.63839444\n",
      "Iteration 26, loss = 0.62161843\n",
      "Iteration 27, loss = 0.60625810\n",
      "Iteration 28, loss = 0.59202668\n",
      "Iteration 29, loss = 0.57884766\n",
      "Iteration 30, loss = 0.56666843\n",
      "Iteration 31, loss = 0.55522810\n",
      "Iteration 32, loss = 0.54460293\n",
      "Iteration 33, loss = 0.53466212\n",
      "Iteration 34, loss = 0.52533201\n",
      "Iteration 35, loss = 0.51648913\n",
      "Iteration 36, loss = 0.50826079\n",
      "Iteration 37, loss = 0.50044316\n",
      "Iteration 38, loss = 0.49308242\n",
      "Iteration 39, loss = 0.48606047\n",
      "Iteration 40, loss = 0.47940953\n",
      "Iteration 41, loss = 0.47321237\n",
      "Iteration 42, loss = 0.46723067\n",
      "Iteration 43, loss = 0.46157214\n",
      "Iteration 44, loss = 0.45618825\n",
      "Iteration 45, loss = 0.45101888\n",
      "Iteration 46, loss = 0.44609363\n",
      "Iteration 47, loss = 0.44140571\n",
      "Iteration 48, loss = 0.43691508\n",
      "Iteration 49, loss = 0.43256924\n",
      "Iteration 50, loss = 0.42848539\n",
      "Iteration 51, loss = 0.42451031\n",
      "Iteration 52, loss = 0.42077822\n",
      "Iteration 53, loss = 0.41711279\n",
      "Iteration 54, loss = 0.41360646\n",
      "Iteration 55, loss = 0.41022348\n",
      "Iteration 56, loss = 0.40697771\n",
      "Iteration 57, loss = 0.40382429\n",
      "Iteration 58, loss = 0.40084631\n",
      "Iteration 59, loss = 0.39791992\n",
      "Iteration 60, loss = 0.39508573\n",
      "Iteration 61, loss = 0.39237229\n",
      "Iteration 62, loss = 0.38974598\n",
      "Iteration 63, loss = 0.38722234\n",
      "Iteration 64, loss = 0.38475442\n",
      "Iteration 65, loss = 0.38236234\n",
      "Iteration 66, loss = 0.38004648\n",
      "Iteration 67, loss = 0.37781348\n",
      "Iteration 68, loss = 0.37561842\n",
      "Iteration 69, loss = 0.37348801\n",
      "Iteration 70, loss = 0.37144673\n",
      "Iteration 71, loss = 0.36943864\n",
      "Iteration 72, loss = 0.36750796\n",
      "Iteration 73, loss = 0.36559458\n",
      "Iteration 74, loss = 0.36375679\n",
      "Iteration 75, loss = 0.36198492\n",
      "Iteration 76, loss = 0.36023493\n",
      "Iteration 77, loss = 0.35852062\n",
      "Iteration 78, loss = 0.35682535\n",
      "Iteration 79, loss = 0.35519778\n",
      "Iteration 80, loss = 0.35362481\n",
      "Iteration 81, loss = 0.35207130\n",
      "Iteration 82, loss = 0.35055581\n",
      "Iteration 83, loss = 0.34908764\n",
      "Iteration 84, loss = 0.34762366\n",
      "Iteration 85, loss = 0.34622062\n",
      "Iteration 86, loss = 0.34481504\n",
      "Iteration 87, loss = 0.34347139\n",
      "Iteration 88, loss = 0.34216082\n",
      "Iteration 89, loss = 0.34081445\n",
      "Iteration 90, loss = 0.33955881\n",
      "Iteration 91, loss = 0.33828689\n",
      "Iteration 92, loss = 0.33706390\n",
      "Iteration 93, loss = 0.33587565\n",
      "Iteration 94, loss = 0.33469226\n",
      "Iteration 95, loss = 0.33351798\n",
      "Iteration 96, loss = 0.33238301\n",
      "Iteration 97, loss = 0.33126484\n",
      "Iteration 98, loss = 0.33016295\n",
      "Iteration 99, loss = 0.32909462\n",
      "Iteration 100, loss = 0.32802641\n",
      "Iteration 101, loss = 0.32697629\n",
      "Iteration 102, loss = 0.32596649\n",
      "Iteration 103, loss = 0.32493499\n",
      "Iteration 104, loss = 0.32395987\n",
      "Iteration 105, loss = 0.32293382\n",
      "Iteration 106, loss = 0.32199027\n",
      "Iteration 107, loss = 0.32105914\n",
      "Iteration 108, loss = 0.32009815\n",
      "Iteration 109, loss = 0.31919519\n",
      "Iteration 110, loss = 0.31826535\n",
      "Iteration 111, loss = 0.31738098\n",
      "Iteration 112, loss = 0.31653228\n",
      "Iteration 113, loss = 0.31564451\n",
      "Iteration 114, loss = 0.31478626\n",
      "Iteration 115, loss = 0.31391611\n",
      "Iteration 116, loss = 0.31309289\n",
      "Iteration 117, loss = 0.31227790\n",
      "Iteration 118, loss = 0.31147144\n",
      "Iteration 119, loss = 0.31064409\n",
      "Iteration 120, loss = 0.30985736\n",
      "Iteration 121, loss = 0.30908157\n",
      "Iteration 122, loss = 0.30831735\n",
      "Iteration 123, loss = 0.30755706\n",
      "Iteration 124, loss = 0.30678908\n",
      "Iteration 125, loss = 0.30605467\n",
      "Iteration 126, loss = 0.30530338\n",
      "Iteration 127, loss = 0.30459046\n",
      "Iteration 128, loss = 0.30383786\n",
      "Iteration 129, loss = 0.30314274\n",
      "Iteration 130, loss = 0.30244944\n",
      "Iteration 131, loss = 0.30174685\n",
      "Iteration 132, loss = 0.30102824\n",
      "Iteration 133, loss = 0.30037006\n",
      "Iteration 134, loss = 0.29968396\n",
      "Iteration 135, loss = 0.29902772\n",
      "Iteration 136, loss = 0.29837166\n",
      "Iteration 137, loss = 0.29772924\n",
      "Iteration 138, loss = 0.29706673\n",
      "Iteration 139, loss = 0.29641678\n",
      "Iteration 140, loss = 0.29578060\n",
      "Iteration 141, loss = 0.29513407\n",
      "Iteration 142, loss = 0.29453316\n",
      "Iteration 143, loss = 0.29389546\n",
      "Iteration 144, loss = 0.29329607\n",
      "Iteration 145, loss = 0.29267752\n",
      "Iteration 146, loss = 0.29209468\n",
      "Iteration 147, loss = 0.29147248\n",
      "Iteration 148, loss = 0.29090329\n",
      "Iteration 149, loss = 0.29032672\n",
      "Iteration 150, loss = 0.28973076\n",
      "Iteration 151, loss = 0.28913491\n",
      "Iteration 152, loss = 0.28858853\n",
      "Iteration 153, loss = 0.28800048\n",
      "Iteration 154, loss = 0.28745309\n",
      "Iteration 155, loss = 0.28691001\n",
      "Iteration 156, loss = 0.28633884\n",
      "Iteration 157, loss = 0.28579109\n",
      "Iteration 158, loss = 0.28521481\n",
      "Iteration 159, loss = 0.28473660\n",
      "Iteration 160, loss = 0.28415096\n",
      "Iteration 161, loss = 0.28362851\n",
      "Iteration 162, loss = 0.28310342\n",
      "Iteration 163, loss = 0.28256275\n",
      "Iteration 164, loss = 0.28205330\n",
      "Iteration 165, loss = 0.28154710\n",
      "Iteration 166, loss = 0.28101887\n",
      "Iteration 167, loss = 0.28051143\n",
      "Iteration 168, loss = 0.28000499\n",
      "Iteration 169, loss = 0.27948723\n",
      "Iteration 170, loss = 0.27898669\n",
      "Iteration 171, loss = 0.27849752\n",
      "Iteration 172, loss = 0.27799736\n",
      "Iteration 173, loss = 0.27748708\n",
      "Iteration 174, loss = 0.27702503\n",
      "Iteration 175, loss = 0.27652764\n",
      "Iteration 176, loss = 0.27604178\n",
      "Iteration 177, loss = 0.27557135\n",
      "Iteration 178, loss = 0.27507767\n",
      "Iteration 179, loss = 0.27461801\n",
      "Iteration 180, loss = 0.27413712\n",
      "Iteration 181, loss = 0.27367258\n",
      "Iteration 182, loss = 0.27323379\n",
      "Iteration 183, loss = 0.27272482\n",
      "Iteration 184, loss = 0.27228584\n",
      "Iteration 185, loss = 0.27183780\n",
      "Iteration 186, loss = 0.27136686\n",
      "Iteration 187, loss = 0.27092593\n",
      "Iteration 188, loss = 0.27048010\n",
      "Iteration 189, loss = 0.27002467\n",
      "Iteration 190, loss = 0.26956685\n",
      "Iteration 191, loss = 0.26914420\n",
      "Iteration 192, loss = 0.26867470\n",
      "Iteration 193, loss = 0.26827182\n",
      "Iteration 194, loss = 0.26781033\n",
      "Iteration 195, loss = 0.26737942\n",
      "Iteration 196, loss = 0.26692486\n",
      "Iteration 197, loss = 0.26649906\n",
      "Iteration 198, loss = 0.26609381\n",
      "Iteration 199, loss = 0.26565673\n",
      "Iteration 200, loss = 0.26523655\n",
      "[CV] END activation=logistic, alpha=0.0001, batch_size=200, hidden_layer_sizes=(100,), solver=sgd; total time= 1.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.27181168\n",
      "Iteration 2, loss = 2.19297248\n",
      "Iteration 3, loss = 2.11347106\n",
      "Iteration 4, loss = 2.02269199\n",
      "Iteration 5, loss = 1.91874465\n",
      "Iteration 6, loss = 1.80334805\n",
      "Iteration 7, loss = 1.68109026\n",
      "Iteration 8, loss = 1.55835688\n",
      "Iteration 9, loss = 1.44097823\n",
      "Iteration 10, loss = 1.33294213\n",
      "Iteration 11, loss = 1.23599685\n",
      "Iteration 12, loss = 1.15043512\n",
      "Iteration 13, loss = 1.07558113\n",
      "Iteration 14, loss = 1.01022779\n",
      "Iteration 15, loss = 0.95313920\n",
      "Iteration 16, loss = 0.90313777\n",
      "Iteration 17, loss = 0.85912893\n",
      "Iteration 18, loss = 0.82027140\n",
      "Iteration 19, loss = 0.78578599\n",
      "Iteration 20, loss = 0.75498235\n",
      "Iteration 21, loss = 0.72733048\n",
      "Iteration 22, loss = 0.70246089\n",
      "Iteration 23, loss = 0.67986876\n",
      "Iteration 24, loss = 0.65937162\n",
      "Iteration 25, loss = 0.64071633\n",
      "Iteration 26, loss = 0.62351686\n",
      "Iteration 27, loss = 0.60774181\n",
      "Iteration 28, loss = 0.59323943\n",
      "Iteration 29, loss = 0.57977768\n",
      "Iteration 30, loss = 0.56729076\n",
      "Iteration 31, loss = 0.55571057\n",
      "Iteration 32, loss = 0.54491507\n",
      "Iteration 33, loss = 0.53480734\n",
      "Iteration 34, loss = 0.52531225\n",
      "Iteration 35, loss = 0.51646804\n",
      "Iteration 36, loss = 0.50814188\n",
      "Iteration 37, loss = 0.50028218\n",
      "Iteration 38, loss = 0.49287643\n",
      "Iteration 39, loss = 0.48589695\n",
      "Iteration 40, loss = 0.47925480\n",
      "Iteration 41, loss = 0.47302837\n",
      "Iteration 42, loss = 0.46705645\n",
      "Iteration 43, loss = 0.46141873\n",
      "Iteration 44, loss = 0.45606752\n",
      "Iteration 45, loss = 0.45093420\n",
      "Iteration 46, loss = 0.44604791\n",
      "Iteration 47, loss = 0.44140592\n",
      "Iteration 48, loss = 0.43696443\n",
      "Iteration 49, loss = 0.43270599\n",
      "Iteration 50, loss = 0.42858351\n",
      "Iteration 51, loss = 0.42476469\n",
      "Iteration 52, loss = 0.42101029\n",
      "Iteration 53, loss = 0.41743118\n",
      "Iteration 54, loss = 0.41392007\n",
      "Iteration 55, loss = 0.41062108\n",
      "Iteration 56, loss = 0.40741000\n",
      "Iteration 57, loss = 0.40431270\n",
      "Iteration 58, loss = 0.40137346\n",
      "Iteration 59, loss = 0.39852123\n",
      "Iteration 60, loss = 0.39575480\n",
      "Iteration 61, loss = 0.39306557\n",
      "Iteration 62, loss = 0.39045659\n",
      "Iteration 63, loss = 0.38798346\n",
      "Iteration 64, loss = 0.38552721\n",
      "Iteration 65, loss = 0.38316765\n",
      "Iteration 66, loss = 0.38087835\n",
      "Iteration 67, loss = 0.37867263\n",
      "Iteration 68, loss = 0.37654040\n",
      "Iteration 69, loss = 0.37443411\n",
      "Iteration 70, loss = 0.37242579\n",
      "Iteration 71, loss = 0.37041955\n",
      "Iteration 72, loss = 0.36854761\n",
      "Iteration 73, loss = 0.36664099\n",
      "Iteration 74, loss = 0.36481942\n",
      "Iteration 75, loss = 0.36303100\n",
      "Iteration 76, loss = 0.36132121\n",
      "Iteration 77, loss = 0.35966841\n",
      "Iteration 78, loss = 0.35796130\n",
      "Iteration 79, loss = 0.35636254\n",
      "Iteration 80, loss = 0.35485857\n",
      "Iteration 81, loss = 0.35325319\n",
      "Iteration 82, loss = 0.35177845\n",
      "Iteration 83, loss = 0.35027296\n",
      "Iteration 84, loss = 0.34886185\n",
      "Iteration 85, loss = 0.34744953\n",
      "Iteration 86, loss = 0.34608989\n",
      "Iteration 87, loss = 0.34470834\n",
      "Iteration 88, loss = 0.34340058\n",
      "Iteration 89, loss = 0.34213280\n",
      "Iteration 90, loss = 0.34086185\n",
      "Iteration 91, loss = 0.33959383\n",
      "Iteration 92, loss = 0.33838350\n",
      "Iteration 93, loss = 0.33720393\n",
      "Iteration 94, loss = 0.33603654\n",
      "Iteration 95, loss = 0.33486226\n",
      "Iteration 96, loss = 0.33373608\n",
      "Iteration 97, loss = 0.33264123\n",
      "Iteration 98, loss = 0.33151381\n",
      "Iteration 99, loss = 0.33043576\n",
      "Iteration 100, loss = 0.32941806\n",
      "Iteration 101, loss = 0.32835764\n",
      "Iteration 102, loss = 0.32733299\n",
      "Iteration 103, loss = 0.32629349\n",
      "Iteration 104, loss = 0.32535800\n",
      "Iteration 105, loss = 0.32433519\n",
      "Iteration 106, loss = 0.32338501\n",
      "Iteration 107, loss = 0.32243899\n",
      "Iteration 108, loss = 0.32151813\n",
      "Iteration 109, loss = 0.32058496\n",
      "Iteration 110, loss = 0.31969549\n",
      "Iteration 111, loss = 0.31879252\n",
      "Iteration 112, loss = 0.31792373\n",
      "Iteration 113, loss = 0.31703021\n",
      "Iteration 114, loss = 0.31619818\n",
      "Iteration 115, loss = 0.31533561\n",
      "Iteration 116, loss = 0.31451309\n",
      "Iteration 117, loss = 0.31369163\n",
      "Iteration 118, loss = 0.31284659\n",
      "Iteration 119, loss = 0.31206064\n",
      "Iteration 120, loss = 0.31127918\n",
      "Iteration 121, loss = 0.31047948\n",
      "Iteration 122, loss = 0.30973854\n",
      "Iteration 123, loss = 0.30896847\n",
      "Iteration 124, loss = 0.30818836\n",
      "Iteration 125, loss = 0.30744667\n",
      "Iteration 126, loss = 0.30674559\n",
      "Iteration 127, loss = 0.30599222\n",
      "Iteration 128, loss = 0.30527867\n",
      "Iteration 129, loss = 0.30457034\n",
      "Iteration 130, loss = 0.30385239\n",
      "Iteration 131, loss = 0.30316562\n",
      "Iteration 132, loss = 0.30246073\n",
      "Iteration 133, loss = 0.30176905\n",
      "Iteration 134, loss = 0.30109727\n",
      "Iteration 135, loss = 0.30044171\n",
      "Iteration 136, loss = 0.29978295\n",
      "Iteration 137, loss = 0.29913835\n",
      "Iteration 138, loss = 0.29846067\n",
      "Iteration 139, loss = 0.29783384\n",
      "Iteration 140, loss = 0.29716909\n",
      "Iteration 141, loss = 0.29652866\n",
      "Iteration 142, loss = 0.29595532\n",
      "Iteration 143, loss = 0.29530851\n",
      "Iteration 144, loss = 0.29469106\n",
      "Iteration 145, loss = 0.29408854\n",
      "Iteration 146, loss = 0.29349025\n",
      "Iteration 147, loss = 0.29290827\n",
      "Iteration 148, loss = 0.29228454\n",
      "Iteration 149, loss = 0.29175622\n",
      "Iteration 150, loss = 0.29114647\n",
      "Iteration 151, loss = 0.29057270\n",
      "Iteration 152, loss = 0.28999926\n",
      "Iteration 153, loss = 0.28945196\n",
      "Iteration 154, loss = 0.28888883\n",
      "Iteration 155, loss = 0.28832971\n",
      "Iteration 156, loss = 0.28774995\n",
      "Iteration 157, loss = 0.28718991\n",
      "Iteration 158, loss = 0.28666726\n",
      "Iteration 159, loss = 0.28613040\n",
      "Iteration 160, loss = 0.28559379\n",
      "Iteration 161, loss = 0.28506457\n",
      "Iteration 162, loss = 0.28453302\n",
      "Iteration 163, loss = 0.28402590\n",
      "Iteration 164, loss = 0.28349474\n",
      "Iteration 165, loss = 0.28296522\n",
      "Iteration 166, loss = 0.28246702\n",
      "Iteration 167, loss = 0.28192624\n",
      "Iteration 168, loss = 0.28142706\n",
      "Iteration 169, loss = 0.28093739\n",
      "Iteration 170, loss = 0.28042751\n",
      "Iteration 171, loss = 0.27992359\n",
      "Iteration 172, loss = 0.27942998\n",
      "Iteration 173, loss = 0.27895889\n",
      "Iteration 174, loss = 0.27847136\n",
      "Iteration 175, loss = 0.27798728\n",
      "Iteration 176, loss = 0.27749185\n",
      "Iteration 177, loss = 0.27701539\n",
      "Iteration 178, loss = 0.27655658\n",
      "Iteration 179, loss = 0.27606934\n",
      "Iteration 180, loss = 0.27560345\n",
      "Iteration 181, loss = 0.27512457\n",
      "Iteration 182, loss = 0.27466673\n",
      "Iteration 183, loss = 0.27420337\n",
      "Iteration 184, loss = 0.27377098\n",
      "Iteration 185, loss = 0.27332329\n",
      "Iteration 186, loss = 0.27283817\n",
      "Iteration 187, loss = 0.27239242\n",
      "Iteration 188, loss = 0.27192260\n",
      "Iteration 189, loss = 0.27150759\n",
      "Iteration 190, loss = 0.27104723\n",
      "Iteration 191, loss = 0.27061556\n",
      "Iteration 192, loss = 0.27019058\n",
      "Iteration 193, loss = 0.26971224\n",
      "Iteration 194, loss = 0.26932711\n",
      "Iteration 195, loss = 0.26886758\n",
      "Iteration 196, loss = 0.26841612\n",
      "Iteration 197, loss = 0.26800481\n",
      "Iteration 198, loss = 0.26756961\n",
      "Iteration 199, loss = 0.26713126\n",
      "Iteration 200, loss = 0.26670909\n",
      "[CV] END activation=logistic, alpha=0.0001, batch_size=200, hidden_layer_sizes=(100,), solver=sgd; total time= 1.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.26919066\n",
      "Iteration 2, loss = 2.17976165\n",
      "Iteration 3, loss = 2.08999378\n",
      "Iteration 4, loss = 1.98996286\n",
      "Iteration 5, loss = 1.87885535\n",
      "Iteration 6, loss = 1.75983529\n",
      "Iteration 7, loss = 1.63785328\n",
      "Iteration 8, loss = 1.51873716\n",
      "Iteration 9, loss = 1.40710313\n",
      "Iteration 10, loss = 1.30577861\n",
      "Iteration 11, loss = 1.21552740\n",
      "Iteration 12, loss = 1.13591454\n",
      "Iteration 13, loss = 1.06598961\n",
      "Iteration 14, loss = 1.00458668\n",
      "Iteration 15, loss = 0.95061367\n",
      "Iteration 16, loss = 0.90299264\n",
      "Iteration 17, loss = 0.86074273\n",
      "Iteration 18, loss = 0.82313278\n",
      "Iteration 19, loss = 0.78945866\n",
      "Iteration 20, loss = 0.75928321\n",
      "Iteration 21, loss = 0.73197405\n",
      "Iteration 22, loss = 0.70730192\n",
      "Iteration 23, loss = 0.68482811\n",
      "Iteration 24, loss = 0.66425588\n",
      "Iteration 25, loss = 0.64551926\n",
      "Iteration 26, loss = 0.62828742\n",
      "Iteration 27, loss = 0.61234825\n",
      "Iteration 28, loss = 0.59761215\n",
      "Iteration 29, loss = 0.58403513\n",
      "Iteration 30, loss = 0.57135968\n",
      "Iteration 31, loss = 0.55956485\n",
      "Iteration 32, loss = 0.54856727\n",
      "Iteration 33, loss = 0.53830209\n",
      "Iteration 34, loss = 0.52867810\n",
      "Iteration 35, loss = 0.51961853\n",
      "Iteration 36, loss = 0.51112355\n",
      "Iteration 37, loss = 0.50318486\n",
      "Iteration 38, loss = 0.49562825\n",
      "Iteration 39, loss = 0.48852068\n",
      "Iteration 40, loss = 0.48177974\n",
      "Iteration 41, loss = 0.47544619\n",
      "Iteration 42, loss = 0.46936303\n",
      "Iteration 43, loss = 0.46361672\n",
      "Iteration 44, loss = 0.45815706\n",
      "Iteration 45, loss = 0.45297325\n",
      "Iteration 46, loss = 0.44803757\n",
      "Iteration 47, loss = 0.44333672\n",
      "Iteration 48, loss = 0.43882650\n",
      "Iteration 49, loss = 0.43448387\n",
      "Iteration 50, loss = 0.43035893\n",
      "Iteration 51, loss = 0.42640231\n",
      "Iteration 52, loss = 0.42264893\n",
      "Iteration 53, loss = 0.41899759\n",
      "Iteration 54, loss = 0.41552125\n",
      "Iteration 55, loss = 0.41214648\n",
      "Iteration 56, loss = 0.40893414\n",
      "Iteration 57, loss = 0.40579362\n",
      "Iteration 58, loss = 0.40285017\n",
      "Iteration 59, loss = 0.39991202\n",
      "Iteration 60, loss = 0.39714295\n",
      "Iteration 61, loss = 0.39443059\n",
      "Iteration 62, loss = 0.39182969\n",
      "Iteration 63, loss = 0.38929191\n",
      "Iteration 64, loss = 0.38686384\n",
      "Iteration 65, loss = 0.38447974\n",
      "Iteration 66, loss = 0.38221051\n",
      "Iteration 67, loss = 0.37999392\n",
      "Iteration 68, loss = 0.37780417\n",
      "Iteration 69, loss = 0.37570299\n",
      "Iteration 70, loss = 0.37366861\n",
      "Iteration 71, loss = 0.37170597\n",
      "Iteration 72, loss = 0.36977606\n",
      "Iteration 73, loss = 0.36792054\n",
      "Iteration 74, loss = 0.36607672\n",
      "Iteration 75, loss = 0.36433372\n",
      "Iteration 76, loss = 0.36256634\n",
      "Iteration 77, loss = 0.36089852\n",
      "Iteration 78, loss = 0.35927750\n",
      "Iteration 79, loss = 0.35762280\n",
      "Iteration 80, loss = 0.35608172\n",
      "Iteration 81, loss = 0.35452600\n",
      "Iteration 82, loss = 0.35305206\n",
      "Iteration 83, loss = 0.35154876\n",
      "Iteration 84, loss = 0.35014640\n",
      "Iteration 85, loss = 0.34872315\n",
      "Iteration 86, loss = 0.34736585\n",
      "Iteration 87, loss = 0.34603272\n",
      "Iteration 88, loss = 0.34469975\n",
      "Iteration 89, loss = 0.34341933\n",
      "Iteration 90, loss = 0.34214970\n",
      "Iteration 91, loss = 0.34089954\n",
      "Iteration 92, loss = 0.33966945\n",
      "Iteration 93, loss = 0.33851070\n",
      "Iteration 94, loss = 0.33731533\n",
      "Iteration 95, loss = 0.33615359\n",
      "Iteration 96, loss = 0.33505885\n",
      "Iteration 97, loss = 0.33390818\n",
      "Iteration 98, loss = 0.33281176\n",
      "Iteration 99, loss = 0.33176493\n",
      "Iteration 100, loss = 0.33069494\n",
      "Iteration 101, loss = 0.32966396\n",
      "Iteration 102, loss = 0.32862829\n",
      "Iteration 103, loss = 0.32766107\n",
      "Iteration 104, loss = 0.32665291\n",
      "Iteration 105, loss = 0.32568985\n",
      "Iteration 106, loss = 0.32472457\n",
      "Iteration 107, loss = 0.32377032\n",
      "Iteration 108, loss = 0.32285990\n",
      "Iteration 109, loss = 0.32190562\n",
      "Iteration 110, loss = 0.32104136\n",
      "Iteration 111, loss = 0.32012158\n",
      "Iteration 112, loss = 0.31924440\n",
      "Iteration 113, loss = 0.31843301\n",
      "Iteration 114, loss = 0.31754489\n",
      "Iteration 115, loss = 0.31671582\n",
      "Iteration 116, loss = 0.31588170\n",
      "Iteration 117, loss = 0.31503840\n",
      "Iteration 118, loss = 0.31426689\n",
      "Iteration 119, loss = 0.31344504\n",
      "Iteration 120, loss = 0.31266124\n",
      "Iteration 121, loss = 0.31187668\n",
      "Iteration 122, loss = 0.31110009\n",
      "Iteration 123, loss = 0.31031890\n",
      "Iteration 124, loss = 0.30959895\n",
      "Iteration 125, loss = 0.30884958\n",
      "Iteration 126, loss = 0.30809793\n",
      "Iteration 127, loss = 0.30738857\n",
      "Iteration 128, loss = 0.30665749\n",
      "Iteration 129, loss = 0.30593960\n",
      "Iteration 130, loss = 0.30524839\n",
      "Iteration 131, loss = 0.30456657\n",
      "Iteration 132, loss = 0.30387298\n",
      "Iteration 133, loss = 0.30317368\n",
      "Iteration 134, loss = 0.30250010\n",
      "Iteration 135, loss = 0.30185187\n",
      "Iteration 136, loss = 0.30116119\n",
      "Iteration 137, loss = 0.30054579\n",
      "Iteration 138, loss = 0.29984366\n",
      "Iteration 139, loss = 0.29925121\n",
      "Iteration 140, loss = 0.29859803\n",
      "Iteration 141, loss = 0.29796133\n",
      "Iteration 142, loss = 0.29734959\n",
      "Iteration 143, loss = 0.29672917\n",
      "Iteration 144, loss = 0.29607203\n",
      "Iteration 145, loss = 0.29547635\n",
      "Iteration 146, loss = 0.29488393\n",
      "Iteration 147, loss = 0.29429809\n",
      "Iteration 148, loss = 0.29368160\n",
      "Iteration 149, loss = 0.29306624\n",
      "Iteration 150, loss = 0.29252873\n",
      "Iteration 151, loss = 0.29193142\n",
      "Iteration 152, loss = 0.29136855\n",
      "Iteration 153, loss = 0.29079925\n",
      "Iteration 154, loss = 0.29022447\n",
      "Iteration 155, loss = 0.28966145\n",
      "Iteration 156, loss = 0.28909905\n",
      "Iteration 157, loss = 0.28855535\n",
      "Iteration 158, loss = 0.28801521\n",
      "Iteration 159, loss = 0.28745233\n",
      "Iteration 160, loss = 0.28695035\n",
      "Iteration 161, loss = 0.28639440\n",
      "Iteration 162, loss = 0.28586591\n",
      "Iteration 163, loss = 0.28533360\n",
      "Iteration 164, loss = 0.28479931\n",
      "Iteration 165, loss = 0.28427335\n",
      "Iteration 166, loss = 0.28376497\n",
      "Iteration 167, loss = 0.28324273\n",
      "Iteration 168, loss = 0.28274021\n",
      "Iteration 169, loss = 0.28220719\n",
      "Iteration 170, loss = 0.28171655\n",
      "Iteration 171, loss = 0.28118492\n",
      "Iteration 172, loss = 0.28071913\n",
      "Iteration 173, loss = 0.28021714\n",
      "Iteration 174, loss = 0.27972687\n",
      "Iteration 175, loss = 0.27922107\n",
      "Iteration 176, loss = 0.27873885\n",
      "Iteration 177, loss = 0.27828066\n",
      "Iteration 178, loss = 0.27778137\n",
      "Iteration 179, loss = 0.27731475\n",
      "Iteration 180, loss = 0.27684854\n",
      "Iteration 181, loss = 0.27634998\n",
      "Iteration 182, loss = 0.27586742\n",
      "Iteration 183, loss = 0.27543367\n",
      "Iteration 184, loss = 0.27494587\n",
      "Iteration 185, loss = 0.27448425\n",
      "Iteration 186, loss = 0.27400923\n",
      "Iteration 187, loss = 0.27354329\n",
      "Iteration 188, loss = 0.27311263\n",
      "Iteration 189, loss = 0.27265296\n",
      "Iteration 190, loss = 0.27218394\n",
      "Iteration 191, loss = 0.27174727\n",
      "Iteration 192, loss = 0.27131037\n",
      "Iteration 193, loss = 0.27088467\n",
      "Iteration 194, loss = 0.27042418\n",
      "Iteration 195, loss = 0.26995806\n",
      "Iteration 196, loss = 0.26953965\n",
      "Iteration 197, loss = 0.26911250\n",
      "Iteration 198, loss = 0.26865452\n",
      "Iteration 199, loss = 0.26822736\n",
      "Iteration 200, loss = 0.26779452\n",
      "[CV] END activation=logistic, alpha=0.0001, batch_size=200, hidden_layer_sizes=(100,), solver=sgd; total time= 1.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.27184681\n",
      "Iteration 2, loss = 2.18671320\n",
      "Iteration 3, loss = 2.10117823\n",
      "Iteration 4, loss = 2.00504610\n",
      "Iteration 5, loss = 1.89662154\n",
      "Iteration 6, loss = 1.77836582\n",
      "Iteration 7, loss = 1.65485260\n",
      "Iteration 8, loss = 1.53232926\n",
      "Iteration 9, loss = 1.41599960\n",
      "Iteration 10, loss = 1.30943469\n",
      "Iteration 11, loss = 1.21425248\n",
      "Iteration 12, loss = 1.13040183\n",
      "Iteration 13, loss = 1.05718994\n",
      "Iteration 14, loss = 0.99349403\n",
      "Iteration 15, loss = 0.93802207\n",
      "Iteration 16, loss = 0.88943636\n",
      "Iteration 17, loss = 0.84687656\n",
      "Iteration 18, loss = 0.80926090\n",
      "Iteration 19, loss = 0.77594551\n",
      "Iteration 20, loss = 0.74624918\n",
      "Iteration 21, loss = 0.71958525\n",
      "Iteration 22, loss = 0.69556032\n",
      "Iteration 23, loss = 0.67382151\n",
      "Iteration 24, loss = 0.65402988\n",
      "Iteration 25, loss = 0.63598414\n",
      "Iteration 26, loss = 0.61941915\n",
      "Iteration 27, loss = 0.60421940\n",
      "Iteration 28, loss = 0.59018198\n",
      "Iteration 29, loss = 0.57714172\n",
      "Iteration 30, loss = 0.56506042\n",
      "Iteration 31, loss = 0.55375099\n",
      "Iteration 32, loss = 0.54334197\n",
      "Iteration 33, loss = 0.53347239\n",
      "Iteration 34, loss = 0.52424776\n",
      "Iteration 35, loss = 0.51566962\n",
      "Iteration 36, loss = 0.50748655\n",
      "Iteration 37, loss = 0.49983883\n",
      "Iteration 38, loss = 0.49257486\n",
      "Iteration 39, loss = 0.48576887\n",
      "Iteration 40, loss = 0.47924838\n",
      "Iteration 41, loss = 0.47310383\n",
      "Iteration 42, loss = 0.46725357\n",
      "Iteration 43, loss = 0.46173517\n",
      "Iteration 44, loss = 0.45645392\n",
      "Iteration 45, loss = 0.45140877\n",
      "Iteration 46, loss = 0.44661488\n",
      "Iteration 47, loss = 0.44201367\n",
      "Iteration 48, loss = 0.43766080\n",
      "Iteration 49, loss = 0.43344547\n",
      "Iteration 50, loss = 0.42941243\n",
      "Iteration 51, loss = 0.42558359\n",
      "Iteration 52, loss = 0.42188519\n",
      "Iteration 53, loss = 0.41831690\n",
      "Iteration 54, loss = 0.41488865\n",
      "Iteration 55, loss = 0.41157394\n",
      "Iteration 56, loss = 0.40844242\n",
      "Iteration 57, loss = 0.40535657\n",
      "Iteration 58, loss = 0.40243677\n",
      "Iteration 59, loss = 0.39958760\n",
      "Iteration 60, loss = 0.39681887\n",
      "Iteration 61, loss = 0.39416813\n",
      "Iteration 62, loss = 0.39159426\n",
      "Iteration 63, loss = 0.38910255\n",
      "Iteration 64, loss = 0.38668939\n",
      "Iteration 65, loss = 0.38432228\n",
      "Iteration 66, loss = 0.38208216\n",
      "Iteration 67, loss = 0.37988526\n",
      "Iteration 68, loss = 0.37773730\n",
      "Iteration 69, loss = 0.37565595\n",
      "Iteration 70, loss = 0.37364877\n",
      "Iteration 71, loss = 0.37163596\n",
      "Iteration 72, loss = 0.36978335\n",
      "Iteration 73, loss = 0.36786682\n",
      "Iteration 74, loss = 0.36606930\n",
      "Iteration 75, loss = 0.36430140\n",
      "Iteration 76, loss = 0.36257557\n",
      "Iteration 77, loss = 0.36088015\n",
      "Iteration 78, loss = 0.35929893\n",
      "Iteration 79, loss = 0.35765355\n",
      "Iteration 80, loss = 0.35610214\n",
      "Iteration 81, loss = 0.35455042\n",
      "Iteration 82, loss = 0.35306775\n",
      "Iteration 83, loss = 0.35159369\n",
      "Iteration 84, loss = 0.35012182\n",
      "Iteration 85, loss = 0.34874539\n",
      "Iteration 86, loss = 0.34732839\n",
      "Iteration 87, loss = 0.34601777\n",
      "Iteration 88, loss = 0.34469364\n",
      "Iteration 89, loss = 0.34339331\n",
      "Iteration 90, loss = 0.34210521\n",
      "Iteration 91, loss = 0.34088782\n",
      "Iteration 92, loss = 0.33968689\n",
      "Iteration 93, loss = 0.33846567\n",
      "Iteration 94, loss = 0.33730922\n",
      "Iteration 95, loss = 0.33614264\n",
      "Iteration 96, loss = 0.33499878\n",
      "Iteration 97, loss = 0.33386154\n",
      "Iteration 98, loss = 0.33278691\n",
      "Iteration 99, loss = 0.33174222\n",
      "Iteration 100, loss = 0.33066152\n",
      "Iteration 101, loss = 0.32959434\n",
      "Iteration 102, loss = 0.32855801\n",
      "Iteration 103, loss = 0.32758145\n",
      "Iteration 104, loss = 0.32652803\n",
      "Iteration 105, loss = 0.32559513\n",
      "Iteration 106, loss = 0.32460676\n",
      "Iteration 107, loss = 0.32367387\n",
      "Iteration 108, loss = 0.32272969\n",
      "Iteration 109, loss = 0.32181073\n",
      "Iteration 110, loss = 0.32089361\n",
      "Iteration 111, loss = 0.31998350\n",
      "Iteration 112, loss = 0.31909190\n",
      "Iteration 113, loss = 0.31828590\n",
      "Iteration 114, loss = 0.31739214\n",
      "Iteration 115, loss = 0.31655151\n",
      "Iteration 116, loss = 0.31570317\n",
      "Iteration 117, loss = 0.31488345\n",
      "Iteration 118, loss = 0.31404946\n",
      "Iteration 119, loss = 0.31325007\n",
      "Iteration 120, loss = 0.31243557\n",
      "Iteration 121, loss = 0.31165514\n",
      "Iteration 122, loss = 0.31087884\n",
      "Iteration 123, loss = 0.31010057\n",
      "Iteration 124, loss = 0.30934434\n",
      "Iteration 125, loss = 0.30858571\n",
      "Iteration 126, loss = 0.30785762\n",
      "Iteration 127, loss = 0.30709107\n",
      "Iteration 128, loss = 0.30637854\n",
      "Iteration 129, loss = 0.30564899\n",
      "Iteration 130, loss = 0.30495592\n",
      "Iteration 131, loss = 0.30424513\n",
      "Iteration 132, loss = 0.30354030\n",
      "Iteration 133, loss = 0.30286907\n",
      "Iteration 134, loss = 0.30218412\n",
      "Iteration 135, loss = 0.30149020\n",
      "Iteration 136, loss = 0.30082873\n",
      "Iteration 137, loss = 0.30017121\n",
      "Iteration 138, loss = 0.29951178\n",
      "Iteration 139, loss = 0.29885767\n",
      "Iteration 140, loss = 0.29818409\n",
      "Iteration 141, loss = 0.29759212\n",
      "Iteration 142, loss = 0.29695575\n",
      "Iteration 143, loss = 0.29630987\n",
      "Iteration 144, loss = 0.29571260\n",
      "Iteration 145, loss = 0.29509125\n",
      "Iteration 146, loss = 0.29449397\n",
      "Iteration 147, loss = 0.29385037\n",
      "Iteration 148, loss = 0.29327201\n",
      "Iteration 149, loss = 0.29266305\n",
      "Iteration 150, loss = 0.29210550\n",
      "Iteration 151, loss = 0.29148244\n",
      "Iteration 152, loss = 0.29090982\n",
      "Iteration 153, loss = 0.29033871\n",
      "Iteration 154, loss = 0.28974837\n",
      "Iteration 155, loss = 0.28918665\n",
      "Iteration 156, loss = 0.28863848\n",
      "Iteration 157, loss = 0.28804832\n",
      "Iteration 158, loss = 0.28751247\n",
      "Iteration 159, loss = 0.28694933\n",
      "Iteration 160, loss = 0.28641152\n",
      "Iteration 161, loss = 0.28585143\n",
      "Iteration 162, loss = 0.28530894\n",
      "Iteration 163, loss = 0.28479670\n",
      "Iteration 164, loss = 0.28422988\n",
      "Iteration 165, loss = 0.28373346\n",
      "Iteration 166, loss = 0.28321285\n",
      "Iteration 167, loss = 0.28266840\n",
      "Iteration 168, loss = 0.28215791\n",
      "Iteration 169, loss = 0.28163899\n",
      "Iteration 170, loss = 0.28111964\n",
      "Iteration 171, loss = 0.28061849\n",
      "Iteration 172, loss = 0.28012052\n",
      "Iteration 173, loss = 0.27960634\n",
      "Iteration 174, loss = 0.27911782\n",
      "Iteration 175, loss = 0.27861220\n",
      "Iteration 176, loss = 0.27812775\n",
      "Iteration 177, loss = 0.27761871\n",
      "Iteration 178, loss = 0.27714436\n",
      "Iteration 179, loss = 0.27666589\n",
      "Iteration 180, loss = 0.27618280\n",
      "Iteration 181, loss = 0.27569293\n",
      "Iteration 182, loss = 0.27522465\n",
      "Iteration 183, loss = 0.27476197\n",
      "Iteration 184, loss = 0.27429624\n",
      "Iteration 185, loss = 0.27380060\n",
      "Iteration 186, loss = 0.27333186\n",
      "Iteration 187, loss = 0.27286642\n",
      "Iteration 188, loss = 0.27241815\n",
      "Iteration 189, loss = 0.27195104\n",
      "Iteration 190, loss = 0.27148159\n",
      "Iteration 191, loss = 0.27104641\n",
      "Iteration 192, loss = 0.27057868\n",
      "Iteration 193, loss = 0.27012617\n",
      "Iteration 194, loss = 0.26969034\n",
      "Iteration 195, loss = 0.26921990\n",
      "Iteration 196, loss = 0.26881908\n",
      "Iteration 197, loss = 0.26834248\n",
      "Iteration 198, loss = 0.26790696\n",
      "Iteration 199, loss = 0.26748676\n",
      "Iteration 200, loss = 0.26705365\n",
      "[CV] END activation=logistic, alpha=0.0001, batch_size=200, hidden_layer_sizes=(100,), solver=sgd; total time= 1.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.13008031\n",
      "Iteration 2, loss = 0.35215338\n",
      "Iteration 3, loss = 0.25535119\n",
      "Iteration 4, loss = 0.20870612\n",
      "Iteration 5, loss = 0.17895707\n",
      "Iteration 6, loss = 0.15381090\n",
      "Iteration 7, loss = 0.13428269\n",
      "Iteration 8, loss = 0.11731660\n",
      "Iteration 9, loss = 0.10281073\n",
      "Iteration 10, loss = 0.08991874\n",
      "Iteration 11, loss = 0.07900985\n",
      "Iteration 12, loss = 0.06915699\n",
      "Iteration 13, loss = 0.06029518\n",
      "Iteration 14, loss = 0.05316939\n",
      "Iteration 15, loss = 0.04687736\n",
      "Iteration 16, loss = 0.04044312\n",
      "Iteration 17, loss = 0.03537415\n",
      "Iteration 18, loss = 0.02995681\n",
      "Iteration 19, loss = 0.02629365\n",
      "Iteration 20, loss = 0.02256080\n",
      "Iteration 21, loss = 0.01980229\n",
      "Iteration 22, loss = 0.01751351\n",
      "Iteration 23, loss = 0.01479135\n",
      "Iteration 24, loss = 0.01276773\n",
      "Iteration 25, loss = 0.01147614\n",
      "Iteration 26, loss = 0.00942099\n",
      "Iteration 27, loss = 0.00823423\n",
      "Iteration 28, loss = 0.00746736\n",
      "Iteration 29, loss = 0.00656141\n",
      "Iteration 30, loss = 0.00626509\n",
      "Iteration 31, loss = 0.00554339\n",
      "Iteration 32, loss = 0.00497688\n",
      "Iteration 33, loss = 0.00433046\n",
      "Iteration 34, loss = 0.00403643\n",
      "Iteration 35, loss = 0.00379358\n",
      "Iteration 36, loss = 0.00350017\n",
      "Iteration 37, loss = 0.00333593\n",
      "Iteration 38, loss = 0.00311343\n",
      "Iteration 39, loss = 0.00301134\n",
      "Iteration 40, loss = 0.00284143\n",
      "Iteration 41, loss = 0.00279508\n",
      "Iteration 42, loss = 0.00265331\n",
      "Iteration 43, loss = 0.00266561\n",
      "Iteration 44, loss = 0.00258411\n",
      "Iteration 45, loss = 0.00247128\n",
      "Iteration 46, loss = 0.00229223\n",
      "Iteration 47, loss = 0.00222414\n",
      "Iteration 48, loss = 0.00217664\n",
      "Iteration 49, loss = 0.00211296\n",
      "Iteration 50, loss = 0.00206327\n",
      "Iteration 51, loss = 0.00200674\n",
      "Iteration 52, loss = 0.00198386\n",
      "Iteration 53, loss = 0.00192542\n",
      "Iteration 54, loss = 0.00189036\n",
      "Iteration 55, loss = 0.00185080\n",
      "Iteration 56, loss = 0.00181903\n",
      "Iteration 57, loss = 0.01484056\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.0001, batch_size=200, hidden_layer_sizes=(400, 100), solver=adam; total time= 1.8min\n",
      "Iteration 1, loss = 1.15549393\n",
      "Iteration 2, loss = 0.36003137\n",
      "Iteration 3, loss = 0.25849752\n",
      "Iteration 4, loss = 0.21054616\n",
      "Iteration 5, loss = 0.17810678\n",
      "Iteration 6, loss = 0.15272334\n",
      "Iteration 7, loss = 0.13241948\n",
      "Iteration 8, loss = 0.11481891\n",
      "Iteration 9, loss = 0.10099357\n",
      "Iteration 10, loss = 0.08767612\n",
      "Iteration 11, loss = 0.07722532\n",
      "Iteration 12, loss = 0.06708658\n",
      "Iteration 13, loss = 0.05922719\n",
      "Iteration 14, loss = 0.05174744\n",
      "Iteration 15, loss = 0.04498377\n",
      "Iteration 16, loss = 0.03939622\n",
      "Iteration 17, loss = 0.03343188\n",
      "Iteration 18, loss = 0.02975298\n",
      "Iteration 19, loss = 0.02570615\n",
      "Iteration 20, loss = 0.02194480\n",
      "Iteration 21, loss = 0.01914898\n",
      "Iteration 22, loss = 0.01629880\n",
      "Iteration 23, loss = 0.01465387\n",
      "Iteration 24, loss = 0.01233921\n",
      "Iteration 25, loss = 0.01080280\n",
      "Iteration 26, loss = 0.00915691\n",
      "Iteration 27, loss = 0.00801536\n",
      "Iteration 28, loss = 0.00706336\n",
      "Iteration 29, loss = 0.00635092\n",
      "Iteration 30, loss = 0.00562165\n",
      "Iteration 31, loss = 0.00516912\n",
      "Iteration 32, loss = 0.00468459\n",
      "Iteration 33, loss = 0.00420509\n",
      "Iteration 34, loss = 0.00394524\n",
      "Iteration 35, loss = 0.00369752\n",
      "Iteration 36, loss = 0.00351126\n",
      "Iteration 37, loss = 0.00325208\n",
      "Iteration 38, loss = 0.00305761\n",
      "Iteration 39, loss = 0.00295901\n",
      "Iteration 40, loss = 0.00283930\n",
      "Iteration 41, loss = 0.00271137\n",
      "Iteration 42, loss = 0.00260825\n",
      "Iteration 43, loss = 0.00254903\n",
      "Iteration 44, loss = 0.00242351\n",
      "Iteration 45, loss = 0.00236313\n",
      "Iteration 46, loss = 0.00229491\n",
      "Iteration 47, loss = 0.00222343\n",
      "Iteration 48, loss = 0.00216445\n",
      "Iteration 49, loss = 0.00211651\n",
      "Iteration 50, loss = 0.00206619\n",
      "Iteration 51, loss = 0.00202948\n",
      "Iteration 52, loss = 0.00197480\n",
      "Iteration 53, loss = 0.00192393\n",
      "Iteration 54, loss = 0.00190319\n",
      "Iteration 55, loss = 0.00187804\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.0001, batch_size=200, hidden_layer_sizes=(400, 100), solver=adam; total time= 1.7min\n",
      "Iteration 1, loss = 1.11043768\n",
      "Iteration 2, loss = 0.35940708\n",
      "Iteration 3, loss = 0.25682761\n",
      "Iteration 4, loss = 0.20881034\n",
      "Iteration 5, loss = 0.17506318\n",
      "Iteration 6, loss = 0.15037034\n",
      "Iteration 7, loss = 0.13031410\n",
      "Iteration 8, loss = 0.11322528\n",
      "Iteration 9, loss = 0.09978739\n",
      "Iteration 10, loss = 0.08814683\n",
      "Iteration 11, loss = 0.07788390\n",
      "Iteration 12, loss = 0.06832854\n",
      "Iteration 13, loss = 0.06060848\n",
      "Iteration 14, loss = 0.05220776\n",
      "Iteration 15, loss = 0.04627658\n",
      "Iteration 16, loss = 0.04069761\n",
      "Iteration 17, loss = 0.03518044\n",
      "Iteration 18, loss = 0.03077198\n",
      "Iteration 19, loss = 0.02664775\n",
      "Iteration 20, loss = 0.02334389\n",
      "Iteration 21, loss = 0.01975268\n",
      "Iteration 22, loss = 0.01771574\n",
      "Iteration 23, loss = 0.01530585\n",
      "Iteration 24, loss = 0.01313967\n",
      "Iteration 25, loss = 0.01138973\n",
      "Iteration 26, loss = 0.01020956\n",
      "Iteration 27, loss = 0.00882364\n",
      "Iteration 28, loss = 0.00783198\n",
      "Iteration 29, loss = 0.00731520\n",
      "Iteration 30, loss = 0.00589351\n",
      "Iteration 31, loss = 0.00547108\n",
      "Iteration 32, loss = 0.00479088\n",
      "Iteration 33, loss = 0.00477532\n",
      "Iteration 34, loss = 0.00419162\n",
      "Iteration 35, loss = 0.00376789\n",
      "Iteration 36, loss = 0.00369583\n",
      "Iteration 37, loss = 0.00335137\n",
      "Iteration 38, loss = 0.00332061\n",
      "Iteration 39, loss = 0.00299679\n",
      "Iteration 40, loss = 0.00292176\n",
      "Iteration 41, loss = 0.00273310\n",
      "Iteration 42, loss = 0.00263028\n",
      "Iteration 43, loss = 0.00254804\n",
      "Iteration 44, loss = 0.00248669\n",
      "Iteration 45, loss = 0.00237549\n",
      "Iteration 46, loss = 0.00230762\n",
      "Iteration 47, loss = 0.00226176\n",
      "Iteration 48, loss = 0.00217310\n",
      "Iteration 49, loss = 0.00214161\n",
      "Iteration 50, loss = 0.00209070\n",
      "Iteration 51, loss = 0.00203107\n",
      "Iteration 52, loss = 0.00198792\n",
      "Iteration 53, loss = 0.00193490\n",
      "Iteration 54, loss = 0.00189632\n",
      "Iteration 55, loss = 0.00185415\n",
      "Iteration 56, loss = 0.00183272\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.0001, batch_size=200, hidden_layer_sizes=(400, 100), solver=adam; total time= 1.7min\n",
      "Iteration 1, loss = 1.12565509\n",
      "Iteration 2, loss = 0.36888632\n",
      "Iteration 3, loss = 0.26317501\n",
      "Iteration 4, loss = 0.21293740\n",
      "Iteration 5, loss = 0.17910737\n",
      "Iteration 6, loss = 0.15490671\n",
      "Iteration 7, loss = 0.13511752\n",
      "Iteration 8, loss = 0.11807700\n",
      "Iteration 9, loss = 0.10437635\n",
      "Iteration 10, loss = 0.09106638\n",
      "Iteration 11, loss = 0.08068439\n",
      "Iteration 12, loss = 0.07008463\n",
      "Iteration 13, loss = 0.06249060\n",
      "Iteration 14, loss = 0.05482977\n",
      "Iteration 15, loss = 0.04814170\n",
      "Iteration 16, loss = 0.04331397\n",
      "Iteration 17, loss = 0.03792347\n",
      "Iteration 18, loss = 0.03305945\n",
      "Iteration 19, loss = 0.02854299\n",
      "Iteration 20, loss = 0.02479007\n",
      "Iteration 21, loss = 0.02186359\n",
      "Iteration 22, loss = 0.01841578\n",
      "Iteration 23, loss = 0.01596127\n",
      "Iteration 24, loss = 0.01395578\n",
      "Iteration 25, loss = 0.01212616\n",
      "Iteration 26, loss = 0.01080968\n",
      "Iteration 27, loss = 0.00912819\n",
      "Iteration 28, loss = 0.00807194\n",
      "Iteration 29, loss = 0.00760275\n",
      "Iteration 30, loss = 0.00628277\n",
      "Iteration 31, loss = 0.00579936\n",
      "Iteration 32, loss = 0.00494352\n",
      "Iteration 33, loss = 0.00452055\n",
      "Iteration 34, loss = 0.00414794\n",
      "Iteration 35, loss = 0.00395732\n",
      "Iteration 36, loss = 0.00365681\n",
      "Iteration 37, loss = 0.00352664\n",
      "Iteration 38, loss = 0.00321536\n",
      "Iteration 39, loss = 0.00307617\n",
      "Iteration 40, loss = 0.00291633\n",
      "Iteration 41, loss = 0.00279537\n",
      "Iteration 42, loss = 0.00269903\n",
      "Iteration 43, loss = 0.00260137\n",
      "Iteration 44, loss = 0.00252910\n",
      "Iteration 45, loss = 0.00242486\n",
      "Iteration 46, loss = 0.00237387\n",
      "Iteration 47, loss = 0.00229531\n",
      "Iteration 48, loss = 0.00227311\n",
      "Iteration 49, loss = 0.00217510\n",
      "Iteration 50, loss = 0.00213752\n",
      "Iteration 51, loss = 0.00207898\n",
      "Iteration 52, loss = 0.00201390\n",
      "Iteration 53, loss = 0.00202562\n",
      "Iteration 54, loss = 0.01645957\n",
      "Iteration 55, loss = 0.00457720\n",
      "Iteration 56, loss = 0.00234998\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.0001, batch_size=200, hidden_layer_sizes=(400, 100), solver=adam; total time= 1.7min\n",
      "Iteration 1, loss = 1.11848551\n",
      "Iteration 2, loss = 0.35765143\n",
      "Iteration 3, loss = 0.26174445\n",
      "Iteration 4, loss = 0.21613422\n",
      "Iteration 5, loss = 0.18171110\n",
      "Iteration 6, loss = 0.15666980\n",
      "Iteration 7, loss = 0.13704723\n",
      "Iteration 8, loss = 0.11781758\n",
      "Iteration 9, loss = 0.10386933\n",
      "Iteration 10, loss = 0.09071147\n",
      "Iteration 11, loss = 0.08033371\n",
      "Iteration 12, loss = 0.07023968\n",
      "Iteration 13, loss = 0.06103066\n",
      "Iteration 14, loss = 0.05442675\n",
      "Iteration 15, loss = 0.04665901\n",
      "Iteration 16, loss = 0.04056203\n",
      "Iteration 17, loss = 0.03583041\n",
      "Iteration 18, loss = 0.03116959\n",
      "Iteration 19, loss = 0.02660739\n",
      "Iteration 20, loss = 0.02391841\n",
      "Iteration 21, loss = 0.02037898\n",
      "Iteration 22, loss = 0.01695836\n",
      "Iteration 23, loss = 0.01502245\n",
      "Iteration 24, loss = 0.01272319\n",
      "Iteration 25, loss = 0.01149806\n",
      "Iteration 26, loss = 0.00938138\n",
      "Iteration 27, loss = 0.00849266\n",
      "Iteration 28, loss = 0.00735241\n",
      "Iteration 29, loss = 0.00667516\n",
      "Iteration 30, loss = 0.00592025\n",
      "Iteration 31, loss = 0.00523237\n",
      "Iteration 32, loss = 0.00470159\n",
      "Iteration 33, loss = 0.00435218\n",
      "Iteration 34, loss = 0.00402734\n",
      "Iteration 35, loss = 0.00379609\n",
      "Iteration 36, loss = 0.00358237\n",
      "Iteration 37, loss = 0.00335749\n",
      "Iteration 38, loss = 0.00323353\n",
      "Iteration 39, loss = 0.00303838\n",
      "Iteration 40, loss = 0.00287687\n",
      "Iteration 41, loss = 0.00272501\n",
      "Iteration 42, loss = 0.00260338\n",
      "Iteration 43, loss = 0.00252840\n",
      "Iteration 44, loss = 0.00246431\n",
      "Iteration 45, loss = 0.00239074\n",
      "Iteration 46, loss = 0.00230424\n",
      "Iteration 47, loss = 0.00227742\n",
      "Iteration 48, loss = 0.00220894\n",
      "Iteration 49, loss = 0.00212808\n",
      "Iteration 50, loss = 0.00208729\n",
      "Iteration 51, loss = 0.00204020\n",
      "Iteration 52, loss = 0.00198286\n",
      "Iteration 53, loss = 0.00194443\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.0001, batch_size=200, hidden_layer_sizes=(400, 100), solver=adam; total time= 1.6min\n",
      "Iteration 1, loss = 2.31263727\n",
      "Iteration 2, loss = 2.29754176\n",
      "Iteration 3, loss = 2.29443482\n",
      "Iteration 4, loss = 2.29137397\n",
      "Iteration 5, loss = 2.28802580\n",
      "Iteration 6, loss = 2.28474050\n",
      "Iteration 7, loss = 2.28098245\n",
      "Iteration 8, loss = 2.27727482\n",
      "Iteration 9, loss = 2.27283839\n",
      "Iteration 10, loss = 2.26825570\n",
      "Iteration 11, loss = 2.26330813\n",
      "Iteration 12, loss = 2.25750576\n",
      "Iteration 13, loss = 2.25110995\n",
      "Iteration 14, loss = 2.24377782\n",
      "Iteration 15, loss = 2.23557606\n",
      "Iteration 16, loss = 2.22607446\n",
      "Iteration 17, loss = 2.21484291\n",
      "Iteration 18, loss = 2.20216017\n",
      "Iteration 19, loss = 2.18715429\n",
      "Iteration 20, loss = 2.16959391\n",
      "Iteration 21, loss = 2.14877141\n",
      "Iteration 22, loss = 2.12426794\n",
      "Iteration 23, loss = 2.09556626\n",
      "Iteration 24, loss = 2.06203934\n",
      "Iteration 25, loss = 2.02347198\n",
      "Iteration 26, loss = 1.97946085\n",
      "Iteration 27, loss = 1.93011836\n",
      "Iteration 28, loss = 1.87617105\n",
      "Iteration 29, loss = 1.81822883\n",
      "Iteration 30, loss = 1.75722123\n",
      "Iteration 31, loss = 1.69388279\n",
      "Iteration 32, loss = 1.62905266\n",
      "Iteration 33, loss = 1.56381045\n",
      "Iteration 34, loss = 1.49887966\n",
      "Iteration 35, loss = 1.43527203\n",
      "Iteration 36, loss = 1.37398504\n",
      "Iteration 37, loss = 1.31578607\n",
      "Iteration 38, loss = 1.26121275\n",
      "Iteration 39, loss = 1.21039129\n",
      "Iteration 40, loss = 1.16394500\n",
      "Iteration 41, loss = 1.12113589\n",
      "Iteration 42, loss = 1.08204004\n",
      "Iteration 43, loss = 1.04633973\n",
      "Iteration 44, loss = 1.01371247\n",
      "Iteration 45, loss = 0.98379101\n",
      "Iteration 46, loss = 0.95642986\n",
      "Iteration 47, loss = 0.93112106\n",
      "Iteration 48, loss = 0.90767409\n",
      "Iteration 49, loss = 0.88582078\n",
      "Iteration 50, loss = 0.86535836\n",
      "Iteration 51, loss = 0.84623872\n",
      "Iteration 52, loss = 0.82809948\n",
      "Iteration 53, loss = 0.81116774\n",
      "Iteration 54, loss = 0.79489156\n",
      "Iteration 55, loss = 0.77953576\n",
      "Iteration 56, loss = 0.76486079\n",
      "Iteration 57, loss = 0.75085086\n",
      "Iteration 58, loss = 0.73760573\n",
      "Iteration 59, loss = 0.72472196\n",
      "Iteration 60, loss = 0.71244894\n",
      "Iteration 61, loss = 0.70073733\n",
      "Iteration 62, loss = 0.68944433\n",
      "Iteration 63, loss = 0.67867082\n",
      "Iteration 64, loss = 0.66812587\n",
      "Iteration 65, loss = 0.65822224\n",
      "Iteration 66, loss = 0.64843949\n",
      "Iteration 67, loss = 0.63931457\n",
      "Iteration 68, loss = 0.63040591\n",
      "Iteration 69, loss = 0.62178824\n",
      "Iteration 70, loss = 0.61344886\n",
      "Iteration 71, loss = 0.60557437\n",
      "Iteration 72, loss = 0.59796072\n",
      "Iteration 73, loss = 0.59055263\n",
      "Iteration 74, loss = 0.58349161\n",
      "Iteration 75, loss = 0.57654156\n",
      "Iteration 76, loss = 0.56993989\n",
      "Iteration 77, loss = 0.56350849\n",
      "Iteration 78, loss = 0.55731581\n",
      "Iteration 79, loss = 0.55138039\n",
      "Iteration 80, loss = 0.54552512\n",
      "Iteration 81, loss = 0.53999591\n",
      "Iteration 82, loss = 0.53447405\n",
      "Iteration 83, loss = 0.52914921\n",
      "Iteration 84, loss = 0.52414232\n",
      "Iteration 85, loss = 0.51919704\n",
      "Iteration 86, loss = 0.51441973\n",
      "Iteration 87, loss = 0.50965269\n",
      "Iteration 88, loss = 0.50529121\n",
      "Iteration 89, loss = 0.50084108\n",
      "Iteration 90, loss = 0.49660658\n",
      "Iteration 91, loss = 0.49241572\n",
      "Iteration 92, loss = 0.48839868\n",
      "Iteration 93, loss = 0.48458888\n",
      "Iteration 94, loss = 0.48070994\n",
      "Iteration 95, loss = 0.47701996\n",
      "Iteration 96, loss = 0.47340526\n",
      "Iteration 97, loss = 0.46988642\n",
      "Iteration 98, loss = 0.46660446\n",
      "Iteration 99, loss = 0.46328915\n",
      "Iteration 100, loss = 0.46002916\n",
      "Iteration 101, loss = 0.45696671\n",
      "Iteration 102, loss = 0.45384869\n",
      "Iteration 103, loss = 0.45095762\n",
      "Iteration 104, loss = 0.44802910\n",
      "Iteration 105, loss = 0.44525447\n",
      "Iteration 106, loss = 0.44251028\n",
      "Iteration 107, loss = 0.43984196\n",
      "Iteration 108, loss = 0.43725447\n",
      "Iteration 109, loss = 0.43470980\n",
      "Iteration 110, loss = 0.43226391\n",
      "Iteration 111, loss = 0.42989612\n",
      "Iteration 112, loss = 0.42757875\n",
      "Iteration 113, loss = 0.42532227\n",
      "Iteration 114, loss = 0.42311210\n",
      "Iteration 115, loss = 0.42093084\n",
      "Iteration 116, loss = 0.41886156\n",
      "Iteration 117, loss = 0.41682449\n",
      "Iteration 118, loss = 0.41478983\n",
      "Iteration 119, loss = 0.41285063\n",
      "Iteration 120, loss = 0.41094569\n",
      "Iteration 121, loss = 0.40906232\n",
      "Iteration 122, loss = 0.40728655\n",
      "Iteration 123, loss = 0.40546526\n",
      "Iteration 124, loss = 0.40382099\n",
      "Iteration 125, loss = 0.40203683\n",
      "Iteration 126, loss = 0.40044512\n",
      "Iteration 127, loss = 0.39876461\n",
      "Iteration 128, loss = 0.39718498\n",
      "Iteration 129, loss = 0.39564057\n",
      "Iteration 130, loss = 0.39413433\n",
      "Iteration 131, loss = 0.39263749\n",
      "Iteration 132, loss = 0.39113756\n",
      "Iteration 133, loss = 0.38980379\n",
      "Iteration 134, loss = 0.38830926\n",
      "Iteration 135, loss = 0.38701430\n",
      "Iteration 136, loss = 0.38563404\n",
      "Iteration 137, loss = 0.38438739\n",
      "Iteration 138, loss = 0.38305823\n",
      "Iteration 139, loss = 0.38175573\n",
      "Iteration 140, loss = 0.38059363\n",
      "Iteration 141, loss = 0.37934423\n",
      "Iteration 142, loss = 0.37818778\n",
      "Iteration 143, loss = 0.37698628\n",
      "Iteration 144, loss = 0.37573975\n",
      "Iteration 145, loss = 0.37471666\n",
      "Iteration 146, loss = 0.37357864\n",
      "Iteration 147, loss = 0.37247961\n",
      "Iteration 148, loss = 0.37139361\n",
      "Iteration 149, loss = 0.37028416\n",
      "Iteration 150, loss = 0.36925338\n",
      "Iteration 151, loss = 0.36821580\n",
      "Iteration 152, loss = 0.36722343\n",
      "Iteration 153, loss = 0.36625580\n",
      "Iteration 154, loss = 0.36525559\n",
      "Iteration 155, loss = 0.36434616\n",
      "Iteration 156, loss = 0.36337202\n",
      "Iteration 157, loss = 0.36238420\n",
      "Iteration 158, loss = 0.36157319\n",
      "Iteration 159, loss = 0.36060377\n",
      "Iteration 160, loss = 0.35970053\n",
      "Iteration 161, loss = 0.35882803\n",
      "Iteration 162, loss = 0.35796892\n",
      "Iteration 163, loss = 0.35709110\n",
      "Iteration 164, loss = 0.35624670\n",
      "Iteration 165, loss = 0.35533101\n",
      "Iteration 166, loss = 0.35452931\n",
      "Iteration 167, loss = 0.35377808\n",
      "Iteration 168, loss = 0.35291219\n",
      "Iteration 169, loss = 0.35220537\n",
      "Iteration 170, loss = 0.35141334\n",
      "Iteration 171, loss = 0.35057840\n",
      "Iteration 172, loss = 0.34986426\n",
      "Iteration 173, loss = 0.34905455\n",
      "Iteration 174, loss = 0.34830719\n",
      "Iteration 175, loss = 0.34753821\n",
      "Iteration 176, loss = 0.34681709\n",
      "Iteration 177, loss = 0.34607842\n",
      "Iteration 178, loss = 0.34544560\n",
      "Iteration 179, loss = 0.34465036\n",
      "Iteration 180, loss = 0.34400897\n",
      "Iteration 181, loss = 0.34326507\n",
      "Iteration 182, loss = 0.34267036\n",
      "Iteration 183, loss = 0.34187047\n",
      "Iteration 184, loss = 0.34117664\n",
      "Iteration 185, loss = 0.34056754\n",
      "Iteration 186, loss = 0.33986655\n",
      "Iteration 187, loss = 0.33921907\n",
      "Iteration 188, loss = 0.33859808\n",
      "Iteration 189, loss = 0.33795408\n",
      "Iteration 190, loss = 0.33730563\n",
      "Iteration 191, loss = 0.33666141\n",
      "Iteration 192, loss = 0.33602447\n",
      "Iteration 193, loss = 0.33541294\n",
      "Iteration 194, loss = 0.33478062\n",
      "Iteration 195, loss = 0.33416020\n",
      "Iteration 196, loss = 0.33355140\n",
      "Iteration 197, loss = 0.33302354\n",
      "Iteration 198, loss = 0.33236358\n",
      "Iteration 199, loss = 0.33182700\n",
      "Iteration 200, loss = 0.33118087\n",
      "[CV] END activation=logistic, alpha=0.0001, batch_size=200, hidden_layer_sizes=(400, 100), solver=sgd; total time= 4.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.31181743\n",
      "Iteration 2, loss = 2.29694378\n",
      "Iteration 3, loss = 2.29341363\n",
      "Iteration 4, loss = 2.29008670\n",
      "Iteration 5, loss = 2.28638253\n",
      "Iteration 6, loss = 2.28271309\n",
      "Iteration 7, loss = 2.27855113\n",
      "Iteration 8, loss = 2.27424331\n",
      "Iteration 9, loss = 2.26951459\n",
      "Iteration 10, loss = 2.26429859\n",
      "Iteration 11, loss = 2.25840807\n",
      "Iteration 12, loss = 2.25188765\n",
      "Iteration 13, loss = 2.24461448\n",
      "Iteration 14, loss = 2.23615951\n",
      "Iteration 15, loss = 2.22637368\n",
      "Iteration 16, loss = 2.21538711\n",
      "Iteration 17, loss = 2.20214760\n",
      "Iteration 18, loss = 2.18710357\n",
      "Iteration 19, loss = 2.16890034\n",
      "Iteration 20, loss = 2.14774819\n",
      "Iteration 21, loss = 2.12254197\n",
      "Iteration 22, loss = 2.09274371\n",
      "Iteration 23, loss = 2.05763919\n",
      "Iteration 24, loss = 2.01680133\n",
      "Iteration 25, loss = 1.96960783\n",
      "Iteration 26, loss = 1.91626691\n",
      "Iteration 27, loss = 1.85712332\n",
      "Iteration 28, loss = 1.79331047\n",
      "Iteration 29, loss = 1.72602929\n",
      "Iteration 30, loss = 1.65689099\n",
      "Iteration 31, loss = 1.58724606\n",
      "Iteration 32, loss = 1.51903555\n",
      "Iteration 33, loss = 1.45341778\n",
      "Iteration 34, loss = 1.39134359\n",
      "Iteration 35, loss = 1.33353049\n",
      "Iteration 36, loss = 1.28008472\n",
      "Iteration 37, loss = 1.23128222\n",
      "Iteration 38, loss = 1.18674830\n",
      "Iteration 39, loss = 1.14608161\n",
      "Iteration 40, loss = 1.10922013\n",
      "Iteration 41, loss = 1.07556373\n",
      "Iteration 42, loss = 1.04478134\n",
      "Iteration 43, loss = 1.01651598\n",
      "Iteration 44, loss = 0.99035716\n",
      "Iteration 45, loss = 0.96609279\n",
      "Iteration 46, loss = 0.94342286\n",
      "Iteration 47, loss = 0.92224753\n",
      "Iteration 48, loss = 0.90227863\n",
      "Iteration 49, loss = 0.88325308\n",
      "Iteration 50, loss = 0.86523268\n",
      "Iteration 51, loss = 0.84805720\n",
      "Iteration 52, loss = 0.83162636\n",
      "Iteration 53, loss = 0.81578266\n",
      "Iteration 54, loss = 0.80073655\n",
      "Iteration 55, loss = 0.78601350\n",
      "Iteration 56, loss = 0.77201478\n",
      "Iteration 57, loss = 0.75831049\n",
      "Iteration 58, loss = 0.74516597\n",
      "Iteration 59, loss = 0.73252692\n",
      "Iteration 60, loss = 0.72016400\n",
      "Iteration 61, loss = 0.70841060\n",
      "Iteration 62, loss = 0.69681719\n",
      "Iteration 63, loss = 0.68582681\n",
      "Iteration 64, loss = 0.67501199\n",
      "Iteration 65, loss = 0.66466434\n",
      "Iteration 66, loss = 0.65468182\n",
      "Iteration 67, loss = 0.64511185\n",
      "Iteration 68, loss = 0.63576454\n",
      "Iteration 69, loss = 0.62676013\n",
      "Iteration 70, loss = 0.61809169\n",
      "Iteration 71, loss = 0.60973914\n",
      "Iteration 72, loss = 0.60171430\n",
      "Iteration 73, loss = 0.59390910\n",
      "Iteration 74, loss = 0.58649112\n",
      "Iteration 75, loss = 0.57920636\n",
      "Iteration 76, loss = 0.57222139\n",
      "Iteration 77, loss = 0.56548674\n",
      "Iteration 78, loss = 0.55906992\n",
      "Iteration 79, loss = 0.55273538\n",
      "Iteration 80, loss = 0.54666316\n",
      "Iteration 81, loss = 0.54082659\n",
      "Iteration 82, loss = 0.53516629\n",
      "Iteration 83, loss = 0.52966250\n",
      "Iteration 84, loss = 0.52433589\n",
      "Iteration 85, loss = 0.51920258\n",
      "Iteration 86, loss = 0.51427364\n",
      "Iteration 87, loss = 0.50949607\n",
      "Iteration 88, loss = 0.50488327\n",
      "Iteration 89, loss = 0.50026422\n",
      "Iteration 90, loss = 0.49593527\n",
      "Iteration 91, loss = 0.49172703\n",
      "Iteration 92, loss = 0.48758518\n",
      "Iteration 93, loss = 0.48358680\n",
      "Iteration 94, loss = 0.47974046\n",
      "Iteration 95, loss = 0.47599941\n",
      "Iteration 96, loss = 0.47234330\n",
      "Iteration 97, loss = 0.46886904\n",
      "Iteration 98, loss = 0.46536824\n",
      "Iteration 99, loss = 0.46206767\n",
      "Iteration 100, loss = 0.45873762\n",
      "Iteration 101, loss = 0.45564026\n",
      "Iteration 102, loss = 0.45256790\n",
      "Iteration 103, loss = 0.44957268\n",
      "Iteration 104, loss = 0.44666069\n",
      "Iteration 105, loss = 0.44382853\n",
      "Iteration 106, loss = 0.44105254\n",
      "Iteration 107, loss = 0.43844365\n",
      "Iteration 108, loss = 0.43574004\n",
      "Iteration 109, loss = 0.43325115\n",
      "Iteration 110, loss = 0.43078863\n",
      "Iteration 111, loss = 0.42834814\n",
      "Iteration 112, loss = 0.42601571\n",
      "Iteration 113, loss = 0.42373162\n",
      "Iteration 114, loss = 0.42154346\n",
      "Iteration 115, loss = 0.41939918\n",
      "Iteration 116, loss = 0.41724639\n",
      "Iteration 117, loss = 0.41515638\n",
      "Iteration 118, loss = 0.41314279\n",
      "Iteration 119, loss = 0.41103441\n",
      "Iteration 120, loss = 0.40916130\n",
      "Iteration 121, loss = 0.40732296\n",
      "Iteration 122, loss = 0.40544391\n",
      "Iteration 123, loss = 0.40373975\n",
      "Iteration 124, loss = 0.40194866\n",
      "Iteration 125, loss = 0.40016780\n",
      "Iteration 126, loss = 0.39846675\n",
      "Iteration 127, loss = 0.39684257\n",
      "Iteration 128, loss = 0.39530179\n",
      "Iteration 129, loss = 0.39366305\n",
      "Iteration 130, loss = 0.39211816\n",
      "Iteration 131, loss = 0.39061238\n",
      "Iteration 132, loss = 0.38912642\n",
      "Iteration 133, loss = 0.38767653\n",
      "Iteration 134, loss = 0.38619721\n",
      "Iteration 135, loss = 0.38483221\n",
      "Iteration 136, loss = 0.38345248\n",
      "Iteration 137, loss = 0.38214412\n",
      "Iteration 138, loss = 0.38078888\n",
      "Iteration 139, loss = 0.37955563\n",
      "Iteration 140, loss = 0.37823621\n",
      "Iteration 141, loss = 0.37697285\n",
      "Iteration 142, loss = 0.37575337\n",
      "Iteration 143, loss = 0.37457227\n",
      "Iteration 144, loss = 0.37340310\n",
      "Iteration 145, loss = 0.37215929\n",
      "Iteration 146, loss = 0.37111036\n",
      "Iteration 147, loss = 0.36986852\n",
      "Iteration 148, loss = 0.36885525\n",
      "Iteration 149, loss = 0.36779341\n",
      "Iteration 150, loss = 0.36660429\n",
      "Iteration 151, loss = 0.36559445\n",
      "Iteration 152, loss = 0.36456144\n",
      "Iteration 153, loss = 0.36354174\n",
      "Iteration 154, loss = 0.36249839\n",
      "Iteration 155, loss = 0.36157303\n",
      "Iteration 156, loss = 0.36061280\n",
      "Iteration 157, loss = 0.35963106\n",
      "Iteration 158, loss = 0.35860935\n",
      "Iteration 159, loss = 0.35779137\n",
      "Iteration 160, loss = 0.35682649\n",
      "Iteration 161, loss = 0.35594146\n",
      "Iteration 162, loss = 0.35489072\n",
      "Iteration 163, loss = 0.35417658\n",
      "Iteration 164, loss = 0.35322682\n",
      "Iteration 165, loss = 0.35236320\n",
      "Iteration 166, loss = 0.35147602\n",
      "Iteration 167, loss = 0.35064581\n",
      "Iteration 168, loss = 0.34984522\n",
      "Iteration 169, loss = 0.34902433\n",
      "Iteration 170, loss = 0.34818366\n",
      "Iteration 171, loss = 0.34741910\n",
      "Iteration 172, loss = 0.34662261\n",
      "Iteration 173, loss = 0.34585764\n",
      "Iteration 174, loss = 0.34499210\n",
      "Iteration 175, loss = 0.34424125\n",
      "Iteration 176, loss = 0.34358662\n",
      "Iteration 177, loss = 0.34274885\n",
      "Iteration 178, loss = 0.34208409\n",
      "Iteration 179, loss = 0.34132581\n",
      "Iteration 180, loss = 0.34058340\n",
      "Iteration 181, loss = 0.33987829\n",
      "Iteration 182, loss = 0.33912292\n",
      "Iteration 183, loss = 0.33847339\n",
      "Iteration 184, loss = 0.33782879\n",
      "Iteration 185, loss = 0.33695923\n",
      "Iteration 186, loss = 0.33646045\n",
      "Iteration 187, loss = 0.33567149\n",
      "Iteration 188, loss = 0.33505808\n",
      "Iteration 189, loss = 0.33443886\n",
      "Iteration 190, loss = 0.33371615\n",
      "Iteration 191, loss = 0.33305658\n",
      "Iteration 192, loss = 0.33244333\n",
      "Iteration 193, loss = 0.33175385\n",
      "Iteration 194, loss = 0.33123765\n",
      "Iteration 195, loss = 0.33056209\n",
      "Iteration 196, loss = 0.32987910\n",
      "Iteration 197, loss = 0.32924614\n",
      "Iteration 198, loss = 0.32865430\n",
      "Iteration 199, loss = 0.32807947\n",
      "Iteration 200, loss = 0.32744402\n",
      "[CV] END activation=logistic, alpha=0.0001, batch_size=200, hidden_layer_sizes=(400, 100), solver=sgd; total time= 4.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.31203892\n",
      "Iteration 2, loss = 2.29833521\n",
      "Iteration 3, loss = 2.29515681\n",
      "Iteration 4, loss = 2.29152905\n",
      "Iteration 5, loss = 2.28785816\n",
      "Iteration 6, loss = 2.28432129\n",
      "Iteration 7, loss = 2.28008340\n",
      "Iteration 8, loss = 2.27585882\n",
      "Iteration 9, loss = 2.27092351\n",
      "Iteration 10, loss = 2.26570629\n",
      "Iteration 11, loss = 2.25982096\n",
      "Iteration 12, loss = 2.25306539\n",
      "Iteration 13, loss = 2.24541340\n",
      "Iteration 14, loss = 2.23658852\n",
      "Iteration 15, loss = 2.22653879\n",
      "Iteration 16, loss = 2.21448845\n",
      "Iteration 17, loss = 2.20061943\n",
      "Iteration 18, loss = 2.18413807\n",
      "Iteration 19, loss = 2.16477919\n",
      "Iteration 20, loss = 2.14185858\n",
      "Iteration 21, loss = 2.11504419\n",
      "Iteration 22, loss = 2.08372464\n",
      "Iteration 23, loss = 2.04802167\n",
      "Iteration 24, loss = 2.00779126\n",
      "Iteration 25, loss = 1.96357612\n",
      "Iteration 26, loss = 1.91587664\n",
      "Iteration 27, loss = 1.86530849\n",
      "Iteration 28, loss = 1.81269564\n",
      "Iteration 29, loss = 1.75868576\n",
      "Iteration 30, loss = 1.70349110\n",
      "Iteration 31, loss = 1.64720650\n",
      "Iteration 32, loss = 1.59043270\n",
      "Iteration 33, loss = 1.53354890\n",
      "Iteration 34, loss = 1.47722077\n",
      "Iteration 35, loss = 1.42227925\n",
      "Iteration 36, loss = 1.36920867\n",
      "Iteration 37, loss = 1.31873832\n",
      "Iteration 38, loss = 1.27092474\n",
      "Iteration 39, loss = 1.22608774\n",
      "Iteration 40, loss = 1.18407817\n",
      "Iteration 41, loss = 1.14496915\n",
      "Iteration 42, loss = 1.10851990\n",
      "Iteration 43, loss = 1.07458361\n",
      "Iteration 44, loss = 1.04306687\n",
      "Iteration 45, loss = 1.01389930\n",
      "Iteration 46, loss = 0.98655545\n",
      "Iteration 47, loss = 0.96114428\n",
      "Iteration 48, loss = 0.93740715\n",
      "Iteration 49, loss = 0.91493747\n",
      "Iteration 50, loss = 0.89416328\n",
      "Iteration 51, loss = 0.87439113\n",
      "Iteration 52, loss = 0.85570021\n",
      "Iteration 53, loss = 0.83783791\n",
      "Iteration 54, loss = 0.82108827\n",
      "Iteration 55, loss = 0.80488181\n",
      "Iteration 56, loss = 0.78961524\n",
      "Iteration 57, loss = 0.77493822\n",
      "Iteration 58, loss = 0.76078419\n",
      "Iteration 59, loss = 0.74720181\n",
      "Iteration 60, loss = 0.73422931\n",
      "Iteration 61, loss = 0.72174455\n",
      "Iteration 62, loss = 0.70974249\n",
      "Iteration 63, loss = 0.69825948\n",
      "Iteration 64, loss = 0.68706213\n",
      "Iteration 65, loss = 0.67640941\n",
      "Iteration 66, loss = 0.66613499\n",
      "Iteration 67, loss = 0.65627174\n",
      "Iteration 68, loss = 0.64671312\n",
      "Iteration 69, loss = 0.63769528\n",
      "Iteration 70, loss = 0.62896281\n",
      "Iteration 71, loss = 0.62054971\n",
      "Iteration 72, loss = 0.61239353\n",
      "Iteration 73, loss = 0.60466412\n",
      "Iteration 74, loss = 0.59715914\n",
      "Iteration 75, loss = 0.59000928\n",
      "Iteration 76, loss = 0.58311862\n",
      "Iteration 77, loss = 0.57641366\n",
      "Iteration 78, loss = 0.57006058\n",
      "Iteration 79, loss = 0.56396356\n",
      "Iteration 80, loss = 0.55795949\n",
      "Iteration 81, loss = 0.55225795\n",
      "Iteration 82, loss = 0.54677840\n",
      "Iteration 83, loss = 0.54130011\n",
      "Iteration 84, loss = 0.53622023\n",
      "Iteration 85, loss = 0.53121514\n",
      "Iteration 86, loss = 0.52641869\n",
      "Iteration 87, loss = 0.52168247\n",
      "Iteration 88, loss = 0.51724188\n",
      "Iteration 89, loss = 0.51287193\n",
      "Iteration 90, loss = 0.50857080\n",
      "Iteration 91, loss = 0.50446756\n",
      "Iteration 92, loss = 0.50039683\n",
      "Iteration 93, loss = 0.49659764\n",
      "Iteration 94, loss = 0.49277571\n",
      "Iteration 95, loss = 0.48912966\n",
      "Iteration 96, loss = 0.48551271\n",
      "Iteration 97, loss = 0.48205157\n",
      "Iteration 98, loss = 0.47868003\n",
      "Iteration 99, loss = 0.47530012\n",
      "Iteration 100, loss = 0.47214181\n",
      "Iteration 101, loss = 0.46896777\n",
      "Iteration 102, loss = 0.46598937\n",
      "Iteration 103, loss = 0.46296253\n",
      "Iteration 104, loss = 0.46013560\n",
      "Iteration 105, loss = 0.45731490\n",
      "Iteration 106, loss = 0.45447834\n",
      "Iteration 107, loss = 0.45176494\n",
      "Iteration 108, loss = 0.44920531\n",
      "Iteration 109, loss = 0.44659635\n",
      "Iteration 110, loss = 0.44411373\n",
      "Iteration 111, loss = 0.44157155\n",
      "Iteration 112, loss = 0.43932985\n",
      "Iteration 113, loss = 0.43687810\n",
      "Iteration 114, loss = 0.43462585\n",
      "Iteration 115, loss = 0.43232501\n",
      "Iteration 116, loss = 0.43017646\n",
      "Iteration 117, loss = 0.42807769\n",
      "Iteration 118, loss = 0.42594043\n",
      "Iteration 119, loss = 0.42392359\n",
      "Iteration 120, loss = 0.42184318\n",
      "Iteration 121, loss = 0.41990482\n",
      "Iteration 122, loss = 0.41809248\n",
      "Iteration 123, loss = 0.41610018\n",
      "Iteration 124, loss = 0.41426445\n",
      "Iteration 125, loss = 0.41242409\n",
      "Iteration 126, loss = 0.41069823\n",
      "Iteration 127, loss = 0.40889696\n",
      "Iteration 128, loss = 0.40723750\n",
      "Iteration 129, loss = 0.40558510\n",
      "Iteration 130, loss = 0.40391014\n",
      "Iteration 131, loss = 0.40225466\n",
      "Iteration 132, loss = 0.40074453\n",
      "Iteration 133, loss = 0.39912803\n",
      "Iteration 134, loss = 0.39760074\n",
      "Iteration 135, loss = 0.39609066\n",
      "Iteration 136, loss = 0.39462274\n",
      "Iteration 137, loss = 0.39315048\n",
      "Iteration 138, loss = 0.39170241\n",
      "Iteration 139, loss = 0.39033462\n",
      "Iteration 140, loss = 0.38892601\n",
      "Iteration 141, loss = 0.38751290\n",
      "Iteration 142, loss = 0.38621436\n",
      "Iteration 143, loss = 0.38483145\n",
      "Iteration 144, loss = 0.38365926\n",
      "Iteration 145, loss = 0.38232621\n",
      "Iteration 146, loss = 0.38113226\n",
      "Iteration 147, loss = 0.37979833\n",
      "Iteration 148, loss = 0.37862478\n",
      "Iteration 149, loss = 0.37738499\n",
      "Iteration 150, loss = 0.37619801\n",
      "Iteration 151, loss = 0.37506323\n",
      "Iteration 152, loss = 0.37400828\n",
      "Iteration 153, loss = 0.37277608\n",
      "Iteration 154, loss = 0.37166262\n",
      "Iteration 155, loss = 0.37054525\n",
      "Iteration 156, loss = 0.36946160\n",
      "Iteration 157, loss = 0.36840695\n",
      "Iteration 158, loss = 0.36731669\n",
      "Iteration 159, loss = 0.36630013\n",
      "Iteration 160, loss = 0.36523063\n",
      "Iteration 161, loss = 0.36423428\n",
      "Iteration 162, loss = 0.36325538\n",
      "Iteration 163, loss = 0.36217351\n",
      "Iteration 164, loss = 0.36132037\n",
      "Iteration 165, loss = 0.36030986\n",
      "Iteration 166, loss = 0.35936252\n",
      "Iteration 167, loss = 0.35840589\n",
      "Iteration 168, loss = 0.35752034\n",
      "Iteration 169, loss = 0.35653919\n",
      "Iteration 170, loss = 0.35563953\n",
      "Iteration 171, loss = 0.35478316\n",
      "Iteration 172, loss = 0.35385849\n",
      "Iteration 173, loss = 0.35298232\n",
      "Iteration 174, loss = 0.35207119\n",
      "Iteration 175, loss = 0.35124522\n",
      "Iteration 176, loss = 0.35042467\n",
      "Iteration 177, loss = 0.34959662\n",
      "Iteration 178, loss = 0.34877368\n",
      "Iteration 179, loss = 0.34791751\n",
      "Iteration 180, loss = 0.34714594\n",
      "Iteration 181, loss = 0.34626396\n",
      "Iteration 182, loss = 0.34555933\n",
      "Iteration 183, loss = 0.34473094\n",
      "Iteration 184, loss = 0.34402802\n",
      "Iteration 185, loss = 0.34317983\n",
      "Iteration 186, loss = 0.34240171\n",
      "Iteration 187, loss = 0.34164599\n",
      "Iteration 188, loss = 0.34097521\n",
      "Iteration 189, loss = 0.34021485\n",
      "Iteration 190, loss = 0.33946404\n",
      "Iteration 191, loss = 0.33872650\n",
      "Iteration 192, loss = 0.33795685\n",
      "Iteration 193, loss = 0.33725086\n",
      "Iteration 194, loss = 0.33658363\n",
      "Iteration 195, loss = 0.33587994\n",
      "Iteration 196, loss = 0.33520858\n",
      "Iteration 197, loss = 0.33445550\n",
      "Iteration 198, loss = 0.33377865\n",
      "Iteration 199, loss = 0.33318531\n",
      "Iteration 200, loss = 0.33247491\n",
      "[CV] END activation=logistic, alpha=0.0001, batch_size=200, hidden_layer_sizes=(400, 100), solver=sgd; total time= 4.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.31222110\n",
      "Iteration 2, loss = 2.29792920\n",
      "Iteration 3, loss = 2.29467121\n",
      "Iteration 4, loss = 2.29130635\n",
      "Iteration 5, loss = 2.28813348\n",
      "Iteration 6, loss = 2.28442558\n",
      "Iteration 7, loss = 2.28079748\n",
      "Iteration 8, loss = 2.27656588\n",
      "Iteration 9, loss = 2.27216521\n",
      "Iteration 10, loss = 2.26730196\n",
      "Iteration 11, loss = 2.26191840\n",
      "Iteration 12, loss = 2.25592347\n",
      "Iteration 13, loss = 2.24904189\n",
      "Iteration 14, loss = 2.24136939\n",
      "Iteration 15, loss = 2.23235081\n",
      "Iteration 16, loss = 2.22201984\n",
      "Iteration 17, loss = 2.20984143\n",
      "Iteration 18, loss = 2.19551821\n",
      "Iteration 19, loss = 2.17876945\n",
      "Iteration 20, loss = 2.15894246\n",
      "Iteration 21, loss = 2.13544058\n",
      "Iteration 22, loss = 2.10774651\n",
      "Iteration 23, loss = 2.07504042\n",
      "Iteration 24, loss = 2.03723963\n",
      "Iteration 25, loss = 1.99419649\n",
      "Iteration 26, loss = 1.94574489\n",
      "Iteration 27, loss = 1.89282585\n",
      "Iteration 28, loss = 1.83586539\n",
      "Iteration 29, loss = 1.77603657\n",
      "Iteration 30, loss = 1.71484881\n",
      "Iteration 31, loss = 1.65283363\n",
      "Iteration 32, loss = 1.59160417\n",
      "Iteration 33, loss = 1.53196676\n",
      "Iteration 34, loss = 1.47488285\n",
      "Iteration 35, loss = 1.42056886\n",
      "Iteration 36, loss = 1.36936201\n",
      "Iteration 37, loss = 1.32114026\n",
      "Iteration 38, loss = 1.27564162\n",
      "Iteration 39, loss = 1.23275343\n",
      "Iteration 40, loss = 1.19202871\n",
      "Iteration 41, loss = 1.15379962\n",
      "Iteration 42, loss = 1.11764534\n",
      "Iteration 43, loss = 1.08358247\n",
      "Iteration 44, loss = 1.05155452\n",
      "Iteration 45, loss = 1.02153132\n",
      "Iteration 46, loss = 0.99339434\n",
      "Iteration 47, loss = 0.96705456\n",
      "Iteration 48, loss = 0.94224570\n",
      "Iteration 49, loss = 0.91878598\n",
      "Iteration 50, loss = 0.89672922\n",
      "Iteration 51, loss = 0.87586138\n",
      "Iteration 52, loss = 0.85619671\n",
      "Iteration 53, loss = 0.83752795\n",
      "Iteration 54, loss = 0.81961960\n",
      "Iteration 55, loss = 0.80279966\n",
      "Iteration 56, loss = 0.78659091\n",
      "Iteration 57, loss = 0.77116276\n",
      "Iteration 58, loss = 0.75642710\n",
      "Iteration 59, loss = 0.74237489\n",
      "Iteration 60, loss = 0.72894149\n",
      "Iteration 61, loss = 0.71605927\n",
      "Iteration 62, loss = 0.70366101\n",
      "Iteration 63, loss = 0.69195243\n",
      "Iteration 64, loss = 0.68071110\n",
      "Iteration 65, loss = 0.66984819\n",
      "Iteration 66, loss = 0.65944944\n",
      "Iteration 67, loss = 0.64951155\n",
      "Iteration 68, loss = 0.64011326\n",
      "Iteration 69, loss = 0.63090921\n",
      "Iteration 70, loss = 0.62210373\n",
      "Iteration 71, loss = 0.61376277\n",
      "Iteration 72, loss = 0.60568891\n",
      "Iteration 73, loss = 0.59795896\n",
      "Iteration 74, loss = 0.59046436\n",
      "Iteration 75, loss = 0.58337383\n",
      "Iteration 76, loss = 0.57644016\n",
      "Iteration 77, loss = 0.56979768\n",
      "Iteration 78, loss = 0.56344928\n",
      "Iteration 79, loss = 0.55721265\n",
      "Iteration 80, loss = 0.55125343\n",
      "Iteration 81, loss = 0.54545757\n",
      "Iteration 82, loss = 0.53991718\n",
      "Iteration 83, loss = 0.53457714\n",
      "Iteration 84, loss = 0.52939340\n",
      "Iteration 85, loss = 0.52433244\n",
      "Iteration 86, loss = 0.51948561\n",
      "Iteration 87, loss = 0.51479854\n",
      "Iteration 88, loss = 0.51019594\n",
      "Iteration 89, loss = 0.50581004\n",
      "Iteration 90, loss = 0.50149586\n",
      "Iteration 91, loss = 0.49728152\n",
      "Iteration 92, loss = 0.49331201\n",
      "Iteration 93, loss = 0.48935410\n",
      "Iteration 94, loss = 0.48557660\n",
      "Iteration 95, loss = 0.48176166\n",
      "Iteration 96, loss = 0.47827608\n",
      "Iteration 97, loss = 0.47472772\n",
      "Iteration 98, loss = 0.47134346\n",
      "Iteration 99, loss = 0.46802314\n",
      "Iteration 100, loss = 0.46486033\n",
      "Iteration 101, loss = 0.46172128\n",
      "Iteration 102, loss = 0.45863186\n",
      "Iteration 103, loss = 0.45572733\n",
      "Iteration 104, loss = 0.45280491\n",
      "Iteration 105, loss = 0.44998916\n",
      "Iteration 106, loss = 0.44725625\n",
      "Iteration 107, loss = 0.44456614\n",
      "Iteration 108, loss = 0.44197864\n",
      "Iteration 109, loss = 0.43942522\n",
      "Iteration 110, loss = 0.43687874\n",
      "Iteration 111, loss = 0.43459543\n",
      "Iteration 112, loss = 0.43215438\n",
      "Iteration 113, loss = 0.42985647\n",
      "Iteration 114, loss = 0.42765786\n",
      "Iteration 115, loss = 0.42533411\n",
      "Iteration 116, loss = 0.42323966\n",
      "Iteration 117, loss = 0.42112780\n",
      "Iteration 118, loss = 0.41909330\n",
      "Iteration 119, loss = 0.41705930\n",
      "Iteration 120, loss = 0.41503284\n",
      "Iteration 121, loss = 0.41326113\n",
      "Iteration 122, loss = 0.41124056\n",
      "Iteration 123, loss = 0.40944246\n",
      "Iteration 124, loss = 0.40764635\n",
      "Iteration 125, loss = 0.40592831\n",
      "Iteration 126, loss = 0.40411259\n",
      "Iteration 127, loss = 0.40243072\n",
      "Iteration 128, loss = 0.40081033\n",
      "Iteration 129, loss = 0.39914536\n",
      "Iteration 130, loss = 0.39759530\n",
      "Iteration 131, loss = 0.39597600\n",
      "Iteration 132, loss = 0.39441198\n",
      "Iteration 133, loss = 0.39298535\n",
      "Iteration 134, loss = 0.39146453\n",
      "Iteration 135, loss = 0.38999301\n",
      "Iteration 136, loss = 0.38858211\n",
      "Iteration 137, loss = 0.38713978\n",
      "Iteration 138, loss = 0.38575558\n",
      "Iteration 139, loss = 0.38450725\n",
      "Iteration 140, loss = 0.38307974\n",
      "Iteration 141, loss = 0.38176994\n",
      "Iteration 142, loss = 0.38056706\n",
      "Iteration 143, loss = 0.37919204\n",
      "Iteration 144, loss = 0.37795230\n",
      "Iteration 145, loss = 0.37671293\n",
      "Iteration 146, loss = 0.37550808\n",
      "Iteration 147, loss = 0.37432190\n",
      "Iteration 148, loss = 0.37318118\n",
      "Iteration 149, loss = 0.37199886\n",
      "Iteration 150, loss = 0.37082403\n",
      "Iteration 151, loss = 0.36974773\n",
      "Iteration 152, loss = 0.36862757\n",
      "Iteration 153, loss = 0.36757664\n",
      "Iteration 154, loss = 0.36652720\n",
      "Iteration 155, loss = 0.36551169\n",
      "Iteration 156, loss = 0.36439717\n",
      "Iteration 157, loss = 0.36331678\n",
      "Iteration 158, loss = 0.36236546\n",
      "Iteration 159, loss = 0.36134167\n",
      "Iteration 160, loss = 0.36029049\n",
      "Iteration 161, loss = 0.35934162\n",
      "Iteration 162, loss = 0.35837311\n",
      "Iteration 163, loss = 0.35744914\n",
      "Iteration 164, loss = 0.35652804\n",
      "Iteration 165, loss = 0.35550972\n",
      "Iteration 166, loss = 0.35467231\n",
      "Iteration 167, loss = 0.35384556\n",
      "Iteration 168, loss = 0.35289051\n",
      "Iteration 169, loss = 0.35201860\n",
      "Iteration 170, loss = 0.35116796\n",
      "Iteration 171, loss = 0.35025449\n",
      "Iteration 172, loss = 0.34946752\n",
      "Iteration 173, loss = 0.34858252\n",
      "Iteration 174, loss = 0.34782906\n",
      "Iteration 175, loss = 0.34696268\n",
      "Iteration 176, loss = 0.34607328\n",
      "Iteration 177, loss = 0.34538509\n",
      "Iteration 178, loss = 0.34447954\n",
      "Iteration 179, loss = 0.34365726\n",
      "Iteration 180, loss = 0.34292939\n",
      "Iteration 181, loss = 0.34220057\n",
      "Iteration 182, loss = 0.34135841\n",
      "Iteration 183, loss = 0.34070377\n",
      "Iteration 184, loss = 0.33984806\n",
      "Iteration 185, loss = 0.33919012\n",
      "Iteration 186, loss = 0.33839978\n",
      "Iteration 187, loss = 0.33762379\n",
      "Iteration 188, loss = 0.33702448\n",
      "Iteration 189, loss = 0.33626916\n",
      "Iteration 190, loss = 0.33563392\n",
      "Iteration 191, loss = 0.33486111\n",
      "Iteration 192, loss = 0.33413251\n",
      "Iteration 193, loss = 0.33350056\n",
      "Iteration 194, loss = 0.33274093\n",
      "Iteration 195, loss = 0.33216308\n",
      "Iteration 196, loss = 0.33147415\n",
      "Iteration 197, loss = 0.33078175\n",
      "Iteration 198, loss = 0.33006929\n",
      "Iteration 199, loss = 0.32944927\n",
      "Iteration 200, loss = 0.32876943\n",
      "[CV] END activation=logistic, alpha=0.0001, batch_size=200, hidden_layer_sizes=(400, 100), solver=sgd; total time= 4.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.30608125\n",
      "Iteration 2, loss = 2.29581007\n",
      "Iteration 3, loss = 2.29229575\n",
      "Iteration 4, loss = 2.28889310\n",
      "Iteration 5, loss = 2.28517760\n",
      "Iteration 6, loss = 2.28135683\n",
      "Iteration 7, loss = 2.27730842\n",
      "Iteration 8, loss = 2.27276574\n",
      "Iteration 9, loss = 2.26777302\n",
      "Iteration 10, loss = 2.26229640\n",
      "Iteration 11, loss = 2.25616464\n",
      "Iteration 12, loss = 2.24945733\n",
      "Iteration 13, loss = 2.24176906\n",
      "Iteration 14, loss = 2.23288298\n",
      "Iteration 15, loss = 2.22257200\n",
      "Iteration 16, loss = 2.21087062\n",
      "Iteration 17, loss = 2.19694292\n",
      "Iteration 18, loss = 2.18069471\n",
      "Iteration 19, loss = 2.16142169\n",
      "Iteration 20, loss = 2.13884602\n",
      "Iteration 21, loss = 2.11174226\n",
      "Iteration 22, loss = 2.07969277\n",
      "Iteration 23, loss = 2.04209408\n",
      "Iteration 24, loss = 1.99830704\n",
      "Iteration 25, loss = 1.94803258\n",
      "Iteration 26, loss = 1.89168343\n",
      "Iteration 27, loss = 1.83018397\n",
      "Iteration 28, loss = 1.76561289\n",
      "Iteration 29, loss = 1.69960462\n",
      "Iteration 30, loss = 1.63418262\n",
      "Iteration 31, loss = 1.57085793\n",
      "Iteration 32, loss = 1.51069645\n",
      "Iteration 33, loss = 1.45396242\n",
      "Iteration 34, loss = 1.40072388\n",
      "Iteration 35, loss = 1.35075253\n",
      "Iteration 36, loss = 1.30371244\n",
      "Iteration 37, loss = 1.25924719\n",
      "Iteration 38, loss = 1.21707460\n",
      "Iteration 39, loss = 1.17743465\n",
      "Iteration 40, loss = 1.13956916\n",
      "Iteration 41, loss = 1.10389653\n",
      "Iteration 42, loss = 1.07030855\n",
      "Iteration 43, loss = 1.03852982\n",
      "Iteration 44, loss = 1.00881522\n",
      "Iteration 45, loss = 0.98076493\n",
      "Iteration 46, loss = 0.95455335\n",
      "Iteration 47, loss = 0.92996656\n",
      "Iteration 48, loss = 0.90684570\n",
      "Iteration 49, loss = 0.88511551\n",
      "Iteration 50, loss = 0.86470933\n",
      "Iteration 51, loss = 0.84547295\n",
      "Iteration 52, loss = 0.82732577\n",
      "Iteration 53, loss = 0.81020457\n",
      "Iteration 54, loss = 0.79406770\n",
      "Iteration 55, loss = 0.77858662\n",
      "Iteration 56, loss = 0.76404154\n",
      "Iteration 57, loss = 0.75019764\n",
      "Iteration 58, loss = 0.73701870\n",
      "Iteration 59, loss = 0.72438235\n",
      "Iteration 60, loss = 0.71246689\n",
      "Iteration 61, loss = 0.70098722\n",
      "Iteration 62, loss = 0.69014269\n",
      "Iteration 63, loss = 0.67961051\n",
      "Iteration 64, loss = 0.66964072\n",
      "Iteration 65, loss = 0.66004769\n",
      "Iteration 66, loss = 0.65084167\n",
      "Iteration 67, loss = 0.64201016\n",
      "Iteration 68, loss = 0.63363297\n",
      "Iteration 69, loss = 0.62531751\n",
      "Iteration 70, loss = 0.61744715\n",
      "Iteration 71, loss = 0.60983989\n",
      "Iteration 72, loss = 0.60245238\n",
      "Iteration 73, loss = 0.59546602\n",
      "Iteration 74, loss = 0.58852675\n",
      "Iteration 75, loss = 0.58189806\n",
      "Iteration 76, loss = 0.57549173\n",
      "Iteration 77, loss = 0.56923762\n",
      "Iteration 78, loss = 0.56316156\n",
      "Iteration 79, loss = 0.55727443\n",
      "Iteration 80, loss = 0.55159140\n",
      "Iteration 81, loss = 0.54604662\n",
      "Iteration 82, loss = 0.54073723\n",
      "Iteration 83, loss = 0.53543261\n",
      "Iteration 84, loss = 0.53035346\n",
      "Iteration 85, loss = 0.52551361\n",
      "Iteration 86, loss = 0.52065361\n",
      "Iteration 87, loss = 0.51599802\n",
      "Iteration 88, loss = 0.51138749\n",
      "Iteration 89, loss = 0.50696058\n",
      "Iteration 90, loss = 0.50276051\n",
      "Iteration 91, loss = 0.49845785\n",
      "Iteration 92, loss = 0.49445709\n",
      "Iteration 93, loss = 0.49046754\n",
      "Iteration 94, loss = 0.48654320\n",
      "Iteration 95, loss = 0.48276686\n",
      "Iteration 96, loss = 0.47906685\n",
      "Iteration 97, loss = 0.47549489\n",
      "Iteration 98, loss = 0.47206577\n",
      "Iteration 99, loss = 0.46860434\n",
      "Iteration 100, loss = 0.46529942\n",
      "Iteration 101, loss = 0.46210858\n",
      "Iteration 102, loss = 0.45892789\n",
      "Iteration 103, loss = 0.45587918\n",
      "Iteration 104, loss = 0.45293794\n",
      "Iteration 105, loss = 0.44996796\n",
      "Iteration 106, loss = 0.44709037\n",
      "Iteration 107, loss = 0.44432688\n",
      "Iteration 108, loss = 0.44167824\n",
      "Iteration 109, loss = 0.43902941\n",
      "Iteration 110, loss = 0.43651639\n",
      "Iteration 111, loss = 0.43393856\n",
      "Iteration 112, loss = 0.43156353\n",
      "Iteration 113, loss = 0.42917831\n",
      "Iteration 114, loss = 0.42678879\n",
      "Iteration 115, loss = 0.42454094\n",
      "Iteration 116, loss = 0.42239020\n",
      "Iteration 117, loss = 0.42022685\n",
      "Iteration 118, loss = 0.41810834\n",
      "Iteration 119, loss = 0.41599475\n",
      "Iteration 120, loss = 0.41401723\n",
      "Iteration 121, loss = 0.41204179\n",
      "Iteration 122, loss = 0.41009620\n",
      "Iteration 123, loss = 0.40821039\n",
      "Iteration 124, loss = 0.40640895\n",
      "Iteration 125, loss = 0.40459455\n",
      "Iteration 126, loss = 0.40283114\n",
      "Iteration 127, loss = 0.40112373\n",
      "Iteration 128, loss = 0.39952102\n",
      "Iteration 129, loss = 0.39780201\n",
      "Iteration 130, loss = 0.39618631\n",
      "Iteration 131, loss = 0.39458968\n",
      "Iteration 132, loss = 0.39305641\n",
      "Iteration 133, loss = 0.39156440\n",
      "Iteration 134, loss = 0.39004231\n",
      "Iteration 135, loss = 0.38864032\n",
      "Iteration 136, loss = 0.38716590\n",
      "Iteration 137, loss = 0.38582276\n",
      "Iteration 138, loss = 0.38454644\n",
      "Iteration 139, loss = 0.38316686\n",
      "Iteration 140, loss = 0.38178060\n",
      "Iteration 141, loss = 0.38057179\n",
      "Iteration 142, loss = 0.37921053\n",
      "Iteration 143, loss = 0.37805163\n",
      "Iteration 144, loss = 0.37682586\n",
      "Iteration 145, loss = 0.37557694\n",
      "Iteration 146, loss = 0.37445552\n",
      "Iteration 147, loss = 0.37324629\n",
      "Iteration 148, loss = 0.37210922\n",
      "Iteration 149, loss = 0.37107241\n",
      "Iteration 150, loss = 0.36988391\n",
      "Iteration 151, loss = 0.36889904\n",
      "Iteration 152, loss = 0.36771292\n",
      "Iteration 153, loss = 0.36670648\n",
      "Iteration 154, loss = 0.36567994\n",
      "Iteration 155, loss = 0.36467313\n",
      "Iteration 156, loss = 0.36368900\n",
      "Iteration 157, loss = 0.36266129\n",
      "Iteration 158, loss = 0.36174410\n",
      "Iteration 159, loss = 0.36064701\n",
      "Iteration 160, loss = 0.35978014\n",
      "Iteration 161, loss = 0.35884525\n",
      "Iteration 162, loss = 0.35792035\n",
      "Iteration 163, loss = 0.35702632\n",
      "Iteration 164, loss = 0.35613415\n",
      "Iteration 165, loss = 0.35528017\n",
      "Iteration 166, loss = 0.35442862\n",
      "Iteration 167, loss = 0.35354617\n",
      "Iteration 168, loss = 0.35268113\n",
      "Iteration 169, loss = 0.35184700\n",
      "Iteration 170, loss = 0.35106333\n",
      "Iteration 171, loss = 0.35017375\n",
      "Iteration 172, loss = 0.34943625\n",
      "Iteration 173, loss = 0.34859147\n",
      "Iteration 174, loss = 0.34783782\n",
      "Iteration 175, loss = 0.34701111\n",
      "Iteration 176, loss = 0.34633892\n",
      "Iteration 177, loss = 0.34557879\n",
      "Iteration 178, loss = 0.34475160\n",
      "Iteration 179, loss = 0.34404703\n",
      "Iteration 180, loss = 0.34323203\n",
      "Iteration 181, loss = 0.34260845\n",
      "Iteration 182, loss = 0.34183140\n",
      "Iteration 183, loss = 0.34115572\n",
      "Iteration 184, loss = 0.34033262\n",
      "Iteration 185, loss = 0.33974363\n",
      "Iteration 186, loss = 0.33905131\n",
      "Iteration 187, loss = 0.33828406\n",
      "Iteration 188, loss = 0.33757127\n",
      "Iteration 189, loss = 0.33709596\n",
      "Iteration 190, loss = 0.33634292\n",
      "Iteration 191, loss = 0.33562819\n",
      "Iteration 192, loss = 0.33500050\n",
      "Iteration 193, loss = 0.33440990\n",
      "Iteration 194, loss = 0.33373669\n",
      "Iteration 195, loss = 0.33305234\n",
      "Iteration 196, loss = 0.33247758\n",
      "Iteration 197, loss = 0.33185085\n",
      "Iteration 198, loss = 0.33126870\n",
      "Iteration 199, loss = 0.33054416\n",
      "Iteration 200, loss = 0.32998870\n",
      "[CV] END activation=logistic, alpha=0.0001, batch_size=200, hidden_layer_sizes=(400, 100), solver=sgd; total time= 4.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.71902183\n",
      "Iteration 2, loss = 0.62824899\n",
      "Iteration 3, loss = 0.38206963\n",
      "Iteration 4, loss = 0.29561186\n",
      "Iteration 5, loss = 0.24126569\n",
      "Iteration 6, loss = 0.20494124\n",
      "Iteration 7, loss = 0.17543543\n",
      "Iteration 8, loss = 0.15216035\n",
      "Iteration 9, loss = 0.13278378\n",
      "Iteration 10, loss = 0.11819818\n",
      "Iteration 11, loss = 0.10463059\n",
      "Iteration 12, loss = 0.09192385\n",
      "Iteration 13, loss = 0.08190911\n",
      "Iteration 14, loss = 0.07240882\n",
      "Iteration 15, loss = 0.06525108\n",
      "Iteration 16, loss = 0.05822025\n",
      "Iteration 17, loss = 0.05268775\n",
      "Iteration 18, loss = 0.04667171\n",
      "Iteration 19, loss = 0.04174010\n",
      "Iteration 20, loss = 0.03756929\n",
      "Iteration 21, loss = 0.03385179\n",
      "Iteration 22, loss = 0.03029833\n",
      "Iteration 23, loss = 0.02667241\n",
      "Iteration 24, loss = 0.02335702\n",
      "Iteration 25, loss = 0.02093995\n",
      "Iteration 26, loss = 0.01898920\n",
      "Iteration 27, loss = 0.01625789\n",
      "Iteration 28, loss = 0.01484805\n",
      "Iteration 29, loss = 0.01252268\n",
      "Iteration 30, loss = 0.01203411\n",
      "Iteration 31, loss = 0.00992088\n",
      "Iteration 32, loss = 0.00811549\n",
      "Iteration 33, loss = 0.00758803\n",
      "Iteration 34, loss = 0.00722644\n",
      "Iteration 35, loss = 0.00640462\n",
      "Iteration 36, loss = 0.00561607\n",
      "Iteration 37, loss = 0.00506314\n",
      "Iteration 38, loss = 0.00465860\n",
      "Iteration 39, loss = 0.00397928\n",
      "Iteration 40, loss = 0.00376703\n",
      "Iteration 41, loss = 0.00299206\n",
      "Iteration 42, loss = 0.00274197\n",
      "Iteration 43, loss = 0.00245828\n",
      "Iteration 44, loss = 0.00219591\n",
      "Iteration 45, loss = 0.00207875\n",
      "Iteration 46, loss = 0.00195279\n",
      "Iteration 47, loss = 0.00187560\n",
      "Iteration 48, loss = 0.00177386\n",
      "Iteration 49, loss = 0.00171612\n",
      "Iteration 50, loss = 0.00165597\n",
      "Iteration 51, loss = 0.00158594\n",
      "Iteration 52, loss = 0.00154170\n",
      "Iteration 53, loss = 0.00150180\n",
      "Iteration 54, loss = 0.00146478\n",
      "Iteration 55, loss = 0.00143447\n",
      "Iteration 56, loss = 0.00140065\n",
      "Iteration 57, loss = 0.00137637\n",
      "Iteration 58, loss = 0.00134957\n",
      "Iteration 59, loss = 0.00132719\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.0001, batch_size=200, hidden_layer_sizes=(100, 100, 100), solver=adam; total time=  45.6s\n",
      "Iteration 1, loss = 1.76766269\n",
      "Iteration 2, loss = 0.66164642\n",
      "Iteration 3, loss = 0.37381123\n",
      "Iteration 4, loss = 0.28156414\n",
      "Iteration 5, loss = 0.23078726\n",
      "Iteration 6, loss = 0.19465004\n",
      "Iteration 7, loss = 0.16715843\n",
      "Iteration 8, loss = 0.14517773\n",
      "Iteration 9, loss = 0.12710896\n",
      "Iteration 10, loss = 0.11205972\n",
      "Iteration 11, loss = 0.09949370\n",
      "Iteration 12, loss = 0.08902474\n",
      "Iteration 13, loss = 0.07861286\n",
      "Iteration 14, loss = 0.06993515\n",
      "Iteration 15, loss = 0.06355718\n",
      "Iteration 16, loss = 0.05675546\n",
      "Iteration 17, loss = 0.04917530\n",
      "Iteration 18, loss = 0.04454034\n",
      "Iteration 19, loss = 0.04031241\n",
      "Iteration 20, loss = 0.03493071\n",
      "Iteration 21, loss = 0.03068428\n",
      "Iteration 22, loss = 0.02713904\n",
      "Iteration 23, loss = 0.02406594\n",
      "Iteration 24, loss = 0.02099586\n",
      "Iteration 25, loss = 0.01841447\n",
      "Iteration 26, loss = 0.01652690\n",
      "Iteration 27, loss = 0.01438604\n",
      "Iteration 28, loss = 0.01237031\n",
      "Iteration 29, loss = 0.01080039\n",
      "Iteration 30, loss = 0.00997236\n",
      "Iteration 31, loss = 0.00888972\n",
      "Iteration 32, loss = 0.00826735\n",
      "Iteration 33, loss = 0.00673939\n",
      "Iteration 34, loss = 0.00582865\n",
      "Iteration 35, loss = 0.00506793\n",
      "Iteration 36, loss = 0.00460726\n",
      "Iteration 37, loss = 0.00420642\n",
      "Iteration 38, loss = 0.00363091\n",
      "Iteration 39, loss = 0.00346339\n",
      "Iteration 40, loss = 0.00319596\n",
      "Iteration 41, loss = 0.00283166\n",
      "Iteration 42, loss = 0.00270029\n",
      "Iteration 43, loss = 0.00244573\n",
      "Iteration 44, loss = 0.00217815\n",
      "Iteration 45, loss = 0.00206251\n",
      "Iteration 46, loss = 0.00193969\n",
      "Iteration 47, loss = 0.00175859\n",
      "Iteration 48, loss = 0.00170895\n",
      "Iteration 49, loss = 0.00164030\n",
      "Iteration 50, loss = 0.00157121\n",
      "Iteration 51, loss = 0.00152089\n",
      "Iteration 52, loss = 0.00148728\n",
      "Iteration 53, loss = 0.00145931\n",
      "Iteration 54, loss = 0.00140755\n",
      "Iteration 55, loss = 0.00138549\n",
      "Iteration 56, loss = 0.00134833\n",
      "Iteration 57, loss = 0.00133516\n",
      "Iteration 58, loss = 0.00130621\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.0001, batch_size=200, hidden_layer_sizes=(100, 100, 100), solver=adam; total time=  46.6s\n",
      "Iteration 1, loss = 1.73725086\n",
      "Iteration 2, loss = 0.70371322\n",
      "Iteration 3, loss = 0.43754580\n",
      "Iteration 4, loss = 0.31268074\n",
      "Iteration 5, loss = 0.24560051\n",
      "Iteration 6, loss = 0.20506493\n",
      "Iteration 7, loss = 0.17481305\n",
      "Iteration 8, loss = 0.15093139\n",
      "Iteration 9, loss = 0.13286276\n",
      "Iteration 10, loss = 0.11872616\n",
      "Iteration 11, loss = 0.10457702\n",
      "Iteration 12, loss = 0.09275164\n",
      "Iteration 13, loss = 0.08433286\n",
      "Iteration 14, loss = 0.07440756\n",
      "Iteration 15, loss = 0.06707328\n",
      "Iteration 16, loss = 0.05988602\n",
      "Iteration 17, loss = 0.05385518\n",
      "Iteration 18, loss = 0.04820004\n",
      "Iteration 19, loss = 0.04339938\n",
      "Iteration 20, loss = 0.03896014\n",
      "Iteration 21, loss = 0.03443605\n",
      "Iteration 22, loss = 0.03141892\n",
      "Iteration 23, loss = 0.02718882\n",
      "Iteration 24, loss = 0.02426211\n",
      "Iteration 25, loss = 0.02113569\n",
      "Iteration 26, loss = 0.01909297\n",
      "Iteration 27, loss = 0.01716262\n",
      "Iteration 28, loss = 0.01550371\n",
      "Iteration 29, loss = 0.01336721\n",
      "Iteration 30, loss = 0.01116617\n",
      "Iteration 31, loss = 0.01047208\n",
      "Iteration 32, loss = 0.00821562\n",
      "Iteration 33, loss = 0.00769079\n",
      "Iteration 34, loss = 0.00705420\n",
      "Iteration 35, loss = 0.00633594\n",
      "Iteration 36, loss = 0.00641653\n",
      "Iteration 37, loss = 0.00556105\n",
      "Iteration 38, loss = 0.00481776\n",
      "Iteration 39, loss = 0.00423110\n",
      "Iteration 40, loss = 0.00459741\n",
      "Iteration 41, loss = 0.00362747\n",
      "Iteration 42, loss = 0.00346143\n",
      "Iteration 43, loss = 0.00297705\n",
      "Iteration 44, loss = 0.00252410\n",
      "Iteration 45, loss = 0.00228467\n",
      "Iteration 46, loss = 0.00208054\n",
      "Iteration 47, loss = 0.00193468\n",
      "Iteration 48, loss = 0.00184543\n",
      "Iteration 49, loss = 0.00176089\n",
      "Iteration 50, loss = 0.00169620\n",
      "Iteration 51, loss = 0.00166146\n",
      "Iteration 52, loss = 0.00179427\n",
      "Iteration 53, loss = 0.01651357\n",
      "Iteration 54, loss = 0.00869021\n",
      "Iteration 55, loss = 0.00541462\n",
      "Iteration 56, loss = 0.00230026\n",
      "Iteration 57, loss = 0.00168527\n",
      "Iteration 58, loss = 0.00159651\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.0001, batch_size=200, hidden_layer_sizes=(100, 100, 100), solver=adam; total time=  46.7s\n",
      "Iteration 1, loss = 1.76491930\n",
      "Iteration 2, loss = 0.70078131\n",
      "Iteration 3, loss = 0.41845697\n",
      "Iteration 4, loss = 0.30225999\n",
      "Iteration 5, loss = 0.23925139\n",
      "Iteration 6, loss = 0.20119193\n",
      "Iteration 7, loss = 0.17239066\n",
      "Iteration 8, loss = 0.15010197\n",
      "Iteration 9, loss = 0.13207005\n",
      "Iteration 10, loss = 0.11781709\n",
      "Iteration 11, loss = 0.10506179\n",
      "Iteration 12, loss = 0.09494933\n",
      "Iteration 13, loss = 0.08428455\n",
      "Iteration 14, loss = 0.07541686\n",
      "Iteration 15, loss = 0.06823051\n",
      "Iteration 16, loss = 0.06092552\n",
      "Iteration 17, loss = 0.05579715\n",
      "Iteration 18, loss = 0.05041383\n",
      "Iteration 19, loss = 0.04670540\n",
      "Iteration 20, loss = 0.04120027\n",
      "Iteration 21, loss = 0.03636810\n",
      "Iteration 22, loss = 0.03318992\n",
      "Iteration 23, loss = 0.02917201\n",
      "Iteration 24, loss = 0.02707709\n",
      "Iteration 25, loss = 0.02344036\n",
      "Iteration 26, loss = 0.02089500\n",
      "Iteration 27, loss = 0.01891729\n",
      "Iteration 28, loss = 0.01686910\n",
      "Iteration 29, loss = 0.01507850\n",
      "Iteration 30, loss = 0.01338253\n",
      "Iteration 31, loss = 0.01167691\n",
      "Iteration 32, loss = 0.01034374\n",
      "Iteration 33, loss = 0.00925593\n",
      "Iteration 34, loss = 0.00822740\n",
      "Iteration 35, loss = 0.00677536\n",
      "Iteration 36, loss = 0.00619933\n",
      "Iteration 37, loss = 0.00574849\n",
      "Iteration 38, loss = 0.00495163\n",
      "Iteration 39, loss = 0.00460119\n",
      "Iteration 40, loss = 0.00427631\n",
      "Iteration 41, loss = 0.00389398\n",
      "Iteration 42, loss = 0.00359608\n",
      "Iteration 43, loss = 0.00331171\n",
      "Iteration 44, loss = 0.00325943\n",
      "Iteration 45, loss = 0.00358059\n",
      "Iteration 46, loss = 0.00265378\n",
      "Iteration 47, loss = 0.00231423\n",
      "Iteration 48, loss = 0.00212360\n",
      "Iteration 49, loss = 0.00198268\n",
      "Iteration 50, loss = 0.00185826\n",
      "Iteration 51, loss = 0.00177891\n",
      "Iteration 52, loss = 0.00168889\n",
      "Iteration 53, loss = 0.00162972\n",
      "Iteration 54, loss = 0.00157039\n",
      "Iteration 55, loss = 0.00152737\n",
      "Iteration 56, loss = 0.00148383\n",
      "Iteration 57, loss = 0.00145762\n",
      "Iteration 58, loss = 0.00142929\n",
      "Iteration 59, loss = 0.00140645\n",
      "Iteration 60, loss = 0.00137445\n",
      "Iteration 61, loss = 0.00135589\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.0001, batch_size=200, hidden_layer_sizes=(100, 100, 100), solver=adam; total time=  48.8s\n",
      "Iteration 1, loss = 1.76153625\n",
      "Iteration 2, loss = 0.69933109\n",
      "Iteration 3, loss = 0.41647110\n",
      "Iteration 4, loss = 0.30155375\n",
      "Iteration 5, loss = 0.23889266\n",
      "Iteration 6, loss = 0.19696762\n",
      "Iteration 7, loss = 0.16758675\n",
      "Iteration 8, loss = 0.14585756\n",
      "Iteration 9, loss = 0.12880265\n",
      "Iteration 10, loss = 0.11355528\n",
      "Iteration 11, loss = 0.10080005\n",
      "Iteration 12, loss = 0.09076895\n",
      "Iteration 13, loss = 0.08051240\n",
      "Iteration 14, loss = 0.07306598\n",
      "Iteration 15, loss = 0.06600643\n",
      "Iteration 16, loss = 0.05820207\n",
      "Iteration 17, loss = 0.05209738\n",
      "Iteration 18, loss = 0.04706559\n",
      "Iteration 19, loss = 0.04205533\n",
      "Iteration 20, loss = 0.03768109\n",
      "Iteration 21, loss = 0.03407505\n",
      "Iteration 22, loss = 0.02982776\n",
      "Iteration 23, loss = 0.02686305\n",
      "Iteration 24, loss = 0.02338564\n",
      "Iteration 25, loss = 0.02054177\n",
      "Iteration 26, loss = 0.01875010\n",
      "Iteration 27, loss = 0.01667329\n",
      "Iteration 28, loss = 0.01488363\n",
      "Iteration 29, loss = 0.01402275\n",
      "Iteration 30, loss = 0.01160820\n",
      "Iteration 31, loss = 0.01026570\n",
      "Iteration 32, loss = 0.00931041\n",
      "Iteration 33, loss = 0.00789306\n",
      "Iteration 34, loss = 0.00740168\n",
      "Iteration 35, loss = 0.00754640\n",
      "Iteration 36, loss = 0.00587756\n",
      "Iteration 37, loss = 0.00536903\n",
      "Iteration 38, loss = 0.00451384\n",
      "Iteration 39, loss = 0.00418715\n",
      "Iteration 40, loss = 0.00393663\n",
      "Iteration 41, loss = 0.00384660\n",
      "Iteration 42, loss = 0.00382773\n",
      "Iteration 43, loss = 0.00332839\n",
      "Iteration 44, loss = 0.00454584\n",
      "Iteration 45, loss = 0.01205871\n",
      "Iteration 46, loss = 0.00711036\n",
      "Iteration 47, loss = 0.00452142\n",
      "Iteration 48, loss = 0.00274746\n",
      "Iteration 49, loss = 0.00239687\n",
      "Iteration 50, loss = 0.00221885\n",
      "Iteration 51, loss = 0.00192477\n",
      "Iteration 52, loss = 0.00180946\n",
      "Iteration 53, loss = 0.00175440\n",
      "Iteration 54, loss = 0.00169064\n",
      "Iteration 55, loss = 0.00162007\n",
      "Iteration 56, loss = 0.00155987\n",
      "Iteration 57, loss = 0.00152435\n",
      "Iteration 58, loss = 0.00149576\n",
      "Iteration 59, loss = 0.00146826\n",
      "Iteration 60, loss = 0.00144344\n",
      "Iteration 61, loss = 0.00142048\n",
      "Iteration 62, loss = 0.00139842\n",
      "Iteration 63, loss = 0.00137519\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.0001, batch_size=200, hidden_layer_sizes=(100, 100, 100), solver=adam; total time=  50.2s\n",
      "Iteration 1, loss = 2.30778533\n",
      "Iteration 2, loss = 2.30173119\n",
      "Iteration 3, loss = 2.30165962\n",
      "Iteration 4, loss = 2.30160266\n",
      "Iteration 5, loss = 2.30150682\n",
      "Iteration 6, loss = 2.30154553\n",
      "Iteration 7, loss = 2.30151337\n",
      "Iteration 8, loss = 2.30152501\n",
      "Iteration 9, loss = 2.30139380\n",
      "Iteration 10, loss = 2.30127283\n",
      "Iteration 11, loss = 2.30139893\n",
      "Iteration 12, loss = 2.30127726\n",
      "Iteration 13, loss = 2.30125147\n",
      "Iteration 14, loss = 2.30117838\n",
      "Iteration 15, loss = 2.30113687\n",
      "Iteration 16, loss = 2.30108211\n",
      "Iteration 17, loss = 2.30107174\n",
      "Iteration 18, loss = 2.30092380\n",
      "Iteration 19, loss = 2.30095025\n",
      "Iteration 20, loss = 2.30095978\n",
      "Iteration 21, loss = 2.30086118\n",
      "Iteration 22, loss = 2.30093582\n",
      "Iteration 23, loss = 2.30077483\n",
      "Iteration 24, loss = 2.30064971\n",
      "Iteration 25, loss = 2.30072440\n",
      "Iteration 26, loss = 2.30071996\n",
      "Iteration 27, loss = 2.30068263\n",
      "Iteration 28, loss = 2.30051566\n",
      "Iteration 29, loss = 2.30042072\n",
      "Iteration 30, loss = 2.30048653\n",
      "Iteration 31, loss = 2.30037641\n",
      "Iteration 32, loss = 2.30044611\n",
      "Iteration 33, loss = 2.30022111\n",
      "Iteration 34, loss = 2.30024961\n",
      "Iteration 35, loss = 2.30015341\n",
      "Iteration 36, loss = 2.30013446\n",
      "Iteration 37, loss = 2.30001061\n",
      "Iteration 38, loss = 2.30005903\n",
      "Iteration 39, loss = 2.29979641\n",
      "Iteration 40, loss = 2.29991806\n",
      "Iteration 41, loss = 2.29988650\n",
      "Iteration 42, loss = 2.29972551\n",
      "Iteration 43, loss = 2.29967580\n",
      "Iteration 44, loss = 2.29967895\n",
      "Iteration 45, loss = 2.29956229\n",
      "Iteration 46, loss = 2.29963331\n",
      "Iteration 47, loss = 2.29944592\n",
      "Iteration 48, loss = 2.29946677\n",
      "Iteration 49, loss = 2.29939359\n",
      "Iteration 50, loss = 2.29926765\n",
      "Iteration 51, loss = 2.29925316\n",
      "Iteration 52, loss = 2.29919421\n",
      "Iteration 53, loss = 2.29902057\n",
      "Iteration 54, loss = 2.29900423\n",
      "Iteration 55, loss = 2.29889490\n",
      "Iteration 56, loss = 2.29892594\n",
      "Iteration 57, loss = 2.29886088\n",
      "Iteration 58, loss = 2.29862406\n",
      "Iteration 59, loss = 2.29862368\n",
      "Iteration 60, loss = 2.29856673\n",
      "Iteration 61, loss = 2.29847834\n",
      "Iteration 62, loss = 2.29834317\n",
      "Iteration 63, loss = 2.29831068\n",
      "Iteration 64, loss = 2.29819061\n",
      "Iteration 65, loss = 2.29811270\n",
      "Iteration 66, loss = 2.29795146\n",
      "Iteration 67, loss = 2.29786641\n",
      "Iteration 68, loss = 2.29782841\n",
      "Iteration 69, loss = 2.29766104\n",
      "Iteration 70, loss = 2.29755839\n",
      "Iteration 71, loss = 2.29742106\n",
      "Iteration 72, loss = 2.29738493\n",
      "Iteration 73, loss = 2.29722589\n",
      "Iteration 74, loss = 2.29716597\n",
      "Iteration 75, loss = 2.29692700\n",
      "Iteration 76, loss = 2.29694440\n",
      "Iteration 77, loss = 2.29660507\n",
      "Iteration 78, loss = 2.29667852\n",
      "Iteration 79, loss = 2.29641879\n",
      "Iteration 80, loss = 2.29611024\n",
      "Iteration 81, loss = 2.29616132\n",
      "Iteration 82, loss = 2.29592963\n",
      "Iteration 83, loss = 2.29584650\n",
      "Iteration 84, loss = 2.29558232\n",
      "Iteration 85, loss = 2.29550706\n",
      "Iteration 86, loss = 2.29531254\n",
      "Iteration 87, loss = 2.29503807\n",
      "Iteration 88, loss = 2.29487901\n",
      "Iteration 89, loss = 2.29465582\n",
      "Iteration 90, loss = 2.29457041\n",
      "Iteration 91, loss = 2.29419916\n",
      "Iteration 92, loss = 2.29392344\n",
      "Iteration 93, loss = 2.29372925\n",
      "Iteration 94, loss = 2.29343064\n",
      "Iteration 95, loss = 2.29318834\n",
      "Iteration 96, loss = 2.29290602\n",
      "Iteration 97, loss = 2.29265700\n",
      "Iteration 98, loss = 2.29231652\n",
      "Iteration 99, loss = 2.29195863\n",
      "Iteration 100, loss = 2.29174691\n",
      "Iteration 101, loss = 2.29137343\n",
      "Iteration 102, loss = 2.29091404\n",
      "Iteration 103, loss = 2.29052861\n",
      "Iteration 104, loss = 2.29012860\n",
      "Iteration 105, loss = 2.28961566\n",
      "Iteration 106, loss = 2.28927177\n",
      "Iteration 107, loss = 2.28874485\n",
      "Iteration 108, loss = 2.28822530\n",
      "Iteration 109, loss = 2.28759333\n",
      "Iteration 110, loss = 2.28708479\n",
      "Iteration 111, loss = 2.28640829\n",
      "Iteration 112, loss = 2.28585625\n",
      "Iteration 113, loss = 2.28504470\n",
      "Iteration 114, loss = 2.28426842\n",
      "Iteration 115, loss = 2.28349106\n",
      "Iteration 116, loss = 2.28273859\n",
      "Iteration 117, loss = 2.28174814\n",
      "Iteration 118, loss = 2.28062824\n",
      "Iteration 119, loss = 2.27953526\n",
      "Iteration 120, loss = 2.27849285\n",
      "Iteration 121, loss = 2.27711470\n",
      "Iteration 122, loss = 2.27569646\n",
      "Iteration 123, loss = 2.27430324\n",
      "Iteration 124, loss = 2.27254207\n",
      "Iteration 125, loss = 2.27085083\n",
      "Iteration 126, loss = 2.26876547\n",
      "Iteration 127, loss = 2.26653632\n",
      "Iteration 128, loss = 2.26421127\n",
      "Iteration 129, loss = 2.26149008\n",
      "Iteration 130, loss = 2.25852060\n",
      "Iteration 131, loss = 2.25516510\n",
      "Iteration 132, loss = 2.25148673\n",
      "Iteration 133, loss = 2.24713132\n",
      "Iteration 134, loss = 2.24227016\n",
      "Iteration 135, loss = 2.23691132\n",
      "Iteration 136, loss = 2.23060479\n",
      "Iteration 137, loss = 2.22360871\n",
      "Iteration 138, loss = 2.21559997\n",
      "Iteration 139, loss = 2.20629706\n",
      "Iteration 140, loss = 2.19558178\n",
      "Iteration 141, loss = 2.18336952\n",
      "Iteration 142, loss = 2.16940884\n",
      "Iteration 143, loss = 2.15341547\n",
      "Iteration 144, loss = 2.13538931\n",
      "Iteration 145, loss = 2.11511037\n",
      "Iteration 146, loss = 2.09269500\n",
      "Iteration 147, loss = 2.06841667\n",
      "Iteration 148, loss = 2.04249519\n",
      "Iteration 149, loss = 2.01558971\n",
      "Iteration 150, loss = 1.98829910\n",
      "Iteration 151, loss = 1.96091675\n",
      "Iteration 152, loss = 1.93386931\n",
      "Iteration 153, loss = 1.90784187\n",
      "Iteration 154, loss = 1.88288382\n",
      "Iteration 155, loss = 1.85881606\n",
      "Iteration 156, loss = 1.83603601\n",
      "Iteration 157, loss = 1.81415423\n",
      "Iteration 158, loss = 1.79335527\n",
      "Iteration 159, loss = 1.77339883\n",
      "Iteration 160, loss = 1.75406007\n",
      "Iteration 161, loss = 1.73551491\n",
      "Iteration 162, loss = 1.71740790\n",
      "Iteration 163, loss = 1.69960567\n",
      "Iteration 164, loss = 1.68198809\n",
      "Iteration 165, loss = 1.66439178\n",
      "Iteration 166, loss = 1.64674525\n",
      "Iteration 167, loss = 1.62898652\n",
      "Iteration 168, loss = 1.61058098\n",
      "Iteration 169, loss = 1.59203172\n",
      "Iteration 170, loss = 1.57276908\n",
      "Iteration 171, loss = 1.55277969\n",
      "Iteration 172, loss = 1.53222345\n",
      "Iteration 173, loss = 1.51094327\n",
      "Iteration 174, loss = 1.48921067\n",
      "Iteration 175, loss = 1.46689093\n",
      "Iteration 176, loss = 1.44434419\n",
      "Iteration 177, loss = 1.42176282\n",
      "Iteration 178, loss = 1.39926427\n",
      "Iteration 179, loss = 1.37708589\n",
      "Iteration 180, loss = 1.35546389\n",
      "Iteration 181, loss = 1.33452204\n",
      "Iteration 182, loss = 1.31434475\n",
      "Iteration 183, loss = 1.29494426\n",
      "Iteration 184, loss = 1.27644850\n",
      "Iteration 185, loss = 1.25882443\n",
      "Iteration 186, loss = 1.24209102\n",
      "Iteration 187, loss = 1.22596369\n",
      "Iteration 188, loss = 1.21071466\n",
      "Iteration 189, loss = 1.19607211\n",
      "Iteration 190, loss = 1.18198463\n",
      "Iteration 191, loss = 1.16846473\n",
      "Iteration 192, loss = 1.15539169\n",
      "Iteration 193, loss = 1.14283593\n",
      "Iteration 194, loss = 1.13060283\n",
      "Iteration 195, loss = 1.11882244\n",
      "Iteration 196, loss = 1.10713845\n",
      "Iteration 197, loss = 1.09576561\n",
      "Iteration 198, loss = 1.08461039\n",
      "Iteration 199, loss = 1.07367143\n",
      "Iteration 200, loss = 1.06291468\n",
      "[CV] END activation=logistic, alpha=0.0001, batch_size=200, hidden_layer_sizes=(100, 100, 100), solver=sgd; total time= 2.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.30827633\n",
      "Iteration 2, loss = 2.30178573\n",
      "Iteration 3, loss = 2.30166604\n",
      "Iteration 4, loss = 2.30149365\n",
      "Iteration 5, loss = 2.30149987\n",
      "Iteration 6, loss = 2.30160246\n",
      "Iteration 7, loss = 2.30139564\n",
      "Iteration 8, loss = 2.30130894\n",
      "Iteration 9, loss = 2.30138951\n",
      "Iteration 10, loss = 2.30110324\n",
      "Iteration 11, loss = 2.30126879\n",
      "Iteration 12, loss = 2.30123696\n",
      "Iteration 13, loss = 2.30107602\n",
      "Iteration 14, loss = 2.30094432\n",
      "Iteration 15, loss = 2.30102460\n",
      "Iteration 16, loss = 2.30097824\n",
      "Iteration 17, loss = 2.30085025\n",
      "Iteration 18, loss = 2.30091295\n",
      "Iteration 19, loss = 2.30077337\n",
      "Iteration 20, loss = 2.30071924\n",
      "Iteration 21, loss = 2.30075171\n",
      "Iteration 22, loss = 2.30056298\n",
      "Iteration 23, loss = 2.30055580\n",
      "Iteration 24, loss = 2.30062201\n",
      "Iteration 25, loss = 2.30055150\n",
      "Iteration 26, loss = 2.30042697\n",
      "Iteration 27, loss = 2.30039653\n",
      "Iteration 28, loss = 2.30037289\n",
      "Iteration 29, loss = 2.30024789\n",
      "Iteration 30, loss = 2.30014973\n",
      "Iteration 31, loss = 2.30003508\n",
      "Iteration 32, loss = 2.29996462\n",
      "Iteration 33, loss = 2.29993813\n",
      "Iteration 34, loss = 2.29988782\n",
      "Iteration 35, loss = 2.29974057\n",
      "Iteration 36, loss = 2.29982855\n",
      "Iteration 37, loss = 2.29968546\n",
      "Iteration 38, loss = 2.29957706\n",
      "Iteration 39, loss = 2.29962838\n",
      "Iteration 40, loss = 2.29949388\n",
      "Iteration 41, loss = 2.29948310\n",
      "Iteration 42, loss = 2.29931180\n",
      "Iteration 43, loss = 2.29919036\n",
      "Iteration 44, loss = 2.29915810\n",
      "Iteration 45, loss = 2.29912597\n",
      "Iteration 46, loss = 2.29903600\n",
      "Iteration 47, loss = 2.29887806\n",
      "Iteration 48, loss = 2.29884115\n",
      "Iteration 49, loss = 2.29874223\n",
      "Iteration 50, loss = 2.29864668\n",
      "Iteration 51, loss = 2.29848706\n",
      "Iteration 52, loss = 2.29842614\n",
      "Iteration 53, loss = 2.29838093\n",
      "Iteration 54, loss = 2.29831615\n",
      "Iteration 55, loss = 2.29802872\n",
      "Iteration 56, loss = 2.29800070\n",
      "Iteration 57, loss = 2.29781011\n",
      "Iteration 58, loss = 2.29781521\n",
      "Iteration 59, loss = 2.29748096\n",
      "Iteration 60, loss = 2.29750164\n",
      "Iteration 61, loss = 2.29736112\n",
      "Iteration 62, loss = 2.29722660\n",
      "Iteration 63, loss = 2.29700461\n",
      "Iteration 64, loss = 2.29692057\n",
      "Iteration 65, loss = 2.29656934\n",
      "Iteration 66, loss = 2.29647263\n",
      "Iteration 67, loss = 2.29631630\n",
      "Iteration 68, loss = 2.29621612\n",
      "Iteration 69, loss = 2.29584759\n",
      "Iteration 70, loss = 2.29588528\n",
      "Iteration 71, loss = 2.29556390\n",
      "Iteration 72, loss = 2.29537828\n",
      "Iteration 73, loss = 2.29510376\n",
      "Iteration 74, loss = 2.29485220\n",
      "Iteration 75, loss = 2.29465342\n",
      "Iteration 76, loss = 2.29438391\n",
      "Iteration 77, loss = 2.29415034\n",
      "Iteration 78, loss = 2.29383686\n",
      "Iteration 79, loss = 2.29340054\n",
      "Iteration 80, loss = 2.29313307\n",
      "Iteration 81, loss = 2.29291143\n",
      "Iteration 82, loss = 2.29254813\n",
      "Iteration 83, loss = 2.29220221\n",
      "Iteration 84, loss = 2.29164621\n",
      "Iteration 85, loss = 2.29133962\n",
      "Iteration 86, loss = 2.29088645\n",
      "Iteration 87, loss = 2.29032485\n",
      "Iteration 88, loss = 2.28986013\n",
      "Iteration 89, loss = 2.28924985\n",
      "Iteration 90, loss = 2.28874649\n",
      "Iteration 91, loss = 2.28810942\n",
      "Iteration 92, loss = 2.28732398\n",
      "Iteration 93, loss = 2.28659483\n",
      "Iteration 94, loss = 2.28578753\n",
      "Iteration 95, loss = 2.28498200\n",
      "Iteration 96, loss = 2.28402343\n",
      "Iteration 97, loss = 2.28298049\n",
      "Iteration 98, loss = 2.28191705\n",
      "Iteration 99, loss = 2.28068254\n",
      "Iteration 100, loss = 2.27941316\n",
      "Iteration 101, loss = 2.27799423\n",
      "Iteration 102, loss = 2.27646750\n",
      "Iteration 103, loss = 2.27468779\n",
      "Iteration 104, loss = 2.27271199\n",
      "Iteration 105, loss = 2.27060739\n",
      "Iteration 106, loss = 2.26818871\n",
      "Iteration 107, loss = 2.26548822\n",
      "Iteration 108, loss = 2.26243745\n",
      "Iteration 109, loss = 2.25910150\n",
      "Iteration 110, loss = 2.25530793\n",
      "Iteration 111, loss = 2.25096173\n",
      "Iteration 112, loss = 2.24602143\n",
      "Iteration 113, loss = 2.24022633\n",
      "Iteration 114, loss = 2.23383994\n",
      "Iteration 115, loss = 2.22650445\n",
      "Iteration 116, loss = 2.21790795\n",
      "Iteration 117, loss = 2.20796471\n",
      "Iteration 118, loss = 2.19683842\n",
      "Iteration 119, loss = 2.18374030\n",
      "Iteration 120, loss = 2.16890167\n",
      "Iteration 121, loss = 2.15216996\n",
      "Iteration 122, loss = 2.13342242\n",
      "Iteration 123, loss = 2.11262838\n",
      "Iteration 124, loss = 2.09045746\n",
      "Iteration 125, loss = 2.06665505\n",
      "Iteration 126, loss = 2.04213590\n",
      "Iteration 127, loss = 2.01730080\n",
      "Iteration 128, loss = 1.99254815\n",
      "Iteration 129, loss = 1.96833398\n",
      "Iteration 130, loss = 1.94537637\n",
      "Iteration 131, loss = 1.92318782\n",
      "Iteration 132, loss = 1.90226112\n",
      "Iteration 133, loss = 1.88236977\n",
      "Iteration 134, loss = 1.86379276\n",
      "Iteration 135, loss = 1.84616917\n",
      "Iteration 136, loss = 1.82959790\n",
      "Iteration 137, loss = 1.81400283\n",
      "Iteration 138, loss = 1.79899494\n",
      "Iteration 139, loss = 1.78482034\n",
      "Iteration 140, loss = 1.77123459\n",
      "Iteration 141, loss = 1.75844985\n",
      "Iteration 142, loss = 1.74595343\n",
      "Iteration 143, loss = 1.73420611\n",
      "Iteration 144, loss = 1.72280197\n",
      "Iteration 145, loss = 1.71177587\n",
      "Iteration 146, loss = 1.70138109\n",
      "Iteration 147, loss = 1.69112118\n",
      "Iteration 148, loss = 1.68130919\n",
      "Iteration 149, loss = 1.67184858\n",
      "Iteration 150, loss = 1.66261677\n",
      "Iteration 151, loss = 1.65355740\n",
      "Iteration 152, loss = 1.64473064\n",
      "Iteration 153, loss = 1.63608217\n",
      "Iteration 154, loss = 1.62769619\n",
      "Iteration 155, loss = 1.61926927\n",
      "Iteration 156, loss = 1.61091517\n",
      "Iteration 157, loss = 1.60271187\n",
      "Iteration 158, loss = 1.59441582\n",
      "Iteration 159, loss = 1.58614714\n",
      "Iteration 160, loss = 1.57767606\n",
      "Iteration 161, loss = 1.56910215\n",
      "Iteration 162, loss = 1.56041807\n",
      "Iteration 163, loss = 1.55141084\n",
      "Iteration 164, loss = 1.54203125\n",
      "Iteration 165, loss = 1.53234814\n",
      "Iteration 166, loss = 1.52225186\n",
      "Iteration 167, loss = 1.51172693\n",
      "Iteration 168, loss = 1.50057405\n",
      "Iteration 169, loss = 1.48868755\n",
      "Iteration 170, loss = 1.47600084\n",
      "Iteration 171, loss = 1.46255663\n",
      "Iteration 172, loss = 1.44814387\n",
      "Iteration 173, loss = 1.43269244\n",
      "Iteration 174, loss = 1.41618932\n",
      "Iteration 175, loss = 1.39861171\n",
      "Iteration 176, loss = 1.38006876\n",
      "Iteration 177, loss = 1.36064377\n",
      "Iteration 178, loss = 1.34043225\n",
      "Iteration 179, loss = 1.31987340\n",
      "Iteration 180, loss = 1.29908440\n",
      "Iteration 181, loss = 1.27841212\n",
      "Iteration 182, loss = 1.25796859\n",
      "Iteration 183, loss = 1.23809375\n",
      "Iteration 184, loss = 1.21893196\n",
      "Iteration 185, loss = 1.20074042\n",
      "Iteration 186, loss = 1.18337941\n",
      "Iteration 187, loss = 1.16696789\n",
      "Iteration 188, loss = 1.15142718\n",
      "Iteration 189, loss = 1.13662926\n",
      "Iteration 190, loss = 1.12271387\n",
      "Iteration 191, loss = 1.10920032\n",
      "Iteration 192, loss = 1.09640549\n",
      "Iteration 193, loss = 1.08405752\n",
      "Iteration 194, loss = 1.07221413\n",
      "Iteration 195, loss = 1.06076134\n",
      "Iteration 196, loss = 1.04945109\n",
      "Iteration 197, loss = 1.03851159\n",
      "Iteration 198, loss = 1.02764101\n",
      "Iteration 199, loss = 1.01696532\n",
      "Iteration 200, loss = 1.00630337\n",
      "[CV] END activation=logistic, alpha=0.0001, batch_size=200, hidden_layer_sizes=(100, 100, 100), solver=sgd; total time= 2.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.30525265\n",
      "Iteration 2, loss = 2.30146888\n",
      "Iteration 3, loss = 2.30155656\n",
      "Iteration 4, loss = 2.30154928\n",
      "Iteration 5, loss = 2.30139314\n",
      "Iteration 6, loss = 2.30137154\n",
      "Iteration 7, loss = 2.30128887\n",
      "Iteration 8, loss = 2.30126790\n",
      "Iteration 9, loss = 2.30121633\n",
      "Iteration 10, loss = 2.30104301\n",
      "Iteration 11, loss = 2.30110341\n",
      "Iteration 12, loss = 2.30109148\n",
      "Iteration 13, loss = 2.30104307\n",
      "Iteration 14, loss = 2.30102626\n",
      "Iteration 15, loss = 2.30100903\n",
      "Iteration 16, loss = 2.30086933\n",
      "Iteration 17, loss = 2.30084905\n",
      "Iteration 18, loss = 2.30069875\n",
      "Iteration 19, loss = 2.30080594\n",
      "Iteration 20, loss = 2.30069895\n",
      "Iteration 21, loss = 2.30068795\n",
      "Iteration 22, loss = 2.30053818\n",
      "Iteration 23, loss = 2.30055456\n",
      "Iteration 24, loss = 2.30049291\n",
      "Iteration 25, loss = 2.30041419\n",
      "Iteration 26, loss = 2.30047872\n",
      "Iteration 27, loss = 2.30024848\n",
      "Iteration 28, loss = 2.30026564\n",
      "Iteration 29, loss = 2.30013883\n",
      "Iteration 30, loss = 2.30021753\n",
      "Iteration 31, loss = 2.30007419\n",
      "Iteration 32, loss = 2.30007249\n",
      "Iteration 33, loss = 2.30001364\n",
      "Iteration 34, loss = 2.29993451\n",
      "Iteration 35, loss = 2.29984485\n",
      "Iteration 36, loss = 2.29972023\n",
      "Iteration 37, loss = 2.29972215\n",
      "Iteration 38, loss = 2.29971249\n",
      "Iteration 39, loss = 2.29954919\n",
      "Iteration 40, loss = 2.29954810\n",
      "Iteration 41, loss = 2.29949434\n",
      "Iteration 42, loss = 2.29941250\n",
      "Iteration 43, loss = 2.29936359\n",
      "Iteration 44, loss = 2.29933493\n",
      "Iteration 45, loss = 2.29907194\n",
      "Iteration 46, loss = 2.29911192\n",
      "Iteration 47, loss = 2.29903027\n",
      "Iteration 48, loss = 2.29905455\n",
      "Iteration 49, loss = 2.29886514\n",
      "Iteration 50, loss = 2.29875819\n",
      "Iteration 51, loss = 2.29862832\n",
      "Iteration 52, loss = 2.29853873\n",
      "Iteration 53, loss = 2.29852518\n",
      "Iteration 54, loss = 2.29839711\n",
      "Iteration 55, loss = 2.29832593\n",
      "Iteration 56, loss = 2.29816692\n",
      "Iteration 57, loss = 2.29816487\n",
      "Iteration 58, loss = 2.29796566\n",
      "Iteration 59, loss = 2.29790947\n",
      "Iteration 60, loss = 2.29778619\n",
      "Iteration 61, loss = 2.29767886\n",
      "Iteration 62, loss = 2.29757027\n",
      "Iteration 63, loss = 2.29739763\n",
      "Iteration 64, loss = 2.29726098\n",
      "Iteration 65, loss = 2.29713306\n",
      "Iteration 66, loss = 2.29691340\n",
      "Iteration 67, loss = 2.29683647\n",
      "Iteration 68, loss = 2.29659851\n",
      "Iteration 69, loss = 2.29655798\n",
      "Iteration 70, loss = 2.29644259\n",
      "Iteration 71, loss = 2.29623189\n",
      "Iteration 72, loss = 2.29613189\n",
      "Iteration 73, loss = 2.29591805\n",
      "Iteration 74, loss = 2.29563079\n",
      "Iteration 75, loss = 2.29551933\n",
      "Iteration 76, loss = 2.29527925\n",
      "Iteration 77, loss = 2.29507183\n",
      "Iteration 78, loss = 2.29482223\n",
      "Iteration 79, loss = 2.29452595\n",
      "Iteration 80, loss = 2.29443230\n",
      "Iteration 81, loss = 2.29411705\n",
      "Iteration 82, loss = 2.29388413\n",
      "Iteration 83, loss = 2.29347579\n",
      "Iteration 84, loss = 2.29341763\n",
      "Iteration 85, loss = 2.29303493\n",
      "Iteration 86, loss = 2.29278528\n",
      "Iteration 87, loss = 2.29236951\n",
      "Iteration 88, loss = 2.29215626\n",
      "Iteration 89, loss = 2.29164471\n",
      "Iteration 90, loss = 2.29138125\n",
      "Iteration 91, loss = 2.29082655\n",
      "Iteration 92, loss = 2.29049660\n",
      "Iteration 93, loss = 2.28996865\n",
      "Iteration 94, loss = 2.28951010\n",
      "Iteration 95, loss = 2.28901609\n",
      "Iteration 96, loss = 2.28842692\n",
      "Iteration 97, loss = 2.28788444\n",
      "Iteration 98, loss = 2.28728531\n",
      "Iteration 99, loss = 2.28668503\n",
      "Iteration 100, loss = 2.28596113\n",
      "Iteration 101, loss = 2.28518999\n",
      "Iteration 102, loss = 2.28435484\n",
      "Iteration 103, loss = 2.28364457\n",
      "Iteration 104, loss = 2.28264360\n",
      "Iteration 105, loss = 2.28178486\n",
      "Iteration 106, loss = 2.28070754\n",
      "Iteration 107, loss = 2.27949926\n",
      "Iteration 108, loss = 2.27828559\n",
      "Iteration 109, loss = 2.27697350\n",
      "Iteration 110, loss = 2.27548226\n",
      "Iteration 111, loss = 2.27398566\n",
      "Iteration 112, loss = 2.27229922\n",
      "Iteration 113, loss = 2.27033693\n",
      "Iteration 114, loss = 2.26835071\n",
      "Iteration 115, loss = 2.26601307\n",
      "Iteration 116, loss = 2.26347650\n",
      "Iteration 117, loss = 2.26082154\n",
      "Iteration 118, loss = 2.25768870\n",
      "Iteration 119, loss = 2.25427205\n",
      "Iteration 120, loss = 2.25047776\n",
      "Iteration 121, loss = 2.24617337\n",
      "Iteration 122, loss = 2.24146018\n",
      "Iteration 123, loss = 2.23599771\n",
      "Iteration 124, loss = 2.22986212\n",
      "Iteration 125, loss = 2.22283316\n",
      "Iteration 126, loss = 2.21494493\n",
      "Iteration 127, loss = 2.20588173\n",
      "Iteration 128, loss = 2.19556192\n",
      "Iteration 129, loss = 2.18360288\n",
      "Iteration 130, loss = 2.17002261\n",
      "Iteration 131, loss = 2.15447581\n",
      "Iteration 132, loss = 2.13657368\n",
      "Iteration 133, loss = 2.11662495\n",
      "Iteration 134, loss = 2.09384162\n",
      "Iteration 135, loss = 2.06841175\n",
      "Iteration 136, loss = 2.04050157\n",
      "Iteration 137, loss = 2.01000607\n",
      "Iteration 138, loss = 1.97737775\n",
      "Iteration 139, loss = 1.94290374\n",
      "Iteration 140, loss = 1.90708704\n",
      "Iteration 141, loss = 1.86993763\n",
      "Iteration 142, loss = 1.83191811\n",
      "Iteration 143, loss = 1.79348423\n",
      "Iteration 144, loss = 1.75466393\n",
      "Iteration 145, loss = 1.71574001\n",
      "Iteration 146, loss = 1.67763384\n",
      "Iteration 147, loss = 1.64020004\n",
      "Iteration 148, loss = 1.60458188\n",
      "Iteration 149, loss = 1.57113736\n",
      "Iteration 150, loss = 1.54014178\n",
      "Iteration 151, loss = 1.51190968\n",
      "Iteration 152, loss = 1.48627494\n",
      "Iteration 153, loss = 1.46331912\n",
      "Iteration 154, loss = 1.44247373\n",
      "Iteration 155, loss = 1.42387457\n",
      "Iteration 156, loss = 1.40705351\n",
      "Iteration 157, loss = 1.39160557\n",
      "Iteration 158, loss = 1.37768547\n",
      "Iteration 159, loss = 1.36464488\n",
      "Iteration 160, loss = 1.35267290\n",
      "Iteration 161, loss = 1.34136061\n",
      "Iteration 162, loss = 1.33081725\n",
      "Iteration 163, loss = 1.32075024\n",
      "Iteration 164, loss = 1.31111326\n",
      "Iteration 165, loss = 1.30181550\n",
      "Iteration 166, loss = 1.29297108\n",
      "Iteration 167, loss = 1.28433193\n",
      "Iteration 168, loss = 1.27574164\n",
      "Iteration 169, loss = 1.26743456\n",
      "Iteration 170, loss = 1.25917114\n",
      "Iteration 171, loss = 1.25096167\n",
      "Iteration 172, loss = 1.24263037\n",
      "Iteration 173, loss = 1.23451893\n",
      "Iteration 174, loss = 1.22619616\n",
      "Iteration 175, loss = 1.21788009\n",
      "Iteration 176, loss = 1.20937526\n",
      "Iteration 177, loss = 1.20077088\n",
      "Iteration 178, loss = 1.19205263\n",
      "Iteration 179, loss = 1.18306851\n",
      "Iteration 180, loss = 1.17377227\n",
      "Iteration 181, loss = 1.16432757\n",
      "Iteration 182, loss = 1.15459881\n",
      "Iteration 183, loss = 1.14448524\n",
      "Iteration 184, loss = 1.13423196\n",
      "Iteration 185, loss = 1.12352625\n",
      "Iteration 186, loss = 1.11244445\n",
      "Iteration 187, loss = 1.10101999\n",
      "Iteration 188, loss = 1.08937350\n",
      "Iteration 189, loss = 1.07713284\n",
      "Iteration 190, loss = 1.06482313\n",
      "Iteration 191, loss = 1.05195301\n",
      "Iteration 192, loss = 1.03899127\n",
      "Iteration 193, loss = 1.02571958\n",
      "Iteration 194, loss = 1.01241698\n",
      "Iteration 195, loss = 0.99894269\n",
      "Iteration 196, loss = 0.98547342\n",
      "Iteration 197, loss = 0.97213179\n",
      "Iteration 198, loss = 0.95893079\n",
      "Iteration 199, loss = 0.94591721\n",
      "Iteration 200, loss = 0.93330911\n",
      "[CV] END activation=logistic, alpha=0.0001, batch_size=200, hidden_layer_sizes=(100, 100, 100), solver=sgd; total time= 2.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.30967267\n",
      "Iteration 2, loss = 2.30155336\n",
      "Iteration 3, loss = 2.30151562\n",
      "Iteration 4, loss = 2.30149477\n",
      "Iteration 5, loss = 2.30144275\n",
      "Iteration 6, loss = 2.30147410\n",
      "Iteration 7, loss = 2.30140858\n",
      "Iteration 8, loss = 2.30137087\n",
      "Iteration 9, loss = 2.30149080\n",
      "Iteration 10, loss = 2.30132363\n",
      "Iteration 11, loss = 2.30122741\n",
      "Iteration 12, loss = 2.30116549\n",
      "Iteration 13, loss = 2.30116060\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.0001, batch_size=200, hidden_layer_sizes=(100, 100, 100), solver=sgd; total time=   9.3s\n",
      "Iteration 1, loss = 2.31043365\n",
      "Iteration 2, loss = 2.30159417\n",
      "Iteration 3, loss = 2.30164401\n",
      "Iteration 4, loss = 2.30165484\n",
      "Iteration 5, loss = 2.30147521\n",
      "Iteration 6, loss = 2.30152409\n",
      "Iteration 7, loss = 2.30155623\n",
      "Iteration 8, loss = 2.30128257\n",
      "Iteration 9, loss = 2.30145236\n",
      "Iteration 10, loss = 2.30130941\n",
      "Iteration 11, loss = 2.30119103\n",
      "Iteration 12, loss = 2.30129587\n",
      "Iteration 13, loss = 2.30121168\n",
      "Iteration 14, loss = 2.30122091\n",
      "Iteration 15, loss = 2.30114296\n",
      "Iteration 16, loss = 2.30115656\n",
      "Iteration 17, loss = 2.30100267\n",
      "Iteration 18, loss = 2.30105455\n",
      "Iteration 19, loss = 2.30090231\n",
      "Iteration 20, loss = 2.30087700\n",
      "Iteration 21, loss = 2.30076824\n",
      "Iteration 22, loss = 2.30084337\n",
      "Iteration 23, loss = 2.30078230\n",
      "Iteration 24, loss = 2.30063049\n",
      "Iteration 25, loss = 2.30055296\n",
      "Iteration 26, loss = 2.30054520\n",
      "Iteration 27, loss = 2.30053312\n",
      "Iteration 28, loss = 2.30049897\n",
      "Iteration 29, loss = 2.30051214\n",
      "Iteration 30, loss = 2.30045154\n",
      "Iteration 31, loss = 2.30030485\n",
      "Iteration 32, loss = 2.30034412\n",
      "Iteration 33, loss = 2.30026312\n",
      "Iteration 34, loss = 2.30023129\n",
      "Iteration 35, loss = 2.30007287\n",
      "Iteration 36, loss = 2.30002278\n",
      "Iteration 37, loss = 2.30004932\n",
      "Iteration 38, loss = 2.29986966\n",
      "Iteration 39, loss = 2.29997695\n",
      "Iteration 40, loss = 2.29984440\n",
      "Iteration 41, loss = 2.29976744\n",
      "Iteration 42, loss = 2.29969030\n",
      "Iteration 43, loss = 2.29956383\n",
      "Iteration 44, loss = 2.29961654\n",
      "Iteration 45, loss = 2.29953079\n",
      "Iteration 46, loss = 2.29946828\n",
      "Iteration 47, loss = 2.29926158\n",
      "Iteration 48, loss = 2.29937012\n",
      "Iteration 49, loss = 2.29916337\n",
      "Iteration 50, loss = 2.29913742\n",
      "Iteration 51, loss = 2.29903409\n",
      "Iteration 52, loss = 2.29890840\n",
      "Iteration 53, loss = 2.29889173\n",
      "Iteration 54, loss = 2.29878019\n",
      "Iteration 55, loss = 2.29876159\n",
      "Iteration 56, loss = 2.29860566\n",
      "Iteration 57, loss = 2.29845439\n",
      "Iteration 58, loss = 2.29845539\n",
      "Iteration 59, loss = 2.29832169\n",
      "Iteration 60, loss = 2.29823817\n",
      "Iteration 61, loss = 2.29813201\n",
      "Iteration 62, loss = 2.29792682\n",
      "Iteration 63, loss = 2.29786191\n",
      "Iteration 64, loss = 2.29776532\n",
      "Iteration 65, loss = 2.29759216\n",
      "Iteration 66, loss = 2.29746476\n",
      "Iteration 67, loss = 2.29740306\n",
      "Iteration 68, loss = 2.29723196\n",
      "Iteration 69, loss = 2.29711082\n",
      "Iteration 70, loss = 2.29687572\n",
      "Iteration 71, loss = 2.29672031\n",
      "Iteration 72, loss = 2.29652116\n",
      "Iteration 73, loss = 2.29638839\n",
      "Iteration 74, loss = 2.29622038\n",
      "Iteration 75, loss = 2.29602426\n",
      "Iteration 76, loss = 2.29593001\n",
      "Iteration 77, loss = 2.29563193\n",
      "Iteration 78, loss = 2.29543959\n",
      "Iteration 79, loss = 2.29522509\n",
      "Iteration 80, loss = 2.29504986\n",
      "Iteration 81, loss = 2.29470718\n",
      "Iteration 82, loss = 2.29450367\n",
      "Iteration 83, loss = 2.29423091\n",
      "Iteration 84, loss = 2.29391114\n",
      "Iteration 85, loss = 2.29367231\n",
      "Iteration 86, loss = 2.29323714\n",
      "Iteration 87, loss = 2.29305827\n",
      "Iteration 88, loss = 2.29258503\n",
      "Iteration 89, loss = 2.29230673\n",
      "Iteration 90, loss = 2.29178919\n",
      "Iteration 91, loss = 2.29144142\n",
      "Iteration 92, loss = 2.29093998\n",
      "Iteration 93, loss = 2.29048397\n",
      "Iteration 94, loss = 2.28998288\n",
      "Iteration 95, loss = 2.28952439\n",
      "Iteration 96, loss = 2.28891572\n",
      "Iteration 97, loss = 2.28822772\n",
      "Iteration 98, loss = 2.28761220\n",
      "Iteration 99, loss = 2.28685952\n",
      "Iteration 100, loss = 2.28599964\n",
      "Iteration 101, loss = 2.28534188\n",
      "Iteration 102, loss = 2.28431448\n",
      "Iteration 103, loss = 2.28321460\n",
      "Iteration 104, loss = 2.28219276\n",
      "Iteration 105, loss = 2.28106163\n",
      "Iteration 106, loss = 2.27981220\n",
      "Iteration 107, loss = 2.27844388\n",
      "Iteration 108, loss = 2.27682679\n",
      "Iteration 109, loss = 2.27504124\n",
      "Iteration 110, loss = 2.27327999\n",
      "Iteration 111, loss = 2.27115430\n",
      "Iteration 112, loss = 2.26875509\n",
      "Iteration 113, loss = 2.26602583\n",
      "Iteration 114, loss = 2.26309696\n",
      "Iteration 115, loss = 2.25982988\n",
      "Iteration 116, loss = 2.25596046\n",
      "Iteration 117, loss = 2.25158128\n",
      "Iteration 118, loss = 2.24661367\n",
      "Iteration 119, loss = 2.24093431\n",
      "Iteration 120, loss = 2.23436011\n",
      "Iteration 121, loss = 2.22667119\n",
      "Iteration 122, loss = 2.21795977\n",
      "Iteration 123, loss = 2.20781505\n",
      "Iteration 124, loss = 2.19610561\n",
      "Iteration 125, loss = 2.18245462\n",
      "Iteration 126, loss = 2.16674523\n",
      "Iteration 127, loss = 2.14887172\n",
      "Iteration 128, loss = 2.12863526\n",
      "Iteration 129, loss = 2.10614148\n",
      "Iteration 130, loss = 2.08188633\n",
      "Iteration 131, loss = 2.05599192\n",
      "Iteration 132, loss = 2.02907183\n",
      "Iteration 133, loss = 2.00204648\n",
      "Iteration 134, loss = 1.97525643\n",
      "Iteration 135, loss = 1.94933886\n",
      "Iteration 136, loss = 1.92472912\n",
      "Iteration 137, loss = 1.90136882\n",
      "Iteration 138, loss = 1.87941307\n",
      "Iteration 139, loss = 1.85925408\n",
      "Iteration 140, loss = 1.84037246\n",
      "Iteration 141, loss = 1.82274568\n",
      "Iteration 142, loss = 1.80634746\n",
      "Iteration 143, loss = 1.79095734\n",
      "Iteration 144, loss = 1.77663158\n",
      "Iteration 145, loss = 1.76297602\n",
      "Iteration 146, loss = 1.75009552\n",
      "Iteration 147, loss = 1.73781510\n",
      "Iteration 148, loss = 1.72614085\n",
      "Iteration 149, loss = 1.71484138\n",
      "Iteration 150, loss = 1.70394998\n",
      "Iteration 151, loss = 1.69355533\n",
      "Iteration 152, loss = 1.68310269\n",
      "Iteration 153, loss = 1.67298281\n",
      "Iteration 154, loss = 1.66274179\n",
      "Iteration 155, loss = 1.65273273\n",
      "Iteration 156, loss = 1.64261813\n",
      "Iteration 157, loss = 1.63228585\n",
      "Iteration 158, loss = 1.62183820\n",
      "Iteration 159, loss = 1.61106621\n",
      "Iteration 160, loss = 1.60009761\n",
      "Iteration 161, loss = 1.58847295\n",
      "Iteration 162, loss = 1.57638670\n",
      "Iteration 163, loss = 1.56380509\n",
      "Iteration 164, loss = 1.55049496\n",
      "Iteration 165, loss = 1.53645206\n",
      "Iteration 166, loss = 1.52175141\n",
      "Iteration 167, loss = 1.50629867\n",
      "Iteration 168, loss = 1.49030471\n",
      "Iteration 169, loss = 1.47352265\n",
      "Iteration 170, loss = 1.45623259\n",
      "Iteration 171, loss = 1.43860681\n",
      "Iteration 172, loss = 1.42071993\n",
      "Iteration 173, loss = 1.40293888\n",
      "Iteration 174, loss = 1.38512559\n",
      "Iteration 175, loss = 1.36759315\n",
      "Iteration 176, loss = 1.35046947\n",
      "Iteration 177, loss = 1.33395159\n",
      "Iteration 178, loss = 1.31794811\n",
      "Iteration 179, loss = 1.30229994\n",
      "Iteration 180, loss = 1.28734219\n",
      "Iteration 181, loss = 1.27301742\n",
      "Iteration 182, loss = 1.25912042\n",
      "Iteration 183, loss = 1.24568795\n",
      "Iteration 184, loss = 1.23268475\n",
      "Iteration 185, loss = 1.21995155\n",
      "Iteration 186, loss = 1.20776584\n",
      "Iteration 187, loss = 1.19561275\n",
      "Iteration 188, loss = 1.18373357\n",
      "Iteration 189, loss = 1.17208956\n",
      "Iteration 190, loss = 1.16046713\n",
      "Iteration 191, loss = 1.14895800\n",
      "Iteration 192, loss = 1.13746740\n",
      "Iteration 193, loss = 1.12589311\n",
      "Iteration 194, loss = 1.11448846\n",
      "Iteration 195, loss = 1.10286197\n",
      "Iteration 196, loss = 1.09117887\n",
      "Iteration 197, loss = 1.07955858\n",
      "Iteration 198, loss = 1.06768969\n",
      "Iteration 199, loss = 1.05569224\n",
      "Iteration 200, loss = 1.04358896\n",
      "[CV] END activation=logistic, alpha=0.0001, batch_size=200, hidden_layer_sizes=(100, 100, 100), solver=sgd; total time= 2.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.67101393\n",
      "Iteration 2, loss = 0.28760841\n",
      "Iteration 3, loss = 0.23126662\n",
      "Iteration 4, loss = 0.19721165\n",
      "Iteration 5, loss = 0.17181083\n",
      "Iteration 6, loss = 0.15372657\n",
      "Iteration 7, loss = 0.13865564\n",
      "Iteration 8, loss = 0.12575215\n",
      "Iteration 9, loss = 0.11503983\n",
      "Iteration 10, loss = 0.10576814\n",
      "Iteration 11, loss = 0.09790350\n",
      "Iteration 12, loss = 0.09101038\n",
      "Iteration 13, loss = 0.08459990\n",
      "Iteration 14, loss = 0.07939091\n",
      "Iteration 15, loss = 0.07479427\n",
      "Iteration 16, loss = 0.07043219\n",
      "Iteration 17, loss = 0.06696591\n",
      "Iteration 18, loss = 0.06348831\n",
      "Iteration 19, loss = 0.06054118\n",
      "Iteration 20, loss = 0.05782985\n",
      "Iteration 21, loss = 0.05512514\n",
      "Iteration 22, loss = 0.05308096\n",
      "Iteration 23, loss = 0.05122583\n",
      "Iteration 24, loss = 0.04952905\n",
      "Iteration 25, loss = 0.04775320\n",
      "Iteration 26, loss = 0.04647226\n",
      "Iteration 27, loss = 0.04516484\n",
      "Iteration 28, loss = 0.04451173\n",
      "Iteration 29, loss = 0.04311043\n",
      "Iteration 30, loss = 0.04230297\n",
      "Iteration 31, loss = 0.04158936\n",
      "Iteration 32, loss = 0.04039388\n",
      "Iteration 33, loss = 0.03983295\n",
      "Iteration 34, loss = 0.03933072\n",
      "Iteration 35, loss = 0.03861802\n",
      "Iteration 36, loss = 0.03810051\n",
      "Iteration 37, loss = 0.03769457\n",
      "Iteration 38, loss = 0.03722147\n",
      "Iteration 39, loss = 0.03684473\n",
      "Iteration 40, loss = 0.03656246\n",
      "Iteration 41, loss = 0.03611364\n",
      "Iteration 42, loss = 0.03550730\n",
      "Iteration 43, loss = 0.03586384\n",
      "Iteration 44, loss = 0.03529538\n",
      "Iteration 45, loss = 0.03464104\n",
      "Iteration 46, loss = 0.03479186\n",
      "Iteration 47, loss = 0.03458435\n",
      "Iteration 48, loss = 0.03432880\n",
      "Iteration 49, loss = 0.03403285\n",
      "Iteration 50, loss = 0.03392670\n",
      "Iteration 51, loss = 0.03418243\n",
      "Iteration 52, loss = 0.03347783\n",
      "Iteration 53, loss = 0.03384673\n",
      "Iteration 54, loss = 0.03335277\n",
      "Iteration 55, loss = 0.03307402\n",
      "Iteration 56, loss = 0.03292239\n",
      "Iteration 57, loss = 0.03307957\n",
      "Iteration 58, loss = 0.03301481\n",
      "Iteration 59, loss = 0.03283767\n",
      "Iteration 60, loss = 0.03271861\n",
      "Iteration 61, loss = 0.03230310\n",
      "Iteration 62, loss = 0.03250426\n",
      "Iteration 63, loss = 0.03277773\n",
      "Iteration 64, loss = 0.03239851\n",
      "Iteration 65, loss = 0.03237269\n",
      "Iteration 66, loss = 0.03241925\n",
      "Iteration 67, loss = 0.03185685\n",
      "Iteration 68, loss = 0.03189662\n",
      "Iteration 69, loss = 0.03182141\n",
      "Iteration 70, loss = 0.03214961\n",
      "Iteration 71, loss = 0.03175830\n",
      "Iteration 72, loss = 0.03160471\n",
      "Iteration 73, loss = 0.03177900\n",
      "Iteration 74, loss = 0.03177172\n",
      "Iteration 75, loss = 0.03226356\n",
      "Iteration 76, loss = 0.03179241\n",
      "Iteration 77, loss = 0.03107533\n",
      "Iteration 78, loss = 0.03157138\n",
      "Iteration 79, loss = 0.03142783\n",
      "Iteration 80, loss = 0.03161939\n",
      "Iteration 81, loss = 0.03124101\n",
      "Iteration 82, loss = 0.03175563\n",
      "Iteration 83, loss = 0.03114632\n",
      "Iteration 84, loss = 0.03123501\n",
      "Iteration 85, loss = 0.03175259\n",
      "Iteration 86, loss = 0.03077178\n",
      "Iteration 87, loss = 0.03093955\n",
      "Iteration 88, loss = 0.03175559\n",
      "Iteration 89, loss = 0.03108527\n",
      "Iteration 90, loss = 0.03082610\n",
      "Iteration 91, loss = 0.03081403\n",
      "Iteration 92, loss = 0.03128677\n",
      "Iteration 93, loss = 0.03117097\n",
      "Iteration 94, loss = 0.03082985\n",
      "Iteration 95, loss = 0.03065182\n",
      "Iteration 96, loss = 0.03083460\n",
      "Iteration 97, loss = 0.03153224\n",
      "Iteration 98, loss = 0.03068774\n",
      "Iteration 99, loss = 0.03072579\n",
      "Iteration 100, loss = 0.03184083\n",
      "Iteration 101, loss = 0.03017709\n",
      "Iteration 102, loss = 0.03019806\n",
      "Iteration 103, loss = 0.03102396\n",
      "Iteration 104, loss = 0.03074321\n",
      "Iteration 105, loss = 0.03056285\n",
      "Iteration 106, loss = 0.03056947\n",
      "Iteration 107, loss = 0.03067451\n",
      "Iteration 108, loss = 0.03069845\n",
      "Iteration 109, loss = 0.03004797\n",
      "Iteration 110, loss = 0.03053744\n",
      "Iteration 111, loss = 0.03047757\n",
      "Iteration 112, loss = 0.03062003\n",
      "Iteration 113, loss = 0.03033832\n",
      "Iteration 114, loss = 0.03066904\n",
      "Iteration 115, loss = 0.03074005\n",
      "Iteration 116, loss = 0.03001512\n",
      "Iteration 117, loss = 0.03045567\n",
      "Iteration 118, loss = 0.03093525\n",
      "Iteration 119, loss = 0.03017460\n",
      "Iteration 120, loss = 0.03021196\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.001, batch_size=50, hidden_layer_sizes=(100,), solver=adam; total time= 3.2min\n",
      "Iteration 1, loss = 0.67579857\n",
      "Iteration 2, loss = 0.29053486\n",
      "Iteration 3, loss = 0.23416246\n",
      "Iteration 4, loss = 0.20010737\n",
      "Iteration 5, loss = 0.17566374\n",
      "Iteration 6, loss = 0.15675086\n",
      "Iteration 7, loss = 0.14142263\n",
      "Iteration 8, loss = 0.12823577\n",
      "Iteration 9, loss = 0.11719030\n",
      "Iteration 10, loss = 0.10769013\n",
      "Iteration 11, loss = 0.09995554\n",
      "Iteration 12, loss = 0.09242417\n",
      "Iteration 13, loss = 0.08634241\n",
      "Iteration 14, loss = 0.08087991\n",
      "Iteration 15, loss = 0.07591900\n",
      "Iteration 16, loss = 0.07230437\n",
      "Iteration 17, loss = 0.06797729\n",
      "Iteration 18, loss = 0.06476840\n",
      "Iteration 19, loss = 0.06123951\n",
      "Iteration 20, loss = 0.05829952\n",
      "Iteration 21, loss = 0.05582472\n",
      "Iteration 22, loss = 0.05395575\n",
      "Iteration 23, loss = 0.05174611\n",
      "Iteration 24, loss = 0.04993908\n",
      "Iteration 25, loss = 0.04821874\n",
      "Iteration 26, loss = 0.04679368\n",
      "Iteration 27, loss = 0.04547820\n",
      "Iteration 28, loss = 0.04402886\n",
      "Iteration 29, loss = 0.04338896\n",
      "Iteration 30, loss = 0.04220505\n",
      "Iteration 31, loss = 0.04173144\n",
      "Iteration 32, loss = 0.04048170\n",
      "Iteration 33, loss = 0.03982291\n",
      "Iteration 34, loss = 0.03940315\n",
      "Iteration 35, loss = 0.03874379\n",
      "Iteration 36, loss = 0.03814412\n",
      "Iteration 37, loss = 0.03791052\n",
      "Iteration 38, loss = 0.03734949\n",
      "Iteration 39, loss = 0.03721765\n",
      "Iteration 40, loss = 0.03641122\n",
      "Iteration 41, loss = 0.03624588\n",
      "Iteration 42, loss = 0.03580603\n",
      "Iteration 43, loss = 0.03555235\n",
      "Iteration 44, loss = 0.03498850\n",
      "Iteration 45, loss = 0.03510495\n",
      "Iteration 46, loss = 0.03521228\n",
      "Iteration 47, loss = 0.03459158\n",
      "Iteration 48, loss = 0.03439254\n",
      "Iteration 49, loss = 0.03409800\n",
      "Iteration 50, loss = 0.03389269\n",
      "Iteration 51, loss = 0.03392734\n",
      "Iteration 52, loss = 0.03375482\n",
      "Iteration 53, loss = 0.03416642\n",
      "Iteration 54, loss = 0.03321582\n",
      "Iteration 55, loss = 0.03292786\n",
      "Iteration 56, loss = 0.03377119\n",
      "Iteration 57, loss = 0.03331390\n",
      "Iteration 58, loss = 0.03287198\n",
      "Iteration 59, loss = 0.03261592\n",
      "Iteration 60, loss = 0.03270765\n",
      "Iteration 61, loss = 0.03302100\n",
      "Iteration 62, loss = 0.03282358\n",
      "Iteration 63, loss = 0.03280686\n",
      "Iteration 64, loss = 0.03251690\n",
      "Iteration 65, loss = 0.03212144\n",
      "Iteration 66, loss = 0.03275680\n",
      "Iteration 67, loss = 0.03229219\n",
      "Iteration 68, loss = 0.03209269\n",
      "Iteration 69, loss = 0.03223125\n",
      "Iteration 70, loss = 0.03213604\n",
      "Iteration 71, loss = 0.03220949\n",
      "Iteration 72, loss = 0.03172541\n",
      "Iteration 73, loss = 0.03157925\n",
      "Iteration 74, loss = 0.03192071\n",
      "Iteration 75, loss = 0.03218681\n",
      "Iteration 76, loss = 0.03169207\n",
      "Iteration 77, loss = 0.03166935\n",
      "Iteration 78, loss = 0.03165103\n",
      "Iteration 79, loss = 0.03169935\n",
      "Iteration 80, loss = 0.03180607\n",
      "Iteration 81, loss = 0.03153062\n",
      "Iteration 82, loss = 0.03144824\n",
      "Iteration 83, loss = 0.03141237\n",
      "Iteration 84, loss = 0.03132452\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.001, batch_size=50, hidden_layer_sizes=(100,), solver=adam; total time= 2.2min\n",
      "Iteration 1, loss = 0.65937973\n",
      "Iteration 2, loss = 0.28787076\n",
      "Iteration 3, loss = 0.23114145\n",
      "Iteration 4, loss = 0.19690901\n",
      "Iteration 5, loss = 0.17242101\n",
      "Iteration 6, loss = 0.15334151\n",
      "Iteration 7, loss = 0.13838128\n",
      "Iteration 8, loss = 0.12592482\n",
      "Iteration 9, loss = 0.11599166\n",
      "Iteration 10, loss = 0.10711494\n",
      "Iteration 11, loss = 0.09920396\n",
      "Iteration 12, loss = 0.09234171\n",
      "Iteration 13, loss = 0.08701679\n",
      "Iteration 14, loss = 0.08158309\n",
      "Iteration 15, loss = 0.07713897\n",
      "Iteration 16, loss = 0.07293085\n",
      "Iteration 17, loss = 0.06995171\n",
      "Iteration 18, loss = 0.06656029\n",
      "Iteration 19, loss = 0.06327812\n",
      "Iteration 20, loss = 0.06053401\n",
      "Iteration 21, loss = 0.05776283\n",
      "Iteration 22, loss = 0.05587211\n",
      "Iteration 23, loss = 0.05357070\n",
      "Iteration 24, loss = 0.05199253\n",
      "Iteration 25, loss = 0.05038207\n",
      "Iteration 26, loss = 0.04875849\n",
      "Iteration 27, loss = 0.04778670\n",
      "Iteration 28, loss = 0.04648633\n",
      "Iteration 29, loss = 0.04547292\n",
      "Iteration 30, loss = 0.04419223\n",
      "Iteration 31, loss = 0.04334099\n",
      "Iteration 32, loss = 0.04234279\n",
      "Iteration 33, loss = 0.04197441\n",
      "Iteration 34, loss = 0.04116514\n",
      "Iteration 35, loss = 0.04039510\n",
      "Iteration 36, loss = 0.03981322\n",
      "Iteration 37, loss = 0.03929844\n",
      "Iteration 38, loss = 0.03884088\n",
      "Iteration 39, loss = 0.03822829\n",
      "Iteration 40, loss = 0.03777792\n",
      "Iteration 41, loss = 0.03767902\n",
      "Iteration 42, loss = 0.03700450\n",
      "Iteration 43, loss = 0.03697694\n",
      "Iteration 44, loss = 0.03635181\n",
      "Iteration 45, loss = 0.03623978\n",
      "Iteration 46, loss = 0.03619076\n",
      "Iteration 47, loss = 0.03544527\n",
      "Iteration 48, loss = 0.03567293\n",
      "Iteration 49, loss = 0.03542810\n",
      "Iteration 50, loss = 0.03542858\n",
      "Iteration 51, loss = 0.03479070\n",
      "Iteration 52, loss = 0.03445400\n",
      "Iteration 53, loss = 0.03493133\n",
      "Iteration 54, loss = 0.03477232\n",
      "Iteration 55, loss = 0.03395519\n",
      "Iteration 56, loss = 0.03438354\n",
      "Iteration 57, loss = 0.03433929\n",
      "Iteration 58, loss = 0.03385865\n",
      "Iteration 59, loss = 0.03370952\n",
      "Iteration 60, loss = 0.03362236\n",
      "Iteration 61, loss = 0.03368575\n",
      "Iteration 62, loss = 0.03371747\n",
      "Iteration 63, loss = 0.03311265\n",
      "Iteration 64, loss = 0.03319761\n",
      "Iteration 65, loss = 0.03285944\n",
      "Iteration 66, loss = 0.03338325\n",
      "Iteration 67, loss = 0.03276350\n",
      "Iteration 68, loss = 0.03285182\n",
      "Iteration 69, loss = 0.03301589\n",
      "Iteration 70, loss = 0.03263358\n",
      "Iteration 71, loss = 0.03267512\n",
      "Iteration 72, loss = 0.03243793\n",
      "Iteration 73, loss = 0.03245680\n",
      "Iteration 74, loss = 0.03235954\n",
      "Iteration 75, loss = 0.03227932\n",
      "Iteration 76, loss = 0.03245485\n",
      "Iteration 77, loss = 0.03205720\n",
      "Iteration 78, loss = 0.03193036\n",
      "Iteration 79, loss = 0.03213639\n",
      "Iteration 80, loss = 0.03206142\n",
      "Iteration 81, loss = 0.03228941\n",
      "Iteration 82, loss = 0.03185166\n",
      "Iteration 83, loss = 0.03188200\n",
      "Iteration 84, loss = 0.03231782\n",
      "Iteration 85, loss = 0.03180899\n",
      "Iteration 86, loss = 0.03169519\n",
      "Iteration 87, loss = 0.03148915\n",
      "Iteration 88, loss = 0.03219695\n",
      "Iteration 89, loss = 0.03141162\n",
      "Iteration 90, loss = 0.03254510\n",
      "Iteration 91, loss = 0.03162842\n",
      "Iteration 92, loss = 0.03144262\n",
      "Iteration 93, loss = 0.03160445\n",
      "Iteration 94, loss = 0.03187548\n",
      "Iteration 95, loss = 0.03148409\n",
      "Iteration 96, loss = 0.03133502\n",
      "Iteration 97, loss = 0.03181234\n",
      "Iteration 98, loss = 0.03129674\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.001, batch_size=50, hidden_layer_sizes=(100,), solver=adam; total time= 2.5min\n",
      "Iteration 1, loss = 0.67316482\n",
      "Iteration 2, loss = 0.28974444\n",
      "Iteration 3, loss = 0.23191594\n",
      "Iteration 4, loss = 0.19720896\n",
      "Iteration 5, loss = 0.17209357\n",
      "Iteration 6, loss = 0.15409021\n",
      "Iteration 7, loss = 0.13906133\n",
      "Iteration 8, loss = 0.12630725\n",
      "Iteration 9, loss = 0.11579765\n",
      "Iteration 10, loss = 0.10702287\n",
      "Iteration 11, loss = 0.09876183\n",
      "Iteration 12, loss = 0.09282163\n",
      "Iteration 13, loss = 0.08677372\n",
      "Iteration 14, loss = 0.08096714\n",
      "Iteration 15, loss = 0.07657081\n",
      "Iteration 16, loss = 0.07268894\n",
      "Iteration 17, loss = 0.06897982\n",
      "Iteration 18, loss = 0.06531766\n",
      "Iteration 19, loss = 0.06257655\n",
      "Iteration 20, loss = 0.05974461\n",
      "Iteration 21, loss = 0.05763923\n",
      "Iteration 22, loss = 0.05517519\n",
      "Iteration 23, loss = 0.05298103\n",
      "Iteration 24, loss = 0.05127550\n",
      "Iteration 25, loss = 0.04983730\n",
      "Iteration 26, loss = 0.04846551\n",
      "Iteration 27, loss = 0.04712313\n",
      "Iteration 28, loss = 0.04573728\n",
      "Iteration 29, loss = 0.04499894\n",
      "Iteration 30, loss = 0.04377521\n",
      "Iteration 31, loss = 0.04303669\n",
      "Iteration 32, loss = 0.04243864\n",
      "Iteration 33, loss = 0.04138735\n",
      "Iteration 34, loss = 0.04049222\n",
      "Iteration 35, loss = 0.04007846\n",
      "Iteration 36, loss = 0.03948115\n",
      "Iteration 37, loss = 0.03913744\n",
      "Iteration 38, loss = 0.03855365\n",
      "Iteration 39, loss = 0.03822511\n",
      "Iteration 40, loss = 0.03762117\n",
      "Iteration 41, loss = 0.03754908\n",
      "Iteration 42, loss = 0.03694478\n",
      "Iteration 43, loss = 0.03655478\n",
      "Iteration 44, loss = 0.03625005\n",
      "Iteration 45, loss = 0.03617551\n",
      "Iteration 46, loss = 0.03585748\n",
      "Iteration 47, loss = 0.03544562\n",
      "Iteration 48, loss = 0.03533823\n",
      "Iteration 49, loss = 0.03528094\n",
      "Iteration 50, loss = 0.03501870\n",
      "Iteration 51, loss = 0.03465981\n",
      "Iteration 52, loss = 0.03498067\n",
      "Iteration 53, loss = 0.03441850\n",
      "Iteration 54, loss = 0.03437025\n",
      "Iteration 55, loss = 0.03463541\n",
      "Iteration 56, loss = 0.03400060\n",
      "Iteration 57, loss = 0.03398392\n",
      "Iteration 58, loss = 0.03413481\n",
      "Iteration 59, loss = 0.03400567\n",
      "Iteration 60, loss = 0.03347584\n",
      "Iteration 61, loss = 0.03363859\n",
      "Iteration 62, loss = 0.03390274\n",
      "Iteration 63, loss = 0.03295496\n",
      "Iteration 64, loss = 0.03322465\n",
      "Iteration 65, loss = 0.03300741\n",
      "Iteration 66, loss = 0.03302255\n",
      "Iteration 67, loss = 0.03314030\n",
      "Iteration 68, loss = 0.03288528\n",
      "Iteration 69, loss = 0.03288409\n",
      "Iteration 70, loss = 0.03259245\n",
      "Iteration 71, loss = 0.03275723\n",
      "Iteration 72, loss = 0.03296359\n",
      "Iteration 73, loss = 0.03302886\n",
      "Iteration 74, loss = 0.03227902\n",
      "Iteration 75, loss = 0.03242727\n",
      "Iteration 76, loss = 0.03227988\n",
      "Iteration 77, loss = 0.03244567\n",
      "Iteration 78, loss = 0.03258965\n",
      "Iteration 79, loss = 0.03209163\n",
      "Iteration 80, loss = 0.03219527\n",
      "Iteration 81, loss = 0.03272045\n",
      "Iteration 82, loss = 0.03192165\n",
      "Iteration 83, loss = 0.03223244\n",
      "Iteration 84, loss = 0.03184523\n",
      "Iteration 85, loss = 0.03202980\n",
      "Iteration 86, loss = 0.03275622\n",
      "Iteration 87, loss = 0.03180075\n",
      "Iteration 88, loss = 0.03180167\n",
      "Iteration 89, loss = 0.03205561\n",
      "Iteration 90, loss = 0.03173793\n",
      "Iteration 91, loss = 0.03206787\n",
      "Iteration 92, loss = 0.03199065\n",
      "Iteration 93, loss = 0.03171009\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.001, batch_size=50, hidden_layer_sizes=(100,), solver=adam; total time= 2.4min\n",
      "Iteration 1, loss = 0.67912485\n",
      "Iteration 2, loss = 0.29385498\n",
      "Iteration 3, loss = 0.23621480\n",
      "Iteration 4, loss = 0.20114299\n",
      "Iteration 5, loss = 0.17626721\n",
      "Iteration 6, loss = 0.15694100\n",
      "Iteration 7, loss = 0.14140256\n",
      "Iteration 8, loss = 0.12901130\n",
      "Iteration 9, loss = 0.11800081\n",
      "Iteration 10, loss = 0.10865344\n",
      "Iteration 11, loss = 0.10077958\n",
      "Iteration 12, loss = 0.09424102\n",
      "Iteration 13, loss = 0.08821287\n",
      "Iteration 14, loss = 0.08262228\n",
      "Iteration 15, loss = 0.07778195\n",
      "Iteration 16, loss = 0.07402435\n",
      "Iteration 17, loss = 0.06999066\n",
      "Iteration 18, loss = 0.06650742\n",
      "Iteration 19, loss = 0.06351244\n",
      "Iteration 20, loss = 0.06117706\n",
      "Iteration 21, loss = 0.05777060\n",
      "Iteration 22, loss = 0.05596125\n",
      "Iteration 23, loss = 0.05383714\n",
      "Iteration 24, loss = 0.05217168\n",
      "Iteration 25, loss = 0.05047390\n",
      "Iteration 26, loss = 0.04907709\n",
      "Iteration 27, loss = 0.04749386\n",
      "Iteration 28, loss = 0.04629925\n",
      "Iteration 29, loss = 0.04486793\n",
      "Iteration 30, loss = 0.04442179\n",
      "Iteration 31, loss = 0.04330775\n",
      "Iteration 32, loss = 0.04231497\n",
      "Iteration 33, loss = 0.04150190\n",
      "Iteration 34, loss = 0.04125706\n",
      "Iteration 35, loss = 0.04036913\n",
      "Iteration 36, loss = 0.03950583\n",
      "Iteration 37, loss = 0.03934073\n",
      "Iteration 38, loss = 0.03863053\n",
      "Iteration 39, loss = 0.03817981\n",
      "Iteration 40, loss = 0.03755907\n",
      "Iteration 41, loss = 0.03739708\n",
      "Iteration 42, loss = 0.03717261\n",
      "Iteration 43, loss = 0.03688259\n",
      "Iteration 44, loss = 0.03624116\n",
      "Iteration 45, loss = 0.03596341\n",
      "Iteration 46, loss = 0.03598910\n",
      "Iteration 47, loss = 0.03538356\n",
      "Iteration 48, loss = 0.03540027\n",
      "Iteration 49, loss = 0.03539828\n",
      "Iteration 50, loss = 0.03486340\n",
      "Iteration 51, loss = 0.03493431\n",
      "Iteration 52, loss = 0.03482449\n",
      "Iteration 53, loss = 0.03424344\n",
      "Iteration 54, loss = 0.03422854\n",
      "Iteration 55, loss = 0.03449876\n",
      "Iteration 56, loss = 0.03440185\n",
      "Iteration 57, loss = 0.03398423\n",
      "Iteration 58, loss = 0.03363647\n",
      "Iteration 59, loss = 0.03372700\n",
      "Iteration 60, loss = 0.03382308\n",
      "Iteration 61, loss = 0.03330648\n",
      "Iteration 62, loss = 0.03316050\n",
      "Iteration 63, loss = 0.03328231\n",
      "Iteration 64, loss = 0.03326104\n",
      "Iteration 65, loss = 0.03388040\n",
      "Iteration 66, loss = 0.03261837\n",
      "Iteration 67, loss = 0.03305314\n",
      "Iteration 68, loss = 0.03277932\n",
      "Iteration 69, loss = 0.03251112\n",
      "Iteration 70, loss = 0.03314664\n",
      "Iteration 71, loss = 0.03277496\n",
      "Iteration 72, loss = 0.03244754\n",
      "Iteration 73, loss = 0.03309809\n",
      "Iteration 74, loss = 0.03221731\n",
      "Iteration 75, loss = 0.03235011\n",
      "Iteration 76, loss = 0.03233587\n",
      "Iteration 77, loss = 0.03227368\n",
      "Iteration 78, loss = 0.03228488\n",
      "Iteration 79, loss = 0.03219483\n",
      "Iteration 80, loss = 0.03292403\n",
      "Iteration 81, loss = 0.03217866\n",
      "Iteration 82, loss = 0.03176238\n",
      "Iteration 83, loss = 0.03198190\n",
      "Iteration 84, loss = 0.03185679\n",
      "Iteration 85, loss = 0.03205840\n",
      "Iteration 86, loss = 0.03187941\n",
      "Iteration 87, loss = 0.03214131\n",
      "Iteration 88, loss = 0.03172688\n",
      "Iteration 89, loss = 0.03185702\n",
      "Iteration 90, loss = 0.03175316\n",
      "Iteration 91, loss = 0.03161799\n",
      "Iteration 92, loss = 0.03141535\n",
      "Iteration 93, loss = 0.03168776\n",
      "Iteration 94, loss = 0.03222617\n",
      "Iteration 95, loss = 0.03153812\n",
      "Iteration 96, loss = 0.03142556\n",
      "Iteration 97, loss = 0.03165278\n",
      "Iteration 98, loss = 0.03126641\n",
      "Iteration 99, loss = 0.03138193\n",
      "Iteration 100, loss = 0.03198203\n",
      "Iteration 101, loss = 0.03101551\n",
      "Iteration 102, loss = 0.03141492\n",
      "Iteration 103, loss = 0.03143650\n",
      "Iteration 104, loss = 0.03172964\n",
      "Iteration 105, loss = 0.03138859\n",
      "Iteration 106, loss = 0.03101526\n",
      "Iteration 107, loss = 0.03100844\n",
      "Iteration 108, loss = 0.03157110\n",
      "Iteration 109, loss = 0.03160573\n",
      "Iteration 110, loss = 0.03095098\n",
      "Iteration 111, loss = 0.03096591\n",
      "Iteration 112, loss = 0.03123337\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.001, batch_size=50, hidden_layer_sizes=(100,), solver=adam; total time= 3.0min\n",
      "Iteration 1, loss = 2.14925970\n",
      "Iteration 2, loss = 1.73085771\n",
      "Iteration 3, loss = 1.28488497\n",
      "Iteration 4, loss = 0.98626939\n",
      "Iteration 5, loss = 0.80939627\n",
      "Iteration 6, loss = 0.69920423\n",
      "Iteration 7, loss = 0.62447066\n",
      "Iteration 8, loss = 0.57102628\n",
      "Iteration 9, loss = 0.53044281\n",
      "Iteration 10, loss = 0.49872630\n",
      "Iteration 11, loss = 0.47311134\n",
      "Iteration 12, loss = 0.45229015\n",
      "Iteration 13, loss = 0.43483736\n",
      "Iteration 14, loss = 0.41995487\n",
      "Iteration 15, loss = 0.40720057\n",
      "Iteration 16, loss = 0.39619002\n",
      "Iteration 17, loss = 0.38663171\n",
      "Iteration 18, loss = 0.37789613\n",
      "Iteration 19, loss = 0.37026172\n",
      "Iteration 20, loss = 0.36338004\n",
      "Iteration 21, loss = 0.35710385\n",
      "Iteration 22, loss = 0.35120569\n",
      "Iteration 23, loss = 0.34599969\n",
      "Iteration 24, loss = 0.34113745\n",
      "Iteration 25, loss = 0.33659642\n",
      "Iteration 26, loss = 0.33248363\n",
      "Iteration 27, loss = 0.32840274\n",
      "Iteration 28, loss = 0.32471509\n",
      "Iteration 29, loss = 0.32114330\n",
      "Iteration 30, loss = 0.31797812\n",
      "Iteration 31, loss = 0.31477335\n",
      "Iteration 32, loss = 0.31175142\n",
      "Iteration 33, loss = 0.30884412\n",
      "Iteration 34, loss = 0.30625503\n",
      "Iteration 35, loss = 0.30359430\n",
      "Iteration 36, loss = 0.30099267\n",
      "Iteration 37, loss = 0.29873019\n",
      "Iteration 38, loss = 0.29628988\n",
      "Iteration 39, loss = 0.29408240\n",
      "Iteration 40, loss = 0.29191035\n",
      "Iteration 41, loss = 0.28975926\n",
      "Iteration 42, loss = 0.28770443\n",
      "Iteration 43, loss = 0.28565232\n",
      "Iteration 44, loss = 0.28376420\n",
      "Iteration 45, loss = 0.28180286\n",
      "Iteration 46, loss = 0.28001381\n",
      "Iteration 47, loss = 0.27814640\n",
      "Iteration 48, loss = 0.27641525\n",
      "Iteration 49, loss = 0.27471379\n",
      "Iteration 50, loss = 0.27303898\n",
      "Iteration 51, loss = 0.27133007\n",
      "Iteration 52, loss = 0.26961083\n",
      "Iteration 53, loss = 0.26810143\n",
      "Iteration 54, loss = 0.26653000\n",
      "Iteration 55, loss = 0.26479223\n",
      "Iteration 56, loss = 0.26331097\n",
      "Iteration 57, loss = 0.26183891\n",
      "Iteration 58, loss = 0.26038594\n",
      "Iteration 59, loss = 0.25892115\n",
      "Iteration 60, loss = 0.25738533\n",
      "Iteration 61, loss = 0.25606671\n",
      "Iteration 62, loss = 0.25459089\n",
      "Iteration 63, loss = 0.25323131\n",
      "Iteration 64, loss = 0.25172162\n",
      "Iteration 65, loss = 0.25046218\n",
      "Iteration 66, loss = 0.24903145\n",
      "Iteration 67, loss = 0.24775590\n",
      "Iteration 68, loss = 0.24645400\n",
      "Iteration 69, loss = 0.24513292\n",
      "Iteration 70, loss = 0.24378253\n",
      "Iteration 71, loss = 0.24244707\n",
      "Iteration 72, loss = 0.24127803\n",
      "Iteration 73, loss = 0.23995855\n",
      "Iteration 74, loss = 0.23873144\n",
      "Iteration 75, loss = 0.23750106\n",
      "Iteration 76, loss = 0.23628016\n",
      "Iteration 77, loss = 0.23501530\n",
      "Iteration 78, loss = 0.23381962\n",
      "Iteration 79, loss = 0.23256172\n",
      "Iteration 80, loss = 0.23144595\n",
      "Iteration 81, loss = 0.23021505\n",
      "Iteration 82, loss = 0.22913988\n",
      "Iteration 83, loss = 0.22794565\n",
      "Iteration 84, loss = 0.22679312\n",
      "Iteration 85, loss = 0.22572135\n",
      "Iteration 86, loss = 0.22451018\n",
      "Iteration 87, loss = 0.22345682\n",
      "Iteration 88, loss = 0.22224616\n",
      "Iteration 89, loss = 0.22127309\n",
      "Iteration 90, loss = 0.22010858\n",
      "Iteration 91, loss = 0.21903775\n",
      "Iteration 92, loss = 0.21799303\n",
      "Iteration 93, loss = 0.21696205\n",
      "Iteration 94, loss = 0.21591525\n",
      "Iteration 95, loss = 0.21488467\n",
      "Iteration 96, loss = 0.21384357\n",
      "Iteration 97, loss = 0.21290896\n",
      "Iteration 98, loss = 0.21184747\n",
      "Iteration 99, loss = 0.21084087\n",
      "Iteration 100, loss = 0.20988846\n",
      "Iteration 101, loss = 0.20884356\n",
      "Iteration 102, loss = 0.20791862\n",
      "Iteration 103, loss = 0.20691024\n",
      "Iteration 104, loss = 0.20600311\n",
      "Iteration 105, loss = 0.20497943\n",
      "Iteration 106, loss = 0.20412940\n",
      "Iteration 107, loss = 0.20315483\n",
      "Iteration 108, loss = 0.20224428\n",
      "Iteration 109, loss = 0.20133778\n",
      "Iteration 110, loss = 0.20046483\n",
      "Iteration 111, loss = 0.19952543\n",
      "Iteration 112, loss = 0.19863570\n",
      "Iteration 113, loss = 0.19780019\n",
      "Iteration 114, loss = 0.19696574\n",
      "Iteration 115, loss = 0.19608665\n",
      "Iteration 116, loss = 0.19518828\n",
      "Iteration 117, loss = 0.19432878\n",
      "Iteration 118, loss = 0.19353370\n",
      "Iteration 119, loss = 0.19263861\n",
      "Iteration 120, loss = 0.19183190\n",
      "Iteration 121, loss = 0.19109325\n",
      "Iteration 122, loss = 0.19022376\n",
      "Iteration 123, loss = 0.18932311\n",
      "Iteration 124, loss = 0.18858786\n",
      "Iteration 125, loss = 0.18785214\n",
      "Iteration 126, loss = 0.18696089\n",
      "Iteration 127, loss = 0.18622429\n",
      "Iteration 128, loss = 0.18544422\n",
      "Iteration 129, loss = 0.18471381\n",
      "Iteration 130, loss = 0.18394006\n",
      "Iteration 131, loss = 0.18314190\n",
      "Iteration 132, loss = 0.18241285\n",
      "Iteration 133, loss = 0.18171838\n",
      "Iteration 134, loss = 0.18097800\n",
      "Iteration 135, loss = 0.18018338\n",
      "Iteration 136, loss = 0.17956896\n",
      "Iteration 137, loss = 0.17876854\n",
      "Iteration 138, loss = 0.17805792\n",
      "Iteration 139, loss = 0.17731283\n",
      "Iteration 140, loss = 0.17673358\n",
      "Iteration 141, loss = 0.17599632\n",
      "Iteration 142, loss = 0.17530705\n",
      "Iteration 143, loss = 0.17466742\n",
      "Iteration 144, loss = 0.17394797\n",
      "Iteration 145, loss = 0.17324691\n",
      "Iteration 146, loss = 0.17260031\n",
      "Iteration 147, loss = 0.17189968\n",
      "Iteration 148, loss = 0.17130097\n",
      "Iteration 149, loss = 0.17067223\n",
      "Iteration 150, loss = 0.16998913\n",
      "Iteration 151, loss = 0.16932273\n",
      "Iteration 152, loss = 0.16871748\n",
      "Iteration 153, loss = 0.16813006\n",
      "Iteration 154, loss = 0.16740725\n",
      "Iteration 155, loss = 0.16679096\n",
      "Iteration 156, loss = 0.16624156\n",
      "Iteration 157, loss = 0.16566248\n",
      "Iteration 158, loss = 0.16503109\n",
      "Iteration 159, loss = 0.16439804\n",
      "Iteration 160, loss = 0.16382223\n",
      "Iteration 161, loss = 0.16319852\n",
      "Iteration 162, loss = 0.16261481\n",
      "Iteration 163, loss = 0.16203822\n",
      "Iteration 164, loss = 0.16142558\n",
      "Iteration 165, loss = 0.16087264\n",
      "Iteration 166, loss = 0.16032688\n",
      "Iteration 167, loss = 0.15977238\n",
      "Iteration 168, loss = 0.15912235\n",
      "Iteration 169, loss = 0.15859676\n",
      "Iteration 170, loss = 0.15805809\n",
      "Iteration 171, loss = 0.15752396\n",
      "Iteration 172, loss = 0.15695823\n",
      "Iteration 173, loss = 0.15632899\n",
      "Iteration 174, loss = 0.15585906\n",
      "Iteration 175, loss = 0.15537207\n",
      "Iteration 176, loss = 0.15477074\n",
      "Iteration 177, loss = 0.15428328\n",
      "Iteration 178, loss = 0.15374087\n",
      "Iteration 179, loss = 0.15327271\n",
      "Iteration 180, loss = 0.15271617\n",
      "Iteration 181, loss = 0.15220679\n",
      "Iteration 182, loss = 0.15166041\n",
      "Iteration 183, loss = 0.15118890\n",
      "Iteration 184, loss = 0.15068677\n",
      "Iteration 185, loss = 0.15018211\n",
      "Iteration 186, loss = 0.14969374\n",
      "Iteration 187, loss = 0.14916201\n",
      "Iteration 188, loss = 0.14869311\n",
      "Iteration 189, loss = 0.14820063\n",
      "Iteration 190, loss = 0.14770308\n",
      "Iteration 191, loss = 0.14721458\n",
      "Iteration 192, loss = 0.14674103\n",
      "Iteration 193, loss = 0.14629016\n",
      "Iteration 194, loss = 0.14579746\n",
      "Iteration 195, loss = 0.14532313\n",
      "Iteration 196, loss = 0.14483488\n",
      "Iteration 197, loss = 0.14441628\n",
      "Iteration 198, loss = 0.14391740\n",
      "Iteration 199, loss = 0.14348281\n",
      "Iteration 200, loss = 0.14307782\n",
      "[CV] END activation=logistic, alpha=0.001, batch_size=50, hidden_layer_sizes=(100,), solver=sgd; total time= 3.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.14516090\n",
      "Iteration 2, loss = 1.72348932\n",
      "Iteration 3, loss = 1.28318671\n",
      "Iteration 4, loss = 0.98401544\n",
      "Iteration 5, loss = 0.80541979\n",
      "Iteration 6, loss = 0.69384591\n",
      "Iteration 7, loss = 0.61847025\n",
      "Iteration 8, loss = 0.56481522\n",
      "Iteration 9, loss = 0.52447946\n",
      "Iteration 10, loss = 0.49325716\n",
      "Iteration 11, loss = 0.46813531\n",
      "Iteration 12, loss = 0.44780955\n",
      "Iteration 13, loss = 0.43100647\n",
      "Iteration 14, loss = 0.41674613\n",
      "Iteration 15, loss = 0.40438620\n",
      "Iteration 16, loss = 0.39380579\n",
      "Iteration 17, loss = 0.38434662\n",
      "Iteration 18, loss = 0.37616263\n",
      "Iteration 19, loss = 0.36879707\n",
      "Iteration 20, loss = 0.36222635\n",
      "Iteration 21, loss = 0.35625543\n",
      "Iteration 22, loss = 0.35081179\n",
      "Iteration 23, loss = 0.34575176\n",
      "Iteration 24, loss = 0.34097197\n",
      "Iteration 25, loss = 0.33667399\n",
      "Iteration 26, loss = 0.33270281\n",
      "Iteration 27, loss = 0.32888868\n",
      "Iteration 28, loss = 0.32537515\n",
      "Iteration 29, loss = 0.32195427\n",
      "Iteration 30, loss = 0.31892266\n",
      "Iteration 31, loss = 0.31588129\n",
      "Iteration 32, loss = 0.31302901\n",
      "Iteration 33, loss = 0.31039229\n",
      "Iteration 34, loss = 0.30779751\n",
      "Iteration 35, loss = 0.30516303\n",
      "Iteration 36, loss = 0.30290224\n",
      "Iteration 37, loss = 0.30063157\n",
      "Iteration 38, loss = 0.29828728\n",
      "Iteration 39, loss = 0.29620375\n",
      "Iteration 40, loss = 0.29410581\n",
      "Iteration 41, loss = 0.29214600\n",
      "Iteration 42, loss = 0.29007468\n",
      "Iteration 43, loss = 0.28816837\n",
      "Iteration 44, loss = 0.28636432\n",
      "Iteration 45, loss = 0.28443376\n",
      "Iteration 46, loss = 0.28272137\n",
      "Iteration 47, loss = 0.28097689\n",
      "Iteration 48, loss = 0.27929937\n",
      "Iteration 49, loss = 0.27763028\n",
      "Iteration 50, loss = 0.27596398\n",
      "Iteration 51, loss = 0.27436068\n",
      "Iteration 52, loss = 0.27273513\n",
      "Iteration 53, loss = 0.27125233\n",
      "Iteration 54, loss = 0.26962279\n",
      "Iteration 55, loss = 0.26815395\n",
      "Iteration 56, loss = 0.26668103\n",
      "Iteration 57, loss = 0.26510974\n",
      "Iteration 58, loss = 0.26369450\n",
      "Iteration 59, loss = 0.26229861\n",
      "Iteration 60, loss = 0.26081774\n",
      "Iteration 61, loss = 0.25944592\n",
      "Iteration 62, loss = 0.25802275\n",
      "Iteration 63, loss = 0.25664438\n",
      "Iteration 64, loss = 0.25527595\n",
      "Iteration 65, loss = 0.25390256\n",
      "Iteration 66, loss = 0.25261339\n",
      "Iteration 67, loss = 0.25122414\n",
      "Iteration 68, loss = 0.24986978\n",
      "Iteration 69, loss = 0.24861371\n",
      "Iteration 70, loss = 0.24734085\n",
      "Iteration 71, loss = 0.24607413\n",
      "Iteration 72, loss = 0.24473814\n",
      "Iteration 73, loss = 0.24351489\n",
      "Iteration 74, loss = 0.24230067\n",
      "Iteration 75, loss = 0.24107045\n",
      "Iteration 76, loss = 0.23983527\n",
      "Iteration 77, loss = 0.23861854\n",
      "Iteration 78, loss = 0.23740215\n",
      "Iteration 79, loss = 0.23621986\n",
      "Iteration 80, loss = 0.23498901\n",
      "Iteration 81, loss = 0.23387249\n",
      "Iteration 82, loss = 0.23270243\n",
      "Iteration 83, loss = 0.23160732\n",
      "Iteration 84, loss = 0.23039657\n",
      "Iteration 85, loss = 0.22930113\n",
      "Iteration 86, loss = 0.22815192\n",
      "Iteration 87, loss = 0.22700643\n",
      "Iteration 88, loss = 0.22589485\n",
      "Iteration 89, loss = 0.22481101\n",
      "Iteration 90, loss = 0.22367505\n",
      "Iteration 91, loss = 0.22270129\n",
      "Iteration 92, loss = 0.22151903\n",
      "Iteration 93, loss = 0.22051937\n",
      "Iteration 94, loss = 0.21946702\n",
      "Iteration 95, loss = 0.21832608\n",
      "Iteration 96, loss = 0.21735696\n",
      "Iteration 97, loss = 0.21629644\n",
      "Iteration 98, loss = 0.21531435\n",
      "Iteration 99, loss = 0.21429780\n",
      "Iteration 100, loss = 0.21329534\n",
      "Iteration 101, loss = 0.21223953\n",
      "Iteration 102, loss = 0.21130689\n",
      "Iteration 103, loss = 0.21032979\n",
      "Iteration 104, loss = 0.20934640\n",
      "Iteration 105, loss = 0.20837441\n",
      "Iteration 106, loss = 0.20734116\n",
      "Iteration 107, loss = 0.20648850\n",
      "Iteration 108, loss = 0.20558239\n",
      "Iteration 109, loss = 0.20454815\n",
      "Iteration 110, loss = 0.20367070\n",
      "Iteration 111, loss = 0.20283064\n",
      "Iteration 112, loss = 0.20183577\n",
      "Iteration 113, loss = 0.20094565\n",
      "Iteration 114, loss = 0.20010372\n",
      "Iteration 115, loss = 0.19921419\n",
      "Iteration 116, loss = 0.19837544\n",
      "Iteration 117, loss = 0.19744771\n",
      "Iteration 118, loss = 0.19660885\n",
      "Iteration 119, loss = 0.19579829\n",
      "Iteration 120, loss = 0.19491000\n",
      "Iteration 121, loss = 0.19399158\n",
      "Iteration 122, loss = 0.19323752\n",
      "Iteration 123, loss = 0.19241264\n",
      "Iteration 124, loss = 0.19149568\n",
      "Iteration 125, loss = 0.19081262\n",
      "Iteration 126, loss = 0.18998833\n",
      "Iteration 127, loss = 0.18913438\n",
      "Iteration 128, loss = 0.18840897\n",
      "Iteration 129, loss = 0.18751790\n",
      "Iteration 130, loss = 0.18682657\n",
      "Iteration 131, loss = 0.18604357\n",
      "Iteration 132, loss = 0.18528099\n",
      "Iteration 133, loss = 0.18448405\n",
      "Iteration 134, loss = 0.18368011\n",
      "Iteration 135, loss = 0.18298741\n",
      "Iteration 136, loss = 0.18226969\n",
      "Iteration 137, loss = 0.18156265\n",
      "Iteration 138, loss = 0.18081123\n",
      "Iteration 139, loss = 0.18008011\n",
      "Iteration 140, loss = 0.17940233\n",
      "Iteration 141, loss = 0.17869411\n",
      "Iteration 142, loss = 0.17796623\n",
      "Iteration 143, loss = 0.17735633\n",
      "Iteration 144, loss = 0.17659571\n",
      "Iteration 145, loss = 0.17582389\n",
      "Iteration 146, loss = 0.17518567\n",
      "Iteration 147, loss = 0.17457605\n",
      "Iteration 148, loss = 0.17378953\n",
      "Iteration 149, loss = 0.17320219\n",
      "Iteration 150, loss = 0.17256344\n",
      "Iteration 151, loss = 0.17187596\n",
      "Iteration 152, loss = 0.17122395\n",
      "Iteration 153, loss = 0.17057516\n",
      "Iteration 154, loss = 0.16993625\n",
      "Iteration 155, loss = 0.16927268\n",
      "Iteration 156, loss = 0.16871539\n",
      "Iteration 157, loss = 0.16808660\n",
      "Iteration 158, loss = 0.16738532\n",
      "Iteration 159, loss = 0.16683877\n",
      "Iteration 160, loss = 0.16621073\n",
      "Iteration 161, loss = 0.16563798\n",
      "Iteration 162, loss = 0.16495624\n",
      "Iteration 163, loss = 0.16437743\n",
      "Iteration 164, loss = 0.16385180\n",
      "Iteration 165, loss = 0.16327856\n",
      "Iteration 166, loss = 0.16266356\n",
      "Iteration 167, loss = 0.16206622\n",
      "Iteration 168, loss = 0.16151927\n",
      "Iteration 169, loss = 0.16095337\n",
      "Iteration 170, loss = 0.16035078\n",
      "Iteration 171, loss = 0.15983908\n",
      "Iteration 172, loss = 0.15916432\n",
      "Iteration 173, loss = 0.15868886\n",
      "Iteration 174, loss = 0.15810008\n",
      "Iteration 175, loss = 0.15758932\n",
      "Iteration 176, loss = 0.15702538\n",
      "Iteration 177, loss = 0.15649989\n",
      "Iteration 178, loss = 0.15595903\n",
      "Iteration 179, loss = 0.15542815\n",
      "Iteration 180, loss = 0.15491839\n",
      "Iteration 181, loss = 0.15436247\n",
      "Iteration 182, loss = 0.15386555\n",
      "Iteration 183, loss = 0.15333702\n",
      "Iteration 184, loss = 0.15277592\n",
      "Iteration 185, loss = 0.15234350\n",
      "Iteration 186, loss = 0.15179731\n",
      "Iteration 187, loss = 0.15123052\n",
      "Iteration 188, loss = 0.15085481\n",
      "Iteration 189, loss = 0.15030911\n",
      "Iteration 190, loss = 0.14981165\n",
      "Iteration 191, loss = 0.14931915\n",
      "Iteration 192, loss = 0.14885809\n",
      "Iteration 193, loss = 0.14832938\n",
      "Iteration 194, loss = 0.14783139\n",
      "Iteration 195, loss = 0.14741744\n",
      "Iteration 196, loss = 0.14684165\n",
      "Iteration 197, loss = 0.14642818\n",
      "Iteration 198, loss = 0.14600336\n",
      "Iteration 199, loss = 0.14555168\n",
      "Iteration 200, loss = 0.14506390\n",
      "[CV] END activation=logistic, alpha=0.001, batch_size=50, hidden_layer_sizes=(100,), solver=sgd; total time= 3.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.14754376\n",
      "Iteration 2, loss = 1.72584033\n",
      "Iteration 3, loss = 1.28470881\n",
      "Iteration 4, loss = 0.98409117\n",
      "Iteration 5, loss = 0.80517449\n",
      "Iteration 6, loss = 0.69403256\n",
      "Iteration 7, loss = 0.61958766\n",
      "Iteration 8, loss = 0.56644565\n",
      "Iteration 9, loss = 0.52653994\n",
      "Iteration 10, loss = 0.49524939\n",
      "Iteration 11, loss = 0.47014166\n",
      "Iteration 12, loss = 0.44964888\n",
      "Iteration 13, loss = 0.43244505\n",
      "Iteration 14, loss = 0.41785860\n",
      "Iteration 15, loss = 0.40528740\n",
      "Iteration 16, loss = 0.39432133\n",
      "Iteration 17, loss = 0.38487357\n",
      "Iteration 18, loss = 0.37630859\n",
      "Iteration 19, loss = 0.36865729\n",
      "Iteration 20, loss = 0.36186226\n",
      "Iteration 21, loss = 0.35564100\n",
      "Iteration 22, loss = 0.34992820\n",
      "Iteration 23, loss = 0.34476811\n",
      "Iteration 24, loss = 0.33987113\n",
      "Iteration 25, loss = 0.33548136\n",
      "Iteration 26, loss = 0.33121691\n",
      "Iteration 27, loss = 0.32742187\n",
      "Iteration 28, loss = 0.32365718\n",
      "Iteration 29, loss = 0.32027468\n",
      "Iteration 30, loss = 0.31695060\n",
      "Iteration 31, loss = 0.31396705\n",
      "Iteration 32, loss = 0.31093425\n",
      "Iteration 33, loss = 0.30816274\n",
      "Iteration 34, loss = 0.30554437\n",
      "Iteration 35, loss = 0.30286042\n",
      "Iteration 36, loss = 0.30046102\n",
      "Iteration 37, loss = 0.29803818\n",
      "Iteration 38, loss = 0.29575489\n",
      "Iteration 39, loss = 0.29345923\n",
      "Iteration 40, loss = 0.29138802\n",
      "Iteration 41, loss = 0.28921909\n",
      "Iteration 42, loss = 0.28725340\n",
      "Iteration 43, loss = 0.28531172\n",
      "Iteration 44, loss = 0.28338368\n",
      "Iteration 45, loss = 0.28144152\n",
      "Iteration 46, loss = 0.27961548\n",
      "Iteration 47, loss = 0.27790471\n",
      "Iteration 48, loss = 0.27605678\n",
      "Iteration 49, loss = 0.27448783\n",
      "Iteration 50, loss = 0.27277002\n",
      "Iteration 51, loss = 0.27115167\n",
      "Iteration 52, loss = 0.26953681\n",
      "Iteration 53, loss = 0.26781176\n",
      "Iteration 54, loss = 0.26625298\n",
      "Iteration 55, loss = 0.26471902\n",
      "Iteration 56, loss = 0.26330064\n",
      "Iteration 57, loss = 0.26179021\n",
      "Iteration 58, loss = 0.26023249\n",
      "Iteration 59, loss = 0.25882894\n",
      "Iteration 60, loss = 0.25743662\n",
      "Iteration 61, loss = 0.25585008\n",
      "Iteration 62, loss = 0.25453746\n",
      "Iteration 63, loss = 0.25314513\n",
      "Iteration 64, loss = 0.25170842\n",
      "Iteration 65, loss = 0.25035093\n",
      "Iteration 66, loss = 0.24905592\n",
      "Iteration 67, loss = 0.24770616\n",
      "Iteration 68, loss = 0.24635030\n",
      "Iteration 69, loss = 0.24512956\n",
      "Iteration 70, loss = 0.24382323\n",
      "Iteration 71, loss = 0.24249101\n",
      "Iteration 72, loss = 0.24119428\n",
      "Iteration 73, loss = 0.23998031\n",
      "Iteration 74, loss = 0.23876122\n",
      "Iteration 75, loss = 0.23749212\n",
      "Iteration 76, loss = 0.23626889\n",
      "Iteration 77, loss = 0.23509037\n",
      "Iteration 78, loss = 0.23380332\n",
      "Iteration 79, loss = 0.23261670\n",
      "Iteration 80, loss = 0.23150992\n",
      "Iteration 81, loss = 0.23030758\n",
      "Iteration 82, loss = 0.22921237\n",
      "Iteration 83, loss = 0.22804035\n",
      "Iteration 84, loss = 0.22679938\n",
      "Iteration 85, loss = 0.22569457\n",
      "Iteration 86, loss = 0.22454628\n",
      "Iteration 87, loss = 0.22347873\n",
      "Iteration 88, loss = 0.22236154\n",
      "Iteration 89, loss = 0.22130767\n",
      "Iteration 90, loss = 0.22016711\n",
      "Iteration 91, loss = 0.21916445\n",
      "Iteration 92, loss = 0.21803768\n",
      "Iteration 93, loss = 0.21707016\n",
      "Iteration 94, loss = 0.21598422\n",
      "Iteration 95, loss = 0.21493140\n",
      "Iteration 96, loss = 0.21395290\n",
      "Iteration 97, loss = 0.21292541\n",
      "Iteration 98, loss = 0.21188482\n",
      "Iteration 99, loss = 0.21091466\n",
      "Iteration 100, loss = 0.20989553\n",
      "Iteration 101, loss = 0.20895672\n",
      "Iteration 102, loss = 0.20789966\n",
      "Iteration 103, loss = 0.20692221\n",
      "Iteration 104, loss = 0.20602125\n",
      "Iteration 105, loss = 0.20507030\n",
      "Iteration 106, loss = 0.20403647\n",
      "Iteration 107, loss = 0.20321710\n",
      "Iteration 108, loss = 0.20227643\n",
      "Iteration 109, loss = 0.20133012\n",
      "Iteration 110, loss = 0.20035840\n",
      "Iteration 111, loss = 0.19947586\n",
      "Iteration 112, loss = 0.19861594\n",
      "Iteration 113, loss = 0.19776564\n",
      "Iteration 114, loss = 0.19688080\n",
      "Iteration 115, loss = 0.19598978\n",
      "Iteration 116, loss = 0.19512081\n",
      "Iteration 117, loss = 0.19425265\n",
      "Iteration 118, loss = 0.19343590\n",
      "Iteration 119, loss = 0.19257797\n",
      "Iteration 120, loss = 0.19176306\n",
      "Iteration 121, loss = 0.19090194\n",
      "Iteration 122, loss = 0.19010179\n",
      "Iteration 123, loss = 0.18923210\n",
      "Iteration 124, loss = 0.18846459\n",
      "Iteration 125, loss = 0.18768666\n",
      "Iteration 126, loss = 0.18686389\n",
      "Iteration 127, loss = 0.18606815\n",
      "Iteration 128, loss = 0.18523928\n",
      "Iteration 129, loss = 0.18453341\n",
      "Iteration 130, loss = 0.18378199\n",
      "Iteration 131, loss = 0.18301256\n",
      "Iteration 132, loss = 0.18222865\n",
      "Iteration 133, loss = 0.18141858\n",
      "Iteration 134, loss = 0.18071433\n",
      "Iteration 135, loss = 0.18002244\n",
      "Iteration 136, loss = 0.17927818\n",
      "Iteration 137, loss = 0.17853519\n",
      "Iteration 138, loss = 0.17780218\n",
      "Iteration 139, loss = 0.17710523\n",
      "Iteration 140, loss = 0.17636131\n",
      "Iteration 141, loss = 0.17560242\n",
      "Iteration 142, loss = 0.17495092\n",
      "Iteration 143, loss = 0.17431955\n",
      "Iteration 144, loss = 0.17356322\n",
      "Iteration 145, loss = 0.17292234\n",
      "Iteration 146, loss = 0.17222393\n",
      "Iteration 147, loss = 0.17147236\n",
      "Iteration 148, loss = 0.17092205\n",
      "Iteration 149, loss = 0.17019405\n",
      "Iteration 150, loss = 0.16950579\n",
      "Iteration 151, loss = 0.16884885\n",
      "Iteration 152, loss = 0.16823398\n",
      "Iteration 153, loss = 0.16761670\n",
      "Iteration 154, loss = 0.16694608\n",
      "Iteration 155, loss = 0.16632330\n",
      "Iteration 156, loss = 0.16571346\n",
      "Iteration 157, loss = 0.16503208\n",
      "Iteration 158, loss = 0.16446033\n",
      "Iteration 159, loss = 0.16383528\n",
      "Iteration 160, loss = 0.16323342\n",
      "Iteration 161, loss = 0.16266033\n",
      "Iteration 162, loss = 0.16203970\n",
      "Iteration 163, loss = 0.16144526\n",
      "Iteration 164, loss = 0.16085402\n",
      "Iteration 165, loss = 0.16030187\n",
      "Iteration 166, loss = 0.15959473\n",
      "Iteration 167, loss = 0.15909879\n",
      "Iteration 168, loss = 0.15848855\n",
      "Iteration 169, loss = 0.15797604\n",
      "Iteration 170, loss = 0.15739861\n",
      "Iteration 171, loss = 0.15684564\n",
      "Iteration 172, loss = 0.15629099\n",
      "Iteration 173, loss = 0.15570945\n",
      "Iteration 174, loss = 0.15510691\n",
      "Iteration 175, loss = 0.15466098\n",
      "Iteration 176, loss = 0.15408724\n",
      "Iteration 177, loss = 0.15352074\n",
      "Iteration 178, loss = 0.15295680\n",
      "Iteration 179, loss = 0.15241737\n",
      "Iteration 180, loss = 0.15194908\n",
      "Iteration 181, loss = 0.15145396\n",
      "Iteration 182, loss = 0.15092969\n",
      "Iteration 183, loss = 0.15041415\n",
      "Iteration 184, loss = 0.14988404\n",
      "Iteration 185, loss = 0.14931981\n",
      "Iteration 186, loss = 0.14884827\n",
      "Iteration 187, loss = 0.14835717\n",
      "Iteration 188, loss = 0.14786398\n",
      "Iteration 189, loss = 0.14735613\n",
      "Iteration 190, loss = 0.14690021\n",
      "Iteration 191, loss = 0.14642209\n",
      "Iteration 192, loss = 0.14589764\n",
      "Iteration 193, loss = 0.14539279\n",
      "Iteration 194, loss = 0.14486529\n",
      "Iteration 195, loss = 0.14448210\n",
      "Iteration 196, loss = 0.14399387\n",
      "Iteration 197, loss = 0.14353648\n",
      "Iteration 198, loss = 0.14304882\n",
      "Iteration 199, loss = 0.14259860\n",
      "Iteration 200, loss = 0.14211342\n",
      "[CV] END activation=logistic, alpha=0.001, batch_size=50, hidden_layer_sizes=(100,), solver=sgd; total time= 3.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.13903956\n",
      "Iteration 2, loss = 1.72627601\n",
      "Iteration 3, loss = 1.28780729\n",
      "Iteration 4, loss = 0.99186528\n",
      "Iteration 5, loss = 0.81479868\n",
      "Iteration 6, loss = 0.70299848\n",
      "Iteration 7, loss = 0.62715731\n",
      "Iteration 8, loss = 0.57250204\n",
      "Iteration 9, loss = 0.53124345\n",
      "Iteration 10, loss = 0.49894384\n",
      "Iteration 11, loss = 0.47308241\n",
      "Iteration 12, loss = 0.45193175\n",
      "Iteration 13, loss = 0.43432718\n",
      "Iteration 14, loss = 0.41944361\n",
      "Iteration 15, loss = 0.40655227\n",
      "Iteration 16, loss = 0.39562067\n",
      "Iteration 17, loss = 0.38582329\n",
      "Iteration 18, loss = 0.37725094\n",
      "Iteration 19, loss = 0.36966008\n",
      "Iteration 20, loss = 0.36262071\n",
      "Iteration 21, loss = 0.35633194\n",
      "Iteration 22, loss = 0.35058852\n",
      "Iteration 23, loss = 0.34539378\n",
      "Iteration 24, loss = 0.34038882\n",
      "Iteration 25, loss = 0.33573923\n",
      "Iteration 26, loss = 0.33163119\n",
      "Iteration 27, loss = 0.32762242\n",
      "Iteration 28, loss = 0.32385908\n",
      "Iteration 29, loss = 0.32028379\n",
      "Iteration 30, loss = 0.31699891\n",
      "Iteration 31, loss = 0.31380272\n",
      "Iteration 32, loss = 0.31076573\n",
      "Iteration 33, loss = 0.30782181\n",
      "Iteration 34, loss = 0.30513816\n",
      "Iteration 35, loss = 0.30236829\n",
      "Iteration 36, loss = 0.29989240\n",
      "Iteration 37, loss = 0.29740295\n",
      "Iteration 38, loss = 0.29504302\n",
      "Iteration 39, loss = 0.29277987\n",
      "Iteration 40, loss = 0.29061315\n",
      "Iteration 41, loss = 0.28830870\n",
      "Iteration 42, loss = 0.28621808\n",
      "Iteration 43, loss = 0.28420033\n",
      "Iteration 44, loss = 0.28221145\n",
      "Iteration 45, loss = 0.28021809\n",
      "Iteration 46, loss = 0.27840703\n",
      "Iteration 47, loss = 0.27643490\n",
      "Iteration 48, loss = 0.27470419\n",
      "Iteration 49, loss = 0.27287018\n",
      "Iteration 50, loss = 0.27109261\n",
      "Iteration 51, loss = 0.26940635\n",
      "Iteration 52, loss = 0.26777967\n",
      "Iteration 53, loss = 0.26605902\n",
      "Iteration 54, loss = 0.26449964\n",
      "Iteration 55, loss = 0.26289673\n",
      "Iteration 56, loss = 0.26132394\n",
      "Iteration 57, loss = 0.25975112\n",
      "Iteration 58, loss = 0.25821978\n",
      "Iteration 59, loss = 0.25671519\n",
      "Iteration 60, loss = 0.25525395\n",
      "Iteration 61, loss = 0.25380973\n",
      "Iteration 62, loss = 0.25231359\n",
      "Iteration 63, loss = 0.25093754\n",
      "Iteration 64, loss = 0.24955775\n",
      "Iteration 65, loss = 0.24820367\n",
      "Iteration 66, loss = 0.24682262\n",
      "Iteration 67, loss = 0.24535403\n",
      "Iteration 68, loss = 0.24413913\n",
      "Iteration 69, loss = 0.24280513\n",
      "Iteration 70, loss = 0.24139487\n",
      "Iteration 71, loss = 0.24023943\n",
      "Iteration 72, loss = 0.23891012\n",
      "Iteration 73, loss = 0.23774557\n",
      "Iteration 74, loss = 0.23639829\n",
      "Iteration 75, loss = 0.23529833\n",
      "Iteration 76, loss = 0.23394442\n",
      "Iteration 77, loss = 0.23276957\n",
      "Iteration 78, loss = 0.23158686\n",
      "Iteration 79, loss = 0.23045055\n",
      "Iteration 80, loss = 0.22921269\n",
      "Iteration 81, loss = 0.22819886\n",
      "Iteration 82, loss = 0.22696269\n",
      "Iteration 83, loss = 0.22585747\n",
      "Iteration 84, loss = 0.22476930\n",
      "Iteration 85, loss = 0.22355953\n",
      "Iteration 86, loss = 0.22259359\n",
      "Iteration 87, loss = 0.22140755\n",
      "Iteration 88, loss = 0.22039712\n",
      "Iteration 89, loss = 0.21937497\n",
      "Iteration 90, loss = 0.21835611\n",
      "Iteration 91, loss = 0.21728052\n",
      "Iteration 92, loss = 0.21621151\n",
      "Iteration 93, loss = 0.21519516\n",
      "Iteration 94, loss = 0.21420383\n",
      "Iteration 95, loss = 0.21316045\n",
      "Iteration 96, loss = 0.21224845\n",
      "Iteration 97, loss = 0.21128729\n",
      "Iteration 98, loss = 0.21023285\n",
      "Iteration 99, loss = 0.20928334\n",
      "Iteration 100, loss = 0.20835468\n",
      "Iteration 101, loss = 0.20739133\n",
      "Iteration 102, loss = 0.20646431\n",
      "Iteration 103, loss = 0.20554057\n",
      "Iteration 104, loss = 0.20459861\n",
      "Iteration 105, loss = 0.20377883\n",
      "Iteration 106, loss = 0.20276620\n",
      "Iteration 107, loss = 0.20194034\n",
      "Iteration 108, loss = 0.20098899\n",
      "Iteration 109, loss = 0.20017431\n",
      "Iteration 110, loss = 0.19931084\n",
      "Iteration 111, loss = 0.19846033\n",
      "Iteration 112, loss = 0.19756820\n",
      "Iteration 113, loss = 0.19670829\n",
      "Iteration 114, loss = 0.19588312\n",
      "Iteration 115, loss = 0.19509398\n",
      "Iteration 116, loss = 0.19423406\n",
      "Iteration 117, loss = 0.19332161\n",
      "Iteration 118, loss = 0.19264095\n",
      "Iteration 119, loss = 0.19172529\n",
      "Iteration 120, loss = 0.19099827\n",
      "Iteration 121, loss = 0.19019544\n",
      "Iteration 122, loss = 0.18947153\n",
      "Iteration 123, loss = 0.18863494\n",
      "Iteration 124, loss = 0.18790498\n",
      "Iteration 125, loss = 0.18710960\n",
      "Iteration 126, loss = 0.18631386\n",
      "Iteration 127, loss = 0.18553040\n",
      "Iteration 128, loss = 0.18489291\n",
      "Iteration 129, loss = 0.18419911\n",
      "Iteration 130, loss = 0.18348840\n",
      "Iteration 131, loss = 0.18271346\n",
      "Iteration 132, loss = 0.18198264\n",
      "Iteration 133, loss = 0.18121071\n",
      "Iteration 134, loss = 0.18055352\n",
      "Iteration 135, loss = 0.17977656\n",
      "Iteration 136, loss = 0.17917498\n",
      "Iteration 137, loss = 0.17844604\n",
      "Iteration 138, loss = 0.17775992\n",
      "Iteration 139, loss = 0.17712955\n",
      "Iteration 140, loss = 0.17640038\n",
      "Iteration 141, loss = 0.17575245\n",
      "Iteration 142, loss = 0.17511314\n",
      "Iteration 143, loss = 0.17446995\n",
      "Iteration 144, loss = 0.17377842\n",
      "Iteration 145, loss = 0.17311968\n",
      "Iteration 146, loss = 0.17251374\n",
      "Iteration 147, loss = 0.17184569\n",
      "Iteration 148, loss = 0.17118938\n",
      "Iteration 149, loss = 0.17055175\n",
      "Iteration 150, loss = 0.16991906\n",
      "Iteration 151, loss = 0.16932593\n",
      "Iteration 152, loss = 0.16865498\n",
      "Iteration 153, loss = 0.16810382\n",
      "Iteration 154, loss = 0.16745215\n",
      "Iteration 155, loss = 0.16688620\n",
      "Iteration 156, loss = 0.16631149\n",
      "Iteration 157, loss = 0.16565596\n",
      "Iteration 158, loss = 0.16512654\n",
      "Iteration 159, loss = 0.16449394\n",
      "Iteration 160, loss = 0.16395287\n",
      "Iteration 161, loss = 0.16331350\n",
      "Iteration 162, loss = 0.16276537\n",
      "Iteration 163, loss = 0.16223282\n",
      "Iteration 164, loss = 0.16158062\n",
      "Iteration 165, loss = 0.16111519\n",
      "Iteration 166, loss = 0.16048518\n",
      "Iteration 167, loss = 0.16000935\n",
      "Iteration 168, loss = 0.15943119\n",
      "Iteration 169, loss = 0.15883372\n",
      "Iteration 170, loss = 0.15835811\n",
      "Iteration 171, loss = 0.15773949\n",
      "Iteration 172, loss = 0.15726518\n",
      "Iteration 173, loss = 0.15671415\n",
      "Iteration 174, loss = 0.15620332\n",
      "Iteration 175, loss = 0.15563790\n",
      "Iteration 176, loss = 0.15522559\n",
      "Iteration 177, loss = 0.15461910\n",
      "Iteration 178, loss = 0.15413384\n",
      "Iteration 179, loss = 0.15362282\n",
      "Iteration 180, loss = 0.15311144\n",
      "Iteration 181, loss = 0.15262308\n",
      "Iteration 182, loss = 0.15206956\n",
      "Iteration 183, loss = 0.15165328\n",
      "Iteration 184, loss = 0.15113629\n",
      "Iteration 185, loss = 0.15063217\n",
      "Iteration 186, loss = 0.15014601\n",
      "Iteration 187, loss = 0.14967014\n",
      "Iteration 188, loss = 0.14922119\n",
      "Iteration 189, loss = 0.14862728\n",
      "Iteration 190, loss = 0.14826818\n",
      "Iteration 191, loss = 0.14778521\n",
      "Iteration 192, loss = 0.14727401\n",
      "Iteration 193, loss = 0.14678759\n",
      "Iteration 194, loss = 0.14635247\n",
      "Iteration 195, loss = 0.14592666\n",
      "Iteration 196, loss = 0.14545726\n",
      "Iteration 197, loss = 0.14497575\n",
      "Iteration 198, loss = 0.14452201\n",
      "Iteration 199, loss = 0.14409915\n",
      "Iteration 200, loss = 0.14365402\n",
      "[CV] END activation=logistic, alpha=0.001, batch_size=50, hidden_layer_sizes=(100,), solver=sgd; total time= 3.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.14833058\n",
      "Iteration 2, loss = 1.73568687\n",
      "Iteration 3, loss = 1.28940305\n",
      "Iteration 4, loss = 0.98810912\n",
      "Iteration 5, loss = 0.81074840\n",
      "Iteration 6, loss = 0.70013651\n",
      "Iteration 7, loss = 0.62580351\n",
      "Iteration 8, loss = 0.57230271\n",
      "Iteration 9, loss = 0.53217149\n",
      "Iteration 10, loss = 0.50064127\n",
      "Iteration 11, loss = 0.47545101\n",
      "Iteration 12, loss = 0.45464654\n",
      "Iteration 13, loss = 0.43738685\n",
      "Iteration 14, loss = 0.42288663\n",
      "Iteration 15, loss = 0.41017230\n",
      "Iteration 16, loss = 0.39921839\n",
      "Iteration 17, loss = 0.38955409\n",
      "Iteration 18, loss = 0.38090722\n",
      "Iteration 19, loss = 0.37313889\n",
      "Iteration 20, loss = 0.36621548\n",
      "Iteration 21, loss = 0.35982756\n",
      "Iteration 22, loss = 0.35409752\n",
      "Iteration 23, loss = 0.34872249\n",
      "Iteration 24, loss = 0.34387516\n",
      "Iteration 25, loss = 0.33921105\n",
      "Iteration 26, loss = 0.33496320\n",
      "Iteration 27, loss = 0.33097235\n",
      "Iteration 28, loss = 0.32706795\n",
      "Iteration 29, loss = 0.32363626\n",
      "Iteration 30, loss = 0.32032338\n",
      "Iteration 31, loss = 0.31708603\n",
      "Iteration 32, loss = 0.31402041\n",
      "Iteration 33, loss = 0.31118400\n",
      "Iteration 34, loss = 0.30827443\n",
      "Iteration 35, loss = 0.30578814\n",
      "Iteration 36, loss = 0.30312026\n",
      "Iteration 37, loss = 0.30080400\n",
      "Iteration 38, loss = 0.29833985\n",
      "Iteration 39, loss = 0.29600618\n",
      "Iteration 40, loss = 0.29385633\n",
      "Iteration 41, loss = 0.29176420\n",
      "Iteration 42, loss = 0.28957593\n",
      "Iteration 43, loss = 0.28759833\n",
      "Iteration 44, loss = 0.28563907\n",
      "Iteration 45, loss = 0.28377212\n",
      "Iteration 46, loss = 0.28185021\n",
      "Iteration 47, loss = 0.27994017\n",
      "Iteration 48, loss = 0.27815378\n",
      "Iteration 49, loss = 0.27638239\n",
      "Iteration 50, loss = 0.27468354\n",
      "Iteration 51, loss = 0.27299236\n",
      "Iteration 52, loss = 0.27137715\n",
      "Iteration 53, loss = 0.26971336\n",
      "Iteration 54, loss = 0.26807507\n",
      "Iteration 55, loss = 0.26651170\n",
      "Iteration 56, loss = 0.26493480\n",
      "Iteration 57, loss = 0.26343513\n",
      "Iteration 58, loss = 0.26189730\n",
      "Iteration 59, loss = 0.26039602\n",
      "Iteration 60, loss = 0.25902601\n",
      "Iteration 61, loss = 0.25749276\n",
      "Iteration 62, loss = 0.25608890\n",
      "Iteration 63, loss = 0.25452732\n",
      "Iteration 64, loss = 0.25328138\n",
      "Iteration 65, loss = 0.25184485\n",
      "Iteration 66, loss = 0.25053250\n",
      "Iteration 67, loss = 0.24918672\n",
      "Iteration 68, loss = 0.24791170\n",
      "Iteration 69, loss = 0.24651595\n",
      "Iteration 70, loss = 0.24524748\n",
      "Iteration 71, loss = 0.24393711\n",
      "Iteration 72, loss = 0.24265564\n",
      "Iteration 73, loss = 0.24134150\n",
      "Iteration 74, loss = 0.24014647\n",
      "Iteration 75, loss = 0.23895339\n",
      "Iteration 76, loss = 0.23771709\n",
      "Iteration 77, loss = 0.23651953\n",
      "Iteration 78, loss = 0.23530606\n",
      "Iteration 79, loss = 0.23420521\n",
      "Iteration 80, loss = 0.23290406\n",
      "Iteration 81, loss = 0.23169136\n",
      "Iteration 82, loss = 0.23071088\n",
      "Iteration 83, loss = 0.22946942\n",
      "Iteration 84, loss = 0.22842116\n",
      "Iteration 85, loss = 0.22729059\n",
      "Iteration 86, loss = 0.22616802\n",
      "Iteration 87, loss = 0.22512215\n",
      "Iteration 88, loss = 0.22409714\n",
      "Iteration 89, loss = 0.22299844\n",
      "Iteration 90, loss = 0.22190699\n",
      "Iteration 91, loss = 0.22086462\n",
      "Iteration 92, loss = 0.21988801\n",
      "Iteration 93, loss = 0.21875502\n",
      "Iteration 94, loss = 0.21780213\n",
      "Iteration 95, loss = 0.21676417\n",
      "Iteration 96, loss = 0.21577622\n",
      "Iteration 97, loss = 0.21480558\n",
      "Iteration 98, loss = 0.21371288\n",
      "Iteration 99, loss = 0.21287072\n",
      "Iteration 100, loss = 0.21187697\n",
      "Iteration 101, loss = 0.21096898\n",
      "Iteration 102, loss = 0.20993370\n",
      "Iteration 103, loss = 0.20903110\n",
      "Iteration 104, loss = 0.20811646\n",
      "Iteration 105, loss = 0.20716281\n",
      "Iteration 106, loss = 0.20628465\n",
      "Iteration 107, loss = 0.20538116\n",
      "Iteration 108, loss = 0.20450937\n",
      "Iteration 109, loss = 0.20361832\n",
      "Iteration 110, loss = 0.20271621\n",
      "Iteration 111, loss = 0.20185238\n",
      "Iteration 112, loss = 0.20101575\n",
      "Iteration 113, loss = 0.20009204\n",
      "Iteration 114, loss = 0.19922041\n",
      "Iteration 115, loss = 0.19832909\n",
      "Iteration 116, loss = 0.19761040\n",
      "Iteration 117, loss = 0.19667069\n",
      "Iteration 118, loss = 0.19587062\n",
      "Iteration 119, loss = 0.19510692\n",
      "Iteration 120, loss = 0.19430027\n",
      "Iteration 121, loss = 0.19346592\n",
      "Iteration 122, loss = 0.19266129\n",
      "Iteration 123, loss = 0.19190187\n",
      "Iteration 124, loss = 0.19111553\n",
      "Iteration 125, loss = 0.19033432\n",
      "Iteration 126, loss = 0.18949791\n",
      "Iteration 127, loss = 0.18876368\n",
      "Iteration 128, loss = 0.18799055\n",
      "Iteration 129, loss = 0.18727504\n",
      "Iteration 130, loss = 0.18648490\n",
      "Iteration 131, loss = 0.18578265\n",
      "Iteration 132, loss = 0.18499653\n",
      "Iteration 133, loss = 0.18427787\n",
      "Iteration 134, loss = 0.18357461\n",
      "Iteration 135, loss = 0.18284684\n",
      "Iteration 136, loss = 0.18211058\n",
      "Iteration 137, loss = 0.18145453\n",
      "Iteration 138, loss = 0.18069811\n",
      "Iteration 139, loss = 0.18001622\n",
      "Iteration 140, loss = 0.17927530\n",
      "Iteration 141, loss = 0.17862594\n",
      "Iteration 142, loss = 0.17792736\n",
      "Iteration 143, loss = 0.17720617\n",
      "Iteration 144, loss = 0.17655864\n",
      "Iteration 145, loss = 0.17590168\n",
      "Iteration 146, loss = 0.17523272\n",
      "Iteration 147, loss = 0.17459725\n",
      "Iteration 148, loss = 0.17387347\n",
      "Iteration 149, loss = 0.17329640\n",
      "Iteration 150, loss = 0.17259820\n",
      "Iteration 151, loss = 0.17196286\n",
      "Iteration 152, loss = 0.17132023\n",
      "Iteration 153, loss = 0.17074340\n",
      "Iteration 154, loss = 0.17007518\n",
      "Iteration 155, loss = 0.16946994\n",
      "Iteration 156, loss = 0.16883074\n",
      "Iteration 157, loss = 0.16823052\n",
      "Iteration 158, loss = 0.16760197\n",
      "Iteration 159, loss = 0.16699997\n",
      "Iteration 160, loss = 0.16640110\n",
      "Iteration 161, loss = 0.16574939\n",
      "Iteration 162, loss = 0.16521378\n",
      "Iteration 163, loss = 0.16458067\n",
      "Iteration 164, loss = 0.16399685\n",
      "Iteration 165, loss = 0.16343682\n",
      "Iteration 166, loss = 0.16281123\n",
      "Iteration 167, loss = 0.16222829\n",
      "Iteration 168, loss = 0.16167786\n",
      "Iteration 169, loss = 0.16111545\n",
      "Iteration 170, loss = 0.16054545\n",
      "Iteration 171, loss = 0.16004835\n",
      "Iteration 172, loss = 0.15947466\n",
      "Iteration 173, loss = 0.15887140\n",
      "Iteration 174, loss = 0.15837306\n",
      "Iteration 175, loss = 0.15782080\n",
      "Iteration 176, loss = 0.15728260\n",
      "Iteration 177, loss = 0.15673068\n",
      "Iteration 178, loss = 0.15618162\n",
      "Iteration 179, loss = 0.15568765\n",
      "Iteration 180, loss = 0.15517438\n",
      "Iteration 181, loss = 0.15462126\n",
      "Iteration 182, loss = 0.15413473\n",
      "Iteration 183, loss = 0.15355971\n",
      "Iteration 184, loss = 0.15307188\n",
      "Iteration 185, loss = 0.15253714\n",
      "Iteration 186, loss = 0.15200461\n",
      "Iteration 187, loss = 0.15153929\n",
      "Iteration 188, loss = 0.15107153\n",
      "Iteration 189, loss = 0.15053203\n",
      "Iteration 190, loss = 0.15000100\n",
      "Iteration 191, loss = 0.14959015\n",
      "Iteration 192, loss = 0.14903869\n",
      "Iteration 193, loss = 0.14859992\n",
      "Iteration 194, loss = 0.14808807\n",
      "Iteration 195, loss = 0.14759824\n",
      "Iteration 196, loss = 0.14709937\n",
      "Iteration 197, loss = 0.14669785\n",
      "Iteration 198, loss = 0.14619303\n",
      "Iteration 199, loss = 0.14575093\n",
      "Iteration 200, loss = 0.14525885\n",
      "[CV] END activation=logistic, alpha=0.001, batch_size=50, hidden_layer_sizes=(100,), solver=sgd; total time= 3.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.65275959\n",
      "Iteration 2, loss = 0.25126047\n",
      "Iteration 3, loss = 0.19369517\n",
      "Iteration 4, loss = 0.15664259\n",
      "Iteration 5, loss = 0.13247953\n",
      "Iteration 6, loss = 0.11440630\n",
      "Iteration 7, loss = 0.10070830\n",
      "Iteration 8, loss = 0.08981303\n",
      "Iteration 9, loss = 0.08169911\n",
      "Iteration 10, loss = 0.07396889\n",
      "Iteration 11, loss = 0.06805801\n",
      "Iteration 12, loss = 0.06265193\n",
      "Iteration 13, loss = 0.06271735\n",
      "Iteration 14, loss = 0.05679805\n",
      "Iteration 15, loss = 0.05403261\n",
      "Iteration 16, loss = 0.05337321\n",
      "Iteration 17, loss = 0.05190660\n",
      "Iteration 18, loss = 0.04916619\n",
      "Iteration 19, loss = 0.04908131\n",
      "Iteration 20, loss = 0.04726194\n",
      "Iteration 21, loss = 0.04749037\n",
      "Iteration 22, loss = 0.04883208\n",
      "Iteration 23, loss = 0.04732315\n",
      "Iteration 24, loss = 0.04257744\n",
      "Iteration 25, loss = 0.04768983\n",
      "Iteration 26, loss = 0.04393857\n",
      "Iteration 27, loss = 0.04168654\n",
      "Iteration 28, loss = 0.04633257\n",
      "Iteration 29, loss = 0.04246539\n",
      "Iteration 30, loss = 0.04528920\n",
      "Iteration 31, loss = 0.04178982\n",
      "Iteration 32, loss = 0.03910878\n",
      "Iteration 33, loss = 0.04463437\n",
      "Iteration 34, loss = 0.04125085\n",
      "Iteration 35, loss = 0.04003036\n",
      "Iteration 36, loss = 0.03868736\n",
      "Iteration 37, loss = 0.04351773\n",
      "Iteration 38, loss = 0.03751533\n",
      "Iteration 39, loss = 0.04040083\n",
      "Iteration 40, loss = 0.04003800\n",
      "Iteration 41, loss = 0.04288483\n",
      "Iteration 42, loss = 0.03622923\n",
      "Iteration 43, loss = 0.03762948\n",
      "Iteration 44, loss = 0.04101934\n",
      "Iteration 45, loss = 0.03679742\n",
      "Iteration 46, loss = 0.03928020\n",
      "Iteration 47, loss = 0.04059709\n",
      "Iteration 48, loss = 0.03606757\n",
      "Iteration 49, loss = 0.03797808\n",
      "Iteration 50, loss = 0.04022189\n",
      "Iteration 51, loss = 0.03431540\n",
      "Iteration 52, loss = 0.03765996\n",
      "Iteration 53, loss = 0.03940989\n",
      "Iteration 54, loss = 0.03542023\n",
      "Iteration 55, loss = 0.03207934\n",
      "Iteration 56, loss = 0.04034486\n",
      "Iteration 57, loss = 0.03635506\n",
      "Iteration 58, loss = 0.03386490\n",
      "Iteration 59, loss = 0.03296081\n",
      "Iteration 60, loss = 0.03890029\n",
      "Iteration 61, loss = 0.03478849\n",
      "Iteration 62, loss = 0.03031610\n",
      "Iteration 63, loss = 0.04012518\n",
      "Iteration 64, loss = 0.03665778\n",
      "Iteration 65, loss = 0.03357926\n",
      "Iteration 66, loss = 0.03032639\n",
      "Iteration 67, loss = 0.03917082\n",
      "Iteration 68, loss = 0.03179315\n",
      "Iteration 69, loss = 0.03045471\n",
      "Iteration 70, loss = 0.03725184\n",
      "Iteration 71, loss = 0.03343558\n",
      "Iteration 72, loss = 0.03099262\n",
      "Iteration 73, loss = 0.03560037\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.001, batch_size=50, hidden_layer_sizes=(400, 100), solver=adam; total time= 6.4min\n",
      "Iteration 1, loss = 0.64220613\n",
      "Iteration 2, loss = 0.25048491\n",
      "Iteration 3, loss = 0.18992696\n",
      "Iteration 4, loss = 0.15538196\n",
      "Iteration 5, loss = 0.12892211\n",
      "Iteration 6, loss = 0.11119712\n",
      "Iteration 7, loss = 0.09714031\n",
      "Iteration 8, loss = 0.08646235\n",
      "Iteration 9, loss = 0.07724165\n",
      "Iteration 10, loss = 0.07168089\n",
      "Iteration 11, loss = 0.06588706\n",
      "Iteration 12, loss = 0.06234766\n",
      "Iteration 13, loss = 0.05954731\n",
      "Iteration 14, loss = 0.05497771\n",
      "Iteration 15, loss = 0.05352477\n",
      "Iteration 16, loss = 0.05285003\n",
      "Iteration 17, loss = 0.05015646\n",
      "Iteration 18, loss = 0.04851169\n",
      "Iteration 19, loss = 0.04996532\n",
      "Iteration 20, loss = 0.04816229\n",
      "Iteration 21, loss = 0.04480141\n",
      "Iteration 22, loss = 0.04791046\n",
      "Iteration 23, loss = 0.04529717\n",
      "Iteration 24, loss = 0.04523862\n",
      "Iteration 25, loss = 0.04450374\n",
      "Iteration 26, loss = 0.04522744\n",
      "Iteration 27, loss = 0.04186310\n",
      "Iteration 28, loss = 0.04539761\n",
      "Iteration 29, loss = 0.04422362\n",
      "Iteration 30, loss = 0.04094295\n",
      "Iteration 31, loss = 0.04145213\n",
      "Iteration 32, loss = 0.04228693\n",
      "Iteration 33, loss = 0.04160281\n",
      "Iteration 34, loss = 0.03898967\n",
      "Iteration 35, loss = 0.04332096\n",
      "Iteration 36, loss = 0.03918072\n",
      "Iteration 37, loss = 0.03838494\n",
      "Iteration 38, loss = 0.04284843\n",
      "Iteration 39, loss = 0.03852612\n",
      "Iteration 40, loss = 0.03922895\n",
      "Iteration 41, loss = 0.04231418\n",
      "Iteration 42, loss = 0.03542844\n",
      "Iteration 43, loss = 0.03996439\n",
      "Iteration 44, loss = 0.03736133\n",
      "Iteration 45, loss = 0.03533873\n",
      "Iteration 46, loss = 0.04397705\n",
      "Iteration 47, loss = 0.03668048\n",
      "Iteration 48, loss = 0.03659939\n",
      "Iteration 49, loss = 0.03464038\n",
      "Iteration 50, loss = 0.04163197\n",
      "Iteration 51, loss = 0.03415977\n",
      "Iteration 52, loss = 0.03640136\n",
      "Iteration 53, loss = 0.03945407\n",
      "Iteration 54, loss = 0.03545039\n",
      "Iteration 55, loss = 0.03456789\n",
      "Iteration 56, loss = 0.03420767\n",
      "Iteration 57, loss = 0.03562785\n",
      "Iteration 58, loss = 0.04130958\n",
      "Iteration 59, loss = 0.03235325\n",
      "Iteration 60, loss = 0.03078707\n",
      "Iteration 61, loss = 0.04364283\n",
      "Iteration 62, loss = 0.03331378\n",
      "Iteration 63, loss = 0.03511930\n",
      "Iteration 64, loss = 0.03543175\n",
      "Iteration 65, loss = 0.03323770\n",
      "Iteration 66, loss = 0.03745714\n",
      "Iteration 67, loss = 0.03069484\n",
      "Iteration 68, loss = 0.03380986\n",
      "Iteration 69, loss = 0.03836830\n",
      "Iteration 70, loss = 0.02968094\n",
      "Iteration 71, loss = 0.03180276\n",
      "Iteration 72, loss = 0.04000481\n",
      "Iteration 73, loss = 0.03103783\n",
      "Iteration 74, loss = 0.03283363\n",
      "Iteration 75, loss = 0.03521918\n",
      "Iteration 76, loss = 0.03157387\n",
      "Iteration 77, loss = 0.03083115\n",
      "Iteration 78, loss = 0.03477115\n",
      "Iteration 79, loss = 0.03023016\n",
      "Iteration 80, loss = 0.03283073\n",
      "Iteration 81, loss = 0.03514270\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.001, batch_size=50, hidden_layer_sizes=(400, 100), solver=adam; total time= 7.0min\n",
      "Iteration 1, loss = 0.63867626\n",
      "Iteration 2, loss = 0.24937993\n",
      "Iteration 3, loss = 0.19030676\n",
      "Iteration 4, loss = 0.15441871\n",
      "Iteration 5, loss = 0.13106407\n",
      "Iteration 6, loss = 0.11204498\n",
      "Iteration 7, loss = 0.09813295\n",
      "Iteration 8, loss = 0.08741953\n",
      "Iteration 9, loss = 0.07957497\n",
      "Iteration 10, loss = 0.07164196\n",
      "Iteration 11, loss = 0.06634959\n",
      "Iteration 12, loss = 0.06357701\n",
      "Iteration 13, loss = 0.05908683\n",
      "Iteration 14, loss = 0.05621711\n",
      "Iteration 15, loss = 0.05413665\n",
      "Iteration 16, loss = 0.05065252\n",
      "Iteration 17, loss = 0.05152839\n",
      "Iteration 18, loss = 0.04705224\n",
      "Iteration 19, loss = 0.05095408\n",
      "Iteration 20, loss = 0.04769619\n",
      "Iteration 21, loss = 0.04740679\n",
      "Iteration 22, loss = 0.04994575\n",
      "Iteration 23, loss = 0.04392463\n",
      "Iteration 24, loss = 0.04448484\n",
      "Iteration 25, loss = 0.04533330\n",
      "Iteration 26, loss = 0.04356268\n",
      "Iteration 27, loss = 0.04275172\n",
      "Iteration 28, loss = 0.04344943\n",
      "Iteration 29, loss = 0.04758191\n",
      "Iteration 30, loss = 0.04121417\n",
      "Iteration 31, loss = 0.04184793\n",
      "Iteration 32, loss = 0.04636013\n",
      "Iteration 33, loss = 0.03942170\n",
      "Iteration 34, loss = 0.04346208\n",
      "Iteration 35, loss = 0.03782814\n",
      "Iteration 36, loss = 0.04106013\n",
      "Iteration 37, loss = 0.04462298\n",
      "Iteration 38, loss = 0.04197157\n",
      "Iteration 39, loss = 0.03645310\n",
      "Iteration 40, loss = 0.03871372\n",
      "Iteration 41, loss = 0.04348823\n",
      "Iteration 42, loss = 0.03728817\n",
      "Iteration 43, loss = 0.03997127\n",
      "Iteration 44, loss = 0.03782096\n",
      "Iteration 45, loss = 0.03781486\n",
      "Iteration 46, loss = 0.04017843\n",
      "Iteration 47, loss = 0.03757293\n",
      "Iteration 48, loss = 0.03470708\n",
      "Iteration 49, loss = 0.04310677\n",
      "Iteration 50, loss = 0.03360234\n",
      "Iteration 51, loss = 0.03333239\n",
      "Iteration 52, loss = 0.04344325\n",
      "Iteration 53, loss = 0.03758303\n",
      "Iteration 54, loss = 0.03342342\n",
      "Iteration 55, loss = 0.03405474\n",
      "Iteration 56, loss = 0.04335001\n",
      "Iteration 57, loss = 0.03319554\n",
      "Iteration 58, loss = 0.03167516\n",
      "Iteration 59, loss = 0.04075960\n",
      "Iteration 60, loss = 0.03492448\n",
      "Iteration 61, loss = 0.03340157\n",
      "Iteration 62, loss = 0.03674314\n",
      "Iteration 63, loss = 0.03723703\n",
      "Iteration 64, loss = 0.03172243\n",
      "Iteration 65, loss = 0.02892945\n",
      "Iteration 66, loss = 0.04313236\n",
      "Iteration 67, loss = 0.03216580\n",
      "Iteration 68, loss = 0.03136838\n",
      "Iteration 69, loss = 0.03556937\n",
      "Iteration 70, loss = 0.03170449\n",
      "Iteration 71, loss = 0.03381924\n",
      "Iteration 72, loss = 0.03501235\n",
      "Iteration 73, loss = 0.03096227\n",
      "Iteration 74, loss = 0.03524177\n",
      "Iteration 75, loss = 0.03143930\n",
      "Iteration 76, loss = 0.03589679\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.001, batch_size=50, hidden_layer_sizes=(400, 100), solver=adam; total time= 6.6min\n",
      "Iteration 1, loss = 0.63912173\n",
      "Iteration 2, loss = 0.24787995\n",
      "Iteration 3, loss = 0.19131999\n",
      "Iteration 4, loss = 0.15537753\n",
      "Iteration 5, loss = 0.12953072\n",
      "Iteration 6, loss = 0.11045296\n",
      "Iteration 7, loss = 0.09720516\n",
      "Iteration 8, loss = 0.08724398\n",
      "Iteration 9, loss = 0.07861133\n",
      "Iteration 10, loss = 0.07217082\n",
      "Iteration 11, loss = 0.06645816\n",
      "Iteration 12, loss = 0.06047721\n",
      "Iteration 13, loss = 0.06031211\n",
      "Iteration 14, loss = 0.05666348\n",
      "Iteration 15, loss = 0.05528819\n",
      "Iteration 16, loss = 0.05181530\n",
      "Iteration 17, loss = 0.05001563\n",
      "Iteration 18, loss = 0.05206343\n",
      "Iteration 19, loss = 0.04670869\n",
      "Iteration 20, loss = 0.04898813\n",
      "Iteration 21, loss = 0.04700304\n",
      "Iteration 22, loss = 0.04584428\n",
      "Iteration 23, loss = 0.04499707\n",
      "Iteration 24, loss = 0.04757984\n",
      "Iteration 25, loss = 0.04505266\n",
      "Iteration 26, loss = 0.04100573\n",
      "Iteration 27, loss = 0.04571976\n",
      "Iteration 28, loss = 0.04421240\n",
      "Iteration 29, loss = 0.04415221\n",
      "Iteration 30, loss = 0.04049607\n",
      "Iteration 31, loss = 0.04134329\n",
      "Iteration 32, loss = 0.04385513\n",
      "Iteration 33, loss = 0.04129951\n",
      "Iteration 34, loss = 0.04365474\n",
      "Iteration 35, loss = 0.03921933\n",
      "Iteration 36, loss = 0.03967021\n",
      "Iteration 37, loss = 0.03937332\n",
      "Iteration 38, loss = 0.03911019\n",
      "Iteration 39, loss = 0.04206145\n",
      "Iteration 40, loss = 0.03801234\n",
      "Iteration 41, loss = 0.04112744\n",
      "Iteration 42, loss = 0.03736562\n",
      "Iteration 43, loss = 0.03608397\n",
      "Iteration 44, loss = 0.03671330\n",
      "Iteration 45, loss = 0.03992801\n",
      "Iteration 46, loss = 0.03739695\n",
      "Iteration 47, loss = 0.03383074\n",
      "Iteration 48, loss = 0.04191086\n",
      "Iteration 49, loss = 0.03735599\n",
      "Iteration 50, loss = 0.03781930\n",
      "Iteration 51, loss = 0.03786857\n",
      "Iteration 52, loss = 0.03233002\n",
      "Iteration 53, loss = 0.04291501\n",
      "Iteration 54, loss = 0.03340003\n",
      "Iteration 55, loss = 0.03311161\n",
      "Iteration 56, loss = 0.03891605\n",
      "Iteration 57, loss = 0.03470148\n",
      "Iteration 58, loss = 0.03485689\n",
      "Iteration 59, loss = 0.03506701\n",
      "Iteration 60, loss = 0.03280822\n",
      "Iteration 61, loss = 0.03706602\n",
      "Iteration 62, loss = 0.03526338\n",
      "Iteration 63, loss = 0.03338984\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.001, batch_size=50, hidden_layer_sizes=(400, 100), solver=adam; total time= 5.4min\n",
      "Iteration 1, loss = 0.64779881\n",
      "Iteration 2, loss = 0.25157970\n",
      "Iteration 3, loss = 0.19335481\n",
      "Iteration 4, loss = 0.15673907\n",
      "Iteration 5, loss = 0.13188566\n",
      "Iteration 6, loss = 0.11380928\n",
      "Iteration 7, loss = 0.10215541\n",
      "Iteration 8, loss = 0.09077526\n",
      "Iteration 9, loss = 0.08093428\n",
      "Iteration 10, loss = 0.07475208\n",
      "Iteration 11, loss = 0.06834714\n",
      "Iteration 12, loss = 0.06424919\n",
      "Iteration 13, loss = 0.05963720\n",
      "Iteration 14, loss = 0.05823783\n",
      "Iteration 15, loss = 0.05408241\n",
      "Iteration 16, loss = 0.05262311\n",
      "Iteration 17, loss = 0.05264836\n",
      "Iteration 18, loss = 0.05028116\n",
      "Iteration 19, loss = 0.05116817\n",
      "Iteration 20, loss = 0.04821189\n",
      "Iteration 21, loss = 0.04822292\n",
      "Iteration 22, loss = 0.04705040\n",
      "Iteration 23, loss = 0.04484591\n",
      "Iteration 24, loss = 0.04523080\n",
      "Iteration 25, loss = 0.04486662\n",
      "Iteration 26, loss = 0.04832896\n",
      "Iteration 27, loss = 0.04153027\n",
      "Iteration 28, loss = 0.03962936\n",
      "Iteration 29, loss = 0.04781985\n",
      "Iteration 30, loss = 0.04043772\n",
      "Iteration 31, loss = 0.04505058\n",
      "Iteration 32, loss = 0.04267914\n",
      "Iteration 33, loss = 0.03782681\n",
      "Iteration 34, loss = 0.04648381\n",
      "Iteration 35, loss = 0.03996699\n",
      "Iteration 36, loss = 0.04411034\n",
      "Iteration 37, loss = 0.04010328\n",
      "Iteration 38, loss = 0.03932561\n",
      "Iteration 39, loss = 0.04089035\n",
      "Iteration 40, loss = 0.03986355\n",
      "Iteration 41, loss = 0.04285570\n",
      "Iteration 42, loss = 0.03720190\n",
      "Iteration 43, loss = 0.03791276\n",
      "Iteration 44, loss = 0.03985267\n",
      "Iteration 45, loss = 0.04024836\n",
      "Iteration 46, loss = 0.04324160\n",
      "Iteration 47, loss = 0.03728232\n",
      "Iteration 48, loss = 0.03902303\n",
      "Iteration 49, loss = 0.03668246\n",
      "Iteration 50, loss = 0.04051779\n",
      "Iteration 51, loss = 0.03872957\n",
      "Iteration 52, loss = 0.04008478\n",
      "Iteration 53, loss = 0.03285345\n",
      "Iteration 54, loss = 0.03341199\n",
      "Iteration 55, loss = 0.04467813\n",
      "Iteration 56, loss = 0.03419708\n",
      "Iteration 57, loss = 0.03815867\n",
      "Iteration 58, loss = 0.03211124\n",
      "Iteration 59, loss = 0.03986004\n",
      "Iteration 60, loss = 0.03570655\n",
      "Iteration 61, loss = 0.03696507\n",
      "Iteration 62, loss = 0.03738206\n",
      "Iteration 63, loss = 0.03064899\n",
      "Iteration 64, loss = 0.03830681\n",
      "Iteration 65, loss = 0.03726036\n",
      "Iteration 66, loss = 0.03171174\n",
      "Iteration 67, loss = 0.03032028\n",
      "Iteration 68, loss = 0.04414833\n",
      "Iteration 69, loss = 0.03414578\n",
      "Iteration 70, loss = 0.03030290\n",
      "Iteration 71, loss = 0.03637834\n",
      "Iteration 72, loss = 0.04000989\n",
      "Iteration 73, loss = 0.02967781\n",
      "Iteration 74, loss = 0.02833104\n",
      "Iteration 75, loss = 0.03973096\n",
      "Iteration 76, loss = 0.03214169\n",
      "Iteration 77, loss = 0.03284519\n",
      "Iteration 78, loss = 0.03387361\n",
      "Iteration 79, loss = 0.03311622\n",
      "Iteration 80, loss = 0.03249861\n",
      "Iteration 81, loss = 0.03123076\n",
      "Iteration 82, loss = 0.03411716\n",
      "Iteration 83, loss = 0.03696766\n",
      "Iteration 84, loss = 0.03031441\n",
      "Iteration 85, loss = 0.02997050\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.001, batch_size=50, hidden_layer_sizes=(400, 100), solver=adam; total time= 7.4min\n",
      "Iteration 1, loss = 2.30094357\n",
      "Iteration 2, loss = 2.28369017\n",
      "Iteration 3, loss = 2.26387554\n",
      "Iteration 4, loss = 2.23291972\n",
      "Iteration 5, loss = 2.17748316\n",
      "Iteration 6, loss = 2.07336314\n",
      "Iteration 7, loss = 1.89544371\n",
      "Iteration 8, loss = 1.66115994\n",
      "Iteration 9, loss = 1.41367925\n",
      "Iteration 10, loss = 1.20244435\n",
      "Iteration 11, loss = 1.04845511\n",
      "Iteration 12, loss = 0.93907523\n",
      "Iteration 13, loss = 0.85699272\n",
      "Iteration 14, loss = 0.79131209\n",
      "Iteration 15, loss = 0.73692256\n",
      "Iteration 16, loss = 0.69020881\n",
      "Iteration 17, loss = 0.65035335\n",
      "Iteration 18, loss = 0.61581386\n",
      "Iteration 19, loss = 0.58613156\n",
      "Iteration 20, loss = 0.56088764\n",
      "Iteration 21, loss = 0.53871959\n",
      "Iteration 22, loss = 0.51943232\n",
      "Iteration 23, loss = 0.50258129\n",
      "Iteration 24, loss = 0.48762602\n",
      "Iteration 25, loss = 0.47437038\n",
      "Iteration 26, loss = 0.46239350\n",
      "Iteration 27, loss = 0.45152212\n",
      "Iteration 28, loss = 0.44204267\n",
      "Iteration 29, loss = 0.43319231\n",
      "Iteration 30, loss = 0.42497736\n",
      "Iteration 31, loss = 0.41779206\n",
      "Iteration 32, loss = 0.41095479\n",
      "Iteration 33, loss = 0.40445566\n",
      "Iteration 34, loss = 0.39849692\n",
      "Iteration 35, loss = 0.39322790\n",
      "Iteration 36, loss = 0.38814649\n",
      "Iteration 37, loss = 0.38316002\n",
      "Iteration 38, loss = 0.37860780\n",
      "Iteration 39, loss = 0.37442669\n",
      "Iteration 40, loss = 0.37045922\n",
      "Iteration 41, loss = 0.36638907\n",
      "Iteration 42, loss = 0.36289092\n",
      "Iteration 43, loss = 0.35905847\n",
      "Iteration 44, loss = 0.35583655\n",
      "Iteration 45, loss = 0.35277096\n",
      "Iteration 46, loss = 0.34960838\n",
      "Iteration 47, loss = 0.34626315\n",
      "Iteration 48, loss = 0.34384511\n",
      "Iteration 49, loss = 0.34098192\n",
      "Iteration 50, loss = 0.33834398\n",
      "Iteration 51, loss = 0.33577032\n",
      "Iteration 52, loss = 0.33337138\n",
      "Iteration 53, loss = 0.33074564\n",
      "Iteration 54, loss = 0.32859932\n",
      "Iteration 55, loss = 0.32633621\n",
      "Iteration 56, loss = 0.32415869\n",
      "Iteration 57, loss = 0.32178226\n",
      "Iteration 58, loss = 0.31977141\n",
      "Iteration 59, loss = 0.31756887\n",
      "Iteration 60, loss = 0.31576733\n",
      "Iteration 61, loss = 0.31376249\n",
      "Iteration 62, loss = 0.31177758\n",
      "Iteration 63, loss = 0.30979379\n",
      "Iteration 64, loss = 0.30791467\n",
      "Iteration 65, loss = 0.30633444\n",
      "Iteration 66, loss = 0.30452110\n",
      "Iteration 67, loss = 0.30265343\n",
      "Iteration 68, loss = 0.30108539\n",
      "Iteration 69, loss = 0.29943296\n",
      "Iteration 70, loss = 0.29782167\n",
      "Iteration 71, loss = 0.29611959\n",
      "Iteration 72, loss = 0.29461270\n",
      "Iteration 73, loss = 0.29309432\n",
      "Iteration 74, loss = 0.29160851\n",
      "Iteration 75, loss = 0.29004652\n",
      "Iteration 76, loss = 0.28861002\n",
      "Iteration 77, loss = 0.28722525\n",
      "Iteration 78, loss = 0.28576574\n",
      "Iteration 79, loss = 0.28439558\n",
      "Iteration 80, loss = 0.28302768\n",
      "Iteration 81, loss = 0.28154323\n",
      "Iteration 82, loss = 0.28030785\n",
      "Iteration 83, loss = 0.27897052\n",
      "Iteration 84, loss = 0.27778346\n",
      "Iteration 85, loss = 0.27647169\n",
      "Iteration 86, loss = 0.27504433\n",
      "Iteration 87, loss = 0.27420252\n",
      "Iteration 88, loss = 0.27267581\n",
      "Iteration 89, loss = 0.27136369\n",
      "Iteration 90, loss = 0.27041971\n",
      "Iteration 91, loss = 0.26912908\n",
      "Iteration 92, loss = 0.26805614\n",
      "Iteration 93, loss = 0.26685370\n",
      "Iteration 94, loss = 0.26562572\n",
      "Iteration 95, loss = 0.26459076\n",
      "Iteration 96, loss = 0.26342375\n",
      "Iteration 97, loss = 0.26241532\n",
      "Iteration 98, loss = 0.26126160\n",
      "Iteration 99, loss = 0.26033074\n",
      "Iteration 100, loss = 0.25914307\n",
      "Iteration 101, loss = 0.25802239\n",
      "Iteration 102, loss = 0.25704844\n",
      "Iteration 103, loss = 0.25601393\n",
      "Iteration 104, loss = 0.25502696\n",
      "Iteration 105, loss = 0.25392881\n",
      "Iteration 106, loss = 0.25289804\n",
      "Iteration 107, loss = 0.25194717\n",
      "Iteration 108, loss = 0.25103803\n",
      "Iteration 109, loss = 0.24999082\n",
      "Iteration 110, loss = 0.24899677\n",
      "Iteration 111, loss = 0.24801781\n",
      "Iteration 112, loss = 0.24708514\n",
      "Iteration 113, loss = 0.24617250\n",
      "Iteration 114, loss = 0.24528548\n",
      "Iteration 115, loss = 0.24431448\n",
      "Iteration 116, loss = 0.24325759\n",
      "Iteration 117, loss = 0.24256423\n",
      "Iteration 118, loss = 0.24137168\n",
      "Iteration 119, loss = 0.24048993\n",
      "Iteration 120, loss = 0.23964885\n",
      "Iteration 121, loss = 0.23867885\n",
      "Iteration 122, loss = 0.23783537\n",
      "Iteration 123, loss = 0.23698647\n",
      "Iteration 124, loss = 0.23612682\n",
      "Iteration 125, loss = 0.23516649\n",
      "Iteration 126, loss = 0.23437004\n",
      "Iteration 127, loss = 0.23342739\n",
      "Iteration 128, loss = 0.23265984\n",
      "Iteration 129, loss = 0.23171867\n",
      "Iteration 130, loss = 0.23086468\n",
      "Iteration 131, loss = 0.23020681\n",
      "Iteration 132, loss = 0.22925529\n",
      "Iteration 133, loss = 0.22846851\n",
      "Iteration 134, loss = 0.22748395\n",
      "Iteration 135, loss = 0.22674126\n",
      "Iteration 136, loss = 0.22600973\n",
      "Iteration 137, loss = 0.22517989\n",
      "Iteration 138, loss = 0.22436691\n",
      "Iteration 139, loss = 0.22338041\n",
      "Iteration 140, loss = 0.22265190\n",
      "Iteration 141, loss = 0.22184220\n",
      "Iteration 142, loss = 0.22111411\n",
      "Iteration 143, loss = 0.22009018\n",
      "Iteration 144, loss = 0.21929628\n",
      "Iteration 145, loss = 0.21869498\n",
      "Iteration 146, loss = 0.21788401\n",
      "Iteration 147, loss = 0.21711648\n",
      "Iteration 148, loss = 0.21638336\n",
      "Iteration 149, loss = 0.21544566\n",
      "Iteration 150, loss = 0.21477504\n",
      "Iteration 151, loss = 0.21397803\n",
      "Iteration 152, loss = 0.21334087\n",
      "Iteration 153, loss = 0.21241232\n",
      "Iteration 154, loss = 0.21165226\n",
      "Iteration 155, loss = 0.21084063\n",
      "Iteration 156, loss = 0.21012895\n",
      "Iteration 157, loss = 0.20938370\n",
      "Iteration 158, loss = 0.20859055\n",
      "Iteration 159, loss = 0.20783559\n",
      "Iteration 160, loss = 0.20701956\n",
      "Iteration 161, loss = 0.20662715\n",
      "Iteration 162, loss = 0.20586703\n",
      "Iteration 163, loss = 0.20493416\n",
      "Iteration 164, loss = 0.20435831\n",
      "Iteration 165, loss = 0.20341765\n",
      "Iteration 166, loss = 0.20298411\n",
      "Iteration 167, loss = 0.20221084\n",
      "Iteration 168, loss = 0.20155962\n",
      "Iteration 169, loss = 0.20070749\n",
      "Iteration 170, loss = 0.20028108\n",
      "Iteration 171, loss = 0.19944593\n",
      "Iteration 172, loss = 0.19889915\n",
      "Iteration 173, loss = 0.19798711\n",
      "Iteration 174, loss = 0.19737837\n",
      "Iteration 175, loss = 0.19665089\n",
      "Iteration 176, loss = 0.19597900\n",
      "Iteration 177, loss = 0.19531400\n",
      "Iteration 178, loss = 0.19457015\n",
      "Iteration 179, loss = 0.19402302\n",
      "Iteration 180, loss = 0.19321496\n",
      "Iteration 181, loss = 0.19264733\n",
      "Iteration 182, loss = 0.19186248\n",
      "Iteration 183, loss = 0.19120976\n",
      "Iteration 184, loss = 0.19042943\n",
      "Iteration 185, loss = 0.18991516\n",
      "Iteration 186, loss = 0.18932700\n",
      "Iteration 187, loss = 0.18867541\n",
      "Iteration 188, loss = 0.18799382\n",
      "Iteration 189, loss = 0.18750485\n",
      "Iteration 190, loss = 0.18674100\n",
      "Iteration 191, loss = 0.18623322\n",
      "Iteration 192, loss = 0.18559622\n",
      "Iteration 193, loss = 0.18496770\n",
      "Iteration 194, loss = 0.18427260\n",
      "Iteration 195, loss = 0.18361189\n",
      "Iteration 196, loss = 0.18298448\n",
      "Iteration 197, loss = 0.18252259\n",
      "Iteration 198, loss = 0.18193608\n",
      "Iteration 199, loss = 0.18125654\n",
      "Iteration 200, loss = 0.18047700\n",
      "[CV] END activation=logistic, alpha=0.001, batch_size=50, hidden_layer_sizes=(400, 100), solver=sgd; total time=10.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.30047392\n",
      "Iteration 2, loss = 2.28288957\n",
      "Iteration 3, loss = 2.26286765\n",
      "Iteration 4, loss = 2.23072912\n",
      "Iteration 5, loss = 2.17172883\n",
      "Iteration 6, loss = 2.05655124\n",
      "Iteration 7, loss = 1.85205852\n",
      "Iteration 8, loss = 1.60091311\n",
      "Iteration 9, loss = 1.38629517\n",
      "Iteration 10, loss = 1.21727941\n",
      "Iteration 11, loss = 1.07949693\n",
      "Iteration 12, loss = 0.96974053\n",
      "Iteration 13, loss = 0.88189222\n",
      "Iteration 14, loss = 0.80993820\n",
      "Iteration 15, loss = 0.74882920\n",
      "Iteration 16, loss = 0.69748624\n",
      "Iteration 17, loss = 0.65381560\n",
      "Iteration 18, loss = 0.61646435\n",
      "Iteration 19, loss = 0.58512178\n",
      "Iteration 20, loss = 0.55826503\n",
      "Iteration 21, loss = 0.53484600\n",
      "Iteration 22, loss = 0.51464340\n",
      "Iteration 23, loss = 0.49686068\n",
      "Iteration 24, loss = 0.48102558\n",
      "Iteration 25, loss = 0.46661598\n",
      "Iteration 26, loss = 0.45400256\n",
      "Iteration 27, loss = 0.44261876\n",
      "Iteration 28, loss = 0.43244175\n",
      "Iteration 29, loss = 0.42322196\n",
      "Iteration 30, loss = 0.41475205\n",
      "Iteration 31, loss = 0.40693441\n",
      "Iteration 32, loss = 0.40005089\n",
      "Iteration 33, loss = 0.39361362\n",
      "Iteration 34, loss = 0.38768922\n",
      "Iteration 35, loss = 0.38206897\n",
      "Iteration 36, loss = 0.37714982\n",
      "Iteration 37, loss = 0.37237485\n",
      "Iteration 38, loss = 0.36816322\n",
      "Iteration 39, loss = 0.36413728\n",
      "Iteration 40, loss = 0.36009760\n",
      "Iteration 41, loss = 0.35661678\n",
      "Iteration 42, loss = 0.35303860\n",
      "Iteration 43, loss = 0.34973667\n",
      "Iteration 44, loss = 0.34691016\n",
      "Iteration 45, loss = 0.34395652\n",
      "Iteration 46, loss = 0.34106941\n",
      "Iteration 47, loss = 0.33839452\n",
      "Iteration 48, loss = 0.33571647\n",
      "Iteration 49, loss = 0.33333481\n",
      "Iteration 50, loss = 0.33098593\n",
      "Iteration 51, loss = 0.32867360\n",
      "Iteration 52, loss = 0.32658058\n",
      "Iteration 53, loss = 0.32428761\n",
      "Iteration 54, loss = 0.32216647\n",
      "Iteration 55, loss = 0.32022358\n",
      "Iteration 56, loss = 0.31830735\n",
      "Iteration 57, loss = 0.31631134\n",
      "Iteration 58, loss = 0.31449610\n",
      "Iteration 59, loss = 0.31260079\n",
      "Iteration 60, loss = 0.31103502\n",
      "Iteration 61, loss = 0.30919599\n",
      "Iteration 62, loss = 0.30751314\n",
      "Iteration 63, loss = 0.30605275\n",
      "Iteration 64, loss = 0.30419622\n",
      "Iteration 65, loss = 0.30270597\n",
      "Iteration 66, loss = 0.30103991\n",
      "Iteration 67, loss = 0.29958326\n",
      "Iteration 68, loss = 0.29816334\n",
      "Iteration 69, loss = 0.29673911\n",
      "Iteration 70, loss = 0.29521440\n",
      "Iteration 71, loss = 0.29391955\n",
      "Iteration 72, loss = 0.29242417\n",
      "Iteration 73, loss = 0.29118440\n",
      "Iteration 74, loss = 0.28974084\n",
      "Iteration 75, loss = 0.28826138\n",
      "Iteration 76, loss = 0.28717409\n",
      "Iteration 77, loss = 0.28578283\n",
      "Iteration 78, loss = 0.28466692\n",
      "Iteration 79, loss = 0.28342025\n",
      "Iteration 80, loss = 0.28206962\n",
      "Iteration 81, loss = 0.28072493\n",
      "Iteration 82, loss = 0.27963173\n",
      "Iteration 83, loss = 0.27856080\n",
      "Iteration 84, loss = 0.27731115\n",
      "Iteration 85, loss = 0.27622193\n",
      "Iteration 86, loss = 0.27504278\n",
      "Iteration 87, loss = 0.27380835\n",
      "Iteration 88, loss = 0.27270283\n",
      "Iteration 89, loss = 0.27160180\n",
      "Iteration 90, loss = 0.27061189\n",
      "Iteration 91, loss = 0.26934153\n",
      "Iteration 92, loss = 0.26840836\n",
      "Iteration 93, loss = 0.26738388\n",
      "Iteration 94, loss = 0.26634850\n",
      "Iteration 95, loss = 0.26517041\n",
      "Iteration 96, loss = 0.26432779\n",
      "Iteration 97, loss = 0.26319876\n",
      "Iteration 98, loss = 0.26216144\n",
      "Iteration 99, loss = 0.26117701\n",
      "Iteration 100, loss = 0.26027228\n",
      "Iteration 101, loss = 0.25927709\n",
      "Iteration 102, loss = 0.25833990\n",
      "Iteration 103, loss = 0.25719253\n",
      "Iteration 104, loss = 0.25628612\n",
      "Iteration 105, loss = 0.25546628\n",
      "Iteration 106, loss = 0.25435944\n",
      "Iteration 107, loss = 0.25346795\n",
      "Iteration 108, loss = 0.25259043\n",
      "Iteration 109, loss = 0.25164152\n",
      "Iteration 110, loss = 0.25077137\n",
      "Iteration 111, loss = 0.24974149\n",
      "Iteration 112, loss = 0.24888486\n",
      "Iteration 113, loss = 0.24793083\n",
      "Iteration 114, loss = 0.24716842\n",
      "Iteration 115, loss = 0.24609929\n",
      "Iteration 116, loss = 0.24524810\n",
      "Iteration 117, loss = 0.24437349\n",
      "Iteration 118, loss = 0.24352186\n",
      "Iteration 119, loss = 0.24261542\n",
      "Iteration 120, loss = 0.24160799\n",
      "Iteration 121, loss = 0.24070545\n",
      "Iteration 122, loss = 0.23996719\n",
      "Iteration 123, loss = 0.23907291\n",
      "Iteration 124, loss = 0.23832846\n",
      "Iteration 125, loss = 0.23733607\n",
      "Iteration 126, loss = 0.23678285\n",
      "Iteration 127, loss = 0.23581817\n",
      "Iteration 128, loss = 0.23468370\n",
      "Iteration 129, loss = 0.23402682\n",
      "Iteration 130, loss = 0.23332087\n",
      "Iteration 131, loss = 0.23228676\n",
      "Iteration 132, loss = 0.23167841\n",
      "Iteration 133, loss = 0.23083037\n",
      "Iteration 134, loss = 0.22987137\n",
      "Iteration 135, loss = 0.22917397\n",
      "Iteration 136, loss = 0.22836447\n",
      "Iteration 137, loss = 0.22742013\n",
      "Iteration 138, loss = 0.22670647\n",
      "Iteration 139, loss = 0.22580338\n",
      "Iteration 140, loss = 0.22511101\n",
      "Iteration 141, loss = 0.22441869\n",
      "Iteration 142, loss = 0.22353737\n",
      "Iteration 143, loss = 0.22267123\n",
      "Iteration 144, loss = 0.22193906\n",
      "Iteration 145, loss = 0.22135629\n",
      "Iteration 146, loss = 0.22030273\n",
      "Iteration 147, loss = 0.21955806\n",
      "Iteration 148, loss = 0.21879482\n",
      "Iteration 149, loss = 0.21805366\n",
      "Iteration 150, loss = 0.21720466\n",
      "Iteration 151, loss = 0.21652734\n",
      "Iteration 152, loss = 0.21570507\n",
      "Iteration 153, loss = 0.21480404\n",
      "Iteration 154, loss = 0.21423360\n",
      "Iteration 155, loss = 0.21346450\n",
      "Iteration 156, loss = 0.21272199\n",
      "Iteration 157, loss = 0.21209339\n",
      "Iteration 158, loss = 0.21126251\n",
      "Iteration 159, loss = 0.21042631\n",
      "Iteration 160, loss = 0.20960694\n",
      "Iteration 161, loss = 0.20896085\n",
      "Iteration 162, loss = 0.20817706\n",
      "Iteration 163, loss = 0.20754754\n",
      "Iteration 164, loss = 0.20674945\n",
      "Iteration 165, loss = 0.20605951\n",
      "Iteration 166, loss = 0.20541641\n",
      "Iteration 167, loss = 0.20460797\n",
      "Iteration 168, loss = 0.20385850\n",
      "Iteration 169, loss = 0.20301627\n",
      "Iteration 170, loss = 0.20254125\n",
      "Iteration 171, loss = 0.20168691\n",
      "Iteration 172, loss = 0.20059046\n",
      "Iteration 173, loss = 0.20029534\n",
      "Iteration 174, loss = 0.19963056\n",
      "Iteration 175, loss = 0.19874066\n",
      "Iteration 176, loss = 0.19808537\n",
      "Iteration 177, loss = 0.19729986\n",
      "Iteration 178, loss = 0.19685896\n",
      "Iteration 179, loss = 0.19618379\n",
      "Iteration 180, loss = 0.19552802\n",
      "Iteration 181, loss = 0.19469752\n",
      "Iteration 182, loss = 0.19395030\n",
      "Iteration 183, loss = 0.19327434\n",
      "Iteration 184, loss = 0.19275027\n",
      "Iteration 185, loss = 0.19205768\n",
      "Iteration 186, loss = 0.19135687\n",
      "Iteration 187, loss = 0.19075435\n",
      "Iteration 188, loss = 0.19010867\n",
      "Iteration 189, loss = 0.18938935\n",
      "Iteration 190, loss = 0.18872784\n",
      "Iteration 191, loss = 0.18801222\n",
      "Iteration 192, loss = 0.18744154\n",
      "Iteration 193, loss = 0.18665833\n",
      "Iteration 194, loss = 0.18611380\n",
      "Iteration 195, loss = 0.18553417\n",
      "Iteration 196, loss = 0.18488287\n",
      "Iteration 197, loss = 0.18423184\n",
      "Iteration 198, loss = 0.18364389\n",
      "Iteration 199, loss = 0.18293003\n",
      "Iteration 200, loss = 0.18225176\n",
      "[CV] END activation=logistic, alpha=0.001, batch_size=50, hidden_layer_sizes=(400, 100), solver=sgd; total time=10.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.30248151\n",
      "Iteration 2, loss = 2.28609365\n",
      "Iteration 3, loss = 2.26878306\n",
      "Iteration 4, loss = 2.24161765\n",
      "Iteration 5, loss = 2.19466687\n",
      "Iteration 6, loss = 2.10476905\n",
      "Iteration 7, loss = 1.93986393\n",
      "Iteration 8, loss = 1.70479253\n",
      "Iteration 9, loss = 1.46564608\n",
      "Iteration 10, loss = 1.26634454\n",
      "Iteration 11, loss = 1.10809443\n",
      "Iteration 12, loss = 0.98441605\n",
      "Iteration 13, loss = 0.88890505\n",
      "Iteration 14, loss = 0.81373072\n",
      "Iteration 15, loss = 0.75296736\n",
      "Iteration 16, loss = 0.70277536\n",
      "Iteration 17, loss = 0.65980782\n",
      "Iteration 18, loss = 0.62317260\n",
      "Iteration 19, loss = 0.59164968\n",
      "Iteration 20, loss = 0.56421312\n",
      "Iteration 21, loss = 0.54020886\n",
      "Iteration 22, loss = 0.51921305\n",
      "Iteration 23, loss = 0.50085511\n",
      "Iteration 24, loss = 0.48480469\n",
      "Iteration 25, loss = 0.47044773\n",
      "Iteration 26, loss = 0.45763352\n",
      "Iteration 27, loss = 0.44601072\n",
      "Iteration 28, loss = 0.43584314\n",
      "Iteration 29, loss = 0.42668407\n",
      "Iteration 30, loss = 0.41845748\n",
      "Iteration 31, loss = 0.41085127\n",
      "Iteration 32, loss = 0.40382132\n",
      "Iteration 33, loss = 0.39748244\n",
      "Iteration 34, loss = 0.39176313\n",
      "Iteration 35, loss = 0.38648067\n",
      "Iteration 36, loss = 0.38129960\n",
      "Iteration 37, loss = 0.37690850\n",
      "Iteration 38, loss = 0.37259891\n",
      "Iteration 39, loss = 0.36846167\n",
      "Iteration 40, loss = 0.36484650\n",
      "Iteration 41, loss = 0.36110001\n",
      "Iteration 42, loss = 0.35789577\n",
      "Iteration 43, loss = 0.35441215\n",
      "Iteration 44, loss = 0.35172876\n",
      "Iteration 45, loss = 0.34853886\n",
      "Iteration 46, loss = 0.34578779\n",
      "Iteration 47, loss = 0.34321766\n",
      "Iteration 48, loss = 0.34045446\n",
      "Iteration 49, loss = 0.33809811\n",
      "Iteration 50, loss = 0.33555257\n",
      "Iteration 51, loss = 0.33332422\n",
      "Iteration 52, loss = 0.33118204\n",
      "Iteration 53, loss = 0.32895145\n",
      "Iteration 54, loss = 0.32663699\n",
      "Iteration 55, loss = 0.32494735\n",
      "Iteration 56, loss = 0.32272892\n",
      "Iteration 57, loss = 0.32085659\n",
      "Iteration 58, loss = 0.31896011\n",
      "Iteration 59, loss = 0.31712846\n",
      "Iteration 60, loss = 0.31526072\n",
      "Iteration 61, loss = 0.31344320\n",
      "Iteration 62, loss = 0.31190990\n",
      "Iteration 63, loss = 0.31024219\n",
      "Iteration 64, loss = 0.30854808\n",
      "Iteration 65, loss = 0.30699779\n",
      "Iteration 66, loss = 0.30526490\n",
      "Iteration 67, loss = 0.30378519\n",
      "Iteration 68, loss = 0.30219101\n",
      "Iteration 69, loss = 0.30072710\n",
      "Iteration 70, loss = 0.29918263\n",
      "Iteration 71, loss = 0.29769250\n",
      "Iteration 72, loss = 0.29631496\n",
      "Iteration 73, loss = 0.29476853\n",
      "Iteration 74, loss = 0.29360607\n",
      "Iteration 75, loss = 0.29212905\n",
      "Iteration 76, loss = 0.29059419\n",
      "Iteration 77, loss = 0.28947473\n",
      "Iteration 78, loss = 0.28803207\n",
      "Iteration 79, loss = 0.28649464\n",
      "Iteration 80, loss = 0.28541000\n",
      "Iteration 81, loss = 0.28423999\n",
      "Iteration 82, loss = 0.28289135\n",
      "Iteration 83, loss = 0.28158190\n",
      "Iteration 84, loss = 0.28046625\n",
      "Iteration 85, loss = 0.27909220\n",
      "Iteration 86, loss = 0.27797809\n",
      "Iteration 87, loss = 0.27701949\n",
      "Iteration 88, loss = 0.27554366\n",
      "Iteration 89, loss = 0.27428672\n",
      "Iteration 90, loss = 0.27322816\n",
      "Iteration 91, loss = 0.27204191\n",
      "Iteration 92, loss = 0.27082285\n",
      "Iteration 93, loss = 0.26992434\n",
      "Iteration 94, loss = 0.26879722\n",
      "Iteration 95, loss = 0.26758200\n",
      "Iteration 96, loss = 0.26657921\n",
      "Iteration 97, loss = 0.26536395\n",
      "Iteration 98, loss = 0.26422048\n",
      "Iteration 99, loss = 0.26336255\n",
      "Iteration 100, loss = 0.26216443\n",
      "Iteration 101, loss = 0.26107616\n",
      "Iteration 102, loss = 0.26003086\n",
      "Iteration 103, loss = 0.25899068\n",
      "Iteration 104, loss = 0.25811231\n",
      "Iteration 105, loss = 0.25707864\n",
      "Iteration 106, loss = 0.25591517\n",
      "Iteration 107, loss = 0.25497063\n",
      "Iteration 108, loss = 0.25390065\n",
      "Iteration 109, loss = 0.25300247\n",
      "Iteration 110, loss = 0.25191854\n",
      "Iteration 111, loss = 0.25104186\n",
      "Iteration 112, loss = 0.24994997\n",
      "Iteration 113, loss = 0.24893242\n",
      "Iteration 114, loss = 0.24817228\n",
      "Iteration 115, loss = 0.24712120\n",
      "Iteration 116, loss = 0.24601078\n",
      "Iteration 117, loss = 0.24505737\n",
      "Iteration 118, loss = 0.24430426\n",
      "Iteration 119, loss = 0.24329901\n",
      "Iteration 120, loss = 0.24231550\n",
      "Iteration 121, loss = 0.24134309\n",
      "Iteration 122, loss = 0.24038698\n",
      "Iteration 123, loss = 0.23953127\n",
      "Iteration 124, loss = 0.23874896\n",
      "Iteration 125, loss = 0.23768400\n",
      "Iteration 126, loss = 0.23683203\n",
      "Iteration 127, loss = 0.23591182\n",
      "Iteration 128, loss = 0.23497931\n",
      "Iteration 129, loss = 0.23407075\n",
      "Iteration 130, loss = 0.23317440\n",
      "Iteration 131, loss = 0.23232275\n",
      "Iteration 132, loss = 0.23137126\n",
      "Iteration 133, loss = 0.23055491\n",
      "Iteration 134, loss = 0.22966546\n",
      "Iteration 135, loss = 0.22864214\n",
      "Iteration 136, loss = 0.22791276\n",
      "Iteration 137, loss = 0.22693976\n",
      "Iteration 138, loss = 0.22620231\n",
      "Iteration 139, loss = 0.22535842\n",
      "Iteration 140, loss = 0.22449307\n",
      "Iteration 141, loss = 0.22343687\n",
      "Iteration 142, loss = 0.22266607\n",
      "Iteration 143, loss = 0.22198937\n",
      "Iteration 144, loss = 0.22095182\n",
      "Iteration 145, loss = 0.22027785\n",
      "Iteration 146, loss = 0.21950872\n",
      "Iteration 147, loss = 0.21850293\n",
      "Iteration 148, loss = 0.21780549\n",
      "Iteration 149, loss = 0.21688415\n",
      "Iteration 150, loss = 0.21602409\n",
      "Iteration 151, loss = 0.21519818\n",
      "Iteration 152, loss = 0.21434598\n",
      "Iteration 153, loss = 0.21364369\n",
      "Iteration 154, loss = 0.21278463\n",
      "Iteration 155, loss = 0.21196220\n",
      "Iteration 156, loss = 0.21129905\n",
      "Iteration 157, loss = 0.21038589\n",
      "Iteration 158, loss = 0.20969167\n",
      "Iteration 159, loss = 0.20882219\n",
      "Iteration 160, loss = 0.20807977\n",
      "Iteration 161, loss = 0.20737169\n",
      "Iteration 162, loss = 0.20669269\n",
      "Iteration 163, loss = 0.20571065\n",
      "Iteration 164, loss = 0.20509824\n",
      "Iteration 165, loss = 0.20422661\n",
      "Iteration 166, loss = 0.20353425\n",
      "Iteration 167, loss = 0.20276690\n",
      "Iteration 168, loss = 0.20198617\n",
      "Iteration 169, loss = 0.20137731\n",
      "Iteration 170, loss = 0.20054550\n",
      "Iteration 171, loss = 0.19985409\n",
      "Iteration 172, loss = 0.19888696\n",
      "Iteration 173, loss = 0.19829940\n",
      "Iteration 174, loss = 0.19757241\n",
      "Iteration 175, loss = 0.19674919\n",
      "Iteration 176, loss = 0.19609036\n",
      "Iteration 177, loss = 0.19548314\n",
      "Iteration 178, loss = 0.19474731\n",
      "Iteration 179, loss = 0.19378656\n",
      "Iteration 180, loss = 0.19344679\n",
      "Iteration 181, loss = 0.19253274\n",
      "Iteration 182, loss = 0.19200295\n",
      "Iteration 183, loss = 0.19123342\n",
      "Iteration 184, loss = 0.19052280\n",
      "Iteration 185, loss = 0.18985331\n",
      "Iteration 186, loss = 0.18909241\n",
      "Iteration 187, loss = 0.18829726\n",
      "Iteration 188, loss = 0.18793751\n",
      "Iteration 189, loss = 0.18712759\n",
      "Iteration 190, loss = 0.18645156\n",
      "Iteration 191, loss = 0.18581193\n",
      "Iteration 192, loss = 0.18514578\n",
      "Iteration 193, loss = 0.18459937\n",
      "Iteration 194, loss = 0.18393584\n",
      "Iteration 195, loss = 0.18316955\n",
      "Iteration 196, loss = 0.18247219\n",
      "Iteration 197, loss = 0.18184125\n",
      "Iteration 198, loss = 0.18129920\n",
      "Iteration 199, loss = 0.18064353\n",
      "Iteration 200, loss = 0.18006487\n",
      "[CV] END activation=logistic, alpha=0.001, batch_size=50, hidden_layer_sizes=(400, 100), solver=sgd; total time=10.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.30144386\n",
      "Iteration 2, loss = 2.28369955\n",
      "Iteration 3, loss = 2.26449591\n",
      "Iteration 4, loss = 2.23482186\n",
      "Iteration 5, loss = 2.18243972\n",
      "Iteration 6, loss = 2.08121428\n",
      "Iteration 7, loss = 1.89434524\n",
      "Iteration 8, loss = 1.63205090\n",
      "Iteration 9, loss = 1.38179274\n",
      "Iteration 10, loss = 1.18834299\n",
      "Iteration 11, loss = 1.04524142\n",
      "Iteration 12, loss = 0.93767760\n",
      "Iteration 13, loss = 0.85341110\n",
      "Iteration 14, loss = 0.78502775\n",
      "Iteration 15, loss = 0.72731825\n",
      "Iteration 16, loss = 0.67897811\n",
      "Iteration 17, loss = 0.63788438\n",
      "Iteration 18, loss = 0.60314206\n",
      "Iteration 19, loss = 0.57376788\n",
      "Iteration 20, loss = 0.54844148\n",
      "Iteration 21, loss = 0.52693123\n",
      "Iteration 22, loss = 0.50807343\n",
      "Iteration 23, loss = 0.49129339\n",
      "Iteration 24, loss = 0.47675417\n",
      "Iteration 25, loss = 0.46401103\n",
      "Iteration 26, loss = 0.45253500\n",
      "Iteration 27, loss = 0.44225645\n",
      "Iteration 28, loss = 0.43280265\n",
      "Iteration 29, loss = 0.42451167\n",
      "Iteration 30, loss = 0.41683671\n",
      "Iteration 31, loss = 0.40984981\n",
      "Iteration 32, loss = 0.40331459\n",
      "Iteration 33, loss = 0.39739701\n",
      "Iteration 34, loss = 0.39187662\n",
      "Iteration 35, loss = 0.38672722\n",
      "Iteration 36, loss = 0.38212443\n",
      "Iteration 37, loss = 0.37757282\n",
      "Iteration 38, loss = 0.37335339\n",
      "Iteration 39, loss = 0.36940638\n",
      "Iteration 40, loss = 0.36558616\n",
      "Iteration 41, loss = 0.36203995\n",
      "Iteration 42, loss = 0.35859721\n",
      "Iteration 43, loss = 0.35552890\n",
      "Iteration 44, loss = 0.35244184\n",
      "Iteration 45, loss = 0.34932637\n",
      "Iteration 46, loss = 0.34665948\n",
      "Iteration 47, loss = 0.34384056\n",
      "Iteration 48, loss = 0.34119003\n",
      "Iteration 49, loss = 0.33860700\n",
      "Iteration 50, loss = 0.33616937\n",
      "Iteration 51, loss = 0.33401908\n",
      "Iteration 52, loss = 0.33139641\n",
      "Iteration 53, loss = 0.32900492\n",
      "Iteration 54, loss = 0.32707857\n",
      "Iteration 55, loss = 0.32481360\n",
      "Iteration 56, loss = 0.32279311\n",
      "Iteration 57, loss = 0.32065989\n",
      "Iteration 58, loss = 0.31863710\n",
      "Iteration 59, loss = 0.31679362\n",
      "Iteration 60, loss = 0.31482893\n",
      "Iteration 61, loss = 0.31299296\n",
      "Iteration 62, loss = 0.31107519\n",
      "Iteration 63, loss = 0.30925684\n",
      "Iteration 64, loss = 0.30732944\n",
      "Iteration 65, loss = 0.30570535\n",
      "Iteration 66, loss = 0.30404417\n",
      "Iteration 67, loss = 0.30233202\n",
      "Iteration 68, loss = 0.30069391\n",
      "Iteration 69, loss = 0.29925649\n",
      "Iteration 70, loss = 0.29756956\n",
      "Iteration 71, loss = 0.29598536\n",
      "Iteration 72, loss = 0.29425822\n",
      "Iteration 73, loss = 0.29303117\n",
      "Iteration 74, loss = 0.29151771\n",
      "Iteration 75, loss = 0.29011020\n",
      "Iteration 76, loss = 0.28855117\n",
      "Iteration 77, loss = 0.28708250\n",
      "Iteration 78, loss = 0.28558596\n",
      "Iteration 79, loss = 0.28415664\n",
      "Iteration 80, loss = 0.28314036\n",
      "Iteration 81, loss = 0.28162075\n",
      "Iteration 82, loss = 0.28033720\n",
      "Iteration 83, loss = 0.27905649\n",
      "Iteration 84, loss = 0.27767763\n",
      "Iteration 85, loss = 0.27632079\n",
      "Iteration 86, loss = 0.27515757\n",
      "Iteration 87, loss = 0.27388400\n",
      "Iteration 88, loss = 0.27280276\n",
      "Iteration 89, loss = 0.27152424\n",
      "Iteration 90, loss = 0.27025238\n",
      "Iteration 91, loss = 0.26902688\n",
      "Iteration 92, loss = 0.26791214\n",
      "Iteration 93, loss = 0.26681958\n",
      "Iteration 94, loss = 0.26564670\n",
      "Iteration 95, loss = 0.26438105\n",
      "Iteration 96, loss = 0.26335471\n",
      "Iteration 97, loss = 0.26216825\n",
      "Iteration 98, loss = 0.26126641\n",
      "Iteration 99, loss = 0.26017422\n",
      "Iteration 100, loss = 0.25900907\n",
      "Iteration 101, loss = 0.25798444\n",
      "Iteration 102, loss = 0.25688241\n",
      "Iteration 103, loss = 0.25589271\n",
      "Iteration 104, loss = 0.25472646\n",
      "Iteration 105, loss = 0.25365591\n",
      "Iteration 106, loss = 0.25279221\n",
      "Iteration 107, loss = 0.25171498\n",
      "Iteration 108, loss = 0.25079994\n",
      "Iteration 109, loss = 0.24977288\n",
      "Iteration 110, loss = 0.24874526\n",
      "Iteration 111, loss = 0.24780602\n",
      "Iteration 112, loss = 0.24681670\n",
      "Iteration 113, loss = 0.24574321\n",
      "Iteration 114, loss = 0.24477523\n",
      "Iteration 115, loss = 0.24392591\n",
      "Iteration 116, loss = 0.24283297\n",
      "Iteration 117, loss = 0.24191875\n",
      "Iteration 118, loss = 0.24093797\n",
      "Iteration 119, loss = 0.23998689\n",
      "Iteration 120, loss = 0.23910780\n",
      "Iteration 121, loss = 0.23813578\n",
      "Iteration 122, loss = 0.23729450\n",
      "Iteration 123, loss = 0.23625665\n",
      "Iteration 124, loss = 0.23533378\n",
      "Iteration 125, loss = 0.23450959\n",
      "Iteration 126, loss = 0.23354670\n",
      "Iteration 127, loss = 0.23286895\n",
      "Iteration 128, loss = 0.23177380\n",
      "Iteration 129, loss = 0.23103266\n",
      "Iteration 130, loss = 0.22989941\n",
      "Iteration 131, loss = 0.22918948\n",
      "Iteration 132, loss = 0.22825794\n",
      "Iteration 133, loss = 0.22719379\n",
      "Iteration 134, loss = 0.22635148\n",
      "Iteration 135, loss = 0.22562440\n",
      "Iteration 136, loss = 0.22481576\n",
      "Iteration 137, loss = 0.22395579\n",
      "Iteration 138, loss = 0.22295322\n",
      "Iteration 139, loss = 0.22215040\n",
      "Iteration 140, loss = 0.22137933\n",
      "Iteration 141, loss = 0.22056565\n",
      "Iteration 142, loss = 0.21973449\n",
      "Iteration 143, loss = 0.21879418\n",
      "Iteration 144, loss = 0.21793657\n",
      "Iteration 145, loss = 0.21726966\n",
      "Iteration 146, loss = 0.21628626\n",
      "Iteration 147, loss = 0.21525085\n",
      "Iteration 148, loss = 0.21470076\n",
      "Iteration 149, loss = 0.21402176\n",
      "Iteration 150, loss = 0.21316812\n",
      "Iteration 151, loss = 0.21230568\n",
      "Iteration 152, loss = 0.21140980\n",
      "Iteration 153, loss = 0.21058539\n",
      "Iteration 154, loss = 0.20971129\n",
      "Iteration 155, loss = 0.20897011\n",
      "Iteration 156, loss = 0.20799794\n",
      "Iteration 157, loss = 0.20748138\n",
      "Iteration 158, loss = 0.20669234\n",
      "Iteration 159, loss = 0.20597755\n",
      "Iteration 160, loss = 0.20496128\n",
      "Iteration 161, loss = 0.20433803\n",
      "Iteration 162, loss = 0.20353552\n",
      "Iteration 163, loss = 0.20272784\n",
      "Iteration 164, loss = 0.20189881\n",
      "Iteration 165, loss = 0.20128916\n",
      "Iteration 166, loss = 0.20043482\n",
      "Iteration 167, loss = 0.19988506\n",
      "Iteration 168, loss = 0.19882223\n",
      "Iteration 169, loss = 0.19811326\n",
      "Iteration 170, loss = 0.19744468\n",
      "Iteration 171, loss = 0.19673272\n",
      "Iteration 172, loss = 0.19597208\n",
      "Iteration 173, loss = 0.19511105\n",
      "Iteration 174, loss = 0.19442024\n",
      "Iteration 175, loss = 0.19385093\n",
      "Iteration 176, loss = 0.19312720\n",
      "Iteration 177, loss = 0.19232326\n",
      "Iteration 178, loss = 0.19163278\n",
      "Iteration 179, loss = 0.19093361\n",
      "Iteration 180, loss = 0.19003864\n",
      "Iteration 181, loss = 0.18948846\n",
      "Iteration 182, loss = 0.18871564\n",
      "Iteration 183, loss = 0.18805819\n",
      "Iteration 184, loss = 0.18734102\n",
      "Iteration 185, loss = 0.18660678\n",
      "Iteration 186, loss = 0.18599678\n",
      "Iteration 187, loss = 0.18543489\n",
      "Iteration 188, loss = 0.18446847\n",
      "Iteration 189, loss = 0.18413092\n",
      "Iteration 190, loss = 0.18320612\n",
      "Iteration 191, loss = 0.18264856\n",
      "Iteration 192, loss = 0.18199522\n",
      "Iteration 193, loss = 0.18124726\n",
      "Iteration 194, loss = 0.18077674\n",
      "Iteration 195, loss = 0.18000530\n",
      "Iteration 196, loss = 0.17946385\n",
      "Iteration 197, loss = 0.17880422\n",
      "Iteration 198, loss = 0.17810747\n",
      "Iteration 199, loss = 0.17730861\n",
      "Iteration 200, loss = 0.17695511\n",
      "[CV] END activation=logistic, alpha=0.001, batch_size=50, hidden_layer_sizes=(400, 100), solver=sgd; total time=10.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.29972865\n",
      "Iteration 2, loss = 2.28177497\n",
      "Iteration 3, loss = 2.26084168\n",
      "Iteration 4, loss = 2.22611524\n",
      "Iteration 5, loss = 2.16325781\n",
      "Iteration 6, loss = 2.04046261\n",
      "Iteration 7, loss = 1.83139260\n",
      "Iteration 8, loss = 1.57724623\n",
      "Iteration 9, loss = 1.35525019\n",
      "Iteration 10, loss = 1.18107771\n",
      "Iteration 11, loss = 1.04469621\n",
      "Iteration 12, loss = 0.93923667\n",
      "Iteration 13, loss = 0.85701324\n",
      "Iteration 14, loss = 0.79072305\n",
      "Iteration 15, loss = 0.73468747\n",
      "Iteration 16, loss = 0.68673674\n",
      "Iteration 17, loss = 0.64536857\n",
      "Iteration 18, loss = 0.60992460\n",
      "Iteration 19, loss = 0.57933917\n",
      "Iteration 20, loss = 0.55319954\n",
      "Iteration 21, loss = 0.53057843\n",
      "Iteration 22, loss = 0.51083655\n",
      "Iteration 23, loss = 0.49362964\n",
      "Iteration 24, loss = 0.47855447\n",
      "Iteration 25, loss = 0.46534193\n",
      "Iteration 26, loss = 0.45354141\n",
      "Iteration 27, loss = 0.44294460\n",
      "Iteration 28, loss = 0.43352435\n",
      "Iteration 29, loss = 0.42512702\n",
      "Iteration 30, loss = 0.41750148\n",
      "Iteration 31, loss = 0.41067769\n",
      "Iteration 32, loss = 0.40402953\n",
      "Iteration 33, loss = 0.39854976\n",
      "Iteration 34, loss = 0.39296818\n",
      "Iteration 35, loss = 0.38816198\n",
      "Iteration 36, loss = 0.38360913\n",
      "Iteration 37, loss = 0.37918124\n",
      "Iteration 38, loss = 0.37537629\n",
      "Iteration 39, loss = 0.37136431\n",
      "Iteration 40, loss = 0.36798562\n",
      "Iteration 41, loss = 0.36457069\n",
      "Iteration 42, loss = 0.36145606\n",
      "Iteration 43, loss = 0.35820025\n",
      "Iteration 44, loss = 0.35534151\n",
      "Iteration 45, loss = 0.35251964\n",
      "Iteration 46, loss = 0.34996472\n",
      "Iteration 47, loss = 0.34741575\n",
      "Iteration 48, loss = 0.34497294\n",
      "Iteration 49, loss = 0.34250768\n",
      "Iteration 50, loss = 0.34034925\n",
      "Iteration 51, loss = 0.33784119\n",
      "Iteration 52, loss = 0.33583127\n",
      "Iteration 53, loss = 0.33377132\n",
      "Iteration 54, loss = 0.33166645\n",
      "Iteration 55, loss = 0.32966354\n",
      "Iteration 56, loss = 0.32781134\n",
      "Iteration 57, loss = 0.32591029\n",
      "Iteration 58, loss = 0.32390610\n",
      "Iteration 59, loss = 0.32206365\n",
      "Iteration 60, loss = 0.32039030\n",
      "Iteration 61, loss = 0.31871071\n",
      "Iteration 62, loss = 0.31690660\n",
      "Iteration 63, loss = 0.31513461\n",
      "Iteration 64, loss = 0.31367464\n",
      "Iteration 65, loss = 0.31201343\n",
      "Iteration 66, loss = 0.31025460\n",
      "Iteration 67, loss = 0.30883182\n",
      "Iteration 68, loss = 0.30710420\n",
      "Iteration 69, loss = 0.30577106\n",
      "Iteration 70, loss = 0.30389340\n",
      "Iteration 71, loss = 0.30276918\n",
      "Iteration 72, loss = 0.30097911\n",
      "Iteration 73, loss = 0.29958882\n",
      "Iteration 74, loss = 0.29826019\n",
      "Iteration 75, loss = 0.29682915\n",
      "Iteration 76, loss = 0.29526023\n",
      "Iteration 77, loss = 0.29391751\n",
      "Iteration 78, loss = 0.29276771\n",
      "Iteration 79, loss = 0.29109320\n",
      "Iteration 80, loss = 0.28992377\n",
      "Iteration 81, loss = 0.28839461\n",
      "Iteration 82, loss = 0.28724698\n",
      "Iteration 83, loss = 0.28592056\n",
      "Iteration 84, loss = 0.28455355\n",
      "Iteration 85, loss = 0.28322715\n",
      "Iteration 86, loss = 0.28209460\n",
      "Iteration 87, loss = 0.28079300\n",
      "Iteration 88, loss = 0.27952588\n",
      "Iteration 89, loss = 0.27827267\n",
      "Iteration 90, loss = 0.27702402\n",
      "Iteration 91, loss = 0.27592728\n",
      "Iteration 92, loss = 0.27455746\n",
      "Iteration 93, loss = 0.27339195\n",
      "Iteration 94, loss = 0.27228030\n",
      "Iteration 95, loss = 0.27112197\n",
      "Iteration 96, loss = 0.26992447\n",
      "Iteration 97, loss = 0.26876661\n",
      "Iteration 98, loss = 0.26763869\n",
      "Iteration 99, loss = 0.26638990\n",
      "Iteration 100, loss = 0.26536140\n",
      "Iteration 101, loss = 0.26404648\n",
      "Iteration 102, loss = 0.26304486\n",
      "Iteration 103, loss = 0.26211261\n",
      "Iteration 104, loss = 0.26095416\n",
      "Iteration 105, loss = 0.25985763\n",
      "Iteration 106, loss = 0.25872653\n",
      "Iteration 107, loss = 0.25758964\n",
      "Iteration 108, loss = 0.25669671\n",
      "Iteration 109, loss = 0.25552220\n",
      "Iteration 110, loss = 0.25448322\n",
      "Iteration 111, loss = 0.25352724\n",
      "Iteration 112, loss = 0.25251352\n",
      "Iteration 113, loss = 0.25147947\n",
      "Iteration 114, loss = 0.25047547\n",
      "Iteration 115, loss = 0.24925579\n",
      "Iteration 116, loss = 0.24839607\n",
      "Iteration 117, loss = 0.24725437\n",
      "Iteration 118, loss = 0.24643209\n",
      "Iteration 119, loss = 0.24533274\n",
      "Iteration 120, loss = 0.24433391\n",
      "Iteration 121, loss = 0.24362749\n",
      "Iteration 122, loss = 0.24251963\n",
      "Iteration 123, loss = 0.24164312\n",
      "Iteration 124, loss = 0.24069223\n",
      "Iteration 125, loss = 0.23966267\n",
      "Iteration 126, loss = 0.23867643\n",
      "Iteration 127, loss = 0.23782287\n",
      "Iteration 128, loss = 0.23684877\n",
      "Iteration 129, loss = 0.23585618\n",
      "Iteration 130, loss = 0.23501987\n",
      "Iteration 131, loss = 0.23394293\n",
      "Iteration 132, loss = 0.23320675\n",
      "Iteration 133, loss = 0.23236263\n",
      "Iteration 134, loss = 0.23149844\n",
      "Iteration 135, loss = 0.23046831\n",
      "Iteration 136, loss = 0.22966725\n",
      "Iteration 137, loss = 0.22867518\n",
      "Iteration 138, loss = 0.22772670\n",
      "Iteration 139, loss = 0.22700597\n",
      "Iteration 140, loss = 0.22603549\n",
      "Iteration 141, loss = 0.22516018\n",
      "Iteration 142, loss = 0.22430496\n",
      "Iteration 143, loss = 0.22346177\n",
      "Iteration 144, loss = 0.22249590\n",
      "Iteration 145, loss = 0.22185430\n",
      "Iteration 146, loss = 0.22089869\n",
      "Iteration 147, loss = 0.21997294\n",
      "Iteration 148, loss = 0.21933233\n",
      "Iteration 149, loss = 0.21854258\n",
      "Iteration 150, loss = 0.21746372\n",
      "Iteration 151, loss = 0.21683133\n",
      "Iteration 152, loss = 0.21590377\n",
      "Iteration 153, loss = 0.21500033\n",
      "Iteration 154, loss = 0.21432962\n",
      "Iteration 155, loss = 0.21344331\n",
      "Iteration 156, loss = 0.21264588\n",
      "Iteration 157, loss = 0.21194655\n",
      "Iteration 158, loss = 0.21114907\n",
      "Iteration 159, loss = 0.21030871\n",
      "Iteration 160, loss = 0.20938717\n",
      "Iteration 161, loss = 0.20878763\n",
      "Iteration 162, loss = 0.20792608\n",
      "Iteration 163, loss = 0.20698179\n",
      "Iteration 164, loss = 0.20642758\n",
      "Iteration 165, loss = 0.20555157\n",
      "Iteration 166, loss = 0.20480240\n",
      "Iteration 167, loss = 0.20414149\n",
      "Iteration 168, loss = 0.20325012\n",
      "Iteration 169, loss = 0.20275444\n",
      "Iteration 170, loss = 0.20192679\n",
      "Iteration 171, loss = 0.20125403\n",
      "Iteration 172, loss = 0.20022180\n",
      "Iteration 173, loss = 0.19954826\n",
      "Iteration 174, loss = 0.19885453\n",
      "Iteration 175, loss = 0.19819827\n",
      "Iteration 176, loss = 0.19724268\n",
      "Iteration 177, loss = 0.19667065\n",
      "Iteration 178, loss = 0.19581418\n",
      "Iteration 179, loss = 0.19518947\n",
      "Iteration 180, loss = 0.19445339\n",
      "Iteration 181, loss = 0.19367305\n",
      "Iteration 182, loss = 0.19308803\n",
      "Iteration 183, loss = 0.19228977\n",
      "Iteration 184, loss = 0.19168077\n",
      "Iteration 185, loss = 0.19108002\n",
      "Iteration 186, loss = 0.19022532\n",
      "Iteration 187, loss = 0.18964533\n",
      "Iteration 188, loss = 0.18891780\n",
      "Iteration 189, loss = 0.18809018\n",
      "Iteration 190, loss = 0.18751088\n",
      "Iteration 191, loss = 0.18681705\n",
      "Iteration 192, loss = 0.18617450\n",
      "Iteration 193, loss = 0.18550914\n",
      "Iteration 194, loss = 0.18502151\n",
      "Iteration 195, loss = 0.18414648\n",
      "Iteration 196, loss = 0.18349794\n",
      "Iteration 197, loss = 0.18280208\n",
      "Iteration 198, loss = 0.18214981\n",
      "Iteration 199, loss = 0.18159440\n",
      "Iteration 200, loss = 0.18079408\n",
      "[CV] END activation=logistic, alpha=0.001, batch_size=50, hidden_layer_sizes=(400, 100), solver=sgd; total time=10.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.07432679\n",
      "Iteration 2, loss = 0.31836073\n",
      "Iteration 3, loss = 0.21743706\n",
      "Iteration 4, loss = 0.17181053\n",
      "Iteration 5, loss = 0.14258078\n",
      "Iteration 6, loss = 0.12238651\n",
      "Iteration 7, loss = 0.10599196\n",
      "Iteration 8, loss = 0.09529166\n",
      "Iteration 9, loss = 0.08581452\n",
      "Iteration 10, loss = 0.07762631\n",
      "Iteration 11, loss = 0.06940434\n",
      "Iteration 12, loss = 0.06313971\n",
      "Iteration 13, loss = 0.05992850\n",
      "Iteration 14, loss = 0.05594390\n",
      "Iteration 15, loss = 0.05027004\n",
      "Iteration 16, loss = 0.05071789\n",
      "Iteration 17, loss = 0.04821962\n",
      "Iteration 18, loss = 0.04630025\n",
      "Iteration 19, loss = 0.04374866\n",
      "Iteration 20, loss = 0.04242065\n",
      "Iteration 21, loss = 0.04273508\n",
      "Iteration 22, loss = 0.03839286\n",
      "Iteration 23, loss = 0.03797796\n",
      "Iteration 24, loss = 0.04024154\n",
      "Iteration 25, loss = 0.03818078\n",
      "Iteration 26, loss = 0.03649166\n",
      "Iteration 27, loss = 0.03569725\n",
      "Iteration 28, loss = 0.04113897\n",
      "Iteration 29, loss = 0.03663067\n",
      "Iteration 30, loss = 0.03578116\n",
      "Iteration 31, loss = 0.03454795\n",
      "Iteration 32, loss = 0.03621067\n",
      "Iteration 33, loss = 0.03936007\n",
      "Iteration 34, loss = 0.03400108\n",
      "Iteration 35, loss = 0.03421951\n",
      "Iteration 36, loss = 0.03181712\n",
      "Iteration 37, loss = 0.03709354\n",
      "Iteration 38, loss = 0.03951957\n",
      "Iteration 39, loss = 0.03236896\n",
      "Iteration 40, loss = 0.03171494\n",
      "Iteration 41, loss = 0.02874969\n",
      "Iteration 42, loss = 0.04367653\n",
      "Iteration 43, loss = 0.02984298\n",
      "Iteration 44, loss = 0.02793618\n",
      "Iteration 45, loss = 0.03536201\n",
      "Iteration 46, loss = 0.03745340\n",
      "Iteration 47, loss = 0.02909785\n",
      "Iteration 48, loss = 0.02763371\n",
      "Iteration 49, loss = 0.04059084\n",
      "Iteration 50, loss = 0.03049314\n",
      "Iteration 51, loss = 0.03073884\n",
      "Iteration 52, loss = 0.03395452\n",
      "Iteration 53, loss = 0.03135428\n",
      "Iteration 54, loss = 0.02715796\n",
      "Iteration 55, loss = 0.03593714\n",
      "Iteration 56, loss = 0.03331334\n",
      "Iteration 57, loss = 0.03064912\n",
      "Iteration 58, loss = 0.02717735\n",
      "Iteration 59, loss = 0.03134547\n",
      "Iteration 60, loss = 0.03670533\n",
      "Iteration 61, loss = 0.02931960\n",
      "Iteration 62, loss = 0.02639457\n",
      "Iteration 63, loss = 0.02645280\n",
      "Iteration 64, loss = 0.04354503\n",
      "Iteration 65, loss = 0.02896987\n",
      "Iteration 66, loss = 0.02671582\n",
      "Iteration 67, loss = 0.03136037\n",
      "Iteration 68, loss = 0.03591810\n",
      "Iteration 69, loss = 0.02810105\n",
      "Iteration 70, loss = 0.02579165\n",
      "Iteration 71, loss = 0.02798588\n",
      "Iteration 72, loss = 0.03881242\n",
      "Iteration 73, loss = 0.02651417\n",
      "Iteration 74, loss = 0.02638674\n",
      "Iteration 75, loss = 0.03984372\n",
      "Iteration 76, loss = 0.02850744\n",
      "Iteration 77, loss = 0.02553070\n",
      "Iteration 78, loss = 0.02480143\n",
      "Iteration 79, loss = 0.04297643\n",
      "Iteration 80, loss = 0.02792454\n",
      "Iteration 81, loss = 0.02540332\n",
      "Iteration 82, loss = 0.02449827\n",
      "Iteration 83, loss = 0.04082172\n",
      "Iteration 84, loss = 0.03069561\n",
      "Iteration 85, loss = 0.02563570\n",
      "Iteration 86, loss = 0.02462181\n",
      "Iteration 87, loss = 0.03791070\n",
      "Iteration 88, loss = 0.03019813\n",
      "Iteration 89, loss = 0.02651723\n",
      "Iteration 90, loss = 0.03502503\n",
      "Iteration 91, loss = 0.02737158\n",
      "Iteration 92, loss = 0.02499528\n",
      "Iteration 93, loss = 0.02573830\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.001, batch_size=50, hidden_layer_sizes=(100, 100, 100), solver=adam; total time= 3.7min\n",
      "Iteration 1, loss = 1.06756269\n",
      "Iteration 2, loss = 0.35614443\n",
      "Iteration 3, loss = 0.23825684\n",
      "Iteration 4, loss = 0.18492995\n",
      "Iteration 5, loss = 0.15189277\n",
      "Iteration 6, loss = 0.12884804\n",
      "Iteration 7, loss = 0.11265844\n",
      "Iteration 8, loss = 0.09921221\n",
      "Iteration 9, loss = 0.08877273\n",
      "Iteration 10, loss = 0.07945606\n",
      "Iteration 11, loss = 0.07238785\n",
      "Iteration 12, loss = 0.06664690\n",
      "Iteration 13, loss = 0.06124374\n",
      "Iteration 14, loss = 0.05746723\n",
      "Iteration 15, loss = 0.05339405\n",
      "Iteration 16, loss = 0.04936167\n",
      "Iteration 17, loss = 0.04709110\n",
      "Iteration 18, loss = 0.04708853\n",
      "Iteration 19, loss = 0.04654767\n",
      "Iteration 20, loss = 0.04015400\n",
      "Iteration 21, loss = 0.04094883\n",
      "Iteration 22, loss = 0.04159351\n",
      "Iteration 23, loss = 0.04018467\n",
      "Iteration 24, loss = 0.03708016\n",
      "Iteration 25, loss = 0.03556875\n",
      "Iteration 26, loss = 0.04140553\n",
      "Iteration 27, loss = 0.03656582\n",
      "Iteration 28, loss = 0.03880468\n",
      "Iteration 29, loss = 0.03529898\n",
      "Iteration 30, loss = 0.03525004\n",
      "Iteration 31, loss = 0.03471017\n",
      "Iteration 32, loss = 0.04142785\n",
      "Iteration 33, loss = 0.03289678\n",
      "Iteration 34, loss = 0.03196213\n",
      "Iteration 35, loss = 0.03584239\n",
      "Iteration 36, loss = 0.04006193\n",
      "Iteration 37, loss = 0.03288259\n",
      "Iteration 38, loss = 0.03340603\n",
      "Iteration 39, loss = 0.03047951\n",
      "Iteration 40, loss = 0.03482983\n",
      "Iteration 41, loss = 0.03565430\n",
      "Iteration 42, loss = 0.03401630\n",
      "Iteration 43, loss = 0.03160014\n",
      "Iteration 44, loss = 0.02882874\n",
      "Iteration 45, loss = 0.03962601\n",
      "Iteration 46, loss = 0.03090346\n",
      "Iteration 47, loss = 0.02948503\n",
      "Iteration 48, loss = 0.02916079\n",
      "Iteration 49, loss = 0.04199754\n",
      "Iteration 50, loss = 0.02844090\n",
      "Iteration 51, loss = 0.02693580\n",
      "Iteration 52, loss = 0.02726209\n",
      "Iteration 53, loss = 0.05037719\n",
      "Iteration 54, loss = 0.02925001\n",
      "Iteration 55, loss = 0.02678185\n",
      "Iteration 56, loss = 0.02605138\n",
      "Iteration 57, loss = 0.03876067\n",
      "Iteration 58, loss = 0.03409478\n",
      "Iteration 59, loss = 0.02838268\n",
      "Iteration 60, loss = 0.02617247\n",
      "Iteration 61, loss = 0.03974891\n",
      "Iteration 62, loss = 0.03060564\n",
      "Iteration 63, loss = 0.02670774\n",
      "Iteration 64, loss = 0.02538340\n",
      "Iteration 65, loss = 0.03838540\n",
      "Iteration 66, loss = 0.03316202\n",
      "Iteration 67, loss = 0.02860393\n",
      "Iteration 68, loss = 0.02909190\n",
      "Iteration 69, loss = 0.03611274\n",
      "Iteration 70, loss = 0.02856976\n",
      "Iteration 71, loss = 0.02570433\n",
      "Iteration 72, loss = 0.03018076\n",
      "Iteration 73, loss = 0.04013780\n",
      "Iteration 74, loss = 0.02691186\n",
      "Iteration 75, loss = 0.02604972\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.001, batch_size=50, hidden_layer_sizes=(100, 100, 100), solver=adam; total time= 2.9min\n",
      "Iteration 1, loss = 1.05541993\n",
      "Iteration 2, loss = 0.33469369\n",
      "Iteration 3, loss = 0.23443690\n",
      "Iteration 4, loss = 0.18575535\n",
      "Iteration 5, loss = 0.15651581\n",
      "Iteration 6, loss = 0.13486107\n",
      "Iteration 7, loss = 0.11566036\n",
      "Iteration 8, loss = 0.10424401\n",
      "Iteration 9, loss = 0.09379700\n",
      "Iteration 10, loss = 0.08466658\n",
      "Iteration 11, loss = 0.07645423\n",
      "Iteration 12, loss = 0.06985365\n",
      "Iteration 13, loss = 0.06380201\n",
      "Iteration 14, loss = 0.05827383\n",
      "Iteration 15, loss = 0.05685168\n",
      "Iteration 16, loss = 0.05280680\n",
      "Iteration 17, loss = 0.04843826\n",
      "Iteration 18, loss = 0.04538505\n",
      "Iteration 19, loss = 0.04563794\n",
      "Iteration 20, loss = 0.04392685\n",
      "Iteration 21, loss = 0.04309752\n",
      "Iteration 22, loss = 0.04179613\n",
      "Iteration 23, loss = 0.04084757\n",
      "Iteration 24, loss = 0.04181521\n",
      "Iteration 25, loss = 0.03920937\n",
      "Iteration 26, loss = 0.03758591\n",
      "Iteration 27, loss = 0.04034441\n",
      "Iteration 28, loss = 0.03559215\n",
      "Iteration 29, loss = 0.03661220\n",
      "Iteration 30, loss = 0.04195767\n",
      "Iteration 31, loss = 0.03481208\n",
      "Iteration 32, loss = 0.03242716\n",
      "Iteration 33, loss = 0.03995693\n",
      "Iteration 34, loss = 0.03547665\n",
      "Iteration 35, loss = 0.03541989\n",
      "Iteration 36, loss = 0.03483638\n",
      "Iteration 37, loss = 0.03100503\n",
      "Iteration 38, loss = 0.04193918\n",
      "Iteration 39, loss = 0.03620806\n",
      "Iteration 40, loss = 0.03186442\n",
      "Iteration 41, loss = 0.03095790\n",
      "Iteration 42, loss = 0.03171356\n",
      "Iteration 43, loss = 0.04158855\n",
      "Iteration 44, loss = 0.03106074\n",
      "Iteration 45, loss = 0.02939967\n",
      "Iteration 46, loss = 0.03779233\n",
      "Iteration 47, loss = 0.03374248\n",
      "Iteration 48, loss = 0.02876981\n",
      "Iteration 49, loss = 0.02911630\n",
      "Iteration 50, loss = 0.03982819\n",
      "Iteration 51, loss = 0.03228532\n",
      "Iteration 52, loss = 0.03321377\n",
      "Iteration 53, loss = 0.03368777\n",
      "Iteration 54, loss = 0.03350850\n",
      "Iteration 55, loss = 0.02921594\n",
      "Iteration 56, loss = 0.03330894\n",
      "Iteration 57, loss = 0.03759905\n",
      "Iteration 58, loss = 0.02860478\n",
      "Iteration 59, loss = 0.02785988\n",
      "Iteration 60, loss = 0.03963277\n",
      "Iteration 61, loss = 0.03128181\n",
      "Iteration 62, loss = 0.02830996\n",
      "Iteration 63, loss = 0.03638321\n",
      "Iteration 64, loss = 0.03508266\n",
      "Iteration 65, loss = 0.02745040\n",
      "Iteration 66, loss = 0.02767583\n",
      "Iteration 67, loss = 0.04101904\n",
      "Iteration 68, loss = 0.02820163\n",
      "Iteration 69, loss = 0.02662494\n",
      "Iteration 70, loss = 0.04049710\n",
      "Iteration 71, loss = 0.03051394\n",
      "Iteration 72, loss = 0.02669403\n",
      "Iteration 73, loss = 0.02622741\n",
      "Iteration 74, loss = 0.04453547\n",
      "Iteration 75, loss = 0.02849837\n",
      "Iteration 76, loss = 0.02644259\n",
      "Iteration 77, loss = 0.02618558\n",
      "Iteration 78, loss = 0.04207539\n",
      "Iteration 79, loss = 0.02742167\n",
      "Iteration 80, loss = 0.02596372\n",
      "Iteration 81, loss = 0.02515094\n",
      "Iteration 82, loss = 0.04424099\n",
      "Iteration 83, loss = 0.03152020\n",
      "Iteration 84, loss = 0.02694107\n",
      "Iteration 85, loss = 0.02659307\n",
      "Iteration 86, loss = 0.03907558\n",
      "Iteration 87, loss = 0.02779022\n",
      "Iteration 88, loss = 0.02677582\n",
      "Iteration 89, loss = 0.02925672\n",
      "Iteration 90, loss = 0.03792139\n",
      "Iteration 91, loss = 0.02619699\n",
      "Iteration 92, loss = 0.02602314\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.001, batch_size=50, hidden_layer_sizes=(100, 100, 100), solver=adam; total time= 3.6min\n",
      "Iteration 1, loss = 1.03376911\n",
      "Iteration 2, loss = 0.31857990\n",
      "Iteration 3, loss = 0.22187114\n",
      "Iteration 4, loss = 0.17552211\n",
      "Iteration 5, loss = 0.14930607\n",
      "Iteration 6, loss = 0.12783772\n",
      "Iteration 7, loss = 0.11313255\n",
      "Iteration 8, loss = 0.10127117\n",
      "Iteration 9, loss = 0.09308953\n",
      "Iteration 10, loss = 0.08220383\n",
      "Iteration 11, loss = 0.07478996\n",
      "Iteration 12, loss = 0.06851832\n",
      "Iteration 13, loss = 0.06367298\n",
      "Iteration 14, loss = 0.05854416\n",
      "Iteration 15, loss = 0.05610629\n",
      "Iteration 16, loss = 0.05292364\n",
      "Iteration 17, loss = 0.05004576\n",
      "Iteration 18, loss = 0.04819695\n",
      "Iteration 19, loss = 0.04554980\n",
      "Iteration 20, loss = 0.04417864\n",
      "Iteration 21, loss = 0.04390706\n",
      "Iteration 22, loss = 0.04491669\n",
      "Iteration 23, loss = 0.03902833\n",
      "Iteration 24, loss = 0.03954394\n",
      "Iteration 25, loss = 0.03989429\n",
      "Iteration 26, loss = 0.04177223\n",
      "Iteration 27, loss = 0.03909196\n",
      "Iteration 28, loss = 0.03419451\n",
      "Iteration 29, loss = 0.04321356\n",
      "Iteration 30, loss = 0.03448435\n",
      "Iteration 31, loss = 0.03831075\n",
      "Iteration 32, loss = 0.03729302\n",
      "Iteration 33, loss = 0.03249289\n",
      "Iteration 34, loss = 0.03409273\n",
      "Iteration 35, loss = 0.04129912\n",
      "Iteration 36, loss = 0.03335123\n",
      "Iteration 37, loss = 0.03199699\n",
      "Iteration 38, loss = 0.03944464\n",
      "Iteration 39, loss = 0.03680750\n",
      "Iteration 40, loss = 0.03414840\n",
      "Iteration 41, loss = 0.03190085\n",
      "Iteration 42, loss = 0.02897145\n",
      "Iteration 43, loss = 0.03760981\n",
      "Iteration 44, loss = 0.03700344\n",
      "Iteration 45, loss = 0.02905537\n",
      "Iteration 46, loss = 0.02809067\n",
      "Iteration 47, loss = 0.04412520\n",
      "Iteration 48, loss = 0.03034892\n",
      "Iteration 49, loss = 0.02797739\n",
      "Iteration 50, loss = 0.04180105\n",
      "Iteration 51, loss = 0.03131631\n",
      "Iteration 52, loss = 0.02783475\n",
      "Iteration 53, loss = 0.03739669\n",
      "Iteration 54, loss = 0.03626774\n",
      "Iteration 55, loss = 0.02796571\n",
      "Iteration 56, loss = 0.02686019\n",
      "Iteration 57, loss = 0.03428918\n",
      "Iteration 58, loss = 0.03889646\n",
      "Iteration 59, loss = 0.03011184\n",
      "Iteration 60, loss = 0.02805364\n",
      "Iteration 61, loss = 0.03560975\n",
      "Iteration 62, loss = 0.02904747\n",
      "Iteration 63, loss = 0.02661503\n",
      "Iteration 64, loss = 0.02645970\n",
      "Iteration 65, loss = 0.04617875\n",
      "Iteration 66, loss = 0.02741866\n",
      "Iteration 67, loss = 0.02596370\n",
      "Iteration 68, loss = 0.03407414\n",
      "Iteration 69, loss = 0.03682724\n",
      "Iteration 70, loss = 0.02797319\n",
      "Iteration 71, loss = 0.02587322\n",
      "Iteration 72, loss = 0.02995406\n",
      "Iteration 73, loss = 0.03857058\n",
      "Iteration 74, loss = 0.02873395\n",
      "Iteration 75, loss = 0.02573411\n",
      "Iteration 76, loss = 0.02745346\n",
      "Iteration 77, loss = 0.04080885\n",
      "Iteration 78, loss = 0.02842823\n",
      "Iteration 79, loss = 0.02588748\n",
      "Iteration 80, loss = 0.02556339\n",
      "Iteration 81, loss = 0.04422874\n",
      "Iteration 82, loss = 0.02683745\n",
      "Iteration 83, loss = 0.02541532\n",
      "Iteration 84, loss = 0.03169275\n",
      "Iteration 85, loss = 0.03810264\n",
      "Iteration 86, loss = 0.02683771\n",
      "Iteration 87, loss = 0.02555843\n",
      "Iteration 88, loss = 0.02808091\n",
      "Iteration 89, loss = 0.04037644\n",
      "Iteration 90, loss = 0.02731536\n",
      "Iteration 91, loss = 0.02639011\n",
      "Iteration 92, loss = 0.03371294\n",
      "Iteration 93, loss = 0.02757697\n",
      "Iteration 94, loss = 0.02527073\n",
      "Iteration 95, loss = 0.02460082\n",
      "Iteration 96, loss = 0.04575942\n",
      "Iteration 97, loss = 0.02706691\n",
      "Iteration 98, loss = 0.02515317\n",
      "Iteration 99, loss = 0.02686982\n",
      "Iteration 100, loss = 0.03766803\n",
      "Iteration 101, loss = 0.02715731\n",
      "Iteration 102, loss = 0.02576561\n",
      "Iteration 103, loss = 0.03802752\n",
      "Iteration 104, loss = 0.02661946\n",
      "Iteration 105, loss = 0.02491932\n",
      "Iteration 106, loss = 0.02445675\n",
      "Iteration 107, loss = 0.04281618\n",
      "Iteration 108, loss = 0.02861267\n",
      "Iteration 109, loss = 0.02559071\n",
      "Iteration 110, loss = 0.02443699\n",
      "Iteration 111, loss = 0.03572986\n",
      "Iteration 112, loss = 0.03349837\n",
      "Iteration 113, loss = 0.02721134\n",
      "Iteration 114, loss = 0.02523107\n",
      "Iteration 115, loss = 0.02412837\n",
      "Iteration 116, loss = 0.03489352\n",
      "Iteration 117, loss = 0.03469885\n",
      "Iteration 118, loss = 0.02578115\n",
      "Iteration 119, loss = 0.02443629\n",
      "Iteration 120, loss = 0.02508177\n",
      "Iteration 121, loss = 0.04237905\n",
      "Iteration 122, loss = 0.02653751\n",
      "Iteration 123, loss = 0.02450763\n",
      "Iteration 124, loss = 0.02623803\n",
      "Iteration 125, loss = 0.03795904\n",
      "Iteration 126, loss = 0.02574296\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.001, batch_size=50, hidden_layer_sizes=(100, 100, 100), solver=adam; total time= 5.0min\n",
      "Iteration 1, loss = 1.05999100\n",
      "Iteration 2, loss = 0.33619149\n",
      "Iteration 3, loss = 0.23686127\n",
      "Iteration 4, loss = 0.18549720\n",
      "Iteration 5, loss = 0.15331944\n",
      "Iteration 6, loss = 0.12888506\n",
      "Iteration 7, loss = 0.11370079\n",
      "Iteration 8, loss = 0.10007524\n",
      "Iteration 9, loss = 0.08788545\n",
      "Iteration 10, loss = 0.08008509\n",
      "Iteration 11, loss = 0.07422610\n",
      "Iteration 12, loss = 0.06629832\n",
      "Iteration 13, loss = 0.06257566\n",
      "Iteration 14, loss = 0.05761965\n",
      "Iteration 15, loss = 0.05306552\n",
      "Iteration 16, loss = 0.05199082\n",
      "Iteration 17, loss = 0.04823152\n",
      "Iteration 18, loss = 0.04451035\n",
      "Iteration 19, loss = 0.04634160\n",
      "Iteration 20, loss = 0.04337950\n",
      "Iteration 21, loss = 0.04112973\n",
      "Iteration 22, loss = 0.04114819\n",
      "Iteration 23, loss = 0.04081603\n",
      "Iteration 24, loss = 0.04004141\n",
      "Iteration 25, loss = 0.04042202\n",
      "Iteration 26, loss = 0.03803336\n",
      "Iteration 27, loss = 0.03550586\n",
      "Iteration 28, loss = 0.03582128\n",
      "Iteration 29, loss = 0.04200064\n",
      "Iteration 30, loss = 0.03349795\n",
      "Iteration 31, loss = 0.03329093\n",
      "Iteration 32, loss = 0.04132432\n",
      "Iteration 33, loss = 0.03282578\n",
      "Iteration 34, loss = 0.03210333\n",
      "Iteration 35, loss = 0.03859175\n",
      "Iteration 36, loss = 0.03816338\n",
      "Iteration 37, loss = 0.03424595\n",
      "Iteration 38, loss = 0.03264611\n",
      "Iteration 39, loss = 0.03809229\n",
      "Iteration 40, loss = 0.03243164\n",
      "Iteration 41, loss = 0.03053059\n",
      "Iteration 42, loss = 0.03348952\n",
      "Iteration 43, loss = 0.03754801\n",
      "Iteration 44, loss = 0.03156765\n",
      "Iteration 45, loss = 0.02983645\n",
      "Iteration 46, loss = 0.03767112\n",
      "Iteration 47, loss = 0.03341400\n",
      "Iteration 48, loss = 0.03022021\n",
      "Iteration 49, loss = 0.03808778\n",
      "Iteration 50, loss = 0.03640778\n",
      "Iteration 51, loss = 0.02971337\n",
      "Iteration 52, loss = 0.02763488\n",
      "Iteration 53, loss = 0.02709901\n",
      "Iteration 54, loss = 0.04691075\n",
      "Iteration 55, loss = 0.02913256\n",
      "Iteration 56, loss = 0.02700496\n",
      "Iteration 57, loss = 0.03151820\n",
      "Iteration 58, loss = 0.04015505\n",
      "Iteration 59, loss = 0.02978950\n",
      "Iteration 60, loss = 0.03251725\n",
      "Iteration 61, loss = 0.03570092\n",
      "Iteration 62, loss = 0.02736487\n",
      "Iteration 63, loss = 0.02658104\n",
      "Iteration 64, loss = 0.03120172\n",
      "Iteration 65, loss = 0.03895731\n",
      "Iteration 66, loss = 0.02825973\n",
      "Iteration 67, loss = 0.02626327\n",
      "Iteration 68, loss = 0.02727191\n",
      "Iteration 69, loss = 0.04174596\n",
      "Iteration 70, loss = 0.02882128\n",
      "Iteration 71, loss = 0.02645621\n",
      "Iteration 72, loss = 0.02539259\n",
      "Iteration 73, loss = 0.04422915\n",
      "Iteration 74, loss = 0.02738860\n",
      "Iteration 75, loss = 0.03002735\n",
      "Iteration 76, loss = 0.03197287\n",
      "Iteration 77, loss = 0.03133642\n",
      "Iteration 78, loss = 0.03085059\n",
      "Iteration 79, loss = 0.02943489\n",
      "Iteration 80, loss = 0.02574874\n",
      "Iteration 81, loss = 0.02494429\n",
      "Iteration 82, loss = 0.04285409\n",
      "Iteration 83, loss = 0.02969988\n",
      "Iteration 84, loss = 0.02588909\n",
      "Iteration 85, loss = 0.02493853\n",
      "Iteration 86, loss = 0.04166444\n",
      "Iteration 87, loss = 0.02900810\n",
      "Iteration 88, loss = 0.02607408\n",
      "Iteration 89, loss = 0.02547431\n",
      "Iteration 90, loss = 0.04150983\n",
      "Iteration 91, loss = 0.02891811\n",
      "Iteration 92, loss = 0.02591946\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.001, batch_size=50, hidden_layer_sizes=(100, 100, 100), solver=adam; total time= 3.6min\n",
      "Iteration 1, loss = 2.30903756\n",
      "Iteration 2, loss = 2.30362248\n",
      "Iteration 3, loss = 2.30361613\n",
      "Iteration 4, loss = 2.30340372\n",
      "Iteration 5, loss = 2.30326915\n",
      "Iteration 6, loss = 2.30262296\n",
      "Iteration 7, loss = 2.30285514\n",
      "Iteration 8, loss = 2.30254548\n",
      "Iteration 9, loss = 2.30265042\n",
      "Iteration 10, loss = 2.30197180\n",
      "Iteration 11, loss = 2.30207426\n",
      "Iteration 12, loss = 2.30177625\n",
      "Iteration 13, loss = 2.30116032\n",
      "Iteration 14, loss = 2.30091704\n",
      "Iteration 15, loss = 2.30092573\n",
      "Iteration 16, loss = 2.30029631\n",
      "Iteration 17, loss = 2.30011794\n",
      "Iteration 18, loss = 2.29959368\n",
      "Iteration 19, loss = 2.29894464\n",
      "Iteration 20, loss = 2.29794091\n",
      "Iteration 21, loss = 2.29761367\n",
      "Iteration 22, loss = 2.29677713\n",
      "Iteration 23, loss = 2.29543938\n",
      "Iteration 24, loss = 2.29441793\n",
      "Iteration 25, loss = 2.29254053\n",
      "Iteration 26, loss = 2.29049192\n",
      "Iteration 27, loss = 2.28817661\n",
      "Iteration 28, loss = 2.28455837\n",
      "Iteration 29, loss = 2.27991907\n",
      "Iteration 30, loss = 2.27275586\n",
      "Iteration 31, loss = 2.26250855\n",
      "Iteration 32, loss = 2.24607141\n",
      "Iteration 33, loss = 2.21841012\n",
      "Iteration 34, loss = 2.17011274\n",
      "Iteration 35, loss = 2.09236277\n",
      "Iteration 36, loss = 1.99149719\n",
      "Iteration 37, loss = 1.89346031\n",
      "Iteration 38, loss = 1.80924042\n",
      "Iteration 39, loss = 1.73201591\n",
      "Iteration 40, loss = 1.64826301\n",
      "Iteration 41, loss = 1.55123229\n",
      "Iteration 42, loss = 1.44696487\n",
      "Iteration 43, loss = 1.35856255\n",
      "Iteration 44, loss = 1.29455697\n",
      "Iteration 45, loss = 1.24892828\n",
      "Iteration 46, loss = 1.21338741\n",
      "Iteration 47, loss = 1.18324527\n",
      "Iteration 48, loss = 1.15617676\n",
      "Iteration 49, loss = 1.13088346\n",
      "Iteration 50, loss = 1.10693322\n",
      "Iteration 51, loss = 1.08373266\n",
      "Iteration 52, loss = 1.06039704\n",
      "Iteration 53, loss = 1.03726174\n",
      "Iteration 54, loss = 1.01398272\n",
      "Iteration 55, loss = 0.99059363\n",
      "Iteration 56, loss = 0.96734159\n",
      "Iteration 57, loss = 0.94469956\n",
      "Iteration 58, loss = 0.92228821\n",
      "Iteration 59, loss = 0.90056576\n",
      "Iteration 60, loss = 0.87957196\n",
      "Iteration 61, loss = 0.85895858\n",
      "Iteration 62, loss = 0.83848649\n",
      "Iteration 63, loss = 0.81816216\n",
      "Iteration 64, loss = 0.79766364\n",
      "Iteration 65, loss = 0.77693504\n",
      "Iteration 66, loss = 0.75563026\n",
      "Iteration 67, loss = 0.73449097\n",
      "Iteration 68, loss = 0.71317229\n",
      "Iteration 69, loss = 0.69219783\n",
      "Iteration 70, loss = 0.67084296\n",
      "Iteration 71, loss = 0.65030401\n",
      "Iteration 72, loss = 0.63059841\n",
      "Iteration 73, loss = 0.61178757\n",
      "Iteration 74, loss = 0.59428410\n",
      "Iteration 75, loss = 0.57747211\n",
      "Iteration 76, loss = 0.56252370\n",
      "Iteration 77, loss = 0.54834512\n",
      "Iteration 78, loss = 0.53552849\n",
      "Iteration 79, loss = 0.52365930\n",
      "Iteration 80, loss = 0.51242808\n",
      "Iteration 81, loss = 0.50233394\n",
      "Iteration 82, loss = 0.49235657\n",
      "Iteration 83, loss = 0.48312721\n",
      "Iteration 84, loss = 0.47481151\n",
      "Iteration 85, loss = 0.46641697\n",
      "Iteration 86, loss = 0.45850060\n",
      "Iteration 87, loss = 0.45087128\n",
      "Iteration 88, loss = 0.44366806\n",
      "Iteration 89, loss = 0.43671093\n",
      "Iteration 90, loss = 0.42982822\n",
      "Iteration 91, loss = 0.42318442\n",
      "Iteration 92, loss = 0.41671369\n",
      "Iteration 93, loss = 0.41058084\n",
      "Iteration 94, loss = 0.40453802\n",
      "Iteration 95, loss = 0.39867750\n",
      "Iteration 96, loss = 0.39301610\n",
      "Iteration 97, loss = 0.38754288\n",
      "Iteration 98, loss = 0.38203177\n",
      "Iteration 99, loss = 0.37698111\n",
      "Iteration 100, loss = 0.37185658\n",
      "Iteration 101, loss = 0.36716010\n",
      "Iteration 102, loss = 0.36241941\n",
      "Iteration 103, loss = 0.35737610\n",
      "Iteration 104, loss = 0.35290315\n",
      "Iteration 105, loss = 0.34894316\n",
      "Iteration 106, loss = 0.34437564\n",
      "Iteration 107, loss = 0.34036238\n",
      "Iteration 108, loss = 0.33630022\n",
      "Iteration 109, loss = 0.33266146\n",
      "Iteration 110, loss = 0.32905385\n",
      "Iteration 111, loss = 0.32532587\n",
      "Iteration 112, loss = 0.32189776\n",
      "Iteration 113, loss = 0.31833203\n",
      "Iteration 114, loss = 0.31499261\n",
      "Iteration 115, loss = 0.31209842\n",
      "Iteration 116, loss = 0.30864391\n",
      "Iteration 117, loss = 0.30555969\n",
      "Iteration 118, loss = 0.30277159\n",
      "Iteration 119, loss = 0.29977224\n",
      "Iteration 120, loss = 0.29712937\n",
      "Iteration 121, loss = 0.29422468\n",
      "Iteration 122, loss = 0.29149693\n",
      "Iteration 123, loss = 0.28870062\n",
      "Iteration 124, loss = 0.28626876\n",
      "Iteration 125, loss = 0.28374990\n",
      "Iteration 126, loss = 0.28116459\n",
      "Iteration 127, loss = 0.27863512\n",
      "Iteration 128, loss = 0.27626300\n",
      "Iteration 129, loss = 0.27393000\n",
      "Iteration 130, loss = 0.27143724\n",
      "Iteration 131, loss = 0.26932943\n",
      "Iteration 132, loss = 0.26701626\n",
      "Iteration 133, loss = 0.26467477\n",
      "Iteration 134, loss = 0.26232324\n",
      "Iteration 135, loss = 0.26015647\n",
      "Iteration 136, loss = 0.25818625\n",
      "Iteration 137, loss = 0.25602106\n",
      "Iteration 138, loss = 0.25386714\n",
      "Iteration 139, loss = 0.25200474\n",
      "Iteration 140, loss = 0.24963409\n",
      "Iteration 141, loss = 0.24781844\n",
      "Iteration 142, loss = 0.24587202\n",
      "Iteration 143, loss = 0.24374985\n",
      "Iteration 144, loss = 0.24164884\n",
      "Iteration 145, loss = 0.23987761\n",
      "Iteration 146, loss = 0.23792224\n",
      "Iteration 147, loss = 0.23592517\n",
      "Iteration 148, loss = 0.23409879\n",
      "Iteration 149, loss = 0.23244019\n",
      "Iteration 150, loss = 0.23053329\n",
      "Iteration 151, loss = 0.22856790\n",
      "Iteration 152, loss = 0.22674725\n",
      "Iteration 153, loss = 0.22500633\n",
      "Iteration 154, loss = 0.22315628\n",
      "Iteration 155, loss = 0.22156530\n",
      "Iteration 156, loss = 0.21993697\n",
      "Iteration 157, loss = 0.21835250\n",
      "Iteration 158, loss = 0.21651385\n",
      "Iteration 159, loss = 0.21477061\n",
      "Iteration 160, loss = 0.21319753\n",
      "Iteration 161, loss = 0.21160112\n",
      "Iteration 162, loss = 0.21005968\n",
      "Iteration 163, loss = 0.20833304\n",
      "Iteration 164, loss = 0.20656234\n",
      "Iteration 165, loss = 0.20505720\n",
      "Iteration 166, loss = 0.20378051\n",
      "Iteration 167, loss = 0.20238716\n",
      "Iteration 168, loss = 0.20068277\n",
      "Iteration 169, loss = 0.19910722\n",
      "Iteration 170, loss = 0.19767105\n",
      "Iteration 171, loss = 0.19612383\n",
      "Iteration 172, loss = 0.19471173\n",
      "Iteration 173, loss = 0.19328724\n",
      "Iteration 174, loss = 0.19196448\n",
      "Iteration 175, loss = 0.19056594\n",
      "Iteration 176, loss = 0.18918964\n",
      "Iteration 177, loss = 0.18767785\n",
      "Iteration 178, loss = 0.18634915\n",
      "Iteration 179, loss = 0.18521770\n",
      "Iteration 180, loss = 0.18382801\n",
      "Iteration 181, loss = 0.18237593\n",
      "Iteration 182, loss = 0.18119714\n",
      "Iteration 183, loss = 0.17974095\n",
      "Iteration 184, loss = 0.17855347\n",
      "Iteration 185, loss = 0.17740144\n",
      "Iteration 186, loss = 0.17604945\n",
      "Iteration 187, loss = 0.17461800\n",
      "Iteration 188, loss = 0.17358041\n",
      "Iteration 189, loss = 0.17235538\n",
      "Iteration 190, loss = 0.17107951\n",
      "Iteration 191, loss = 0.16987302\n",
      "Iteration 192, loss = 0.16906001\n",
      "Iteration 193, loss = 0.16731672\n",
      "Iteration 194, loss = 0.16650640\n",
      "Iteration 195, loss = 0.16522521\n",
      "Iteration 196, loss = 0.16435297\n",
      "Iteration 197, loss = 0.16311101\n",
      "Iteration 198, loss = 0.16198461\n",
      "Iteration 199, loss = 0.16063350\n",
      "Iteration 200, loss = 0.15970675\n",
      "[CV] END activation=logistic, alpha=0.001, batch_size=50, hidden_layer_sizes=(100, 100, 100), solver=sgd; total time= 5.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.30874827\n",
      "Iteration 2, loss = 2.30373025\n",
      "Iteration 3, loss = 2.30344154\n",
      "Iteration 4, loss = 2.30335829\n",
      "Iteration 5, loss = 2.30321407\n",
      "Iteration 6, loss = 2.30283944\n",
      "Iteration 7, loss = 2.30282211\n",
      "Iteration 8, loss = 2.30257763\n",
      "Iteration 9, loss = 2.30240445\n",
      "Iteration 10, loss = 2.30184267\n",
      "Iteration 11, loss = 2.30193482\n",
      "Iteration 12, loss = 2.30158886\n",
      "Iteration 13, loss = 2.30128277\n",
      "Iteration 14, loss = 2.30103717\n",
      "Iteration 15, loss = 2.30034881\n",
      "Iteration 16, loss = 2.30021321\n",
      "Iteration 17, loss = 2.29995098\n",
      "Iteration 18, loss = 2.29941163\n",
      "Iteration 19, loss = 2.29887015\n",
      "Iteration 20, loss = 2.29829616\n",
      "Iteration 21, loss = 2.29755480\n",
      "Iteration 22, loss = 2.29668986\n",
      "Iteration 23, loss = 2.29583026\n",
      "Iteration 24, loss = 2.29456946\n",
      "Iteration 25, loss = 2.29314104\n",
      "Iteration 26, loss = 2.29161846\n",
      "Iteration 27, loss = 2.28953158\n",
      "Iteration 28, loss = 2.28701537\n",
      "Iteration 29, loss = 2.28371925\n",
      "Iteration 30, loss = 2.27944798\n",
      "Iteration 31, loss = 2.27366250\n",
      "Iteration 32, loss = 2.26548901\n",
      "Iteration 33, loss = 2.25421569\n",
      "Iteration 34, loss = 2.23695678\n",
      "Iteration 35, loss = 2.20887289\n",
      "Iteration 36, loss = 2.16209483\n",
      "Iteration 37, loss = 2.08278982\n",
      "Iteration 38, loss = 1.96238978\n",
      "Iteration 39, loss = 1.81375149\n",
      "Iteration 40, loss = 1.65493772\n",
      "Iteration 41, loss = 1.49350262\n",
      "Iteration 42, loss = 1.34913961\n",
      "Iteration 43, loss = 1.23502580\n",
      "Iteration 44, loss = 1.14452362\n",
      "Iteration 45, loss = 1.06917434\n",
      "Iteration 46, loss = 1.00486794\n",
      "Iteration 47, loss = 0.95103960\n",
      "Iteration 48, loss = 0.90645666\n",
      "Iteration 49, loss = 0.86943170\n",
      "Iteration 50, loss = 0.83859690\n",
      "Iteration 51, loss = 0.81216527\n",
      "Iteration 52, loss = 0.78937971\n",
      "Iteration 53, loss = 0.76869748\n",
      "Iteration 54, loss = 0.74981808\n",
      "Iteration 55, loss = 0.73269023\n",
      "Iteration 56, loss = 0.71623379\n",
      "Iteration 57, loss = 0.70078781\n",
      "Iteration 58, loss = 0.68568455\n",
      "Iteration 59, loss = 0.67130811\n",
      "Iteration 60, loss = 0.65753977\n",
      "Iteration 61, loss = 0.64443911\n",
      "Iteration 62, loss = 0.63173175\n",
      "Iteration 63, loss = 0.61969567\n",
      "Iteration 64, loss = 0.60856676\n",
      "Iteration 65, loss = 0.59748463\n",
      "Iteration 66, loss = 0.58739605\n",
      "Iteration 67, loss = 0.57730189\n",
      "Iteration 68, loss = 0.56805930\n",
      "Iteration 69, loss = 0.55913177\n",
      "Iteration 70, loss = 0.55062634\n",
      "Iteration 71, loss = 0.54248351\n",
      "Iteration 72, loss = 0.53440214\n",
      "Iteration 73, loss = 0.52696847\n",
      "Iteration 74, loss = 0.51959083\n",
      "Iteration 75, loss = 0.51258213\n",
      "Iteration 76, loss = 0.50585957\n",
      "Iteration 77, loss = 0.49909716\n",
      "Iteration 78, loss = 0.49259213\n",
      "Iteration 79, loss = 0.48649368\n",
      "Iteration 80, loss = 0.48031622\n",
      "Iteration 81, loss = 0.47434537\n",
      "Iteration 82, loss = 0.46857351\n",
      "Iteration 83, loss = 0.46281917\n",
      "Iteration 84, loss = 0.45715663\n",
      "Iteration 85, loss = 0.45169169\n",
      "Iteration 86, loss = 0.44631813\n",
      "Iteration 87, loss = 0.44083171\n",
      "Iteration 88, loss = 0.43570730\n",
      "Iteration 89, loss = 0.43035696\n",
      "Iteration 90, loss = 0.42545591\n",
      "Iteration 91, loss = 0.42035964\n",
      "Iteration 92, loss = 0.41541019\n",
      "Iteration 93, loss = 0.41070614\n",
      "Iteration 94, loss = 0.40602570\n",
      "Iteration 95, loss = 0.40112159\n",
      "Iteration 96, loss = 0.39648070\n",
      "Iteration 97, loss = 0.39184804\n",
      "Iteration 98, loss = 0.38756301\n",
      "Iteration 99, loss = 0.38290324\n",
      "Iteration 100, loss = 0.37886644\n",
      "Iteration 101, loss = 0.37458462\n",
      "Iteration 102, loss = 0.37039573\n",
      "Iteration 103, loss = 0.36639395\n",
      "Iteration 104, loss = 0.36229119\n",
      "Iteration 105, loss = 0.35854811\n",
      "Iteration 106, loss = 0.35441376\n",
      "Iteration 107, loss = 0.35096088\n",
      "Iteration 108, loss = 0.34698281\n",
      "Iteration 109, loss = 0.34362703\n",
      "Iteration 110, loss = 0.33999588\n",
      "Iteration 111, loss = 0.33645772\n",
      "Iteration 112, loss = 0.33292427\n",
      "Iteration 113, loss = 0.32972359\n",
      "Iteration 114, loss = 0.32608613\n",
      "Iteration 115, loss = 0.32302162\n",
      "Iteration 116, loss = 0.31972482\n",
      "Iteration 117, loss = 0.31666322\n",
      "Iteration 118, loss = 0.31351465\n",
      "Iteration 119, loss = 0.31063207\n",
      "Iteration 120, loss = 0.30748463\n",
      "Iteration 121, loss = 0.30442786\n",
      "Iteration 122, loss = 0.30170631\n",
      "Iteration 123, loss = 0.29856431\n",
      "Iteration 124, loss = 0.29580825\n",
      "Iteration 125, loss = 0.29315427\n",
      "Iteration 126, loss = 0.29031020\n",
      "Iteration 127, loss = 0.28762026\n",
      "Iteration 128, loss = 0.28514266\n",
      "Iteration 129, loss = 0.28236788\n",
      "Iteration 130, loss = 0.27984610\n",
      "Iteration 131, loss = 0.27750268\n",
      "Iteration 132, loss = 0.27487382\n",
      "Iteration 133, loss = 0.27235051\n",
      "Iteration 134, loss = 0.27002167\n",
      "Iteration 135, loss = 0.26758390\n",
      "Iteration 136, loss = 0.26538347\n",
      "Iteration 137, loss = 0.26302356\n",
      "Iteration 138, loss = 0.26059237\n",
      "Iteration 139, loss = 0.25838411\n",
      "Iteration 140, loss = 0.25625034\n",
      "Iteration 141, loss = 0.25406712\n",
      "Iteration 142, loss = 0.25174046\n",
      "Iteration 143, loss = 0.24978569\n",
      "Iteration 144, loss = 0.24774659\n",
      "Iteration 145, loss = 0.24562975\n",
      "Iteration 146, loss = 0.24357629\n",
      "Iteration 147, loss = 0.24134388\n",
      "Iteration 148, loss = 0.23951570\n",
      "Iteration 149, loss = 0.23737114\n",
      "Iteration 150, loss = 0.23565467\n",
      "Iteration 151, loss = 0.23367296\n",
      "Iteration 152, loss = 0.23179847\n",
      "Iteration 153, loss = 0.22984377\n",
      "Iteration 154, loss = 0.22797548\n",
      "Iteration 155, loss = 0.22626664\n",
      "Iteration 156, loss = 0.22437476\n",
      "Iteration 157, loss = 0.22288797\n",
      "Iteration 158, loss = 0.22088317\n",
      "Iteration 159, loss = 0.21930326\n",
      "Iteration 160, loss = 0.21745569\n",
      "Iteration 161, loss = 0.21571646\n",
      "Iteration 162, loss = 0.21410951\n",
      "Iteration 163, loss = 0.21239121\n",
      "Iteration 164, loss = 0.21076858\n",
      "Iteration 165, loss = 0.20914288\n",
      "Iteration 166, loss = 0.20756974\n",
      "Iteration 167, loss = 0.20603006\n",
      "Iteration 168, loss = 0.20439977\n",
      "Iteration 169, loss = 0.20291304\n",
      "Iteration 170, loss = 0.20131330\n",
      "Iteration 171, loss = 0.19992754\n",
      "Iteration 172, loss = 0.19845846\n",
      "Iteration 173, loss = 0.19687769\n",
      "Iteration 174, loss = 0.19545871\n",
      "Iteration 175, loss = 0.19389495\n",
      "Iteration 176, loss = 0.19268856\n",
      "Iteration 177, loss = 0.19124480\n",
      "Iteration 178, loss = 0.18991756\n",
      "Iteration 179, loss = 0.18837123\n",
      "Iteration 180, loss = 0.18723986\n",
      "Iteration 181, loss = 0.18569722\n",
      "Iteration 182, loss = 0.18433532\n",
      "Iteration 183, loss = 0.18312104\n",
      "Iteration 184, loss = 0.18196090\n",
      "Iteration 185, loss = 0.18065206\n",
      "Iteration 186, loss = 0.17931249\n",
      "Iteration 187, loss = 0.17806404\n",
      "Iteration 188, loss = 0.17677177\n",
      "Iteration 189, loss = 0.17555731\n",
      "Iteration 190, loss = 0.17454268\n",
      "Iteration 191, loss = 0.17324760\n",
      "Iteration 192, loss = 0.17207968\n",
      "Iteration 193, loss = 0.17093340\n",
      "Iteration 194, loss = 0.16982316\n",
      "Iteration 195, loss = 0.16862414\n",
      "Iteration 196, loss = 0.16765302\n",
      "Iteration 197, loss = 0.16632142\n",
      "Iteration 198, loss = 0.16518448\n",
      "Iteration 199, loss = 0.16420554\n",
      "Iteration 200, loss = 0.16298835\n",
      "[CV] END activation=logistic, alpha=0.001, batch_size=50, hidden_layer_sizes=(100, 100, 100), solver=sgd; total time= 6.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.30623336\n",
      "Iteration 2, loss = 2.30326315\n",
      "Iteration 3, loss = 2.30298075\n",
      "Iteration 4, loss = 2.30303744\n",
      "Iteration 5, loss = 2.30274591\n",
      "Iteration 6, loss = 2.30245042\n",
      "Iteration 7, loss = 2.30225117\n",
      "Iteration 8, loss = 2.30179550\n",
      "Iteration 9, loss = 2.30157700\n",
      "Iteration 10, loss = 2.30138112\n",
      "Iteration 11, loss = 2.30114107\n",
      "Iteration 12, loss = 2.30091789\n",
      "Iteration 13, loss = 2.30074584\n",
      "Iteration 14, loss = 2.29992907\n",
      "Iteration 15, loss = 2.29964542\n",
      "Iteration 16, loss = 2.29889420\n",
      "Iteration 17, loss = 2.29849230\n",
      "Iteration 18, loss = 2.29779085\n",
      "Iteration 19, loss = 2.29662786\n",
      "Iteration 20, loss = 2.29578575\n",
      "Iteration 21, loss = 2.29470719\n",
      "Iteration 22, loss = 2.29303006\n",
      "Iteration 23, loss = 2.29105419\n",
      "Iteration 24, loss = 2.28868272\n",
      "Iteration 25, loss = 2.28547969\n",
      "Iteration 26, loss = 2.28102512\n",
      "Iteration 27, loss = 2.27456778\n",
      "Iteration 28, loss = 2.26518472\n",
      "Iteration 29, loss = 2.24963259\n",
      "Iteration 30, loss = 2.22377210\n",
      "Iteration 31, loss = 2.17938808\n",
      "Iteration 32, loss = 2.10754876\n",
      "Iteration 33, loss = 2.01704620\n",
      "Iteration 34, loss = 1.93311661\n",
      "Iteration 35, loss = 1.86536411\n",
      "Iteration 36, loss = 1.80996081\n",
      "Iteration 37, loss = 1.76188431\n",
      "Iteration 38, loss = 1.71515304\n",
      "Iteration 39, loss = 1.66254265\n",
      "Iteration 40, loss = 1.59690474\n",
      "Iteration 41, loss = 1.51130165\n",
      "Iteration 42, loss = 1.41346811\n",
      "Iteration 43, loss = 1.32499656\n",
      "Iteration 44, loss = 1.25827322\n",
      "Iteration 45, loss = 1.20721906\n",
      "Iteration 46, loss = 1.16367290\n",
      "Iteration 47, loss = 1.12351356\n",
      "Iteration 48, loss = 1.08406318\n",
      "Iteration 49, loss = 1.04524370\n",
      "Iteration 50, loss = 1.00666678\n",
      "Iteration 51, loss = 0.96774909\n",
      "Iteration 52, loss = 0.92855749\n",
      "Iteration 53, loss = 0.88819320\n",
      "Iteration 54, loss = 0.84710870\n",
      "Iteration 55, loss = 0.80676347\n",
      "Iteration 56, loss = 0.76873487\n",
      "Iteration 57, loss = 0.73469561\n",
      "Iteration 58, loss = 0.70569520\n",
      "Iteration 59, loss = 0.68075088\n",
      "Iteration 60, loss = 0.65944734\n",
      "Iteration 61, loss = 0.64094723\n",
      "Iteration 62, loss = 0.62468551\n",
      "Iteration 63, loss = 0.61002546\n",
      "Iteration 64, loss = 0.59691556\n",
      "Iteration 65, loss = 0.58470206\n",
      "Iteration 66, loss = 0.57369773\n",
      "Iteration 67, loss = 0.56316554\n",
      "Iteration 68, loss = 0.55328152\n",
      "Iteration 69, loss = 0.54439206\n",
      "Iteration 70, loss = 0.53550098\n",
      "Iteration 71, loss = 0.52728154\n",
      "Iteration 72, loss = 0.51940823\n",
      "Iteration 73, loss = 0.51167318\n",
      "Iteration 74, loss = 0.50454110\n",
      "Iteration 75, loss = 0.49743222\n",
      "Iteration 76, loss = 0.49047421\n",
      "Iteration 77, loss = 0.48385792\n",
      "Iteration 78, loss = 0.47750361\n",
      "Iteration 79, loss = 0.47120353\n",
      "Iteration 80, loss = 0.46488835\n",
      "Iteration 81, loss = 0.45889029\n",
      "Iteration 82, loss = 0.45300832\n",
      "Iteration 83, loss = 0.44710697\n",
      "Iteration 84, loss = 0.44124353\n",
      "Iteration 85, loss = 0.43564247\n",
      "Iteration 86, loss = 0.43010413\n",
      "Iteration 87, loss = 0.42443587\n",
      "Iteration 88, loss = 0.41933583\n",
      "Iteration 89, loss = 0.41400590\n",
      "Iteration 90, loss = 0.40873365\n",
      "Iteration 91, loss = 0.40337941\n",
      "Iteration 92, loss = 0.39838105\n",
      "Iteration 93, loss = 0.39333725\n",
      "Iteration 94, loss = 0.38850116\n",
      "Iteration 95, loss = 0.38360000\n",
      "Iteration 96, loss = 0.37898199\n",
      "Iteration 97, loss = 0.37431993\n",
      "Iteration 98, loss = 0.36975179\n",
      "Iteration 99, loss = 0.36569830\n",
      "Iteration 100, loss = 0.36126876\n",
      "Iteration 101, loss = 0.35720819\n",
      "Iteration 102, loss = 0.35304200\n",
      "Iteration 103, loss = 0.34920924\n",
      "Iteration 104, loss = 0.34525090\n",
      "Iteration 105, loss = 0.34147870\n",
      "Iteration 106, loss = 0.33787229\n",
      "Iteration 107, loss = 0.33442734\n",
      "Iteration 108, loss = 0.33097896\n",
      "Iteration 109, loss = 0.32748311\n",
      "Iteration 110, loss = 0.32445354\n",
      "Iteration 111, loss = 0.32127696\n",
      "Iteration 112, loss = 0.31822243\n",
      "Iteration 113, loss = 0.31504001\n",
      "Iteration 114, loss = 0.31207181\n",
      "Iteration 115, loss = 0.30937205\n",
      "Iteration 116, loss = 0.30624357\n",
      "Iteration 117, loss = 0.30352804\n",
      "Iteration 118, loss = 0.30068019\n",
      "Iteration 119, loss = 0.29806840\n",
      "Iteration 120, loss = 0.29571950\n",
      "Iteration 121, loss = 0.29284364\n",
      "Iteration 122, loss = 0.29016641\n",
      "Iteration 123, loss = 0.28763205\n",
      "Iteration 124, loss = 0.28519499\n",
      "Iteration 125, loss = 0.28270866\n",
      "Iteration 126, loss = 0.28037907\n",
      "Iteration 127, loss = 0.27786032\n",
      "Iteration 128, loss = 0.27563910\n",
      "Iteration 129, loss = 0.27344046\n",
      "Iteration 130, loss = 0.27103550\n",
      "Iteration 131, loss = 0.26870953\n",
      "Iteration 132, loss = 0.26645299\n",
      "Iteration 133, loss = 0.26440436\n",
      "Iteration 134, loss = 0.26204337\n",
      "Iteration 135, loss = 0.25997146\n",
      "Iteration 136, loss = 0.25791417\n",
      "Iteration 137, loss = 0.25581276\n",
      "Iteration 138, loss = 0.25362663\n",
      "Iteration 139, loss = 0.25168038\n",
      "Iteration 140, loss = 0.24973029\n",
      "Iteration 141, loss = 0.24765116\n",
      "Iteration 142, loss = 0.24564080\n",
      "Iteration 143, loss = 0.24380736\n",
      "Iteration 144, loss = 0.24182731\n",
      "Iteration 145, loss = 0.23980528\n",
      "Iteration 146, loss = 0.23805285\n",
      "Iteration 147, loss = 0.23617384\n",
      "Iteration 148, loss = 0.23446869\n",
      "Iteration 149, loss = 0.23251173\n",
      "Iteration 150, loss = 0.23095589\n",
      "Iteration 151, loss = 0.22899595\n",
      "Iteration 152, loss = 0.22730035\n",
      "Iteration 153, loss = 0.22530395\n",
      "Iteration 154, loss = 0.22360367\n",
      "Iteration 155, loss = 0.22218858\n",
      "Iteration 156, loss = 0.22024340\n",
      "Iteration 157, loss = 0.21879875\n",
      "Iteration 158, loss = 0.21708787\n",
      "Iteration 159, loss = 0.21556584\n",
      "Iteration 160, loss = 0.21375404\n",
      "Iteration 161, loss = 0.21201123\n",
      "Iteration 162, loss = 0.21058133\n",
      "Iteration 163, loss = 0.20921507\n",
      "Iteration 164, loss = 0.20760809\n",
      "Iteration 165, loss = 0.20593066\n",
      "Iteration 166, loss = 0.20447214\n",
      "Iteration 167, loss = 0.20304605\n",
      "Iteration 168, loss = 0.20153547\n",
      "Iteration 169, loss = 0.20002895\n",
      "Iteration 170, loss = 0.19855990\n",
      "Iteration 171, loss = 0.19714918\n",
      "Iteration 172, loss = 0.19575363\n",
      "Iteration 173, loss = 0.19447993\n",
      "Iteration 174, loss = 0.19302905\n",
      "Iteration 175, loss = 0.19170817\n",
      "Iteration 176, loss = 0.19008985\n",
      "Iteration 177, loss = 0.18890524\n",
      "Iteration 178, loss = 0.18767215\n",
      "Iteration 179, loss = 0.18632739\n",
      "Iteration 180, loss = 0.18486317\n",
      "Iteration 181, loss = 0.18370801\n",
      "Iteration 182, loss = 0.18233470\n",
      "Iteration 183, loss = 0.18112254\n",
      "Iteration 184, loss = 0.17989314\n",
      "Iteration 185, loss = 0.17867609\n",
      "Iteration 186, loss = 0.17725282\n",
      "Iteration 187, loss = 0.17599468\n",
      "Iteration 188, loss = 0.17504193\n",
      "Iteration 189, loss = 0.17368014\n",
      "Iteration 190, loss = 0.17263492\n",
      "Iteration 191, loss = 0.17134744\n",
      "Iteration 192, loss = 0.17026894\n",
      "Iteration 193, loss = 0.16882293\n",
      "Iteration 194, loss = 0.16783855\n",
      "Iteration 195, loss = 0.16681354\n",
      "Iteration 196, loss = 0.16562277\n",
      "Iteration 197, loss = 0.16448483\n",
      "Iteration 198, loss = 0.16332547\n",
      "Iteration 199, loss = 0.16218891\n",
      "Iteration 200, loss = 0.16137461\n",
      "[CV] END activation=logistic, alpha=0.001, batch_size=50, hidden_layer_sizes=(100, 100, 100), solver=sgd; total time= 5.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.30518541\n",
      "Iteration 2, loss = 2.30372816\n",
      "Iteration 3, loss = 2.30323148\n",
      "Iteration 4, loss = 2.30328829\n",
      "Iteration 5, loss = 2.30339551\n",
      "Iteration 6, loss = 2.30286661\n",
      "Iteration 7, loss = 2.30287934\n",
      "Iteration 8, loss = 2.30238954\n",
      "Iteration 9, loss = 2.30240083\n",
      "Iteration 10, loss = 2.30205236\n",
      "Iteration 11, loss = 2.30195081\n",
      "Iteration 12, loss = 2.30189368\n",
      "Iteration 13, loss = 2.30149551\n",
      "Iteration 14, loss = 2.30067339\n",
      "Iteration 15, loss = 2.30056025\n",
      "Iteration 16, loss = 2.30018027\n",
      "Iteration 17, loss = 2.29973602\n",
      "Iteration 18, loss = 2.29932735\n",
      "Iteration 19, loss = 2.29879393\n",
      "Iteration 20, loss = 2.29812450\n",
      "Iteration 21, loss = 2.29726872\n",
      "Iteration 22, loss = 2.29627897\n",
      "Iteration 23, loss = 2.29528183\n",
      "Iteration 24, loss = 2.29414676\n",
      "Iteration 25, loss = 2.29220043\n",
      "Iteration 26, loss = 2.29019011\n",
      "Iteration 27, loss = 2.28774062\n",
      "Iteration 28, loss = 2.28416733\n",
      "Iteration 29, loss = 2.27931775\n",
      "Iteration 30, loss = 2.27240214\n",
      "Iteration 31, loss = 2.26237624\n",
      "Iteration 32, loss = 2.24623207\n",
      "Iteration 33, loss = 2.21915908\n",
      "Iteration 34, loss = 2.17272545\n",
      "Iteration 35, loss = 2.09624924\n",
      "Iteration 36, loss = 1.99436668\n",
      "Iteration 37, loss = 1.89263216\n",
      "Iteration 38, loss = 1.80665059\n",
      "Iteration 39, loss = 1.73191478\n",
      "Iteration 40, loss = 1.65986686\n",
      "Iteration 41, loss = 1.58232009\n",
      "Iteration 42, loss = 1.49355054\n",
      "Iteration 43, loss = 1.39418258\n",
      "Iteration 44, loss = 1.29617261\n",
      "Iteration 45, loss = 1.21322421\n",
      "Iteration 46, loss = 1.14514196\n",
      "Iteration 47, loss = 1.08693087\n",
      "Iteration 48, loss = 1.03272717\n",
      "Iteration 49, loss = 0.98093334\n",
      "Iteration 50, loss = 0.93293553\n",
      "Iteration 51, loss = 0.89073987\n",
      "Iteration 52, loss = 0.85536114\n",
      "Iteration 53, loss = 0.82562101\n",
      "Iteration 54, loss = 0.80073239\n",
      "Iteration 55, loss = 0.77882661\n",
      "Iteration 56, loss = 0.75972625\n",
      "Iteration 57, loss = 0.74232978\n",
      "Iteration 58, loss = 0.72627480\n",
      "Iteration 59, loss = 0.71137974\n",
      "Iteration 60, loss = 0.69726197\n",
      "Iteration 61, loss = 0.68361087\n",
      "Iteration 62, loss = 0.67073495\n",
      "Iteration 63, loss = 0.65799527\n",
      "Iteration 64, loss = 0.64548632\n",
      "Iteration 65, loss = 0.63353016\n",
      "Iteration 66, loss = 0.62190627\n",
      "Iteration 67, loss = 0.61062834\n",
      "Iteration 68, loss = 0.59952010\n",
      "Iteration 69, loss = 0.58885083\n",
      "Iteration 70, loss = 0.57882297\n",
      "Iteration 71, loss = 0.56896480\n",
      "Iteration 72, loss = 0.55954517\n",
      "Iteration 73, loss = 0.55075852\n",
      "Iteration 74, loss = 0.54240074\n",
      "Iteration 75, loss = 0.53429090\n",
      "Iteration 76, loss = 0.52636872\n",
      "Iteration 77, loss = 0.51891836\n",
      "Iteration 78, loss = 0.51177462\n",
      "Iteration 79, loss = 0.50513499\n",
      "Iteration 80, loss = 0.49828640\n",
      "Iteration 81, loss = 0.49243627\n",
      "Iteration 82, loss = 0.48615613\n",
      "Iteration 83, loss = 0.48028365\n",
      "Iteration 84, loss = 0.47437071\n",
      "Iteration 85, loss = 0.46893436\n",
      "Iteration 86, loss = 0.46362912\n",
      "Iteration 87, loss = 0.45795532\n",
      "Iteration 88, loss = 0.45296127\n",
      "Iteration 89, loss = 0.44799392\n",
      "Iteration 90, loss = 0.44298101\n",
      "Iteration 91, loss = 0.43785502\n",
      "Iteration 92, loss = 0.43302699\n",
      "Iteration 93, loss = 0.42817709\n",
      "Iteration 94, loss = 0.42342039\n",
      "Iteration 95, loss = 0.41880821\n",
      "Iteration 96, loss = 0.41424824\n",
      "Iteration 97, loss = 0.40956264\n",
      "Iteration 98, loss = 0.40495232\n",
      "Iteration 99, loss = 0.40051257\n",
      "Iteration 100, loss = 0.39573908\n",
      "Iteration 101, loss = 0.39135041\n",
      "Iteration 102, loss = 0.38706628\n",
      "Iteration 103, loss = 0.38266279\n",
      "Iteration 104, loss = 0.37822935\n",
      "Iteration 105, loss = 0.37370945\n",
      "Iteration 106, loss = 0.36964198\n",
      "Iteration 107, loss = 0.36543565\n",
      "Iteration 108, loss = 0.36108646\n",
      "Iteration 109, loss = 0.35683636\n",
      "Iteration 110, loss = 0.35297096\n",
      "Iteration 111, loss = 0.34869261\n",
      "Iteration 112, loss = 0.34467737\n",
      "Iteration 113, loss = 0.34067075\n",
      "Iteration 114, loss = 0.33683028\n",
      "Iteration 115, loss = 0.33294486\n",
      "Iteration 116, loss = 0.32942979\n",
      "Iteration 117, loss = 0.32547839\n",
      "Iteration 118, loss = 0.32176410\n",
      "Iteration 119, loss = 0.31830391\n",
      "Iteration 120, loss = 0.31458698\n",
      "Iteration 121, loss = 0.31111403\n",
      "Iteration 122, loss = 0.30778380\n",
      "Iteration 123, loss = 0.30441122\n",
      "Iteration 124, loss = 0.30134719\n",
      "Iteration 125, loss = 0.29783240\n",
      "Iteration 126, loss = 0.29450801\n",
      "Iteration 127, loss = 0.29165318\n",
      "Iteration 128, loss = 0.28848436\n",
      "Iteration 129, loss = 0.28558690\n",
      "Iteration 130, loss = 0.28271665\n",
      "Iteration 131, loss = 0.27974337\n",
      "Iteration 132, loss = 0.27659232\n",
      "Iteration 133, loss = 0.27389768\n",
      "Iteration 134, loss = 0.27126215\n",
      "Iteration 135, loss = 0.26842427\n",
      "Iteration 136, loss = 0.26569000\n",
      "Iteration 137, loss = 0.26297189\n",
      "Iteration 138, loss = 0.26046284\n",
      "Iteration 139, loss = 0.25801070\n",
      "Iteration 140, loss = 0.25519004\n",
      "Iteration 141, loss = 0.25280894\n",
      "Iteration 142, loss = 0.25039252\n",
      "Iteration 143, loss = 0.24783060\n",
      "Iteration 144, loss = 0.24565110\n",
      "Iteration 145, loss = 0.24331598\n",
      "Iteration 146, loss = 0.24098665\n",
      "Iteration 147, loss = 0.23880386\n",
      "Iteration 148, loss = 0.23653001\n",
      "Iteration 149, loss = 0.23423080\n",
      "Iteration 150, loss = 0.23222927\n",
      "Iteration 151, loss = 0.22999944\n",
      "Iteration 152, loss = 0.22783716\n",
      "Iteration 153, loss = 0.22583065\n",
      "Iteration 154, loss = 0.22395737\n",
      "Iteration 155, loss = 0.22183302\n",
      "Iteration 156, loss = 0.21998554\n",
      "Iteration 157, loss = 0.21792416\n",
      "Iteration 158, loss = 0.21603987\n",
      "Iteration 159, loss = 0.21422481\n",
      "Iteration 160, loss = 0.21224699\n",
      "Iteration 161, loss = 0.21041923\n",
      "Iteration 162, loss = 0.20883977\n",
      "Iteration 163, loss = 0.20673640\n",
      "Iteration 164, loss = 0.20518762\n",
      "Iteration 165, loss = 0.20336328\n",
      "Iteration 166, loss = 0.20186592\n",
      "Iteration 167, loss = 0.20009543\n",
      "Iteration 168, loss = 0.19849726\n",
      "Iteration 169, loss = 0.19670120\n",
      "Iteration 170, loss = 0.19541366\n",
      "Iteration 171, loss = 0.19359869\n",
      "Iteration 172, loss = 0.19197317\n",
      "Iteration 173, loss = 0.19073714\n",
      "Iteration 174, loss = 0.18902784\n",
      "Iteration 175, loss = 0.18751043\n",
      "Iteration 176, loss = 0.18617178\n",
      "Iteration 177, loss = 0.18473499\n",
      "Iteration 178, loss = 0.18329007\n",
      "Iteration 179, loss = 0.18177504\n",
      "Iteration 180, loss = 0.18025406\n",
      "Iteration 181, loss = 0.17900689\n",
      "Iteration 182, loss = 0.17758465\n",
      "Iteration 183, loss = 0.17622900\n",
      "Iteration 184, loss = 0.17498931\n",
      "Iteration 185, loss = 0.17360742\n",
      "Iteration 186, loss = 0.17253034\n",
      "Iteration 187, loss = 0.17093130\n",
      "Iteration 188, loss = 0.16987838\n",
      "Iteration 189, loss = 0.16845492\n",
      "Iteration 190, loss = 0.16734254\n",
      "Iteration 191, loss = 0.16597181\n",
      "Iteration 192, loss = 0.16502491\n",
      "Iteration 193, loss = 0.16364054\n",
      "Iteration 194, loss = 0.16255305\n",
      "Iteration 195, loss = 0.16123205\n",
      "Iteration 196, loss = 0.16001368\n",
      "Iteration 197, loss = 0.15915856\n",
      "Iteration 198, loss = 0.15786943\n",
      "Iteration 199, loss = 0.15686476\n",
      "Iteration 200, loss = 0.15561250\n",
      "[CV] END activation=logistic, alpha=0.001, batch_size=50, hidden_layer_sizes=(100, 100, 100), solver=sgd; total time= 5.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.30503395\n",
      "Iteration 2, loss = 2.30344802\n",
      "Iteration 3, loss = 2.30311787\n",
      "Iteration 4, loss = 2.30321800\n",
      "Iteration 5, loss = 2.30298616\n",
      "Iteration 6, loss = 2.30269237\n",
      "Iteration 7, loss = 2.30256701\n",
      "Iteration 8, loss = 2.30238618\n",
      "Iteration 9, loss = 2.30200330\n",
      "Iteration 10, loss = 2.30168556\n",
      "Iteration 11, loss = 2.30178685\n",
      "Iteration 12, loss = 2.30126212\n",
      "Iteration 13, loss = 2.30093537\n",
      "Iteration 14, loss = 2.30049787\n",
      "Iteration 15, loss = 2.30022781\n",
      "Iteration 16, loss = 2.30000347\n",
      "Iteration 17, loss = 2.29941438\n",
      "Iteration 18, loss = 2.29892382\n",
      "Iteration 19, loss = 2.29802170\n",
      "Iteration 20, loss = 2.29722428\n",
      "Iteration 21, loss = 2.29639408\n",
      "Iteration 22, loss = 2.29513787\n",
      "Iteration 23, loss = 2.29388280\n",
      "Iteration 24, loss = 2.29221991\n",
      "Iteration 25, loss = 2.29016867\n",
      "Iteration 26, loss = 2.28740811\n",
      "Iteration 27, loss = 2.28363125\n",
      "Iteration 28, loss = 2.27864576\n",
      "Iteration 29, loss = 2.27105848\n",
      "Iteration 30, loss = 2.25997395\n",
      "Iteration 31, loss = 2.24158563\n",
      "Iteration 32, loss = 2.21015717\n",
      "Iteration 33, loss = 2.15500171\n",
      "Iteration 34, loss = 2.06759112\n",
      "Iteration 35, loss = 1.96218262\n",
      "Iteration 36, loss = 1.86971269\n",
      "Iteration 37, loss = 1.79742350\n",
      "Iteration 38, loss = 1.73829620\n",
      "Iteration 39, loss = 1.68160744\n",
      "Iteration 40, loss = 1.61786186\n",
      "Iteration 41, loss = 1.54023517\n",
      "Iteration 42, loss = 1.45340145\n",
      "Iteration 43, loss = 1.37113496\n",
      "Iteration 44, loss = 1.29845279\n",
      "Iteration 45, loss = 1.23085461\n",
      "Iteration 46, loss = 1.16030651\n",
      "Iteration 47, loss = 1.08571970\n",
      "Iteration 48, loss = 1.01345923\n",
      "Iteration 49, loss = 0.95126088\n",
      "Iteration 50, loss = 0.90203587\n",
      "Iteration 51, loss = 0.86399826\n",
      "Iteration 52, loss = 0.83360277\n",
      "Iteration 53, loss = 0.80871569\n",
      "Iteration 54, loss = 0.78726448\n",
      "Iteration 55, loss = 0.76874132\n",
      "Iteration 56, loss = 0.75182028\n",
      "Iteration 57, loss = 0.73618686\n",
      "Iteration 58, loss = 0.72152606\n",
      "Iteration 59, loss = 0.70778648\n",
      "Iteration 60, loss = 0.69473924\n",
      "Iteration 61, loss = 0.68176395\n",
      "Iteration 62, loss = 0.66968628\n",
      "Iteration 63, loss = 0.65790225\n",
      "Iteration 64, loss = 0.64675397\n",
      "Iteration 65, loss = 0.63583697\n",
      "Iteration 66, loss = 0.62548912\n",
      "Iteration 67, loss = 0.61542462\n",
      "Iteration 68, loss = 0.60579225\n",
      "Iteration 69, loss = 0.59677873\n",
      "Iteration 70, loss = 0.58774055\n",
      "Iteration 71, loss = 0.57907462\n",
      "Iteration 72, loss = 0.57087403\n",
      "Iteration 73, loss = 0.56289317\n",
      "Iteration 74, loss = 0.55518367\n",
      "Iteration 75, loss = 0.54768839\n",
      "Iteration 76, loss = 0.54053009\n",
      "Iteration 77, loss = 0.53367863\n",
      "Iteration 78, loss = 0.52697754\n",
      "Iteration 79, loss = 0.52026729\n",
      "Iteration 80, loss = 0.51388244\n",
      "Iteration 81, loss = 0.50769161\n",
      "Iteration 82, loss = 0.50163740\n",
      "Iteration 83, loss = 0.49574335\n",
      "Iteration 84, loss = 0.48994583\n",
      "Iteration 85, loss = 0.48428584\n",
      "Iteration 86, loss = 0.47861950\n",
      "Iteration 87, loss = 0.47330835\n",
      "Iteration 88, loss = 0.46808195\n",
      "Iteration 89, loss = 0.46261452\n",
      "Iteration 90, loss = 0.45745425\n",
      "Iteration 91, loss = 0.45242619\n",
      "Iteration 92, loss = 0.44739878\n",
      "Iteration 93, loss = 0.44220895\n",
      "Iteration 94, loss = 0.43722252\n",
      "Iteration 95, loss = 0.43227730\n",
      "Iteration 96, loss = 0.42735091\n",
      "Iteration 97, loss = 0.42301176\n",
      "Iteration 98, loss = 0.41806590\n",
      "Iteration 99, loss = 0.41322191\n",
      "Iteration 100, loss = 0.40854875\n",
      "Iteration 101, loss = 0.40362838\n",
      "Iteration 102, loss = 0.39906106\n",
      "Iteration 103, loss = 0.39458880\n",
      "Iteration 104, loss = 0.39000056\n",
      "Iteration 105, loss = 0.38540807\n",
      "Iteration 106, loss = 0.38096462\n",
      "Iteration 107, loss = 0.37663044\n",
      "Iteration 108, loss = 0.37222920\n",
      "Iteration 109, loss = 0.36770936\n",
      "Iteration 110, loss = 0.36342787\n",
      "Iteration 111, loss = 0.35901491\n",
      "Iteration 112, loss = 0.35491333\n",
      "Iteration 113, loss = 0.35079103\n",
      "Iteration 114, loss = 0.34662835\n",
      "Iteration 115, loss = 0.34251408\n",
      "Iteration 116, loss = 0.33840418\n",
      "Iteration 117, loss = 0.33463341\n",
      "Iteration 118, loss = 0.33052096\n",
      "Iteration 119, loss = 0.32670907\n",
      "Iteration 120, loss = 0.32282917\n",
      "Iteration 121, loss = 0.31905621\n",
      "Iteration 122, loss = 0.31537854\n",
      "Iteration 123, loss = 0.31168481\n",
      "Iteration 124, loss = 0.30819111\n",
      "Iteration 125, loss = 0.30461353\n",
      "Iteration 126, loss = 0.30112508\n",
      "Iteration 127, loss = 0.29795748\n",
      "Iteration 128, loss = 0.29465316\n",
      "Iteration 129, loss = 0.29135983\n",
      "Iteration 130, loss = 0.28795060\n",
      "Iteration 131, loss = 0.28471935\n",
      "Iteration 132, loss = 0.28176067\n",
      "Iteration 133, loss = 0.27880278\n",
      "Iteration 134, loss = 0.27551660\n",
      "Iteration 135, loss = 0.27268620\n",
      "Iteration 136, loss = 0.27011493\n",
      "Iteration 137, loss = 0.26728371\n",
      "Iteration 138, loss = 0.26438294\n",
      "Iteration 139, loss = 0.26177238\n",
      "Iteration 140, loss = 0.25905003\n",
      "Iteration 141, loss = 0.25655846\n",
      "Iteration 142, loss = 0.25395704\n",
      "Iteration 143, loss = 0.25149202\n",
      "Iteration 144, loss = 0.24907078\n",
      "Iteration 145, loss = 0.24680892\n",
      "Iteration 146, loss = 0.24447824\n",
      "Iteration 147, loss = 0.24210934\n",
      "Iteration 148, loss = 0.23973925\n",
      "Iteration 149, loss = 0.23776269\n",
      "Iteration 150, loss = 0.23541929\n",
      "Iteration 151, loss = 0.23337952\n",
      "Iteration 152, loss = 0.23129189\n",
      "Iteration 153, loss = 0.22910536\n",
      "Iteration 154, loss = 0.22689390\n",
      "Iteration 155, loss = 0.22518806\n",
      "Iteration 156, loss = 0.22318987\n",
      "Iteration 157, loss = 0.22136393\n",
      "Iteration 158, loss = 0.21925613\n",
      "Iteration 159, loss = 0.21760758\n",
      "Iteration 160, loss = 0.21576279\n",
      "Iteration 161, loss = 0.21389140\n",
      "Iteration 162, loss = 0.21222862\n",
      "Iteration 163, loss = 0.21031208\n",
      "Iteration 164, loss = 0.20872245\n",
      "Iteration 165, loss = 0.20698169\n",
      "Iteration 166, loss = 0.20531551\n",
      "Iteration 167, loss = 0.20374215\n",
      "Iteration 168, loss = 0.20216412\n",
      "Iteration 169, loss = 0.20042933\n",
      "Iteration 170, loss = 0.19911354\n",
      "Iteration 171, loss = 0.19755657\n",
      "Iteration 172, loss = 0.19581036\n",
      "Iteration 173, loss = 0.19452080\n",
      "Iteration 174, loss = 0.19286362\n",
      "Iteration 175, loss = 0.19145819\n",
      "Iteration 176, loss = 0.19006794\n",
      "Iteration 177, loss = 0.18853434\n",
      "Iteration 178, loss = 0.18732141\n",
      "Iteration 179, loss = 0.18578006\n",
      "Iteration 180, loss = 0.18441751\n",
      "Iteration 181, loss = 0.18307793\n",
      "Iteration 182, loss = 0.18188348\n",
      "Iteration 183, loss = 0.18031621\n",
      "Iteration 184, loss = 0.17910485\n",
      "Iteration 185, loss = 0.17768936\n",
      "Iteration 186, loss = 0.17658011\n",
      "Iteration 187, loss = 0.17524143\n",
      "Iteration 188, loss = 0.17397114\n",
      "Iteration 189, loss = 0.17268428\n",
      "Iteration 190, loss = 0.17154876\n",
      "Iteration 191, loss = 0.17029509\n",
      "Iteration 192, loss = 0.16905908\n",
      "Iteration 193, loss = 0.16796177\n",
      "Iteration 194, loss = 0.16693619\n",
      "Iteration 195, loss = 0.16569161\n",
      "Iteration 196, loss = 0.16445866\n",
      "Iteration 197, loss = 0.16342171\n",
      "Iteration 198, loss = 0.16233904\n",
      "Iteration 199, loss = 0.16115415\n",
      "Iteration 200, loss = 0.16022941\n",
      "[CV] END activation=logistic, alpha=0.001, batch_size=50, hidden_layer_sizes=(100, 100, 100), solver=sgd; total time= 5.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.86744425\n",
      "Iteration 2, loss = 0.34284351\n",
      "Iteration 3, loss = 0.27103675\n",
      "Iteration 4, loss = 0.23273032\n",
      "Iteration 5, loss = 0.20569005\n",
      "Iteration 6, loss = 0.18436738\n",
      "Iteration 7, loss = 0.16760289\n",
      "Iteration 8, loss = 0.15275543\n",
      "Iteration 9, loss = 0.14082053\n",
      "Iteration 10, loss = 0.12968976\n",
      "Iteration 11, loss = 0.12047689\n",
      "Iteration 12, loss = 0.11165942\n",
      "Iteration 13, loss = 0.10353716\n",
      "Iteration 14, loss = 0.09692862\n",
      "Iteration 15, loss = 0.09100640\n",
      "Iteration 16, loss = 0.08530516\n",
      "Iteration 17, loss = 0.07985276\n",
      "Iteration 18, loss = 0.07551950\n",
      "Iteration 19, loss = 0.07112874\n",
      "Iteration 20, loss = 0.06710867\n",
      "Iteration 21, loss = 0.06353470\n",
      "Iteration 22, loss = 0.06056231\n",
      "Iteration 23, loss = 0.05714542\n",
      "Iteration 24, loss = 0.05451488\n",
      "Iteration 25, loss = 0.05210886\n",
      "Iteration 26, loss = 0.04954144\n",
      "Iteration 27, loss = 0.04758074\n",
      "Iteration 28, loss = 0.04553592\n",
      "Iteration 29, loss = 0.04318504\n",
      "Iteration 30, loss = 0.04160207\n",
      "Iteration 31, loss = 0.03983854\n",
      "Iteration 32, loss = 0.03841244\n",
      "Iteration 33, loss = 0.03702523\n",
      "Iteration 34, loss = 0.03603408\n",
      "Iteration 35, loss = 0.03445466\n",
      "Iteration 36, loss = 0.03339059\n",
      "Iteration 37, loss = 0.03222155\n",
      "Iteration 38, loss = 0.03137198\n",
      "Iteration 39, loss = 0.03042178\n",
      "Iteration 40, loss = 0.02970637\n",
      "Iteration 41, loss = 0.02896398\n",
      "Iteration 42, loss = 0.02808380\n",
      "Iteration 43, loss = 0.02751194\n",
      "Iteration 44, loss = 0.02691822\n",
      "Iteration 45, loss = 0.02640663\n",
      "Iteration 46, loss = 0.02576970\n",
      "Iteration 47, loss = 0.02539517\n",
      "Iteration 48, loss = 0.02493888\n",
      "Iteration 49, loss = 0.02456841\n",
      "Iteration 50, loss = 0.02406968\n",
      "Iteration 51, loss = 0.02393708\n",
      "Iteration 52, loss = 0.02340787\n",
      "Iteration 53, loss = 0.02341880\n",
      "Iteration 54, loss = 0.02272456\n",
      "Iteration 55, loss = 0.02273892\n",
      "Iteration 56, loss = 0.02229033\n",
      "Iteration 57, loss = 0.02211362\n",
      "Iteration 58, loss = 0.02185122\n",
      "Iteration 59, loss = 0.02163863\n",
      "Iteration 60, loss = 0.02135899\n",
      "Iteration 61, loss = 0.02125980\n",
      "Iteration 62, loss = 0.02123705\n",
      "Iteration 63, loss = 0.02102816\n",
      "Iteration 64, loss = 0.02070706\n",
      "Iteration 65, loss = 0.02054421\n",
      "Iteration 66, loss = 0.02058178\n",
      "Iteration 67, loss = 0.02053035\n",
      "Iteration 68, loss = 0.02017636\n",
      "Iteration 69, loss = 0.02012744\n",
      "Iteration 70, loss = 0.02000654\n",
      "Iteration 71, loss = 0.01992090\n",
      "Iteration 72, loss = 0.01982953\n",
      "Iteration 73, loss = 0.01975810\n",
      "Iteration 74, loss = 0.01958512\n",
      "Iteration 75, loss = 0.01956820\n",
      "Iteration 76, loss = 0.01945118\n",
      "Iteration 77, loss = 0.01933791\n",
      "Iteration 78, loss = 0.01938401\n",
      "Iteration 79, loss = 0.01934285\n",
      "Iteration 80, loss = 0.01911854\n",
      "Iteration 81, loss = 0.01916562\n",
      "Iteration 82, loss = 0.01905528\n",
      "Iteration 83, loss = 0.01896989\n",
      "Iteration 84, loss = 0.01891741\n",
      "Iteration 85, loss = 0.01890590\n",
      "Iteration 86, loss = 0.01878979\n",
      "Iteration 87, loss = 0.01871027\n",
      "Iteration 88, loss = 0.01873284\n",
      "Iteration 89, loss = 0.01880040\n",
      "Iteration 90, loss = 0.01869697\n",
      "Iteration 91, loss = 0.01854601\n",
      "Iteration 92, loss = 0.01876901\n",
      "Iteration 93, loss = 0.01863098\n",
      "Iteration 94, loss = 0.01833751\n",
      "Iteration 95, loss = 0.01824924\n",
      "Iteration 96, loss = 0.01856390\n",
      "Iteration 97, loss = 0.01852074\n",
      "Iteration 98, loss = 0.01828012\n",
      "Iteration 99, loss = 0.01819472\n",
      "Iteration 100, loss = 0.01828448\n",
      "Iteration 101, loss = 0.01831615\n",
      "Iteration 102, loss = 0.01807948\n",
      "Iteration 103, loss = 0.01837871\n",
      "Iteration 104, loss = 0.01828116\n",
      "Iteration 105, loss = 0.01822688\n",
      "Iteration 106, loss = 0.01801232\n",
      "Iteration 107, loss = 0.01784521\n",
      "Iteration 108, loss = 0.01814426\n",
      "Iteration 109, loss = 0.01794181\n",
      "Iteration 110, loss = 0.01832722\n",
      "Iteration 111, loss = 0.01818579\n",
      "Iteration 112, loss = 0.01793184\n",
      "Iteration 113, loss = 0.01776728\n",
      "Iteration 114, loss = 0.01776467\n",
      "Iteration 115, loss = 0.01803031\n",
      "Iteration 116, loss = 0.01804023\n",
      "Iteration 117, loss = 0.01789164\n",
      "Iteration 118, loss = 0.01771243\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.001, batch_size=100, hidden_layer_sizes=(100,), solver=adam; total time= 1.6min\n",
      "Iteration 1, loss = 0.86736309\n",
      "Iteration 2, loss = 0.34628571\n",
      "Iteration 3, loss = 0.27407902\n",
      "Iteration 4, loss = 0.23606719\n",
      "Iteration 5, loss = 0.20931652\n",
      "Iteration 6, loss = 0.18812410\n",
      "Iteration 7, loss = 0.17035382\n",
      "Iteration 8, loss = 0.15561492\n",
      "Iteration 9, loss = 0.14332904\n",
      "Iteration 10, loss = 0.13239712\n",
      "Iteration 11, loss = 0.12302249\n",
      "Iteration 12, loss = 0.11406493\n",
      "Iteration 13, loss = 0.10703915\n",
      "Iteration 14, loss = 0.09992873\n",
      "Iteration 15, loss = 0.09393139\n",
      "Iteration 16, loss = 0.08835583\n",
      "Iteration 17, loss = 0.08283949\n",
      "Iteration 18, loss = 0.07842309\n",
      "Iteration 19, loss = 0.07384372\n",
      "Iteration 20, loss = 0.07027079\n",
      "Iteration 21, loss = 0.06612173\n",
      "Iteration 22, loss = 0.06280982\n",
      "Iteration 23, loss = 0.05995511\n",
      "Iteration 24, loss = 0.05687628\n",
      "Iteration 25, loss = 0.05413744\n",
      "Iteration 26, loss = 0.05153302\n",
      "Iteration 27, loss = 0.04936079\n",
      "Iteration 28, loss = 0.04709457\n",
      "Iteration 29, loss = 0.04500149\n",
      "Iteration 30, loss = 0.04336880\n",
      "Iteration 31, loss = 0.04162661\n",
      "Iteration 32, loss = 0.04000269\n",
      "Iteration 33, loss = 0.03833890\n",
      "Iteration 34, loss = 0.03730336\n",
      "Iteration 35, loss = 0.03581715\n",
      "Iteration 36, loss = 0.03480166\n",
      "Iteration 37, loss = 0.03350160\n",
      "Iteration 38, loss = 0.03262051\n",
      "Iteration 39, loss = 0.03150581\n",
      "Iteration 40, loss = 0.03073535\n",
      "Iteration 41, loss = 0.03001470\n",
      "Iteration 42, loss = 0.02939068\n",
      "Iteration 43, loss = 0.02863016\n",
      "Iteration 44, loss = 0.02777333\n",
      "Iteration 45, loss = 0.02730127\n",
      "Iteration 46, loss = 0.02665551\n",
      "Iteration 47, loss = 0.02634431\n",
      "Iteration 48, loss = 0.02581066\n",
      "Iteration 49, loss = 0.02543798\n",
      "Iteration 50, loss = 0.02498820\n",
      "Iteration 51, loss = 0.02467204\n",
      "Iteration 52, loss = 0.02402415\n",
      "Iteration 53, loss = 0.02370534\n",
      "Iteration 54, loss = 0.02362927\n",
      "Iteration 55, loss = 0.02328656\n",
      "Iteration 56, loss = 0.02290563\n",
      "Iteration 57, loss = 0.02261644\n",
      "Iteration 58, loss = 0.02235260\n",
      "Iteration 59, loss = 0.02222702\n",
      "Iteration 60, loss = 0.02207982\n",
      "Iteration 61, loss = 0.02171787\n",
      "Iteration 62, loss = 0.02138913\n",
      "Iteration 63, loss = 0.02126791\n",
      "Iteration 64, loss = 0.02124890\n",
      "Iteration 65, loss = 0.02118929\n",
      "Iteration 66, loss = 0.02093863\n",
      "Iteration 67, loss = 0.02079871\n",
      "Iteration 68, loss = 0.02050193\n",
      "Iteration 69, loss = 0.02051297\n",
      "Iteration 70, loss = 0.02029078\n",
      "Iteration 71, loss = 0.02013287\n",
      "Iteration 72, loss = 0.02026088\n",
      "Iteration 73, loss = 0.01999839\n",
      "Iteration 74, loss = 0.01987640\n",
      "Iteration 75, loss = 0.01994611\n",
      "Iteration 76, loss = 0.01995425\n",
      "Iteration 77, loss = 0.01965219\n",
      "Iteration 78, loss = 0.01954119\n",
      "Iteration 79, loss = 0.01954844\n",
      "Iteration 80, loss = 0.01936061\n",
      "Iteration 81, loss = 0.01927643\n",
      "Iteration 82, loss = 0.01940258\n",
      "Iteration 83, loss = 0.01913347\n",
      "Iteration 84, loss = 0.01926725\n",
      "Iteration 85, loss = 0.01907177\n",
      "Iteration 86, loss = 0.01919013\n",
      "Iteration 87, loss = 0.01883385\n",
      "Iteration 88, loss = 0.01872254\n",
      "Iteration 89, loss = 0.01909443\n",
      "Iteration 90, loss = 0.01871200\n",
      "Iteration 91, loss = 0.01873804\n",
      "Iteration 92, loss = 0.01878153\n",
      "Iteration 93, loss = 0.01865974\n",
      "Iteration 94, loss = 0.01899356\n",
      "Iteration 95, loss = 0.01866527\n",
      "Iteration 96, loss = 0.01841659\n",
      "Iteration 97, loss = 0.01847402\n",
      "Iteration 98, loss = 0.01843625\n",
      "Iteration 99, loss = 0.01833872\n",
      "Iteration 100, loss = 0.01849728\n",
      "Iteration 101, loss = 0.01846755\n",
      "Iteration 102, loss = 0.01857910\n",
      "Iteration 103, loss = 0.01825938\n",
      "Iteration 104, loss = 0.01821289\n",
      "Iteration 105, loss = 0.01807350\n",
      "Iteration 106, loss = 0.01824248\n",
      "Iteration 107, loss = 0.01811198\n",
      "Iteration 108, loss = 0.01832013\n",
      "Iteration 109, loss = 0.01859028\n",
      "Iteration 110, loss = 0.01809221\n",
      "Iteration 111, loss = 0.01802694\n",
      "Iteration 112, loss = 0.01796626\n",
      "Iteration 113, loss = 0.01779197\n",
      "Iteration 114, loss = 0.01805183\n",
      "Iteration 115, loss = 0.01801710\n",
      "Iteration 116, loss = 0.01841385\n",
      "Iteration 117, loss = 0.01825035\n",
      "Iteration 118, loss = 0.01782152\n",
      "Iteration 119, loss = 0.01780679\n",
      "Iteration 120, loss = 0.01784553\n",
      "Iteration 121, loss = 0.01778323\n",
      "Iteration 122, loss = 0.01797555\n",
      "Iteration 123, loss = 0.01801845\n",
      "Iteration 124, loss = 0.01794431\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.001, batch_size=100, hidden_layer_sizes=(100,), solver=adam; total time= 1.7min\n",
      "Iteration 1, loss = 0.85994421\n",
      "Iteration 2, loss = 0.34187902\n",
      "Iteration 3, loss = 0.26881725\n",
      "Iteration 4, loss = 0.22948390\n",
      "Iteration 5, loss = 0.20231750\n",
      "Iteration 6, loss = 0.18149329\n",
      "Iteration 7, loss = 0.16474963\n",
      "Iteration 8, loss = 0.15094206\n",
      "Iteration 9, loss = 0.13879160\n",
      "Iteration 10, loss = 0.12775903\n",
      "Iteration 11, loss = 0.11925643\n",
      "Iteration 12, loss = 0.11119232\n",
      "Iteration 13, loss = 0.10340927\n",
      "Iteration 14, loss = 0.09689397\n",
      "Iteration 15, loss = 0.09115227\n",
      "Iteration 16, loss = 0.08511082\n",
      "Iteration 17, loss = 0.08029147\n",
      "Iteration 18, loss = 0.07581954\n",
      "Iteration 19, loss = 0.07146119\n",
      "Iteration 20, loss = 0.06757170\n",
      "Iteration 21, loss = 0.06396397\n",
      "Iteration 22, loss = 0.06049697\n",
      "Iteration 23, loss = 0.05768974\n",
      "Iteration 24, loss = 0.05472308\n",
      "Iteration 25, loss = 0.05217242\n",
      "Iteration 26, loss = 0.04951649\n",
      "Iteration 27, loss = 0.04737762\n",
      "Iteration 28, loss = 0.04565810\n",
      "Iteration 29, loss = 0.04351124\n",
      "Iteration 30, loss = 0.04171841\n",
      "Iteration 31, loss = 0.03991386\n",
      "Iteration 32, loss = 0.03799114\n",
      "Iteration 33, loss = 0.03712391\n",
      "Iteration 34, loss = 0.03585545\n",
      "Iteration 35, loss = 0.03443212\n",
      "Iteration 36, loss = 0.03359398\n",
      "Iteration 37, loss = 0.03205917\n",
      "Iteration 38, loss = 0.03109187\n",
      "Iteration 39, loss = 0.03030280\n",
      "Iteration 40, loss = 0.02951466\n",
      "Iteration 41, loss = 0.02868938\n",
      "Iteration 42, loss = 0.02806713\n",
      "Iteration 43, loss = 0.02728799\n",
      "Iteration 44, loss = 0.02690128\n",
      "Iteration 45, loss = 0.02626229\n",
      "Iteration 46, loss = 0.02569684\n",
      "Iteration 47, loss = 0.02514438\n",
      "Iteration 48, loss = 0.02492087\n",
      "Iteration 49, loss = 0.02442716\n",
      "Iteration 50, loss = 0.02393943\n",
      "Iteration 51, loss = 0.02381075\n",
      "Iteration 52, loss = 0.02332692\n",
      "Iteration 53, loss = 0.02293543\n",
      "Iteration 54, loss = 0.02286547\n",
      "Iteration 55, loss = 0.02269637\n",
      "Iteration 56, loss = 0.02224059\n",
      "Iteration 57, loss = 0.02192419\n",
      "Iteration 58, loss = 0.02165186\n",
      "Iteration 59, loss = 0.02135635\n",
      "Iteration 60, loss = 0.02138240\n",
      "Iteration 61, loss = 0.02109575\n",
      "Iteration 62, loss = 0.02095693\n",
      "Iteration 63, loss = 0.02078587\n",
      "Iteration 64, loss = 0.02057974\n",
      "Iteration 65, loss = 0.02029852\n",
      "Iteration 66, loss = 0.02029141\n",
      "Iteration 67, loss = 0.02026615\n",
      "Iteration 68, loss = 0.02013736\n",
      "Iteration 69, loss = 0.01992474\n",
      "Iteration 70, loss = 0.01991720\n",
      "Iteration 71, loss = 0.01971421\n",
      "Iteration 72, loss = 0.01962590\n",
      "Iteration 73, loss = 0.01954827\n",
      "Iteration 74, loss = 0.01954929\n",
      "Iteration 75, loss = 0.01920056\n",
      "Iteration 76, loss = 0.01907740\n",
      "Iteration 77, loss = 0.01914854\n",
      "Iteration 78, loss = 0.01921521\n",
      "Iteration 79, loss = 0.01897191\n",
      "Iteration 80, loss = 0.01893469\n",
      "Iteration 81, loss = 0.01885352\n",
      "Iteration 82, loss = 0.01878802\n",
      "Iteration 83, loss = 0.01864065\n",
      "Iteration 84, loss = 0.01882877\n",
      "Iteration 85, loss = 0.01870244\n",
      "Iteration 86, loss = 0.01837735\n",
      "Iteration 87, loss = 0.01858911\n",
      "Iteration 88, loss = 0.01856459\n",
      "Iteration 89, loss = 0.01834802\n",
      "Iteration 90, loss = 0.01860966\n",
      "Iteration 91, loss = 0.01828472\n",
      "Iteration 92, loss = 0.01819025\n",
      "Iteration 93, loss = 0.01825536\n",
      "Iteration 94, loss = 0.01852084\n",
      "Iteration 95, loss = 0.01819767\n",
      "Iteration 96, loss = 0.01794430\n",
      "Iteration 97, loss = 0.01807399\n",
      "Iteration 98, loss = 0.01800805\n",
      "Iteration 99, loss = 0.01805808\n",
      "Iteration 100, loss = 0.01797912\n",
      "Iteration 101, loss = 0.01805740\n",
      "Iteration 102, loss = 0.01788575\n",
      "Iteration 103, loss = 0.01790918\n",
      "Iteration 104, loss = 0.01805507\n",
      "Iteration 105, loss = 0.01770863\n",
      "Iteration 106, loss = 0.01796432\n",
      "Iteration 107, loss = 0.01834981\n",
      "Iteration 108, loss = 0.01777292\n",
      "Iteration 109, loss = 0.01758604\n",
      "Iteration 110, loss = 0.01757491\n",
      "Iteration 111, loss = 0.01784743\n",
      "Iteration 112, loss = 0.01762774\n",
      "Iteration 113, loss = 0.01745051\n",
      "Iteration 114, loss = 0.01766939\n",
      "Iteration 115, loss = 0.01771215\n",
      "Iteration 116, loss = 0.01751935\n",
      "Iteration 117, loss = 0.01758635\n",
      "Iteration 118, loss = 0.01764687\n",
      "Iteration 119, loss = 0.01753248\n",
      "Iteration 120, loss = 0.01744160\n",
      "Iteration 121, loss = 0.01767210\n",
      "Iteration 122, loss = 0.01762711\n",
      "Iteration 123, loss = 0.01731641\n",
      "Iteration 124, loss = 0.01723794\n",
      "Iteration 125, loss = 0.01753861\n",
      "Iteration 126, loss = 0.01761882\n",
      "Iteration 127, loss = 0.01780613\n",
      "Iteration 128, loss = 0.01804415\n",
      "Iteration 129, loss = 0.01714803\n",
      "Iteration 130, loss = 0.01716516\n",
      "Iteration 131, loss = 0.01723959\n",
      "Iteration 132, loss = 0.01736670\n",
      "Iteration 133, loss = 0.01717349\n",
      "Iteration 134, loss = 0.01728238\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.001, batch_size=100, hidden_layer_sizes=(100,), solver=adam; total time= 1.8min\n",
      "Iteration 1, loss = 0.86943503\n",
      "Iteration 2, loss = 0.34302818\n",
      "Iteration 3, loss = 0.26978844\n",
      "Iteration 4, loss = 0.23147418\n",
      "Iteration 5, loss = 0.20529198\n",
      "Iteration 6, loss = 0.18398694\n",
      "Iteration 7, loss = 0.16718836\n",
      "Iteration 8, loss = 0.15296300\n",
      "Iteration 9, loss = 0.14063737\n",
      "Iteration 10, loss = 0.13033587\n",
      "Iteration 11, loss = 0.12082083\n",
      "Iteration 12, loss = 0.11270722\n",
      "Iteration 13, loss = 0.10517764\n",
      "Iteration 14, loss = 0.09845033\n",
      "Iteration 15, loss = 0.09296292\n",
      "Iteration 16, loss = 0.08698353\n",
      "Iteration 17, loss = 0.08206375\n",
      "Iteration 18, loss = 0.07725197\n",
      "Iteration 19, loss = 0.07339258\n",
      "Iteration 20, loss = 0.06954371\n",
      "Iteration 21, loss = 0.06608275\n",
      "Iteration 22, loss = 0.06237434\n",
      "Iteration 23, loss = 0.05956390\n",
      "Iteration 24, loss = 0.05641825\n",
      "Iteration 25, loss = 0.05408694\n",
      "Iteration 26, loss = 0.05149781\n",
      "Iteration 27, loss = 0.04923271\n",
      "Iteration 28, loss = 0.04740462\n",
      "Iteration 29, loss = 0.04496812\n",
      "Iteration 30, loss = 0.04335429\n",
      "Iteration 31, loss = 0.04173567\n",
      "Iteration 32, loss = 0.04016167\n",
      "Iteration 33, loss = 0.03856572\n",
      "Iteration 34, loss = 0.03722616\n",
      "Iteration 35, loss = 0.03628160\n",
      "Iteration 36, loss = 0.03460977\n",
      "Iteration 37, loss = 0.03366166\n",
      "Iteration 38, loss = 0.03268335\n",
      "Iteration 39, loss = 0.03158810\n",
      "Iteration 40, loss = 0.03085369\n",
      "Iteration 41, loss = 0.02998622\n",
      "Iteration 42, loss = 0.02915333\n",
      "Iteration 43, loss = 0.02842650\n",
      "Iteration 44, loss = 0.02796973\n",
      "Iteration 45, loss = 0.02713530\n",
      "Iteration 46, loss = 0.02653178\n",
      "Iteration 47, loss = 0.02632214\n",
      "Iteration 48, loss = 0.02589466\n",
      "Iteration 49, loss = 0.02522694\n",
      "Iteration 50, loss = 0.02489914\n",
      "Iteration 51, loss = 0.02452869\n",
      "Iteration 52, loss = 0.02413401\n",
      "Iteration 53, loss = 0.02384819\n",
      "Iteration 54, loss = 0.02330579\n",
      "Iteration 55, loss = 0.02318599\n",
      "Iteration 56, loss = 0.02273633\n",
      "Iteration 57, loss = 0.02258134\n",
      "Iteration 58, loss = 0.02253359\n",
      "Iteration 59, loss = 0.02213119\n",
      "Iteration 60, loss = 0.02184158\n",
      "Iteration 61, loss = 0.02176406\n",
      "Iteration 62, loss = 0.02139714\n",
      "Iteration 63, loss = 0.02154585\n",
      "Iteration 64, loss = 0.02107607\n",
      "Iteration 65, loss = 0.02095362\n",
      "Iteration 66, loss = 0.02087451\n",
      "Iteration 67, loss = 0.02075719\n",
      "Iteration 68, loss = 0.02060307\n",
      "Iteration 69, loss = 0.02034525\n",
      "Iteration 70, loss = 0.02057881\n",
      "Iteration 71, loss = 0.02021437\n",
      "Iteration 72, loss = 0.02011225\n",
      "Iteration 73, loss = 0.02005107\n",
      "Iteration 74, loss = 0.01991516\n",
      "Iteration 75, loss = 0.01990725\n",
      "Iteration 76, loss = 0.01980766\n",
      "Iteration 77, loss = 0.01966045\n",
      "Iteration 78, loss = 0.01939296\n",
      "Iteration 79, loss = 0.01930847\n",
      "Iteration 80, loss = 0.01949033\n",
      "Iteration 81, loss = 0.01936411\n",
      "Iteration 82, loss = 0.01937519\n",
      "Iteration 83, loss = 0.01914762\n",
      "Iteration 84, loss = 0.01934984\n",
      "Iteration 85, loss = 0.01899928\n",
      "Iteration 86, loss = 0.01884533\n",
      "Iteration 87, loss = 0.01903232\n",
      "Iteration 88, loss = 0.01895207\n",
      "Iteration 89, loss = 0.01895084\n",
      "Iteration 90, loss = 0.01889733\n",
      "Iteration 91, loss = 0.01862424\n",
      "Iteration 92, loss = 0.01882978\n",
      "Iteration 93, loss = 0.01892354\n",
      "Iteration 94, loss = 0.01858404\n",
      "Iteration 95, loss = 0.01862702\n",
      "Iteration 96, loss = 0.01834429\n",
      "Iteration 97, loss = 0.01858508\n",
      "Iteration 98, loss = 0.01856585\n",
      "Iteration 99, loss = 0.01856287\n",
      "Iteration 100, loss = 0.01882436\n",
      "Iteration 101, loss = 0.01833035\n",
      "Iteration 102, loss = 0.01836018\n",
      "Iteration 103, loss = 0.01827348\n",
      "Iteration 104, loss = 0.01826643\n",
      "Iteration 105, loss = 0.01831713\n",
      "Iteration 106, loss = 0.01824252\n",
      "Iteration 107, loss = 0.01831613\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.001, batch_size=100, hidden_layer_sizes=(100,), solver=adam; total time= 1.5min\n",
      "Iteration 1, loss = 0.85566624\n",
      "Iteration 2, loss = 0.34233824\n",
      "Iteration 3, loss = 0.27203576\n",
      "Iteration 4, loss = 0.23420583\n",
      "Iteration 5, loss = 0.20762088\n",
      "Iteration 6, loss = 0.18633444\n",
      "Iteration 7, loss = 0.16994058\n",
      "Iteration 8, loss = 0.15487873\n",
      "Iteration 9, loss = 0.14279446\n",
      "Iteration 10, loss = 0.13176487\n",
      "Iteration 11, loss = 0.12225217\n",
      "Iteration 12, loss = 0.11380477\n",
      "Iteration 13, loss = 0.10609333\n",
      "Iteration 14, loss = 0.09945825\n",
      "Iteration 15, loss = 0.09337031\n",
      "Iteration 16, loss = 0.08785787\n",
      "Iteration 17, loss = 0.08272901\n",
      "Iteration 18, loss = 0.07813089\n",
      "Iteration 19, loss = 0.07340058\n",
      "Iteration 20, loss = 0.06961106\n",
      "Iteration 21, loss = 0.06585599\n",
      "Iteration 22, loss = 0.06227156\n",
      "Iteration 23, loss = 0.05927918\n",
      "Iteration 24, loss = 0.05669985\n",
      "Iteration 25, loss = 0.05373276\n",
      "Iteration 26, loss = 0.05124709\n",
      "Iteration 27, loss = 0.04879224\n",
      "Iteration 28, loss = 0.04659180\n",
      "Iteration 29, loss = 0.04486404\n",
      "Iteration 30, loss = 0.04315621\n",
      "Iteration 31, loss = 0.04132530\n",
      "Iteration 32, loss = 0.03961670\n",
      "Iteration 33, loss = 0.03840266\n",
      "Iteration 34, loss = 0.03677557\n",
      "Iteration 35, loss = 0.03565278\n",
      "Iteration 36, loss = 0.03449563\n",
      "Iteration 37, loss = 0.03318674\n",
      "Iteration 38, loss = 0.03230334\n",
      "Iteration 39, loss = 0.03140669\n",
      "Iteration 40, loss = 0.03042089\n",
      "Iteration 41, loss = 0.02958139\n",
      "Iteration 42, loss = 0.02893289\n",
      "Iteration 43, loss = 0.02816469\n",
      "Iteration 44, loss = 0.02763031\n",
      "Iteration 45, loss = 0.02708721\n",
      "Iteration 46, loss = 0.02632578\n",
      "Iteration 47, loss = 0.02599100\n",
      "Iteration 48, loss = 0.02548070\n",
      "Iteration 49, loss = 0.02507779\n",
      "Iteration 50, loss = 0.02453154\n",
      "Iteration 51, loss = 0.02429094\n",
      "Iteration 52, loss = 0.02388655\n",
      "Iteration 53, loss = 0.02352809\n",
      "Iteration 54, loss = 0.02320561\n",
      "Iteration 55, loss = 0.02298241\n",
      "Iteration 56, loss = 0.02250043\n",
      "Iteration 57, loss = 0.02220552\n",
      "Iteration 58, loss = 0.02221430\n",
      "Iteration 59, loss = 0.02197879\n",
      "Iteration 60, loss = 0.02174219\n",
      "Iteration 61, loss = 0.02141696\n",
      "Iteration 62, loss = 0.02134977\n",
      "Iteration 63, loss = 0.02107550\n",
      "Iteration 64, loss = 0.02095228\n",
      "Iteration 65, loss = 0.02063925\n",
      "Iteration 66, loss = 0.02064187\n",
      "Iteration 67, loss = 0.02046416\n",
      "Iteration 68, loss = 0.02043050\n",
      "Iteration 69, loss = 0.02025977\n",
      "Iteration 70, loss = 0.02021368\n",
      "Iteration 71, loss = 0.01997158\n",
      "Iteration 72, loss = 0.01982862\n",
      "Iteration 73, loss = 0.01969827\n",
      "Iteration 74, loss = 0.01969253\n",
      "Iteration 75, loss = 0.01970247\n",
      "Iteration 76, loss = 0.01946052\n",
      "Iteration 77, loss = 0.01971372\n",
      "Iteration 78, loss = 0.01927327\n",
      "Iteration 79, loss = 0.01930855\n",
      "Iteration 80, loss = 0.01910069\n",
      "Iteration 81, loss = 0.01900844\n",
      "Iteration 82, loss = 0.01904143\n",
      "Iteration 83, loss = 0.01905413\n",
      "Iteration 84, loss = 0.01897125\n",
      "Iteration 85, loss = 0.01874874\n",
      "Iteration 86, loss = 0.01890410\n",
      "Iteration 87, loss = 0.01873741\n",
      "Iteration 88, loss = 0.01876030\n",
      "Iteration 89, loss = 0.01846827\n",
      "Iteration 90, loss = 0.01868132\n",
      "Iteration 91, loss = 0.01871730\n",
      "Iteration 92, loss = 0.01909550\n",
      "Iteration 93, loss = 0.01828316\n",
      "Iteration 94, loss = 0.01835250\n",
      "Iteration 95, loss = 0.01825681\n",
      "Iteration 96, loss = 0.01826426\n",
      "Iteration 97, loss = 0.01826313\n",
      "Iteration 98, loss = 0.01833504\n",
      "Iteration 99, loss = 0.01831509\n",
      "Iteration 100, loss = 0.01824500\n",
      "Iteration 101, loss = 0.01811359\n",
      "Iteration 102, loss = 0.01820861\n",
      "Iteration 103, loss = 0.01817624\n",
      "Iteration 104, loss = 0.01804446\n",
      "Iteration 105, loss = 0.01797383\n",
      "Iteration 106, loss = 0.01804689\n",
      "Iteration 107, loss = 0.01817214\n",
      "Iteration 108, loss = 0.01802859\n",
      "Iteration 109, loss = 0.01778581\n",
      "Iteration 110, loss = 0.01793235\n",
      "Iteration 111, loss = 0.01791834\n",
      "Iteration 112, loss = 0.01787478\n",
      "Iteration 113, loss = 0.01809809\n",
      "Iteration 114, loss = 0.01768825\n",
      "Iteration 115, loss = 0.01786699\n",
      "Iteration 116, loss = 0.01765672\n",
      "Iteration 117, loss = 0.01767950\n",
      "Iteration 118, loss = 0.01766619\n",
      "Iteration 119, loss = 0.01775564\n",
      "Iteration 120, loss = 0.01789169\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.001, batch_size=100, hidden_layer_sizes=(100,), solver=adam; total time= 1.6min\n",
      "Iteration 1, loss = 2.24127255\n",
      "Iteration 2, loss = 2.07422150\n",
      "Iteration 3, loss = 1.87313767\n",
      "Iteration 4, loss = 1.63650428\n",
      "Iteration 5, loss = 1.40306310\n",
      "Iteration 6, loss = 1.20543426\n",
      "Iteration 7, loss = 1.05135384\n",
      "Iteration 8, loss = 0.93379539\n",
      "Iteration 9, loss = 0.84368988\n",
      "Iteration 10, loss = 0.77332392\n",
      "Iteration 11, loss = 0.71722516\n",
      "Iteration 12, loss = 0.67166320\n",
      "Iteration 13, loss = 0.63390870\n",
      "Iteration 14, loss = 0.60222502\n",
      "Iteration 15, loss = 0.57524288\n",
      "Iteration 16, loss = 0.55201466\n",
      "Iteration 17, loss = 0.53176443\n",
      "Iteration 18, loss = 0.51401929\n",
      "Iteration 19, loss = 0.49831479\n",
      "Iteration 20, loss = 0.48435416\n",
      "Iteration 21, loss = 0.47177953\n",
      "Iteration 22, loss = 0.46050630\n",
      "Iteration 23, loss = 0.45036896\n",
      "Iteration 24, loss = 0.44110002\n",
      "Iteration 25, loss = 0.43255823\n",
      "Iteration 26, loss = 0.42482414\n",
      "Iteration 27, loss = 0.41760609\n",
      "Iteration 28, loss = 0.41098100\n",
      "Iteration 29, loss = 0.40486939\n",
      "Iteration 30, loss = 0.39916664\n",
      "Iteration 31, loss = 0.39376289\n",
      "Iteration 32, loss = 0.38881093\n",
      "Iteration 33, loss = 0.38412609\n",
      "Iteration 34, loss = 0.37973410\n",
      "Iteration 35, loss = 0.37553279\n",
      "Iteration 36, loss = 0.37162937\n",
      "Iteration 37, loss = 0.36783336\n",
      "Iteration 38, loss = 0.36437740\n",
      "Iteration 39, loss = 0.36098467\n",
      "Iteration 40, loss = 0.35789208\n",
      "Iteration 41, loss = 0.35471175\n",
      "Iteration 42, loss = 0.35179694\n",
      "Iteration 43, loss = 0.34898525\n",
      "Iteration 44, loss = 0.34630744\n",
      "Iteration 45, loss = 0.34373409\n",
      "Iteration 46, loss = 0.34125219\n",
      "Iteration 47, loss = 0.33887256\n",
      "Iteration 48, loss = 0.33656460\n",
      "Iteration 49, loss = 0.33437747\n",
      "Iteration 50, loss = 0.33221879\n",
      "Iteration 51, loss = 0.33012113\n",
      "Iteration 52, loss = 0.32813030\n",
      "Iteration 53, loss = 0.32617904\n",
      "Iteration 54, loss = 0.32428769\n",
      "Iteration 55, loss = 0.32248573\n",
      "Iteration 56, loss = 0.32065812\n",
      "Iteration 57, loss = 0.31897628\n",
      "Iteration 58, loss = 0.31724938\n",
      "Iteration 59, loss = 0.31561667\n",
      "Iteration 60, loss = 0.31397636\n",
      "Iteration 61, loss = 0.31243504\n",
      "Iteration 62, loss = 0.31093906\n",
      "Iteration 63, loss = 0.30943221\n",
      "Iteration 64, loss = 0.30796523\n",
      "Iteration 65, loss = 0.30648268\n",
      "Iteration 66, loss = 0.30516119\n",
      "Iteration 67, loss = 0.30378423\n",
      "Iteration 68, loss = 0.30243324\n",
      "Iteration 69, loss = 0.30112726\n",
      "Iteration 70, loss = 0.29982206\n",
      "Iteration 71, loss = 0.29858148\n",
      "Iteration 72, loss = 0.29730342\n",
      "Iteration 73, loss = 0.29608294\n",
      "Iteration 74, loss = 0.29496119\n",
      "Iteration 75, loss = 0.29373258\n",
      "Iteration 76, loss = 0.29252748\n",
      "Iteration 77, loss = 0.29142990\n",
      "Iteration 78, loss = 0.29032896\n",
      "Iteration 79, loss = 0.28915442\n",
      "Iteration 80, loss = 0.28809302\n",
      "Iteration 81, loss = 0.28703433\n",
      "Iteration 82, loss = 0.28597138\n",
      "Iteration 83, loss = 0.28491380\n",
      "Iteration 84, loss = 0.28388955\n",
      "Iteration 85, loss = 0.28289105\n",
      "Iteration 86, loss = 0.28187892\n",
      "Iteration 87, loss = 0.28086870\n",
      "Iteration 88, loss = 0.27988653\n",
      "Iteration 89, loss = 0.27894021\n",
      "Iteration 90, loss = 0.27799731\n",
      "Iteration 91, loss = 0.27699779\n",
      "Iteration 92, loss = 0.27607703\n",
      "Iteration 93, loss = 0.27517534\n",
      "Iteration 94, loss = 0.27423284\n",
      "Iteration 95, loss = 0.27328287\n",
      "Iteration 96, loss = 0.27244476\n",
      "Iteration 97, loss = 0.27153180\n",
      "Iteration 98, loss = 0.27064868\n",
      "Iteration 99, loss = 0.26978877\n",
      "Iteration 100, loss = 0.26892570\n",
      "Iteration 101, loss = 0.26806485\n",
      "Iteration 102, loss = 0.26723515\n",
      "Iteration 103, loss = 0.26638544\n",
      "Iteration 104, loss = 0.26553940\n",
      "Iteration 105, loss = 0.26475326\n",
      "Iteration 106, loss = 0.26390125\n",
      "Iteration 107, loss = 0.26315170\n",
      "Iteration 108, loss = 0.26229020\n",
      "Iteration 109, loss = 0.26145124\n",
      "Iteration 110, loss = 0.26070063\n",
      "Iteration 111, loss = 0.25992582\n",
      "Iteration 112, loss = 0.25917348\n",
      "Iteration 113, loss = 0.25836775\n",
      "Iteration 114, loss = 0.25763342\n",
      "Iteration 115, loss = 0.25682323\n",
      "Iteration 116, loss = 0.25607304\n",
      "Iteration 117, loss = 0.25532887\n",
      "Iteration 118, loss = 0.25455787\n",
      "Iteration 119, loss = 0.25390544\n",
      "Iteration 120, loss = 0.25313488\n",
      "Iteration 121, loss = 0.25240120\n",
      "Iteration 122, loss = 0.25164791\n",
      "Iteration 123, loss = 0.25094041\n",
      "Iteration 124, loss = 0.25026161\n",
      "Iteration 125, loss = 0.24951030\n",
      "Iteration 126, loss = 0.24881388\n",
      "Iteration 127, loss = 0.24808879\n",
      "Iteration 128, loss = 0.24739833\n",
      "Iteration 129, loss = 0.24673299\n",
      "Iteration 130, loss = 0.24599250\n",
      "Iteration 131, loss = 0.24533783\n",
      "Iteration 132, loss = 0.24467933\n",
      "Iteration 133, loss = 0.24400285\n",
      "Iteration 134, loss = 0.24329915\n",
      "Iteration 135, loss = 0.24265157\n",
      "Iteration 136, loss = 0.24198062\n",
      "Iteration 137, loss = 0.24129402\n",
      "Iteration 138, loss = 0.24067786\n",
      "Iteration 139, loss = 0.24003526\n",
      "Iteration 140, loss = 0.23935553\n",
      "Iteration 141, loss = 0.23871184\n",
      "Iteration 142, loss = 0.23807476\n",
      "Iteration 143, loss = 0.23746771\n",
      "Iteration 144, loss = 0.23678931\n",
      "Iteration 145, loss = 0.23618522\n",
      "Iteration 146, loss = 0.23557833\n",
      "Iteration 147, loss = 0.23492815\n",
      "Iteration 148, loss = 0.23428615\n",
      "Iteration 149, loss = 0.23370247\n",
      "Iteration 150, loss = 0.23307271\n",
      "Iteration 151, loss = 0.23241655\n",
      "Iteration 152, loss = 0.23188429\n",
      "Iteration 153, loss = 0.23124911\n",
      "Iteration 154, loss = 0.23061001\n",
      "Iteration 155, loss = 0.23004617\n",
      "Iteration 156, loss = 0.22942588\n",
      "Iteration 157, loss = 0.22886011\n",
      "Iteration 158, loss = 0.22820568\n",
      "Iteration 159, loss = 0.22771440\n",
      "Iteration 160, loss = 0.22708493\n",
      "Iteration 161, loss = 0.22652789\n",
      "Iteration 162, loss = 0.22596178\n",
      "Iteration 163, loss = 0.22535552\n",
      "Iteration 164, loss = 0.22477297\n",
      "Iteration 165, loss = 0.22422662\n",
      "Iteration 166, loss = 0.22362934\n",
      "Iteration 167, loss = 0.22306709\n",
      "Iteration 168, loss = 0.22255043\n",
      "Iteration 169, loss = 0.22194203\n",
      "Iteration 170, loss = 0.22142341\n",
      "Iteration 171, loss = 0.22084152\n",
      "Iteration 172, loss = 0.22031527\n",
      "Iteration 173, loss = 0.21976765\n",
      "Iteration 174, loss = 0.21924273\n",
      "Iteration 175, loss = 0.21867256\n",
      "Iteration 176, loss = 0.21810955\n",
      "Iteration 177, loss = 0.21759447\n",
      "Iteration 178, loss = 0.21704555\n",
      "Iteration 179, loss = 0.21641640\n",
      "Iteration 180, loss = 0.21596470\n",
      "Iteration 181, loss = 0.21547171\n",
      "Iteration 182, loss = 0.21490042\n",
      "Iteration 183, loss = 0.21439758\n",
      "Iteration 184, loss = 0.21384892\n",
      "Iteration 185, loss = 0.21340598\n",
      "Iteration 186, loss = 0.21284994\n",
      "Iteration 187, loss = 0.21231699\n",
      "Iteration 188, loss = 0.21181509\n",
      "Iteration 189, loss = 0.21134315\n",
      "Iteration 190, loss = 0.21084098\n",
      "Iteration 191, loss = 0.21029306\n",
      "Iteration 192, loss = 0.20982952\n",
      "Iteration 193, loss = 0.20929708\n",
      "Iteration 194, loss = 0.20880471\n",
      "Iteration 195, loss = 0.20829155\n",
      "Iteration 196, loss = 0.20778318\n",
      "Iteration 197, loss = 0.20732642\n",
      "Iteration 198, loss = 0.20684107\n",
      "Iteration 199, loss = 0.20631886\n",
      "Iteration 200, loss = 0.20590112\n",
      "[CV] END activation=logistic, alpha=0.001, batch_size=100, hidden_layer_sizes=(100,), solver=sgd; total time= 1.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.24688328\n",
      "Iteration 2, loss = 2.07583257\n",
      "Iteration 3, loss = 1.87231079\n",
      "Iteration 4, loss = 1.63933723\n",
      "Iteration 5, loss = 1.41376462\n",
      "Iteration 6, loss = 1.22270501\n",
      "Iteration 7, loss = 1.07192083\n",
      "Iteration 8, loss = 0.95570170\n",
      "Iteration 9, loss = 0.86531861\n",
      "Iteration 10, loss = 0.79413144\n",
      "Iteration 11, loss = 0.73678923\n",
      "Iteration 12, loss = 0.69013511\n",
      "Iteration 13, loss = 0.65132472\n",
      "Iteration 14, loss = 0.61851766\n",
      "Iteration 15, loss = 0.59057981\n",
      "Iteration 16, loss = 0.56633268\n",
      "Iteration 17, loss = 0.54517745\n",
      "Iteration 18, loss = 0.52665506\n",
      "Iteration 19, loss = 0.51015143\n",
      "Iteration 20, loss = 0.49539754\n",
      "Iteration 21, loss = 0.48219997\n",
      "Iteration 22, loss = 0.47035824\n",
      "Iteration 23, loss = 0.45953973\n",
      "Iteration 24, loss = 0.44962149\n",
      "Iteration 25, loss = 0.44081500\n",
      "Iteration 26, loss = 0.43249814\n",
      "Iteration 27, loss = 0.42489666\n",
      "Iteration 28, loss = 0.41791663\n",
      "Iteration 29, loss = 0.41138134\n",
      "Iteration 30, loss = 0.40537917\n",
      "Iteration 31, loss = 0.39975790\n",
      "Iteration 32, loss = 0.39443542\n",
      "Iteration 33, loss = 0.38958036\n",
      "Iteration 34, loss = 0.38498212\n",
      "Iteration 35, loss = 0.38058360\n",
      "Iteration 36, loss = 0.37645461\n",
      "Iteration 37, loss = 0.37263609\n",
      "Iteration 38, loss = 0.36893459\n",
      "Iteration 39, loss = 0.36542252\n",
      "Iteration 40, loss = 0.36207565\n",
      "Iteration 41, loss = 0.35889372\n",
      "Iteration 42, loss = 0.35591706\n",
      "Iteration 43, loss = 0.35297778\n",
      "Iteration 44, loss = 0.35025528\n",
      "Iteration 45, loss = 0.34759219\n",
      "Iteration 46, loss = 0.34506926\n",
      "Iteration 47, loss = 0.34257444\n",
      "Iteration 48, loss = 0.34030897\n",
      "Iteration 49, loss = 0.33807441\n",
      "Iteration 50, loss = 0.33581323\n",
      "Iteration 51, loss = 0.33372346\n",
      "Iteration 52, loss = 0.33169725\n",
      "Iteration 53, loss = 0.32972017\n",
      "Iteration 54, loss = 0.32773414\n",
      "Iteration 55, loss = 0.32593432\n",
      "Iteration 56, loss = 0.32413441\n",
      "Iteration 57, loss = 0.32240209\n",
      "Iteration 58, loss = 0.32067221\n",
      "Iteration 59, loss = 0.31905175\n",
      "Iteration 60, loss = 0.31747160\n",
      "Iteration 61, loss = 0.31588370\n",
      "Iteration 62, loss = 0.31436945\n",
      "Iteration 63, loss = 0.31285954\n",
      "Iteration 64, loss = 0.31142304\n",
      "Iteration 65, loss = 0.31004620\n",
      "Iteration 66, loss = 0.30864173\n",
      "Iteration 67, loss = 0.30727437\n",
      "Iteration 68, loss = 0.30597964\n",
      "Iteration 69, loss = 0.30461776\n",
      "Iteration 70, loss = 0.30337623\n",
      "Iteration 71, loss = 0.30215207\n",
      "Iteration 72, loss = 0.30092698\n",
      "Iteration 73, loss = 0.29975143\n",
      "Iteration 74, loss = 0.29856514\n",
      "Iteration 75, loss = 0.29740320\n",
      "Iteration 76, loss = 0.29628721\n",
      "Iteration 77, loss = 0.29513881\n",
      "Iteration 78, loss = 0.29400877\n",
      "Iteration 79, loss = 0.29299205\n",
      "Iteration 80, loss = 0.29189261\n",
      "Iteration 81, loss = 0.29090651\n",
      "Iteration 82, loss = 0.28981655\n",
      "Iteration 83, loss = 0.28883927\n",
      "Iteration 84, loss = 0.28782670\n",
      "Iteration 85, loss = 0.28685104\n",
      "Iteration 86, loss = 0.28585405\n",
      "Iteration 87, loss = 0.28491536\n",
      "Iteration 88, loss = 0.28392980\n",
      "Iteration 89, loss = 0.28302395\n",
      "Iteration 90, loss = 0.28209615\n",
      "Iteration 91, loss = 0.28116551\n",
      "Iteration 92, loss = 0.28028749\n",
      "Iteration 93, loss = 0.27936112\n",
      "Iteration 94, loss = 0.27852710\n",
      "Iteration 95, loss = 0.27762345\n",
      "Iteration 96, loss = 0.27678459\n",
      "Iteration 97, loss = 0.27592389\n",
      "Iteration 98, loss = 0.27505794\n",
      "Iteration 99, loss = 0.27427401\n",
      "Iteration 100, loss = 0.27337815\n",
      "Iteration 101, loss = 0.27254624\n",
      "Iteration 102, loss = 0.27169087\n",
      "Iteration 103, loss = 0.27091124\n",
      "Iteration 104, loss = 0.27012842\n",
      "Iteration 105, loss = 0.26930705\n",
      "Iteration 106, loss = 0.26849833\n",
      "Iteration 107, loss = 0.26772720\n",
      "Iteration 108, loss = 0.26695523\n",
      "Iteration 109, loss = 0.26614379\n",
      "Iteration 110, loss = 0.26540605\n",
      "Iteration 111, loss = 0.26459180\n",
      "Iteration 112, loss = 0.26394121\n",
      "Iteration 113, loss = 0.26315323\n",
      "Iteration 114, loss = 0.26242828\n",
      "Iteration 115, loss = 0.26167206\n",
      "Iteration 116, loss = 0.26089184\n",
      "Iteration 117, loss = 0.26021842\n",
      "Iteration 118, loss = 0.25947587\n",
      "Iteration 119, loss = 0.25877512\n",
      "Iteration 120, loss = 0.25802847\n",
      "Iteration 121, loss = 0.25731555\n",
      "Iteration 122, loss = 0.25659627\n",
      "Iteration 123, loss = 0.25592341\n",
      "Iteration 124, loss = 0.25515502\n",
      "Iteration 125, loss = 0.25446459\n",
      "Iteration 126, loss = 0.25380580\n",
      "Iteration 127, loss = 0.25311988\n",
      "Iteration 128, loss = 0.25240330\n",
      "Iteration 129, loss = 0.25182319\n",
      "Iteration 130, loss = 0.25111128\n",
      "Iteration 131, loss = 0.25038537\n",
      "Iteration 132, loss = 0.24975142\n",
      "Iteration 133, loss = 0.24908333\n",
      "Iteration 134, loss = 0.24839427\n",
      "Iteration 135, loss = 0.24776520\n",
      "Iteration 136, loss = 0.24707453\n",
      "Iteration 137, loss = 0.24641102\n",
      "Iteration 138, loss = 0.24582306\n",
      "Iteration 139, loss = 0.24512600\n",
      "Iteration 140, loss = 0.24450258\n",
      "Iteration 141, loss = 0.24388437\n",
      "Iteration 142, loss = 0.24322265\n",
      "Iteration 143, loss = 0.24260305\n",
      "Iteration 144, loss = 0.24197202\n",
      "Iteration 145, loss = 0.24134715\n",
      "Iteration 146, loss = 0.24073906\n",
      "Iteration 147, loss = 0.24013609\n",
      "Iteration 148, loss = 0.23948907\n",
      "Iteration 149, loss = 0.23885860\n",
      "Iteration 150, loss = 0.23822199\n",
      "Iteration 151, loss = 0.23769624\n",
      "Iteration 152, loss = 0.23704927\n",
      "Iteration 153, loss = 0.23650043\n",
      "Iteration 154, loss = 0.23588849\n",
      "Iteration 155, loss = 0.23523172\n",
      "Iteration 156, loss = 0.23464068\n",
      "Iteration 157, loss = 0.23408428\n",
      "Iteration 158, loss = 0.23348032\n",
      "Iteration 159, loss = 0.23288158\n",
      "Iteration 160, loss = 0.23232263\n",
      "Iteration 161, loss = 0.23175686\n",
      "Iteration 162, loss = 0.23117094\n",
      "Iteration 163, loss = 0.23058332\n",
      "Iteration 164, loss = 0.23002847\n",
      "Iteration 165, loss = 0.22944400\n",
      "Iteration 166, loss = 0.22892655\n",
      "Iteration 167, loss = 0.22831996\n",
      "Iteration 168, loss = 0.22780318\n",
      "Iteration 169, loss = 0.22720192\n",
      "Iteration 170, loss = 0.22671420\n",
      "Iteration 171, loss = 0.22613275\n",
      "Iteration 172, loss = 0.22557695\n",
      "Iteration 173, loss = 0.22504601\n",
      "Iteration 174, loss = 0.22450178\n",
      "Iteration 175, loss = 0.22394292\n",
      "Iteration 176, loss = 0.22342669\n",
      "Iteration 177, loss = 0.22285360\n",
      "Iteration 178, loss = 0.22233154\n",
      "Iteration 179, loss = 0.22180814\n",
      "Iteration 180, loss = 0.22126223\n",
      "Iteration 181, loss = 0.22077003\n",
      "Iteration 182, loss = 0.22021072\n",
      "Iteration 183, loss = 0.21969445\n",
      "Iteration 184, loss = 0.21915107\n",
      "Iteration 185, loss = 0.21864638\n",
      "Iteration 186, loss = 0.21818772\n",
      "Iteration 187, loss = 0.21766525\n",
      "Iteration 188, loss = 0.21712442\n",
      "Iteration 189, loss = 0.21664381\n",
      "Iteration 190, loss = 0.21609895\n",
      "Iteration 191, loss = 0.21560651\n",
      "Iteration 192, loss = 0.21511218\n",
      "Iteration 193, loss = 0.21464605\n",
      "Iteration 194, loss = 0.21413961\n",
      "Iteration 195, loss = 0.21366948\n",
      "Iteration 196, loss = 0.21316186\n",
      "Iteration 197, loss = 0.21265034\n",
      "Iteration 198, loss = 0.21219817\n",
      "Iteration 199, loss = 0.21171423\n",
      "Iteration 200, loss = 0.21118713\n",
      "[CV] END activation=logistic, alpha=0.001, batch_size=100, hidden_layer_sizes=(100,), solver=sgd; total time= 1.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.23909243\n",
      "Iteration 2, loss = 2.06898383\n",
      "Iteration 3, loss = 1.86310751\n",
      "Iteration 4, loss = 1.62528005\n",
      "Iteration 5, loss = 1.39449098\n",
      "Iteration 6, loss = 1.20167515\n",
      "Iteration 7, loss = 1.05140722\n",
      "Iteration 8, loss = 0.93679378\n",
      "Iteration 9, loss = 0.84817502\n",
      "Iteration 10, loss = 0.77865104\n",
      "Iteration 11, loss = 0.72303133\n",
      "Iteration 12, loss = 0.67755258\n",
      "Iteration 13, loss = 0.63980526\n",
      "Iteration 14, loss = 0.60798626\n",
      "Iteration 15, loss = 0.58065571\n",
      "Iteration 16, loss = 0.55703078\n",
      "Iteration 17, loss = 0.53658167\n",
      "Iteration 18, loss = 0.51839476\n",
      "Iteration 19, loss = 0.50237506\n",
      "Iteration 20, loss = 0.48791445\n",
      "Iteration 21, loss = 0.47501910\n",
      "Iteration 22, loss = 0.46345229\n",
      "Iteration 23, loss = 0.45283472\n",
      "Iteration 24, loss = 0.44329796\n",
      "Iteration 25, loss = 0.43451260\n",
      "Iteration 26, loss = 0.42648340\n",
      "Iteration 27, loss = 0.41902399\n",
      "Iteration 28, loss = 0.41224671\n",
      "Iteration 29, loss = 0.40581761\n",
      "Iteration 30, loss = 0.39994972\n",
      "Iteration 31, loss = 0.39444465\n",
      "Iteration 32, loss = 0.38930536\n",
      "Iteration 33, loss = 0.38451472\n",
      "Iteration 34, loss = 0.38001697\n",
      "Iteration 35, loss = 0.37573766\n",
      "Iteration 36, loss = 0.37167682\n",
      "Iteration 37, loss = 0.36784374\n",
      "Iteration 38, loss = 0.36416687\n",
      "Iteration 39, loss = 0.36086770\n",
      "Iteration 40, loss = 0.35753089\n",
      "Iteration 41, loss = 0.35454183\n",
      "Iteration 42, loss = 0.35147953\n",
      "Iteration 43, loss = 0.34860335\n",
      "Iteration 44, loss = 0.34594613\n",
      "Iteration 45, loss = 0.34336144\n",
      "Iteration 46, loss = 0.34080190\n",
      "Iteration 47, loss = 0.33844715\n",
      "Iteration 48, loss = 0.33605428\n",
      "Iteration 49, loss = 0.33380897\n",
      "Iteration 50, loss = 0.33172075\n",
      "Iteration 51, loss = 0.32959808\n",
      "Iteration 52, loss = 0.32750842\n",
      "Iteration 53, loss = 0.32567938\n",
      "Iteration 54, loss = 0.32375895\n",
      "Iteration 55, loss = 0.32190536\n",
      "Iteration 56, loss = 0.32011098\n",
      "Iteration 57, loss = 0.31836124\n",
      "Iteration 58, loss = 0.31670631\n",
      "Iteration 59, loss = 0.31506598\n",
      "Iteration 60, loss = 0.31342471\n",
      "Iteration 61, loss = 0.31190135\n",
      "Iteration 62, loss = 0.31036142\n",
      "Iteration 63, loss = 0.30888041\n",
      "Iteration 64, loss = 0.30742463\n",
      "Iteration 65, loss = 0.30598675\n",
      "Iteration 66, loss = 0.30456287\n",
      "Iteration 67, loss = 0.30330604\n",
      "Iteration 68, loss = 0.30195027\n",
      "Iteration 69, loss = 0.30063764\n",
      "Iteration 70, loss = 0.29939972\n",
      "Iteration 71, loss = 0.29808940\n",
      "Iteration 72, loss = 0.29685463\n",
      "Iteration 73, loss = 0.29563257\n",
      "Iteration 74, loss = 0.29445968\n",
      "Iteration 75, loss = 0.29330159\n",
      "Iteration 76, loss = 0.29215633\n",
      "Iteration 77, loss = 0.29099302\n",
      "Iteration 78, loss = 0.28990940\n",
      "Iteration 79, loss = 0.28878294\n",
      "Iteration 80, loss = 0.28773300\n",
      "Iteration 81, loss = 0.28667686\n",
      "Iteration 82, loss = 0.28560816\n",
      "Iteration 83, loss = 0.28464909\n",
      "Iteration 84, loss = 0.28361173\n",
      "Iteration 85, loss = 0.28260263\n",
      "Iteration 86, loss = 0.28155926\n",
      "Iteration 87, loss = 0.28057460\n",
      "Iteration 88, loss = 0.27964526\n",
      "Iteration 89, loss = 0.27867680\n",
      "Iteration 90, loss = 0.27773173\n",
      "Iteration 91, loss = 0.27684877\n",
      "Iteration 92, loss = 0.27589984\n",
      "Iteration 93, loss = 0.27492672\n",
      "Iteration 94, loss = 0.27406126\n",
      "Iteration 95, loss = 0.27316093\n",
      "Iteration 96, loss = 0.27226879\n",
      "Iteration 97, loss = 0.27134550\n",
      "Iteration 98, loss = 0.27055142\n",
      "Iteration 99, loss = 0.26967470\n",
      "Iteration 100, loss = 0.26878024\n",
      "Iteration 101, loss = 0.26797500\n",
      "Iteration 102, loss = 0.26714668\n",
      "Iteration 103, loss = 0.26630939\n",
      "Iteration 104, loss = 0.26548899\n",
      "Iteration 105, loss = 0.26470038\n",
      "Iteration 106, loss = 0.26388514\n",
      "Iteration 107, loss = 0.26306463\n",
      "Iteration 108, loss = 0.26227008\n",
      "Iteration 109, loss = 0.26149935\n",
      "Iteration 110, loss = 0.26066186\n",
      "Iteration 111, loss = 0.25992295\n",
      "Iteration 112, loss = 0.25914524\n",
      "Iteration 113, loss = 0.25836058\n",
      "Iteration 114, loss = 0.25764095\n",
      "Iteration 115, loss = 0.25687364\n",
      "Iteration 116, loss = 0.25611721\n",
      "Iteration 117, loss = 0.25536780\n",
      "Iteration 118, loss = 0.25460762\n",
      "Iteration 119, loss = 0.25386523\n",
      "Iteration 120, loss = 0.25314796\n",
      "Iteration 121, loss = 0.25246593\n",
      "Iteration 122, loss = 0.25171101\n",
      "Iteration 123, loss = 0.25099270\n",
      "Iteration 124, loss = 0.25029687\n",
      "Iteration 125, loss = 0.24953799\n",
      "Iteration 126, loss = 0.24889548\n",
      "Iteration 127, loss = 0.24815393\n",
      "Iteration 128, loss = 0.24745106\n",
      "Iteration 129, loss = 0.24678760\n",
      "Iteration 130, loss = 0.24609309\n",
      "Iteration 131, loss = 0.24527536\n",
      "Iteration 132, loss = 0.24471202\n",
      "Iteration 133, loss = 0.24398015\n",
      "Iteration 134, loss = 0.24337214\n",
      "Iteration 135, loss = 0.24268346\n",
      "Iteration 136, loss = 0.24200780\n",
      "Iteration 137, loss = 0.24137215\n",
      "Iteration 138, loss = 0.24063673\n",
      "Iteration 139, loss = 0.24000873\n",
      "Iteration 140, loss = 0.23937618\n",
      "Iteration 141, loss = 0.23873604\n",
      "Iteration 142, loss = 0.23807735\n",
      "Iteration 143, loss = 0.23742151\n",
      "Iteration 144, loss = 0.23678582\n",
      "Iteration 145, loss = 0.23616429\n",
      "Iteration 146, loss = 0.23551002\n",
      "Iteration 147, loss = 0.23482847\n",
      "Iteration 148, loss = 0.23423137\n",
      "Iteration 149, loss = 0.23361458\n",
      "Iteration 150, loss = 0.23298494\n",
      "Iteration 151, loss = 0.23229454\n",
      "Iteration 152, loss = 0.23177141\n",
      "Iteration 153, loss = 0.23110186\n",
      "Iteration 154, loss = 0.23051178\n",
      "Iteration 155, loss = 0.22990132\n",
      "Iteration 156, loss = 0.22931345\n",
      "Iteration 157, loss = 0.22869027\n",
      "Iteration 158, loss = 0.22809138\n",
      "Iteration 159, loss = 0.22750752\n",
      "Iteration 160, loss = 0.22684700\n",
      "Iteration 161, loss = 0.22631577\n",
      "Iteration 162, loss = 0.22573079\n",
      "Iteration 163, loss = 0.22511704\n",
      "Iteration 164, loss = 0.22452345\n",
      "Iteration 165, loss = 0.22399875\n",
      "Iteration 166, loss = 0.22340685\n",
      "Iteration 167, loss = 0.22279942\n",
      "Iteration 168, loss = 0.22224457\n",
      "Iteration 169, loss = 0.22169925\n",
      "Iteration 170, loss = 0.22111864\n",
      "Iteration 171, loss = 0.22057050\n",
      "Iteration 172, loss = 0.22000322\n",
      "Iteration 173, loss = 0.21946271\n",
      "Iteration 174, loss = 0.21889864\n",
      "Iteration 175, loss = 0.21832961\n",
      "Iteration 176, loss = 0.21778739\n",
      "Iteration 177, loss = 0.21724282\n",
      "Iteration 178, loss = 0.21671016\n",
      "Iteration 179, loss = 0.21615432\n",
      "Iteration 180, loss = 0.21559104\n",
      "Iteration 181, loss = 0.21509696\n",
      "Iteration 182, loss = 0.21453772\n",
      "Iteration 183, loss = 0.21402557\n",
      "Iteration 184, loss = 0.21346565\n",
      "Iteration 185, loss = 0.21296130\n",
      "Iteration 186, loss = 0.21240001\n",
      "Iteration 187, loss = 0.21192924\n",
      "Iteration 188, loss = 0.21139592\n",
      "Iteration 189, loss = 0.21088583\n",
      "Iteration 190, loss = 0.21032085\n",
      "Iteration 191, loss = 0.20984632\n",
      "Iteration 192, loss = 0.20933824\n",
      "Iteration 193, loss = 0.20884792\n",
      "Iteration 194, loss = 0.20829937\n",
      "Iteration 195, loss = 0.20783474\n",
      "Iteration 196, loss = 0.20729292\n",
      "Iteration 197, loss = 0.20683581\n",
      "Iteration 198, loss = 0.20633648\n",
      "Iteration 199, loss = 0.20583951\n",
      "Iteration 200, loss = 0.20535540\n",
      "[CV] END activation=logistic, alpha=0.001, batch_size=100, hidden_layer_sizes=(100,), solver=sgd; total time= 1.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.23616264\n",
      "Iteration 2, loss = 2.06303180\n",
      "Iteration 3, loss = 1.85567019\n",
      "Iteration 4, loss = 1.61924182\n",
      "Iteration 5, loss = 1.39303903\n",
      "Iteration 6, loss = 1.20407479\n",
      "Iteration 7, loss = 1.05613269\n",
      "Iteration 8, loss = 0.94199931\n",
      "Iteration 9, loss = 0.85340098\n",
      "Iteration 10, loss = 0.78344763\n",
      "Iteration 11, loss = 0.72722923\n",
      "Iteration 12, loss = 0.68107583\n",
      "Iteration 13, loss = 0.64277587\n",
      "Iteration 14, loss = 0.61036471\n",
      "Iteration 15, loss = 0.58277691\n",
      "Iteration 16, loss = 0.55887338\n",
      "Iteration 17, loss = 0.53809762\n",
      "Iteration 18, loss = 0.51983835\n",
      "Iteration 19, loss = 0.50368015\n",
      "Iteration 20, loss = 0.48925904\n",
      "Iteration 21, loss = 0.47638238\n",
      "Iteration 22, loss = 0.46474803\n",
      "Iteration 23, loss = 0.45417711\n",
      "Iteration 24, loss = 0.44467314\n",
      "Iteration 25, loss = 0.43585060\n",
      "Iteration 26, loss = 0.42789816\n",
      "Iteration 27, loss = 0.42049992\n",
      "Iteration 28, loss = 0.41371345\n",
      "Iteration 29, loss = 0.40741107\n",
      "Iteration 30, loss = 0.40159354\n",
      "Iteration 31, loss = 0.39608639\n",
      "Iteration 32, loss = 0.39095844\n",
      "Iteration 33, loss = 0.38621281\n",
      "Iteration 34, loss = 0.38169871\n",
      "Iteration 35, loss = 0.37751556\n",
      "Iteration 36, loss = 0.37344019\n",
      "Iteration 37, loss = 0.36970526\n",
      "Iteration 38, loss = 0.36612583\n",
      "Iteration 39, loss = 0.36274762\n",
      "Iteration 40, loss = 0.35943470\n",
      "Iteration 41, loss = 0.35632212\n",
      "Iteration 42, loss = 0.35336175\n",
      "Iteration 43, loss = 0.35051767\n",
      "Iteration 44, loss = 0.34783739\n",
      "Iteration 45, loss = 0.34524804\n",
      "Iteration 46, loss = 0.34275355\n",
      "Iteration 47, loss = 0.34039108\n",
      "Iteration 48, loss = 0.33799462\n",
      "Iteration 49, loss = 0.33580474\n",
      "Iteration 50, loss = 0.33368808\n",
      "Iteration 51, loss = 0.33153051\n",
      "Iteration 52, loss = 0.32952561\n",
      "Iteration 53, loss = 0.32762229\n",
      "Iteration 54, loss = 0.32568982\n",
      "Iteration 55, loss = 0.32383991\n",
      "Iteration 56, loss = 0.32210481\n",
      "Iteration 57, loss = 0.32034490\n",
      "Iteration 58, loss = 0.31863727\n",
      "Iteration 59, loss = 0.31697719\n",
      "Iteration 60, loss = 0.31544342\n",
      "Iteration 61, loss = 0.31386514\n",
      "Iteration 62, loss = 0.31231643\n",
      "Iteration 63, loss = 0.31084887\n",
      "Iteration 64, loss = 0.30932600\n",
      "Iteration 65, loss = 0.30796507\n",
      "Iteration 66, loss = 0.30659336\n",
      "Iteration 67, loss = 0.30517712\n",
      "Iteration 68, loss = 0.30388454\n",
      "Iteration 69, loss = 0.30256478\n",
      "Iteration 70, loss = 0.30128909\n",
      "Iteration 71, loss = 0.30004021\n",
      "Iteration 72, loss = 0.29875160\n",
      "Iteration 73, loss = 0.29757231\n",
      "Iteration 74, loss = 0.29636277\n",
      "Iteration 75, loss = 0.29518796\n",
      "Iteration 76, loss = 0.29403866\n",
      "Iteration 77, loss = 0.29288821\n",
      "Iteration 78, loss = 0.29180963\n",
      "Iteration 79, loss = 0.29072779\n",
      "Iteration 80, loss = 0.28962336\n",
      "Iteration 81, loss = 0.28854815\n",
      "Iteration 82, loss = 0.28750275\n",
      "Iteration 83, loss = 0.28643412\n",
      "Iteration 84, loss = 0.28534216\n",
      "Iteration 85, loss = 0.28439518\n",
      "Iteration 86, loss = 0.28342306\n",
      "Iteration 87, loss = 0.28239885\n",
      "Iteration 88, loss = 0.28144975\n",
      "Iteration 89, loss = 0.28049003\n",
      "Iteration 90, loss = 0.27953415\n",
      "Iteration 91, loss = 0.27857451\n",
      "Iteration 92, loss = 0.27765141\n",
      "Iteration 93, loss = 0.27672326\n",
      "Iteration 94, loss = 0.27584771\n",
      "Iteration 95, loss = 0.27495849\n",
      "Iteration 96, loss = 0.27401945\n",
      "Iteration 97, loss = 0.27317666\n",
      "Iteration 98, loss = 0.27230161\n",
      "Iteration 99, loss = 0.27144015\n",
      "Iteration 100, loss = 0.27057583\n",
      "Iteration 101, loss = 0.26972473\n",
      "Iteration 102, loss = 0.26887857\n",
      "Iteration 103, loss = 0.26805271\n",
      "Iteration 104, loss = 0.26722312\n",
      "Iteration 105, loss = 0.26638873\n",
      "Iteration 106, loss = 0.26561339\n",
      "Iteration 107, loss = 0.26475586\n",
      "Iteration 108, loss = 0.26398510\n",
      "Iteration 109, loss = 0.26318888\n",
      "Iteration 110, loss = 0.26238351\n",
      "Iteration 111, loss = 0.26169520\n",
      "Iteration 112, loss = 0.26082237\n",
      "Iteration 113, loss = 0.26013818\n",
      "Iteration 114, loss = 0.25936354\n",
      "Iteration 115, loss = 0.25860020\n",
      "Iteration 116, loss = 0.25783768\n",
      "Iteration 117, loss = 0.25712400\n",
      "Iteration 118, loss = 0.25637015\n",
      "Iteration 119, loss = 0.25556936\n",
      "Iteration 120, loss = 0.25489039\n",
      "Iteration 121, loss = 0.25412077\n",
      "Iteration 122, loss = 0.25346025\n",
      "Iteration 123, loss = 0.25267376\n",
      "Iteration 124, loss = 0.25206098\n",
      "Iteration 125, loss = 0.25131870\n",
      "Iteration 126, loss = 0.25060461\n",
      "Iteration 127, loss = 0.24989277\n",
      "Iteration 128, loss = 0.24922638\n",
      "Iteration 129, loss = 0.24855271\n",
      "Iteration 130, loss = 0.24783867\n",
      "Iteration 131, loss = 0.24716115\n",
      "Iteration 132, loss = 0.24648724\n",
      "Iteration 133, loss = 0.24580747\n",
      "Iteration 134, loss = 0.24517948\n",
      "Iteration 135, loss = 0.24447331\n",
      "Iteration 136, loss = 0.24381232\n",
      "Iteration 137, loss = 0.24316585\n",
      "Iteration 138, loss = 0.24252060\n",
      "Iteration 139, loss = 0.24186488\n",
      "Iteration 140, loss = 0.24123090\n",
      "Iteration 141, loss = 0.24054182\n",
      "Iteration 142, loss = 0.23996216\n",
      "Iteration 143, loss = 0.23932637\n",
      "Iteration 144, loss = 0.23868088\n",
      "Iteration 145, loss = 0.23802340\n",
      "Iteration 146, loss = 0.23744207\n",
      "Iteration 147, loss = 0.23680932\n",
      "Iteration 148, loss = 0.23615246\n",
      "Iteration 149, loss = 0.23556098\n",
      "Iteration 150, loss = 0.23495271\n",
      "Iteration 151, loss = 0.23434550\n",
      "Iteration 152, loss = 0.23372207\n",
      "Iteration 153, loss = 0.23317792\n",
      "Iteration 154, loss = 0.23258596\n",
      "Iteration 155, loss = 0.23194317\n",
      "Iteration 156, loss = 0.23136857\n",
      "Iteration 157, loss = 0.23076063\n",
      "Iteration 158, loss = 0.23018864\n",
      "Iteration 159, loss = 0.22959702\n",
      "Iteration 160, loss = 0.22901786\n",
      "Iteration 161, loss = 0.22845941\n",
      "Iteration 162, loss = 0.22784450\n",
      "Iteration 163, loss = 0.22729486\n",
      "Iteration 164, loss = 0.22671108\n",
      "Iteration 165, loss = 0.22620655\n",
      "Iteration 166, loss = 0.22562089\n",
      "Iteration 167, loss = 0.22503640\n",
      "Iteration 168, loss = 0.22451594\n",
      "Iteration 169, loss = 0.22392690\n",
      "Iteration 170, loss = 0.22338525\n",
      "Iteration 171, loss = 0.22285025\n",
      "Iteration 172, loss = 0.22226853\n",
      "Iteration 173, loss = 0.22173893\n",
      "Iteration 174, loss = 0.22118661\n",
      "Iteration 175, loss = 0.22065613\n",
      "Iteration 176, loss = 0.22012749\n",
      "Iteration 177, loss = 0.21959377\n",
      "Iteration 178, loss = 0.21904466\n",
      "Iteration 179, loss = 0.21854349\n",
      "Iteration 180, loss = 0.21802583\n",
      "Iteration 181, loss = 0.21745281\n",
      "Iteration 182, loss = 0.21698020\n",
      "Iteration 183, loss = 0.21646069\n",
      "Iteration 184, loss = 0.21588035\n",
      "Iteration 185, loss = 0.21545935\n",
      "Iteration 186, loss = 0.21486655\n",
      "Iteration 187, loss = 0.21435815\n",
      "Iteration 188, loss = 0.21388731\n",
      "Iteration 189, loss = 0.21338210\n",
      "Iteration 190, loss = 0.21287021\n",
      "Iteration 191, loss = 0.21238708\n",
      "Iteration 192, loss = 0.21190837\n",
      "Iteration 193, loss = 0.21141035\n",
      "Iteration 194, loss = 0.21087300\n",
      "Iteration 195, loss = 0.21038245\n",
      "Iteration 196, loss = 0.20989875\n",
      "Iteration 197, loss = 0.20945231\n",
      "Iteration 198, loss = 0.20897065\n",
      "Iteration 199, loss = 0.20849419\n",
      "Iteration 200, loss = 0.20800580\n",
      "[CV] END activation=logistic, alpha=0.001, batch_size=100, hidden_layer_sizes=(100,), solver=sgd; total time= 1.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.23363586\n",
      "Iteration 2, loss = 2.06763326\n",
      "Iteration 3, loss = 1.86253237\n",
      "Iteration 4, loss = 1.62401281\n",
      "Iteration 5, loss = 1.39352633\n",
      "Iteration 6, loss = 1.20059613\n",
      "Iteration 7, loss = 1.05112940\n",
      "Iteration 8, loss = 0.93646242\n",
      "Iteration 9, loss = 0.84823863\n",
      "Iteration 10, loss = 0.77930664\n",
      "Iteration 11, loss = 0.72424755\n",
      "Iteration 12, loss = 0.67948128\n",
      "Iteration 13, loss = 0.64231202\n",
      "Iteration 14, loss = 0.61108653\n",
      "Iteration 15, loss = 0.58446303\n",
      "Iteration 16, loss = 0.56145568\n",
      "Iteration 17, loss = 0.54133071\n",
      "Iteration 18, loss = 0.52360299\n",
      "Iteration 19, loss = 0.50788327\n",
      "Iteration 20, loss = 0.49378628\n",
      "Iteration 21, loss = 0.48116255\n",
      "Iteration 22, loss = 0.46968811\n",
      "Iteration 23, loss = 0.45921789\n",
      "Iteration 24, loss = 0.44971036\n",
      "Iteration 25, loss = 0.44099289\n",
      "Iteration 26, loss = 0.43295275\n",
      "Iteration 27, loss = 0.42558161\n",
      "Iteration 28, loss = 0.41862304\n",
      "Iteration 29, loss = 0.41223146\n",
      "Iteration 30, loss = 0.40632537\n",
      "Iteration 31, loss = 0.40070389\n",
      "Iteration 32, loss = 0.39552033\n",
      "Iteration 33, loss = 0.39055910\n",
      "Iteration 34, loss = 0.38593138\n",
      "Iteration 35, loss = 0.38169995\n",
      "Iteration 36, loss = 0.37749739\n",
      "Iteration 37, loss = 0.37361045\n",
      "Iteration 38, loss = 0.36992104\n",
      "Iteration 39, loss = 0.36640541\n",
      "Iteration 40, loss = 0.36308325\n",
      "Iteration 41, loss = 0.35987341\n",
      "Iteration 42, loss = 0.35688138\n",
      "Iteration 43, loss = 0.35388560\n",
      "Iteration 44, loss = 0.35115356\n",
      "Iteration 45, loss = 0.34842680\n",
      "Iteration 46, loss = 0.34590955\n",
      "Iteration 47, loss = 0.34348379\n",
      "Iteration 48, loss = 0.34099772\n",
      "Iteration 49, loss = 0.33873864\n",
      "Iteration 50, loss = 0.33652523\n",
      "Iteration 51, loss = 0.33446055\n",
      "Iteration 52, loss = 0.33231740\n",
      "Iteration 53, loss = 0.33034009\n",
      "Iteration 54, loss = 0.32835426\n",
      "Iteration 55, loss = 0.32644883\n",
      "Iteration 56, loss = 0.32463749\n",
      "Iteration 57, loss = 0.32283885\n",
      "Iteration 58, loss = 0.32113716\n",
      "Iteration 59, loss = 0.31947194\n",
      "Iteration 60, loss = 0.31789115\n",
      "Iteration 61, loss = 0.31627165\n",
      "Iteration 62, loss = 0.31467538\n",
      "Iteration 63, loss = 0.31309360\n",
      "Iteration 64, loss = 0.31172711\n",
      "Iteration 65, loss = 0.31028780\n",
      "Iteration 66, loss = 0.30885348\n",
      "Iteration 67, loss = 0.30743537\n",
      "Iteration 68, loss = 0.30618226\n",
      "Iteration 69, loss = 0.30474737\n",
      "Iteration 70, loss = 0.30350072\n",
      "Iteration 71, loss = 0.30226971\n",
      "Iteration 72, loss = 0.30100111\n",
      "Iteration 73, loss = 0.29975362\n",
      "Iteration 74, loss = 0.29854793\n",
      "Iteration 75, loss = 0.29734316\n",
      "Iteration 76, loss = 0.29623575\n",
      "Iteration 77, loss = 0.29507487\n",
      "Iteration 78, loss = 0.29400551\n",
      "Iteration 79, loss = 0.29286889\n",
      "Iteration 80, loss = 0.29178346\n",
      "Iteration 81, loss = 0.29072011\n",
      "Iteration 82, loss = 0.28968127\n",
      "Iteration 83, loss = 0.28863470\n",
      "Iteration 84, loss = 0.28762147\n",
      "Iteration 85, loss = 0.28666006\n",
      "Iteration 86, loss = 0.28560058\n",
      "Iteration 87, loss = 0.28462238\n",
      "Iteration 88, loss = 0.28363085\n",
      "Iteration 89, loss = 0.28268916\n",
      "Iteration 90, loss = 0.28172853\n",
      "Iteration 91, loss = 0.28083224\n",
      "Iteration 92, loss = 0.27987536\n",
      "Iteration 93, loss = 0.27896818\n",
      "Iteration 94, loss = 0.27809834\n",
      "Iteration 95, loss = 0.27722509\n",
      "Iteration 96, loss = 0.27629791\n",
      "Iteration 97, loss = 0.27541744\n",
      "Iteration 98, loss = 0.27456974\n",
      "Iteration 99, loss = 0.27367780\n",
      "Iteration 100, loss = 0.27287175\n",
      "Iteration 101, loss = 0.27206125\n",
      "Iteration 102, loss = 0.27119026\n",
      "Iteration 103, loss = 0.27036055\n",
      "Iteration 104, loss = 0.26961164\n",
      "Iteration 105, loss = 0.26874662\n",
      "Iteration 106, loss = 0.26792105\n",
      "Iteration 107, loss = 0.26720405\n",
      "Iteration 108, loss = 0.26636608\n",
      "Iteration 109, loss = 0.26558819\n",
      "Iteration 110, loss = 0.26476467\n",
      "Iteration 111, loss = 0.26407237\n",
      "Iteration 112, loss = 0.26329055\n",
      "Iteration 113, loss = 0.26250599\n",
      "Iteration 114, loss = 0.26172611\n",
      "Iteration 115, loss = 0.26097773\n",
      "Iteration 116, loss = 0.26033226\n",
      "Iteration 117, loss = 0.25950494\n",
      "Iteration 118, loss = 0.25882352\n",
      "Iteration 119, loss = 0.25804976\n",
      "Iteration 120, loss = 0.25721449\n",
      "Iteration 121, loss = 0.25665596\n",
      "Iteration 122, loss = 0.25596274\n",
      "Iteration 123, loss = 0.25520907\n",
      "Iteration 124, loss = 0.25451401\n",
      "Iteration 125, loss = 0.25376790\n",
      "Iteration 126, loss = 0.25304864\n",
      "Iteration 127, loss = 0.25238303\n",
      "Iteration 128, loss = 0.25171616\n",
      "Iteration 129, loss = 0.25102441\n",
      "Iteration 130, loss = 0.25031349\n",
      "Iteration 131, loss = 0.24968083\n",
      "Iteration 132, loss = 0.24894094\n",
      "Iteration 133, loss = 0.24832494\n",
      "Iteration 134, loss = 0.24771078\n",
      "Iteration 135, loss = 0.24697069\n",
      "Iteration 136, loss = 0.24631701\n",
      "Iteration 137, loss = 0.24569245\n",
      "Iteration 138, loss = 0.24502580\n",
      "Iteration 139, loss = 0.24439960\n",
      "Iteration 140, loss = 0.24371311\n",
      "Iteration 141, loss = 0.24307803\n",
      "Iteration 142, loss = 0.24242654\n",
      "Iteration 143, loss = 0.24179112\n",
      "Iteration 144, loss = 0.24116755\n",
      "Iteration 145, loss = 0.24054792\n",
      "Iteration 146, loss = 0.23988744\n",
      "Iteration 147, loss = 0.23932905\n",
      "Iteration 148, loss = 0.23866071\n",
      "Iteration 149, loss = 0.23802436\n",
      "Iteration 150, loss = 0.23741315\n",
      "Iteration 151, loss = 0.23678656\n",
      "Iteration 152, loss = 0.23621271\n",
      "Iteration 153, loss = 0.23558994\n",
      "Iteration 154, loss = 0.23495279\n",
      "Iteration 155, loss = 0.23437180\n",
      "Iteration 156, loss = 0.23380192\n",
      "Iteration 157, loss = 0.23316206\n",
      "Iteration 158, loss = 0.23258369\n",
      "Iteration 159, loss = 0.23193510\n",
      "Iteration 160, loss = 0.23141967\n",
      "Iteration 161, loss = 0.23083072\n",
      "Iteration 162, loss = 0.23019095\n",
      "Iteration 163, loss = 0.22964383\n",
      "Iteration 164, loss = 0.22902276\n",
      "Iteration 165, loss = 0.22849353\n",
      "Iteration 166, loss = 0.22787003\n",
      "Iteration 167, loss = 0.22733069\n",
      "Iteration 168, loss = 0.22679033\n",
      "Iteration 169, loss = 0.22618640\n",
      "Iteration 170, loss = 0.22560470\n",
      "Iteration 171, loss = 0.22505984\n",
      "Iteration 172, loss = 0.22448450\n",
      "Iteration 173, loss = 0.22398711\n",
      "Iteration 174, loss = 0.22337875\n",
      "Iteration 175, loss = 0.22288526\n",
      "Iteration 176, loss = 0.22228670\n",
      "Iteration 177, loss = 0.22173938\n",
      "Iteration 178, loss = 0.22119499\n",
      "Iteration 179, loss = 0.22067931\n",
      "Iteration 180, loss = 0.22011729\n",
      "Iteration 181, loss = 0.21957967\n",
      "Iteration 182, loss = 0.21901667\n",
      "Iteration 183, loss = 0.21851875\n",
      "Iteration 184, loss = 0.21794667\n",
      "Iteration 185, loss = 0.21743694\n",
      "Iteration 186, loss = 0.21691033\n",
      "Iteration 187, loss = 0.21634919\n",
      "Iteration 188, loss = 0.21584310\n",
      "Iteration 189, loss = 0.21529490\n",
      "Iteration 190, loss = 0.21477570\n",
      "Iteration 191, loss = 0.21429570\n",
      "Iteration 192, loss = 0.21375960\n",
      "Iteration 193, loss = 0.21328007\n",
      "Iteration 194, loss = 0.21274912\n",
      "Iteration 195, loss = 0.21228142\n",
      "Iteration 196, loss = 0.21177570\n",
      "Iteration 197, loss = 0.21122197\n",
      "Iteration 198, loss = 0.21074496\n",
      "Iteration 199, loss = 0.21026284\n",
      "Iteration 200, loss = 0.20970956\n",
      "[CV] END activation=logistic, alpha=0.001, batch_size=100, hidden_layer_sizes=(100,), solver=sgd; total time= 1.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.83697288\n",
      "Iteration 2, loss = 0.28621062\n",
      "Iteration 3, loss = 0.21764469\n",
      "Iteration 4, loss = 0.17806070\n",
      "Iteration 5, loss = 0.14947761\n",
      "Iteration 6, loss = 0.12783488\n",
      "Iteration 7, loss = 0.11144658\n",
      "Iteration 8, loss = 0.09630821\n",
      "Iteration 9, loss = 0.08438737\n",
      "Iteration 10, loss = 0.07600735\n",
      "Iteration 11, loss = 0.06704404\n",
      "Iteration 12, loss = 0.06041177\n",
      "Iteration 13, loss = 0.05433349\n",
      "Iteration 14, loss = 0.04969333\n",
      "Iteration 15, loss = 0.04606533\n",
      "Iteration 16, loss = 0.04212943\n",
      "Iteration 17, loss = 0.03807457\n",
      "Iteration 18, loss = 0.03651895\n",
      "Iteration 19, loss = 0.03448414\n",
      "Iteration 20, loss = 0.03155298\n",
      "Iteration 21, loss = 0.03042654\n",
      "Iteration 22, loss = 0.02990466\n",
      "Iteration 23, loss = 0.02913043\n",
      "Iteration 24, loss = 0.02946032\n",
      "Iteration 25, loss = 0.02701872\n",
      "Iteration 26, loss = 0.02701007\n",
      "Iteration 27, loss = 0.02681433\n",
      "Iteration 28, loss = 0.02425591\n",
      "Iteration 29, loss = 0.02406889\n",
      "Iteration 30, loss = 0.02911654\n",
      "Iteration 31, loss = 0.02727882\n",
      "Iteration 32, loss = 0.02341669\n",
      "Iteration 33, loss = 0.02222607\n",
      "Iteration 34, loss = 0.02948047\n",
      "Iteration 35, loss = 0.02804988\n",
      "Iteration 36, loss = 0.02227085\n",
      "Iteration 37, loss = 0.02069679\n",
      "Iteration 38, loss = 0.01998212\n",
      "Iteration 39, loss = 0.01953239\n",
      "Iteration 40, loss = 0.03542851\n",
      "Iteration 41, loss = 0.02561643\n",
      "Iteration 42, loss = 0.02089077\n",
      "Iteration 43, loss = 0.01978837\n",
      "Iteration 44, loss = 0.01901861\n",
      "Iteration 45, loss = 0.01847499\n",
      "Iteration 46, loss = 0.03967112\n",
      "Iteration 47, loss = 0.02376385\n",
      "Iteration 48, loss = 0.02023480\n",
      "Iteration 49, loss = 0.01924490\n",
      "Iteration 50, loss = 0.01857087\n",
      "Iteration 51, loss = 0.01814504\n",
      "Iteration 52, loss = 0.02576367\n",
      "Iteration 53, loss = 0.03376114\n",
      "Iteration 54, loss = 0.02089420\n",
      "Iteration 55, loss = 0.01904004\n",
      "Iteration 56, loss = 0.01830731\n",
      "Iteration 57, loss = 0.01774842\n",
      "Iteration 58, loss = 0.01770219\n",
      "Iteration 59, loss = 0.04446060\n",
      "Iteration 60, loss = 0.02206101\n",
      "Iteration 61, loss = 0.01898911\n",
      "Iteration 62, loss = 0.01825580\n",
      "Iteration 63, loss = 0.01772479\n",
      "Iteration 64, loss = 0.01725407\n",
      "Iteration 65, loss = 0.01694572\n",
      "Iteration 66, loss = 0.03220142\n",
      "Iteration 67, loss = 0.02860624\n",
      "Iteration 68, loss = 0.01955562\n",
      "Iteration 69, loss = 0.01801793\n",
      "Iteration 70, loss = 0.01743061\n",
      "Iteration 71, loss = 0.01698127\n",
      "Iteration 72, loss = 0.01657043\n",
      "Iteration 73, loss = 0.03586814\n",
      "Iteration 74, loss = 0.02445221\n",
      "Iteration 75, loss = 0.01905760\n",
      "Iteration 76, loss = 0.01763609\n",
      "Iteration 77, loss = 0.01703521\n",
      "Iteration 78, loss = 0.01659092\n",
      "Iteration 79, loss = 0.01627832\n",
      "Iteration 80, loss = 0.02088381\n",
      "Iteration 81, loss = 0.03639617\n",
      "Iteration 82, loss = 0.01894156\n",
      "Iteration 83, loss = 0.01740628\n",
      "Iteration 84, loss = 0.01678051\n",
      "Iteration 85, loss = 0.01630965\n",
      "Iteration 86, loss = 0.01606784\n",
      "Iteration 87, loss = 0.01653963\n",
      "Iteration 88, loss = 0.03801786\n",
      "Iteration 89, loss = 0.01932922\n",
      "Iteration 90, loss = 0.01723848\n",
      "Iteration 91, loss = 0.01653981\n",
      "Iteration 92, loss = 0.01607905\n",
      "Iteration 93, loss = 0.01573854\n",
      "Iteration 94, loss = 0.01570188\n",
      "Iteration 95, loss = 0.03841039\n",
      "Iteration 96, loss = 0.01917069\n",
      "Iteration 97, loss = 0.01687223\n",
      "Iteration 98, loss = 0.01625859\n",
      "Iteration 99, loss = 0.01581212\n",
      "Iteration 100, loss = 0.01550263\n",
      "Iteration 101, loss = 0.03683278\n",
      "Iteration 102, loss = 0.01933485\n",
      "Iteration 103, loss = 0.01712718\n",
      "Iteration 104, loss = 0.01645264\n",
      "Iteration 105, loss = 0.01585585\n",
      "Iteration 106, loss = 0.01554455\n",
      "Iteration 107, loss = 0.01534395\n",
      "Iteration 108, loss = 0.03593523\n",
      "Iteration 109, loss = 0.02110818\n",
      "Iteration 110, loss = 0.01711776\n",
      "Iteration 111, loss = 0.01620953\n",
      "Iteration 112, loss = 0.01570500\n",
      "Iteration 113, loss = 0.01539938\n",
      "Iteration 114, loss = 0.01512880\n",
      "Iteration 115, loss = 0.02325650\n",
      "Iteration 116, loss = 0.02646819\n",
      "Iteration 117, loss = 0.01807367\n",
      "Iteration 118, loss = 0.01627506\n",
      "Iteration 119, loss = 0.01559497\n",
      "Iteration 120, loss = 0.01514946\n",
      "Iteration 121, loss = 0.01485553\n",
      "Iteration 122, loss = 0.01923031\n",
      "Iteration 123, loss = 0.03109002\n",
      "Iteration 124, loss = 0.01839975\n",
      "Iteration 125, loss = 0.01611622\n",
      "Iteration 126, loss = 0.01545567\n",
      "Iteration 127, loss = 0.01507055\n",
      "Iteration 128, loss = 0.01491139\n",
      "Iteration 129, loss = 0.01472296\n",
      "Iteration 130, loss = 0.02477451\n",
      "Iteration 131, loss = 0.02573949\n",
      "Iteration 132, loss = 0.01775672\n",
      "Iteration 133, loss = 0.01600348\n",
      "Iteration 134, loss = 0.01530925\n",
      "Iteration 135, loss = 0.01492965\n",
      "Iteration 136, loss = 0.01467694\n",
      "Iteration 137, loss = 0.02289974\n",
      "Iteration 138, loss = 0.02787316\n",
      "Iteration 139, loss = 0.01724523\n",
      "Iteration 140, loss = 0.01577977\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.001, batch_size=100, hidden_layer_sizes=(400, 100), solver=adam; total time= 7.4min\n",
      "Iteration 1, loss = 0.84360762\n",
      "Iteration 2, loss = 0.28566084\n",
      "Iteration 3, loss = 0.21474439\n",
      "Iteration 4, loss = 0.17710795\n",
      "Iteration 5, loss = 0.15073817\n",
      "Iteration 6, loss = 0.12891345\n",
      "Iteration 7, loss = 0.11297569\n",
      "Iteration 8, loss = 0.09810795\n",
      "Iteration 9, loss = 0.08694275\n",
      "Iteration 10, loss = 0.07654144\n",
      "Iteration 11, loss = 0.06792121\n",
      "Iteration 12, loss = 0.06007264\n",
      "Iteration 13, loss = 0.05430290\n",
      "Iteration 14, loss = 0.05003882\n",
      "Iteration 15, loss = 0.04517212\n",
      "Iteration 16, loss = 0.03998439\n",
      "Iteration 17, loss = 0.03896181\n",
      "Iteration 18, loss = 0.03602761\n",
      "Iteration 19, loss = 0.03313710\n",
      "Iteration 20, loss = 0.03190987\n",
      "Iteration 21, loss = 0.02964949\n",
      "Iteration 22, loss = 0.02932523\n",
      "Iteration 23, loss = 0.02792123\n",
      "Iteration 24, loss = 0.02826253\n",
      "Iteration 25, loss = 0.02790657\n",
      "Iteration 26, loss = 0.02615439\n",
      "Iteration 27, loss = 0.02583817\n",
      "Iteration 28, loss = 0.02516861\n",
      "Iteration 29, loss = 0.02645960\n",
      "Iteration 30, loss = 0.02759194\n",
      "Iteration 31, loss = 0.03297035\n",
      "Iteration 32, loss = 0.02383854\n",
      "Iteration 33, loss = 0.02355642\n",
      "Iteration 34, loss = 0.02156830\n",
      "Iteration 35, loss = 0.02051644\n",
      "Iteration 36, loss = 0.01995659\n",
      "Iteration 37, loss = 0.03504760\n",
      "Iteration 38, loss = 0.02651923\n",
      "Iteration 39, loss = 0.02224235\n",
      "Iteration 40, loss = 0.02035427\n",
      "Iteration 41, loss = 0.01976164\n",
      "Iteration 42, loss = 0.02044038\n",
      "Iteration 43, loss = 0.03750746\n",
      "Iteration 44, loss = 0.02404825\n",
      "Iteration 45, loss = 0.02037714\n",
      "Iteration 46, loss = 0.01933031\n",
      "Iteration 47, loss = 0.01874142\n",
      "Iteration 48, loss = 0.01845644\n",
      "Iteration 49, loss = 0.03437101\n",
      "Iteration 50, loss = 0.02863252\n",
      "Iteration 51, loss = 0.02094774\n",
      "Iteration 52, loss = 0.01928392\n",
      "Iteration 53, loss = 0.01852607\n",
      "Iteration 54, loss = 0.01799531\n",
      "Iteration 55, loss = 0.01837914\n",
      "Iteration 56, loss = 0.03879470\n",
      "Iteration 57, loss = 0.02166586\n",
      "Iteration 58, loss = 0.01905642\n",
      "Iteration 59, loss = 0.01834623\n",
      "Iteration 60, loss = 0.01766677\n",
      "Iteration 61, loss = 0.01722776\n",
      "Iteration 62, loss = 0.03415982\n",
      "Iteration 63, loss = 0.02837431\n",
      "Iteration 64, loss = 0.02021393\n",
      "Iteration 65, loss = 0.01830242\n",
      "Iteration 66, loss = 0.01767144\n",
      "Iteration 67, loss = 0.01718270\n",
      "Iteration 68, loss = 0.01733430\n",
      "Iteration 69, loss = 0.03631925\n",
      "Iteration 70, loss = 0.02061964\n",
      "Iteration 71, loss = 0.01855307\n",
      "Iteration 72, loss = 0.01748662\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.001, batch_size=100, hidden_layer_sizes=(400, 100), solver=adam; total time=395.9min\n",
      "Iteration 1, loss = 0.82538894\n",
      "Iteration 2, loss = 0.28313681\n",
      "Iteration 3, loss = 0.21588635\n",
      "Iteration 4, loss = 0.17548813\n",
      "Iteration 5, loss = 0.14660043\n",
      "Iteration 6, loss = 0.12563631\n",
      "Iteration 7, loss = 0.10859053\n",
      "Iteration 8, loss = 0.09614239\n",
      "Iteration 9, loss = 0.08449984\n",
      "Iteration 10, loss = 0.07582873\n",
      "Iteration 11, loss = 0.06637680\n",
      "Iteration 12, loss = 0.05994875\n",
      "Iteration 13, loss = 0.05404688\n",
      "Iteration 14, loss = 0.04849800\n",
      "Iteration 15, loss = 0.04503753\n",
      "Iteration 16, loss = 0.04129437\n",
      "Iteration 17, loss = 0.03857839\n",
      "Iteration 18, loss = 0.03627919\n",
      "Iteration 19, loss = 0.03431152\n",
      "Iteration 20, loss = 0.03235801\n",
      "Iteration 21, loss = 0.03184155\n",
      "Iteration 22, loss = 0.02886926\n",
      "Iteration 23, loss = 0.02767532\n",
      "Iteration 24, loss = 0.02689428\n",
      "Iteration 25, loss = 0.03024339\n",
      "Iteration 26, loss = 0.02971232\n",
      "Iteration 27, loss = 0.02457388\n",
      "Iteration 28, loss = 0.02320583\n",
      "Iteration 29, loss = 0.02324239\n",
      "Iteration 30, loss = 0.02886697\n",
      "Iteration 31, loss = 0.03135271\n",
      "Iteration 32, loss = 0.02465774\n",
      "Iteration 33, loss = 0.02188625\n",
      "Iteration 34, loss = 0.02093765\n",
      "Iteration 35, loss = 0.02268712\n",
      "Iteration 36, loss = 0.03787659\n",
      "Iteration 37, loss = 0.02382665\n",
      "Iteration 38, loss = 0.02143929\n",
      "Iteration 39, loss = 0.02045357\n",
      "Iteration 40, loss = 0.01969226\n",
      "Iteration 41, loss = 0.01935280\n",
      "Iteration 42, loss = 0.03833602\n",
      "Iteration 43, loss = 0.02350462\n",
      "Iteration 44, loss = 0.02031450\n",
      "Iteration 45, loss = 0.01935886\n",
      "Iteration 46, loss = 0.01869377\n",
      "Iteration 47, loss = 0.01839766\n",
      "Iteration 48, loss = 0.03644582\n",
      "Iteration 49, loss = 0.02807169\n",
      "Iteration 50, loss = 0.02034554\n",
      "Iteration 51, loss = 0.01918654\n",
      "Iteration 52, loss = 0.01851723\n",
      "Iteration 53, loss = 0.01811647\n",
      "Iteration 54, loss = 0.01773914\n",
      "Iteration 55, loss = 0.02797882\n",
      "Iteration 56, loss = 0.03293840\n",
      "Iteration 57, loss = 0.02101803\n",
      "Iteration 58, loss = 0.01880992\n",
      "Iteration 59, loss = 0.01812876\n",
      "Iteration 60, loss = 0.01752649\n",
      "Iteration 61, loss = 0.01730964\n",
      "Iteration 62, loss = 0.02661437\n",
      "Iteration 63, loss = 0.03117333\n",
      "Iteration 64, loss = 0.02176419\n",
      "Iteration 65, loss = 0.01838060\n",
      "Iteration 66, loss = 0.01759052\n",
      "Iteration 67, loss = 0.01704174\n",
      "Iteration 68, loss = 0.01669289\n",
      "Iteration 69, loss = 0.01997352\n",
      "Iteration 70, loss = 0.03912220\n",
      "Iteration 71, loss = 0.02143489\n",
      "Iteration 72, loss = 0.01825873\n",
      "Iteration 73, loss = 0.01750748\n",
      "Iteration 74, loss = 0.01696406\n",
      "Iteration 75, loss = 0.01659962\n",
      "Iteration 76, loss = 0.01646930\n",
      "Iteration 77, loss = 0.03790865\n",
      "Iteration 78, loss = 0.02352802\n",
      "Iteration 79, loss = 0.01810322\n",
      "Iteration 80, loss = 0.01726762\n",
      "Iteration 81, loss = 0.01674539\n",
      "Iteration 82, loss = 0.01635976\n",
      "Iteration 83, loss = 0.01664983\n",
      "Iteration 84, loss = 0.03686463\n",
      "Iteration 85, loss = 0.02024317\n",
      "Iteration 86, loss = 0.01747218\n",
      "Iteration 87, loss = 0.01677430\n",
      "Iteration 88, loss = 0.01628708\n",
      "Iteration 89, loss = 0.01596490\n",
      "Iteration 90, loss = 0.03227764\n",
      "Iteration 91, loss = 0.02227582\n",
      "Iteration 92, loss = 0.01811483\n",
      "Iteration 93, loss = 0.01691746\n",
      "Iteration 94, loss = 0.01627553\n",
      "Iteration 95, loss = 0.01596749\n",
      "Iteration 96, loss = 0.01582149\n",
      "Iteration 97, loss = 0.03126271\n",
      "Iteration 98, loss = 0.02054125\n",
      "Iteration 99, loss = 0.01742101\n",
      "Iteration 100, loss = 0.01638178\n",
      "Iteration 101, loss = 0.01590026\n",
      "Iteration 102, loss = 0.01553604\n",
      "Iteration 103, loss = 0.01533505\n",
      "Iteration 104, loss = 0.03396431\n",
      "Iteration 105, loss = 0.02037426\n",
      "Iteration 106, loss = 0.01666446\n",
      "Iteration 107, loss = 0.01596235\n",
      "Iteration 108, loss = 0.01554841\n",
      "Iteration 109, loss = 0.01528061\n",
      "Iteration 110, loss = 0.02833769\n",
      "Iteration 111, loss = 0.02266459\n",
      "Iteration 112, loss = 0.01721152\n",
      "Iteration 113, loss = 0.01614010\n",
      "Iteration 114, loss = 0.01559059\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.001, batch_size=100, hidden_layer_sizes=(400, 100), solver=adam; total time= 7.8min\n",
      "Iteration 1, loss = 0.82075790\n",
      "Iteration 2, loss = 0.28359471\n",
      "Iteration 3, loss = 0.21708298\n",
      "Iteration 4, loss = 0.17860961\n",
      "Iteration 5, loss = 0.15106150\n",
      "Iteration 6, loss = 0.13025839\n",
      "Iteration 7, loss = 0.11385170\n",
      "Iteration 8, loss = 0.09876512\n",
      "Iteration 9, loss = 0.08689603\n",
      "Iteration 10, loss = 0.07663720\n",
      "Iteration 11, loss = 0.06915953\n",
      "Iteration 12, loss = 0.06093211\n",
      "Iteration 13, loss = 0.05568516\n",
      "Iteration 14, loss = 0.05015313\n",
      "Iteration 15, loss = 0.04517049\n",
      "Iteration 16, loss = 0.04136349\n",
      "Iteration 17, loss = 0.03945403\n",
      "Iteration 18, loss = 0.03599809\n",
      "Iteration 19, loss = 0.03408376\n",
      "Iteration 20, loss = 0.03362420\n",
      "Iteration 21, loss = 0.02950428\n",
      "Iteration 22, loss = 0.02861446\n",
      "Iteration 23, loss = 0.02955591\n",
      "Iteration 24, loss = 0.02813620\n",
      "Iteration 25, loss = 0.02710761\n",
      "Iteration 26, loss = 0.02609354\n",
      "Iteration 27, loss = 0.02523435\n",
      "Iteration 28, loss = 0.02484386\n",
      "Iteration 29, loss = 0.02879383\n",
      "Iteration 30, loss = 0.03029294\n",
      "Iteration 31, loss = 0.02594301\n",
      "Iteration 32, loss = 0.02346599\n",
      "Iteration 33, loss = 0.02210965\n",
      "Iteration 34, loss = 0.02611129\n",
      "Iteration 35, loss = 0.03007120\n",
      "Iteration 36, loss = 0.02308611\n",
      "Iteration 37, loss = 0.02106921\n",
      "Iteration 38, loss = 0.02001470\n",
      "Iteration 39, loss = 0.01938338\n",
      "Iteration 40, loss = 0.03640776\n",
      "Iteration 41, loss = 0.02757479\n",
      "Iteration 42, loss = 0.02111136\n",
      "Iteration 43, loss = 0.01982123\n",
      "Iteration 44, loss = 0.01916823\n",
      "Iteration 45, loss = 0.01865966\n",
      "Iteration 46, loss = 0.01852613\n",
      "Iteration 47, loss = 0.04116120\n",
      "Iteration 48, loss = 0.02298924\n",
      "Iteration 49, loss = 0.02010270\n",
      "Iteration 50, loss = 0.01890795\n",
      "Iteration 51, loss = 0.01834634\n",
      "Iteration 52, loss = 0.01822812\n",
      "Iteration 53, loss = 0.03542355\n",
      "Iteration 54, loss = 0.02641068\n",
      "Iteration 55, loss = 0.02011737\n",
      "Iteration 56, loss = 0.01872753\n",
      "Iteration 57, loss = 0.01812677\n",
      "Iteration 58, loss = 0.01763210\n",
      "Iteration 59, loss = 0.01787835\n",
      "Iteration 60, loss = 0.03746769\n",
      "Iteration 61, loss = 0.02141208\n",
      "Iteration 62, loss = 0.01930105\n",
      "Iteration 63, loss = 0.01804220\n",
      "Iteration 64, loss = 0.01742972\n",
      "Iteration 65, loss = 0.01755311\n",
      "Iteration 66, loss = 0.04053492\n",
      "Iteration 67, loss = 0.02089120\n",
      "Iteration 68, loss = 0.01863342\n",
      "Iteration 69, loss = 0.01780895\n",
      "Iteration 70, loss = 0.01725278\n",
      "Iteration 71, loss = 0.01679470\n",
      "Iteration 72, loss = 0.01695703\n",
      "Iteration 73, loss = 0.03974709\n",
      "Iteration 74, loss = 0.02039769\n",
      "Iteration 75, loss = 0.01807799\n",
      "Iteration 76, loss = 0.01737594\n",
      "Iteration 77, loss = 0.01687112\n",
      "Iteration 78, loss = 0.01642064\n",
      "Iteration 79, loss = 0.01624442\n",
      "Iteration 80, loss = 0.03689472\n",
      "Iteration 81, loss = 0.02181052\n",
      "Iteration 82, loss = 0.01837013\n",
      "Iteration 83, loss = 0.01713628\n",
      "Iteration 84, loss = 0.01661430\n",
      "Iteration 85, loss = 0.01627985\n",
      "Iteration 86, loss = 0.01605255\n",
      "Iteration 87, loss = 0.03728610\n",
      "Iteration 88, loss = 0.02148171\n",
      "Iteration 89, loss = 0.01766073\n",
      "Iteration 90, loss = 0.01690325\n",
      "Iteration 91, loss = 0.01633651\n",
      "Iteration 92, loss = 0.01596569\n",
      "Iteration 93, loss = 0.01790193\n",
      "Iteration 94, loss = 0.03137502\n",
      "Iteration 95, loss = 0.01917273\n",
      "Iteration 96, loss = 0.01686050\n",
      "Iteration 97, loss = 0.01619890\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.001, batch_size=100, hidden_layer_sizes=(400, 100), solver=adam; total time= 6.5min\n",
      "Iteration 1, loss = 0.83013304\n",
      "Iteration 2, loss = 0.28454011\n",
      "Iteration 3, loss = 0.21608711\n",
      "Iteration 4, loss = 0.17661818\n",
      "Iteration 5, loss = 0.14931356\n",
      "Iteration 6, loss = 0.12863936\n",
      "Iteration 7, loss = 0.11032553\n",
      "Iteration 8, loss = 0.09570332\n",
      "Iteration 9, loss = 0.08445829\n",
      "Iteration 10, loss = 0.07509382\n",
      "Iteration 11, loss = 0.06643303\n",
      "Iteration 12, loss = 0.05947998\n",
      "Iteration 13, loss = 0.05302534\n",
      "Iteration 14, loss = 0.04815018\n",
      "Iteration 15, loss = 0.04458883\n",
      "Iteration 16, loss = 0.04024053\n",
      "Iteration 17, loss = 0.03678255\n",
      "Iteration 18, loss = 0.03424111\n",
      "Iteration 19, loss = 0.03272496\n",
      "Iteration 20, loss = 0.03111123\n",
      "Iteration 21, loss = 0.03006347\n",
      "Iteration 22, loss = 0.02849531\n",
      "Iteration 23, loss = 0.02687230\n",
      "Iteration 24, loss = 0.02554140\n",
      "Iteration 25, loss = 0.02754613\n",
      "Iteration 26, loss = 0.02785807\n",
      "Iteration 27, loss = 0.02458695\n",
      "Iteration 28, loss = 0.02314055\n",
      "Iteration 29, loss = 0.02987173\n",
      "Iteration 30, loss = 0.02280490\n",
      "Iteration 31, loss = 0.02194207\n",
      "Iteration 32, loss = 0.02336250\n",
      "Iteration 33, loss = 0.03526442\n",
      "Iteration 34, loss = 0.02412854\n",
      "Iteration 35, loss = 0.02146300\n",
      "Iteration 36, loss = 0.02026215\n",
      "Iteration 37, loss = 0.01967788\n",
      "Iteration 38, loss = 0.03266728\n",
      "Iteration 39, loss = 0.02464273\n",
      "Iteration 40, loss = 0.02090091\n",
      "Iteration 41, loss = 0.01964704\n",
      "Iteration 42, loss = 0.01913081\n",
      "Iteration 43, loss = 0.03080147\n",
      "Iteration 44, loss = 0.03374037\n",
      "Iteration 45, loss = 0.02102176\n",
      "Iteration 46, loss = 0.01978914\n",
      "Iteration 47, loss = 0.01904882\n",
      "Iteration 48, loss = 0.01852357\n",
      "Iteration 49, loss = 0.01813392\n",
      "Iteration 50, loss = 0.03911926\n",
      "Iteration 51, loss = 0.02708032\n",
      "Iteration 52, loss = 0.02018135\n",
      "Iteration 53, loss = 0.01886281\n",
      "Iteration 54, loss = 0.01827788\n",
      "Iteration 55, loss = 0.01777450\n",
      "Iteration 56, loss = 0.01741345\n",
      "Iteration 57, loss = 0.03554699\n",
      "Iteration 58, loss = 0.02545854\n",
      "Iteration 59, loss = 0.02011273\n",
      "Iteration 60, loss = 0.01846211\n",
      "Iteration 61, loss = 0.01772606\n",
      "Iteration 62, loss = 0.01721712\n",
      "Iteration 63, loss = 0.01688140\n",
      "Iteration 64, loss = 0.01909869\n",
      "Iteration 65, loss = 0.04249355\n",
      "Iteration 66, loss = 0.02023534\n",
      "Iteration 67, loss = 0.01853833\n",
      "Iteration 68, loss = 0.01762455\n",
      "Iteration 69, loss = 0.01711779\n",
      "Iteration 70, loss = 0.01680818\n",
      "Iteration 71, loss = 0.03090017\n",
      "Iteration 72, loss = 0.02477226\n",
      "Iteration 73, loss = 0.01865506\n",
      "Iteration 74, loss = 0.01742397\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.001, batch_size=100, hidden_layer_sizes=(400, 100), solver=adam; total time= 4.8min\n",
      "Iteration 1, loss = 2.30561315\n",
      "Iteration 2, loss = 2.29364568\n",
      "Iteration 3, loss = 2.28615375\n",
      "Iteration 4, loss = 2.27815336\n",
      "Iteration 5, loss = 2.26847354\n",
      "Iteration 6, loss = 2.25670128\n",
      "Iteration 7, loss = 2.24163613\n",
      "Iteration 8, loss = 2.22259503\n",
      "Iteration 9, loss = 2.19633869\n",
      "Iteration 10, loss = 2.16045149\n",
      "Iteration 11, loss = 2.11050799\n",
      "Iteration 12, loss = 2.04171821\n",
      "Iteration 13, loss = 1.95105153\n",
      "Iteration 14, loss = 1.83969162\n",
      "Iteration 15, loss = 1.71267942\n",
      "Iteration 16, loss = 1.57901276\n",
      "Iteration 17, loss = 1.44778956\n",
      "Iteration 18, loss = 1.32773076\n",
      "Iteration 19, loss = 1.22437542\n",
      "Iteration 20, loss = 1.13767249\n",
      "Iteration 21, loss = 1.06522405\n",
      "Iteration 22, loss = 1.00421015\n",
      "Iteration 23, loss = 0.95204605\n",
      "Iteration 24, loss = 0.90649583\n",
      "Iteration 25, loss = 0.86607571\n",
      "Iteration 26, loss = 0.82971883\n",
      "Iteration 27, loss = 0.79668691\n",
      "Iteration 28, loss = 0.76643876\n",
      "Iteration 29, loss = 0.73863348\n",
      "Iteration 30, loss = 0.71303517\n",
      "Iteration 31, loss = 0.68960561\n",
      "Iteration 32, loss = 0.66796400\n",
      "Iteration 33, loss = 0.64819775\n",
      "Iteration 34, loss = 0.63013654\n",
      "Iteration 35, loss = 0.61345220\n",
      "Iteration 36, loss = 0.59801687\n",
      "Iteration 37, loss = 0.58374644\n",
      "Iteration 38, loss = 0.57087355\n",
      "Iteration 39, loss = 0.55862714\n",
      "Iteration 40, loss = 0.54741983\n",
      "Iteration 41, loss = 0.53676601\n",
      "Iteration 42, loss = 0.52700113\n",
      "Iteration 43, loss = 0.51769749\n",
      "Iteration 44, loss = 0.50907279\n",
      "Iteration 45, loss = 0.50072095\n",
      "Iteration 46, loss = 0.49309777\n",
      "Iteration 47, loss = 0.48559930\n",
      "Iteration 48, loss = 0.47876425\n",
      "Iteration 49, loss = 0.47198027\n",
      "Iteration 50, loss = 0.46590881\n",
      "Iteration 51, loss = 0.45985777\n",
      "Iteration 52, loss = 0.45427933\n",
      "Iteration 53, loss = 0.44877810\n",
      "Iteration 54, loss = 0.44353486\n",
      "Iteration 55, loss = 0.43880865\n",
      "Iteration 56, loss = 0.43409846\n",
      "Iteration 57, loss = 0.42967951\n",
      "Iteration 58, loss = 0.42525445\n",
      "Iteration 59, loss = 0.42108942\n",
      "Iteration 60, loss = 0.41719810\n",
      "Iteration 61, loss = 0.41347994\n",
      "Iteration 62, loss = 0.40996552\n",
      "Iteration 63, loss = 0.40639048\n",
      "Iteration 64, loss = 0.40316547\n",
      "Iteration 65, loss = 0.39993529\n",
      "Iteration 66, loss = 0.39696773\n",
      "Iteration 67, loss = 0.39394861\n",
      "Iteration 68, loss = 0.39116182\n",
      "Iteration 69, loss = 0.38840001\n",
      "Iteration 70, loss = 0.38577123\n",
      "Iteration 71, loss = 0.38328171\n",
      "Iteration 72, loss = 0.38089160\n",
      "Iteration 73, loss = 0.37842573\n",
      "Iteration 74, loss = 0.37627630\n",
      "Iteration 75, loss = 0.37398880\n",
      "Iteration 76, loss = 0.37187378\n",
      "Iteration 77, loss = 0.36974472\n",
      "Iteration 78, loss = 0.36765229\n",
      "Iteration 79, loss = 0.36584008\n",
      "Iteration 80, loss = 0.36385568\n",
      "Iteration 81, loss = 0.36190953\n",
      "Iteration 82, loss = 0.36023916\n",
      "Iteration 83, loss = 0.35847979\n",
      "Iteration 84, loss = 0.35672194\n",
      "Iteration 85, loss = 0.35506858\n",
      "Iteration 86, loss = 0.35341493\n",
      "Iteration 87, loss = 0.35175633\n",
      "Iteration 88, loss = 0.35034573\n",
      "Iteration 89, loss = 0.34872843\n",
      "Iteration 90, loss = 0.34729843\n",
      "Iteration 91, loss = 0.34582354\n",
      "Iteration 92, loss = 0.34433197\n",
      "Iteration 93, loss = 0.34300442\n",
      "Iteration 94, loss = 0.34163960\n",
      "Iteration 95, loss = 0.34023265\n",
      "Iteration 96, loss = 0.33898918\n",
      "Iteration 97, loss = 0.33758042\n",
      "Iteration 98, loss = 0.33638951\n",
      "Iteration 99, loss = 0.33513207\n",
      "Iteration 100, loss = 0.33383223\n",
      "Iteration 101, loss = 0.33265763\n",
      "Iteration 102, loss = 0.33145304\n",
      "Iteration 103, loss = 0.33027898\n",
      "Iteration 104, loss = 0.32908986\n",
      "Iteration 105, loss = 0.32788636\n",
      "Iteration 106, loss = 0.32671940\n",
      "Iteration 107, loss = 0.32572086\n",
      "Iteration 108, loss = 0.32441504\n",
      "Iteration 109, loss = 0.32350690\n",
      "Iteration 110, loss = 0.32237106\n",
      "Iteration 111, loss = 0.32118687\n",
      "Iteration 112, loss = 0.32025908\n",
      "Iteration 113, loss = 0.31911072\n",
      "Iteration 114, loss = 0.31812453\n",
      "Iteration 115, loss = 0.31719295\n",
      "Iteration 116, loss = 0.31608381\n",
      "Iteration 117, loss = 0.31519199\n",
      "Iteration 118, loss = 0.31419460\n",
      "Iteration 119, loss = 0.31310012\n",
      "Iteration 120, loss = 0.31219621\n",
      "Iteration 121, loss = 0.31133650\n",
      "Iteration 122, loss = 0.31031507\n",
      "Iteration 123, loss = 0.30935469\n",
      "Iteration 124, loss = 0.30831829\n",
      "Iteration 125, loss = 0.30759880\n",
      "Iteration 126, loss = 0.30661424\n",
      "Iteration 127, loss = 0.30561245\n",
      "Iteration 128, loss = 0.30470663\n",
      "Iteration 129, loss = 0.30396185\n",
      "Iteration 130, loss = 0.30307956\n",
      "Iteration 131, loss = 0.30217692\n",
      "Iteration 132, loss = 0.30133161\n",
      "Iteration 133, loss = 0.30039312\n",
      "Iteration 134, loss = 0.29963454\n",
      "Iteration 135, loss = 0.29877423\n",
      "Iteration 136, loss = 0.29793153\n",
      "Iteration 137, loss = 0.29702262\n",
      "Iteration 138, loss = 0.29632946\n",
      "Iteration 139, loss = 0.29546862\n",
      "Iteration 140, loss = 0.29470846\n",
      "Iteration 141, loss = 0.29381408\n",
      "Iteration 142, loss = 0.29303428\n",
      "Iteration 143, loss = 0.29224551\n",
      "Iteration 144, loss = 0.29150357\n",
      "Iteration 145, loss = 0.29077738\n",
      "Iteration 146, loss = 0.28995994\n",
      "Iteration 147, loss = 0.28907850\n",
      "Iteration 148, loss = 0.28833630\n",
      "Iteration 149, loss = 0.28762715\n",
      "Iteration 150, loss = 0.28682132\n",
      "Iteration 151, loss = 0.28615588\n",
      "Iteration 152, loss = 0.28534825\n",
      "Iteration 153, loss = 0.28458078\n",
      "Iteration 154, loss = 0.28400155\n",
      "Iteration 155, loss = 0.28324840\n",
      "Iteration 156, loss = 0.28245689\n",
      "Iteration 157, loss = 0.28183716\n",
      "Iteration 158, loss = 0.28111789\n",
      "Iteration 159, loss = 0.28035511\n",
      "Iteration 160, loss = 0.27970815\n",
      "Iteration 161, loss = 0.27899052\n",
      "Iteration 162, loss = 0.27831385\n",
      "Iteration 163, loss = 0.27771970\n",
      "Iteration 164, loss = 0.27691183\n",
      "Iteration 165, loss = 0.27630176\n",
      "Iteration 166, loss = 0.27560774\n",
      "Iteration 167, loss = 0.27492204\n",
      "Iteration 168, loss = 0.27430009\n",
      "Iteration 169, loss = 0.27368069\n",
      "Iteration 170, loss = 0.27297977\n",
      "Iteration 171, loss = 0.27244685\n",
      "Iteration 172, loss = 0.27168323\n",
      "Iteration 173, loss = 0.27104265\n",
      "Iteration 174, loss = 0.27038536\n",
      "Iteration 175, loss = 0.26970280\n",
      "Iteration 176, loss = 0.26920920\n",
      "Iteration 177, loss = 0.26850532\n",
      "Iteration 178, loss = 0.26788111\n",
      "Iteration 179, loss = 0.26729632\n",
      "Iteration 180, loss = 0.26662142\n",
      "Iteration 181, loss = 0.26600906\n",
      "Iteration 182, loss = 0.26553842\n",
      "Iteration 183, loss = 0.26486493\n",
      "Iteration 184, loss = 0.26419971\n",
      "Iteration 185, loss = 0.26366219\n",
      "Iteration 186, loss = 0.26303736\n",
      "Iteration 187, loss = 0.26247505\n",
      "Iteration 188, loss = 0.26185828\n",
      "Iteration 189, loss = 0.26135570\n",
      "Iteration 190, loss = 0.26071667\n",
      "Iteration 191, loss = 0.26008792\n",
      "Iteration 192, loss = 0.25945719\n",
      "Iteration 193, loss = 0.25903361\n",
      "Iteration 194, loss = 0.25826311\n",
      "Iteration 195, loss = 0.25781205\n",
      "Iteration 196, loss = 0.25729718\n",
      "Iteration 197, loss = 0.25671635\n",
      "Iteration 198, loss = 0.25619872\n",
      "Iteration 199, loss = 0.25556197\n",
      "Iteration 200, loss = 0.25518348\n",
      "[CV] END activation=logistic, alpha=0.001, batch_size=100, hidden_layer_sizes=(400, 100), solver=sgd; total time= 9.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.30208443\n",
      "Iteration 2, loss = 2.29355843\n",
      "Iteration 3, loss = 2.28654754\n",
      "Iteration 4, loss = 2.27810177\n",
      "Iteration 5, loss = 2.26852768\n",
      "Iteration 6, loss = 2.25677872\n",
      "Iteration 7, loss = 2.24144538\n",
      "Iteration 8, loss = 2.22092679\n",
      "Iteration 9, loss = 2.19302053\n",
      "Iteration 10, loss = 2.15411175\n",
      "Iteration 11, loss = 2.09975688\n",
      "Iteration 12, loss = 2.02629798\n",
      "Iteration 13, loss = 1.93165398\n",
      "Iteration 14, loss = 1.82047811\n",
      "Iteration 15, loss = 1.69946673\n",
      "Iteration 16, loss = 1.57789316\n",
      "Iteration 17, loss = 1.46246047\n",
      "Iteration 18, loss = 1.35752650\n",
      "Iteration 19, loss = 1.26330308\n",
      "Iteration 20, loss = 1.18080219\n",
      "Iteration 21, loss = 1.10905335\n",
      "Iteration 22, loss = 1.04739501\n",
      "Iteration 23, loss = 0.99464503\n",
      "Iteration 24, loss = 0.94941155\n",
      "Iteration 25, loss = 0.91015923\n",
      "Iteration 26, loss = 0.87547955\n",
      "Iteration 27, loss = 0.84462226\n",
      "Iteration 28, loss = 0.81632210\n",
      "Iteration 29, loss = 0.79038692\n",
      "Iteration 30, loss = 0.76580852\n",
      "Iteration 31, loss = 0.74271674\n",
      "Iteration 32, loss = 0.72081091\n",
      "Iteration 33, loss = 0.70010553\n",
      "Iteration 34, loss = 0.68018829\n",
      "Iteration 35, loss = 0.66112357\n",
      "Iteration 36, loss = 0.64302094\n",
      "Iteration 37, loss = 0.62572801\n",
      "Iteration 38, loss = 0.60945269\n",
      "Iteration 39, loss = 0.59379041\n",
      "Iteration 40, loss = 0.57936983\n",
      "Iteration 41, loss = 0.56578777\n",
      "Iteration 42, loss = 0.55294932\n",
      "Iteration 43, loss = 0.54110847\n",
      "Iteration 44, loss = 0.52969092\n",
      "Iteration 45, loss = 0.51929310\n",
      "Iteration 46, loss = 0.50949088\n",
      "Iteration 47, loss = 0.50008542\n",
      "Iteration 48, loss = 0.49152257\n",
      "Iteration 49, loss = 0.48341170\n",
      "Iteration 50, loss = 0.47568074\n",
      "Iteration 51, loss = 0.46860573\n",
      "Iteration 52, loss = 0.46191416\n",
      "Iteration 53, loss = 0.45565331\n",
      "Iteration 54, loss = 0.44964857\n",
      "Iteration 55, loss = 0.44396743\n",
      "Iteration 56, loss = 0.43865094\n",
      "Iteration 57, loss = 0.43346450\n",
      "Iteration 58, loss = 0.42881134\n",
      "Iteration 59, loss = 0.42426475\n",
      "Iteration 60, loss = 0.42000406\n",
      "Iteration 61, loss = 0.41588654\n",
      "Iteration 62, loss = 0.41195492\n",
      "Iteration 63, loss = 0.40832052\n",
      "Iteration 64, loss = 0.40480216\n",
      "Iteration 65, loss = 0.40132730\n",
      "Iteration 66, loss = 0.39822271\n",
      "Iteration 67, loss = 0.39516396\n",
      "Iteration 68, loss = 0.39199462\n",
      "Iteration 69, loss = 0.38938600\n",
      "Iteration 70, loss = 0.38664914\n",
      "Iteration 71, loss = 0.38393893\n",
      "Iteration 72, loss = 0.38141334\n",
      "Iteration 73, loss = 0.37904766\n",
      "Iteration 74, loss = 0.37673205\n",
      "Iteration 75, loss = 0.37450104\n",
      "Iteration 76, loss = 0.37234982\n",
      "Iteration 77, loss = 0.37021450\n",
      "Iteration 78, loss = 0.36803055\n",
      "Iteration 79, loss = 0.36617191\n",
      "Iteration 80, loss = 0.36422344\n",
      "Iteration 81, loss = 0.36230801\n",
      "Iteration 82, loss = 0.36059424\n",
      "Iteration 83, loss = 0.35886517\n",
      "Iteration 84, loss = 0.35712208\n",
      "Iteration 85, loss = 0.35546723\n",
      "Iteration 86, loss = 0.35374813\n",
      "Iteration 87, loss = 0.35213809\n",
      "Iteration 88, loss = 0.35059561\n",
      "Iteration 89, loss = 0.34915870\n",
      "Iteration 90, loss = 0.34769014\n",
      "Iteration 91, loss = 0.34613206\n",
      "Iteration 92, loss = 0.34485354\n",
      "Iteration 93, loss = 0.34338104\n",
      "Iteration 94, loss = 0.34209698\n",
      "Iteration 95, loss = 0.34081036\n",
      "Iteration 96, loss = 0.33942625\n",
      "Iteration 97, loss = 0.33813274\n",
      "Iteration 98, loss = 0.33690221\n",
      "Iteration 99, loss = 0.33558771\n",
      "Iteration 100, loss = 0.33453724\n",
      "Iteration 101, loss = 0.33338911\n",
      "Iteration 102, loss = 0.33213085\n",
      "Iteration 103, loss = 0.33101130\n",
      "Iteration 104, loss = 0.32983187\n",
      "Iteration 105, loss = 0.32865333\n",
      "Iteration 106, loss = 0.32771174\n",
      "Iteration 107, loss = 0.32659887\n",
      "Iteration 108, loss = 0.32567883\n",
      "Iteration 109, loss = 0.32446587\n",
      "Iteration 110, loss = 0.32337560\n",
      "Iteration 111, loss = 0.32246048\n",
      "Iteration 112, loss = 0.32151990\n",
      "Iteration 113, loss = 0.32053642\n",
      "Iteration 114, loss = 0.31942366\n",
      "Iteration 115, loss = 0.31858552\n",
      "Iteration 116, loss = 0.31746370\n",
      "Iteration 117, loss = 0.31665191\n",
      "Iteration 118, loss = 0.31568802\n",
      "Iteration 119, loss = 0.31486625\n",
      "Iteration 120, loss = 0.31397851\n",
      "Iteration 121, loss = 0.31301383\n",
      "Iteration 122, loss = 0.31211427\n",
      "Iteration 123, loss = 0.31120478\n",
      "Iteration 124, loss = 0.31047557\n",
      "Iteration 125, loss = 0.30945550\n",
      "Iteration 126, loss = 0.30856365\n",
      "Iteration 127, loss = 0.30776399\n",
      "Iteration 128, loss = 0.30698525\n",
      "Iteration 129, loss = 0.30619133\n",
      "Iteration 130, loss = 0.30537183\n",
      "Iteration 131, loss = 0.30449569\n",
      "Iteration 132, loss = 0.30373722\n",
      "Iteration 133, loss = 0.30297129\n",
      "Iteration 134, loss = 0.30217869\n",
      "Iteration 135, loss = 0.30139052\n",
      "Iteration 136, loss = 0.30068940\n",
      "Iteration 137, loss = 0.29982986\n",
      "Iteration 138, loss = 0.29927386\n",
      "Iteration 139, loss = 0.29831328\n",
      "Iteration 140, loss = 0.29759835\n",
      "Iteration 141, loss = 0.29684258\n",
      "Iteration 142, loss = 0.29618056\n",
      "Iteration 143, loss = 0.29538288\n",
      "Iteration 144, loss = 0.29479676\n",
      "Iteration 145, loss = 0.29393351\n",
      "Iteration 146, loss = 0.29322776\n",
      "Iteration 147, loss = 0.29268732\n",
      "Iteration 148, loss = 0.29185880\n",
      "Iteration 149, loss = 0.29122116\n",
      "Iteration 150, loss = 0.29041542\n",
      "Iteration 151, loss = 0.28991707\n",
      "Iteration 152, loss = 0.28922933\n",
      "Iteration 153, loss = 0.28850987\n",
      "Iteration 154, loss = 0.28777625\n",
      "Iteration 155, loss = 0.28716428\n",
      "Iteration 156, loss = 0.28651757\n",
      "Iteration 157, loss = 0.28580557\n",
      "Iteration 158, loss = 0.28512609\n",
      "Iteration 159, loss = 0.28456109\n",
      "Iteration 160, loss = 0.28386797\n",
      "Iteration 161, loss = 0.28335712\n",
      "Iteration 162, loss = 0.28256736\n",
      "Iteration 163, loss = 0.28198080\n",
      "Iteration 164, loss = 0.28136792\n",
      "Iteration 165, loss = 0.28072797\n",
      "Iteration 166, loss = 0.28015135\n",
      "Iteration 167, loss = 0.27952369\n",
      "Iteration 168, loss = 0.27883695\n",
      "Iteration 169, loss = 0.27821581\n",
      "Iteration 170, loss = 0.27761258\n",
      "Iteration 171, loss = 0.27709796\n",
      "Iteration 172, loss = 0.27635212\n",
      "Iteration 173, loss = 0.27586866\n",
      "Iteration 174, loss = 0.27521180\n",
      "Iteration 175, loss = 0.27467541\n",
      "Iteration 176, loss = 0.27413800\n",
      "Iteration 177, loss = 0.27347874\n",
      "Iteration 178, loss = 0.27285787\n",
      "Iteration 179, loss = 0.27230378\n",
      "Iteration 180, loss = 0.27176739\n",
      "Iteration 181, loss = 0.27113782\n",
      "Iteration 182, loss = 0.27062401\n",
      "Iteration 183, loss = 0.26996000\n",
      "Iteration 184, loss = 0.26943888\n",
      "Iteration 185, loss = 0.26883510\n",
      "Iteration 186, loss = 0.26835135\n",
      "Iteration 187, loss = 0.26774392\n",
      "Iteration 188, loss = 0.26716444\n",
      "Iteration 189, loss = 0.26661398\n",
      "Iteration 190, loss = 0.26601049\n",
      "Iteration 191, loss = 0.26544312\n",
      "Iteration 192, loss = 0.26485856\n",
      "Iteration 193, loss = 0.26427580\n",
      "Iteration 194, loss = 0.26381584\n",
      "Iteration 195, loss = 0.26331598\n",
      "Iteration 196, loss = 0.26272551\n",
      "Iteration 197, loss = 0.26223963\n",
      "Iteration 198, loss = 0.26157340\n",
      "Iteration 199, loss = 0.26109072\n",
      "Iteration 200, loss = 0.26044292\n",
      "[CV] END activation=logistic, alpha=0.001, batch_size=100, hidden_layer_sizes=(400, 100), solver=sgd; total time= 9.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.30690953\n",
      "Iteration 2, loss = 2.29464466\n",
      "Iteration 3, loss = 2.28813009\n",
      "Iteration 4, loss = 2.28088070\n",
      "Iteration 5, loss = 2.27233617\n",
      "Iteration 6, loss = 2.26227863\n",
      "Iteration 7, loss = 2.24972522\n",
      "Iteration 8, loss = 2.23335836\n",
      "Iteration 9, loss = 2.21165086\n",
      "Iteration 10, loss = 2.18241666\n",
      "Iteration 11, loss = 2.14168505\n",
      "Iteration 12, loss = 2.08524423\n",
      "Iteration 13, loss = 2.00722951\n",
      "Iteration 14, loss = 1.90629471\n",
      "Iteration 15, loss = 1.78470206\n",
      "Iteration 16, loss = 1.65408078\n",
      "Iteration 17, loss = 1.52541016\n",
      "Iteration 18, loss = 1.40626776\n",
      "Iteration 19, loss = 1.30022471\n",
      "Iteration 20, loss = 1.20691596\n",
      "Iteration 21, loss = 1.12628593\n",
      "Iteration 22, loss = 1.05669481\n",
      "Iteration 23, loss = 0.99667807\n",
      "Iteration 24, loss = 0.94462924\n",
      "Iteration 25, loss = 0.89902198\n",
      "Iteration 26, loss = 0.85887458\n",
      "Iteration 27, loss = 0.82283360\n",
      "Iteration 28, loss = 0.79051263\n",
      "Iteration 29, loss = 0.76101813\n",
      "Iteration 30, loss = 0.73413096\n",
      "Iteration 31, loss = 0.70946582\n",
      "Iteration 32, loss = 0.68658096\n",
      "Iteration 33, loss = 0.66549709\n",
      "Iteration 34, loss = 0.64598353\n",
      "Iteration 35, loss = 0.62775817\n",
      "Iteration 36, loss = 0.61113637\n",
      "Iteration 37, loss = 0.59545567\n",
      "Iteration 38, loss = 0.58101861\n",
      "Iteration 39, loss = 0.56762892\n",
      "Iteration 40, loss = 0.55495568\n",
      "Iteration 41, loss = 0.54305786\n",
      "Iteration 42, loss = 0.53207823\n",
      "Iteration 43, loss = 0.52159705\n",
      "Iteration 44, loss = 0.51187726\n",
      "Iteration 45, loss = 0.50266252\n",
      "Iteration 46, loss = 0.49392941\n",
      "Iteration 47, loss = 0.48576661\n",
      "Iteration 48, loss = 0.47806806\n",
      "Iteration 49, loss = 0.47065400\n",
      "Iteration 50, loss = 0.46374651\n",
      "Iteration 51, loss = 0.45713321\n",
      "Iteration 52, loss = 0.45081814\n",
      "Iteration 53, loss = 0.44488718\n",
      "Iteration 54, loss = 0.43932087\n",
      "Iteration 55, loss = 0.43399982\n",
      "Iteration 56, loss = 0.42883607\n",
      "Iteration 57, loss = 0.42411692\n",
      "Iteration 58, loss = 0.41946265\n",
      "Iteration 59, loss = 0.41510607\n",
      "Iteration 60, loss = 0.41100786\n",
      "Iteration 61, loss = 0.40691098\n",
      "Iteration 62, loss = 0.40316272\n",
      "Iteration 63, loss = 0.39941749\n",
      "Iteration 64, loss = 0.39604667\n",
      "Iteration 65, loss = 0.39270301\n",
      "Iteration 66, loss = 0.38963937\n",
      "Iteration 67, loss = 0.38653694\n",
      "Iteration 68, loss = 0.38361008\n",
      "Iteration 69, loss = 0.38073696\n",
      "Iteration 70, loss = 0.37801059\n",
      "Iteration 71, loss = 0.37535887\n",
      "Iteration 72, loss = 0.37291773\n",
      "Iteration 73, loss = 0.37039509\n",
      "Iteration 74, loss = 0.36813711\n",
      "Iteration 75, loss = 0.36598514\n",
      "Iteration 76, loss = 0.36368963\n",
      "Iteration 77, loss = 0.36167375\n",
      "Iteration 78, loss = 0.35943386\n",
      "Iteration 79, loss = 0.35749309\n",
      "Iteration 80, loss = 0.35560957\n",
      "Iteration 81, loss = 0.35364961\n",
      "Iteration 82, loss = 0.35189379\n",
      "Iteration 83, loss = 0.35006238\n",
      "Iteration 84, loss = 0.34834965\n",
      "Iteration 85, loss = 0.34677822\n",
      "Iteration 86, loss = 0.34499929\n",
      "Iteration 87, loss = 0.34339837\n",
      "Iteration 88, loss = 0.34178400\n",
      "Iteration 89, loss = 0.34034598\n",
      "Iteration 90, loss = 0.33886538\n",
      "Iteration 91, loss = 0.33731148\n",
      "Iteration 92, loss = 0.33592021\n",
      "Iteration 93, loss = 0.33450661\n",
      "Iteration 94, loss = 0.33319409\n",
      "Iteration 95, loss = 0.33183458\n",
      "Iteration 96, loss = 0.33045474\n",
      "Iteration 97, loss = 0.32918219\n",
      "Iteration 98, loss = 0.32791119\n",
      "Iteration 99, loss = 0.32674164\n",
      "Iteration 100, loss = 0.32535462\n",
      "Iteration 101, loss = 0.32429154\n",
      "Iteration 102, loss = 0.32307073\n",
      "Iteration 103, loss = 0.32178948\n",
      "Iteration 104, loss = 0.32077617\n",
      "Iteration 105, loss = 0.31972805\n",
      "Iteration 106, loss = 0.31859200\n",
      "Iteration 107, loss = 0.31740274\n",
      "Iteration 108, loss = 0.31625182\n",
      "Iteration 109, loss = 0.31521874\n",
      "Iteration 110, loss = 0.31434568\n",
      "Iteration 111, loss = 0.31327657\n",
      "Iteration 112, loss = 0.31216343\n",
      "Iteration 113, loss = 0.31127318\n",
      "Iteration 114, loss = 0.31023314\n",
      "Iteration 115, loss = 0.30919879\n",
      "Iteration 116, loss = 0.30832928\n",
      "Iteration 117, loss = 0.30728683\n",
      "Iteration 118, loss = 0.30646726\n",
      "Iteration 119, loss = 0.30547017\n",
      "Iteration 120, loss = 0.30454418\n",
      "Iteration 121, loss = 0.30368961\n",
      "Iteration 122, loss = 0.30287949\n",
      "Iteration 123, loss = 0.30191687\n",
      "Iteration 124, loss = 0.30101199\n",
      "Iteration 125, loss = 0.30015134\n",
      "Iteration 126, loss = 0.29927023\n",
      "Iteration 127, loss = 0.29848508\n",
      "Iteration 128, loss = 0.29771400\n",
      "Iteration 129, loss = 0.29680585\n",
      "Iteration 130, loss = 0.29600194\n",
      "Iteration 131, loss = 0.29530433\n",
      "Iteration 132, loss = 0.29445581\n",
      "Iteration 133, loss = 0.29357674\n",
      "Iteration 134, loss = 0.29292725\n",
      "Iteration 135, loss = 0.29210153\n",
      "Iteration 136, loss = 0.29143800\n",
      "Iteration 137, loss = 0.29066568\n",
      "Iteration 138, loss = 0.28990095\n",
      "Iteration 139, loss = 0.28912988\n",
      "Iteration 140, loss = 0.28832808\n",
      "Iteration 141, loss = 0.28769883\n",
      "Iteration 142, loss = 0.28685600\n",
      "Iteration 143, loss = 0.28631377\n",
      "Iteration 144, loss = 0.28555500\n",
      "Iteration 145, loss = 0.28467685\n",
      "Iteration 146, loss = 0.28407319\n",
      "Iteration 147, loss = 0.28344933\n",
      "Iteration 148, loss = 0.28277684\n",
      "Iteration 149, loss = 0.28211063\n",
      "Iteration 150, loss = 0.28131355\n",
      "Iteration 151, loss = 0.28076710\n",
      "Iteration 152, loss = 0.28006442\n",
      "Iteration 153, loss = 0.27937083\n",
      "Iteration 154, loss = 0.27876066\n",
      "Iteration 155, loss = 0.27806157\n",
      "Iteration 156, loss = 0.27741503\n",
      "Iteration 157, loss = 0.27682548\n",
      "Iteration 158, loss = 0.27608199\n",
      "Iteration 159, loss = 0.27555414\n",
      "Iteration 160, loss = 0.27490465\n",
      "Iteration 161, loss = 0.27428427\n",
      "Iteration 162, loss = 0.27363930\n",
      "Iteration 163, loss = 0.27302495\n",
      "Iteration 164, loss = 0.27246101\n",
      "Iteration 165, loss = 0.27181412\n",
      "Iteration 166, loss = 0.27117623\n",
      "Iteration 167, loss = 0.27053691\n",
      "Iteration 168, loss = 0.26997651\n",
      "Iteration 169, loss = 0.26946879\n",
      "Iteration 170, loss = 0.26876226\n",
      "Iteration 171, loss = 0.26817245\n",
      "Iteration 172, loss = 0.26772653\n",
      "Iteration 173, loss = 0.26719891\n",
      "Iteration 174, loss = 0.26650037\n",
      "Iteration 175, loss = 0.26593123\n",
      "Iteration 176, loss = 0.26545711\n",
      "Iteration 177, loss = 0.26476710\n",
      "Iteration 178, loss = 0.26425784\n",
      "Iteration 179, loss = 0.26371770\n",
      "Iteration 180, loss = 0.26312208\n",
      "Iteration 181, loss = 0.26243847\n",
      "Iteration 182, loss = 0.26208461\n",
      "Iteration 183, loss = 0.26139603\n",
      "Iteration 184, loss = 0.26092180\n",
      "Iteration 185, loss = 0.26029396\n",
      "Iteration 186, loss = 0.25976608\n",
      "Iteration 187, loss = 0.25936453\n",
      "Iteration 188, loss = 0.25875558\n",
      "Iteration 189, loss = 0.25821745\n",
      "Iteration 190, loss = 0.25776816\n",
      "Iteration 191, loss = 0.25715337\n",
      "Iteration 192, loss = 0.25653419\n",
      "Iteration 193, loss = 0.25612308\n",
      "Iteration 194, loss = 0.25556182\n",
      "Iteration 195, loss = 0.25503123\n",
      "Iteration 196, loss = 0.25446095\n",
      "Iteration 197, loss = 0.25406217\n",
      "Iteration 198, loss = 0.25353172\n",
      "Iteration 199, loss = 0.25297613\n",
      "Iteration 200, loss = 0.25250882\n",
      "[CV] END activation=logistic, alpha=0.001, batch_size=100, hidden_layer_sizes=(400, 100), solver=sgd; total time= 8.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.30324328\n",
      "Iteration 2, loss = 2.29322415\n",
      "Iteration 3, loss = 2.28614552\n",
      "Iteration 4, loss = 2.27843479\n",
      "Iteration 5, loss = 2.26929983\n",
      "Iteration 6, loss = 2.25828325\n",
      "Iteration 7, loss = 2.24423011\n",
      "Iteration 8, loss = 2.22561968\n",
      "Iteration 9, loss = 2.20111075\n",
      "Iteration 10, loss = 2.16731047\n",
      "Iteration 11, loss = 2.11993739\n",
      "Iteration 12, loss = 2.05377756\n",
      "Iteration 13, loss = 1.96393821\n",
      "Iteration 14, loss = 1.85008757\n",
      "Iteration 15, loss = 1.72023108\n",
      "Iteration 16, loss = 1.58836096\n",
      "Iteration 17, loss = 1.46508626\n",
      "Iteration 18, loss = 1.35485631\n",
      "Iteration 19, loss = 1.25733860\n",
      "Iteration 20, loss = 1.17223533\n",
      "Iteration 21, loss = 1.09827308\n",
      "Iteration 22, loss = 1.03420616\n",
      "Iteration 23, loss = 0.97803917\n",
      "Iteration 24, loss = 0.92868221\n",
      "Iteration 25, loss = 0.88469828\n",
      "Iteration 26, loss = 0.84492302\n",
      "Iteration 27, loss = 0.80918578\n",
      "Iteration 28, loss = 0.77630808\n",
      "Iteration 29, loss = 0.74623520\n",
      "Iteration 30, loss = 0.71882387\n",
      "Iteration 31, loss = 0.69351741\n",
      "Iteration 32, loss = 0.67025234\n",
      "Iteration 33, loss = 0.64905449\n",
      "Iteration 34, loss = 0.62932715\n",
      "Iteration 35, loss = 0.61136085\n",
      "Iteration 36, loss = 0.59490974\n",
      "Iteration 37, loss = 0.57958902\n",
      "Iteration 38, loss = 0.56565130\n",
      "Iteration 39, loss = 0.55268810\n",
      "Iteration 40, loss = 0.54071198\n",
      "Iteration 41, loss = 0.52945628\n",
      "Iteration 42, loss = 0.51928999\n",
      "Iteration 43, loss = 0.50959153\n",
      "Iteration 44, loss = 0.50057504\n",
      "Iteration 45, loss = 0.49215987\n",
      "Iteration 46, loss = 0.48419154\n",
      "Iteration 47, loss = 0.47684865\n",
      "Iteration 48, loss = 0.46982809\n",
      "Iteration 49, loss = 0.46319529\n",
      "Iteration 50, loss = 0.45703676\n",
      "Iteration 51, loss = 0.45126844\n",
      "Iteration 52, loss = 0.44574221\n",
      "Iteration 53, loss = 0.44041209\n",
      "Iteration 54, loss = 0.43546033\n",
      "Iteration 55, loss = 0.43092170\n",
      "Iteration 56, loss = 0.42658975\n",
      "Iteration 57, loss = 0.42214222\n",
      "Iteration 58, loss = 0.41818093\n",
      "Iteration 59, loss = 0.41435723\n",
      "Iteration 60, loss = 0.41068151\n",
      "Iteration 61, loss = 0.40717983\n",
      "Iteration 62, loss = 0.40384647\n",
      "Iteration 63, loss = 0.40065013\n",
      "Iteration 64, loss = 0.39742060\n",
      "Iteration 65, loss = 0.39464860\n",
      "Iteration 66, loss = 0.39185263\n",
      "Iteration 67, loss = 0.38901106\n",
      "Iteration 68, loss = 0.38652700\n",
      "Iteration 69, loss = 0.38388603\n",
      "Iteration 70, loss = 0.38158892\n",
      "Iteration 71, loss = 0.37920912\n",
      "Iteration 72, loss = 0.37699125\n",
      "Iteration 73, loss = 0.37474193\n",
      "Iteration 74, loss = 0.37267136\n",
      "Iteration 75, loss = 0.37066493\n",
      "Iteration 76, loss = 0.36865614\n",
      "Iteration 77, loss = 0.36679231\n",
      "Iteration 78, loss = 0.36483808\n",
      "Iteration 79, loss = 0.36291638\n",
      "Iteration 80, loss = 0.36123701\n",
      "Iteration 81, loss = 0.35945511\n",
      "Iteration 82, loss = 0.35781713\n",
      "Iteration 83, loss = 0.35605922\n",
      "Iteration 84, loss = 0.35461758\n",
      "Iteration 85, loss = 0.35290050\n",
      "Iteration 86, loss = 0.35144192\n",
      "Iteration 87, loss = 0.34991879\n",
      "Iteration 88, loss = 0.34831842\n",
      "Iteration 89, loss = 0.34702625\n",
      "Iteration 90, loss = 0.34558275\n",
      "Iteration 91, loss = 0.34424719\n",
      "Iteration 92, loss = 0.34281852\n",
      "Iteration 93, loss = 0.34155067\n",
      "Iteration 94, loss = 0.34023876\n",
      "Iteration 95, loss = 0.33881002\n",
      "Iteration 96, loss = 0.33764437\n",
      "Iteration 97, loss = 0.33641850\n",
      "Iteration 98, loss = 0.33508060\n",
      "Iteration 99, loss = 0.33397697\n",
      "Iteration 100, loss = 0.33270469\n",
      "Iteration 101, loss = 0.33158459\n",
      "Iteration 102, loss = 0.33049763\n",
      "Iteration 103, loss = 0.32930321\n",
      "Iteration 104, loss = 0.32816463\n",
      "Iteration 105, loss = 0.32702671\n",
      "Iteration 106, loss = 0.32598793\n",
      "Iteration 107, loss = 0.32487593\n",
      "Iteration 108, loss = 0.32384072\n",
      "Iteration 109, loss = 0.32270015\n",
      "Iteration 110, loss = 0.32179172\n",
      "Iteration 111, loss = 0.32068054\n",
      "Iteration 112, loss = 0.31965792\n",
      "Iteration 113, loss = 0.31870403\n",
      "Iteration 114, loss = 0.31768388\n",
      "Iteration 115, loss = 0.31654146\n",
      "Iteration 116, loss = 0.31576767\n",
      "Iteration 117, loss = 0.31461738\n",
      "Iteration 118, loss = 0.31385275\n",
      "Iteration 119, loss = 0.31286797\n",
      "Iteration 120, loss = 0.31184535\n",
      "Iteration 121, loss = 0.31088404\n",
      "Iteration 122, loss = 0.31002160\n",
      "Iteration 123, loss = 0.30908062\n",
      "Iteration 124, loss = 0.30813674\n",
      "Iteration 125, loss = 0.30733146\n",
      "Iteration 126, loss = 0.30644984\n",
      "Iteration 127, loss = 0.30547646\n",
      "Iteration 128, loss = 0.30482406\n",
      "Iteration 129, loss = 0.30391626\n",
      "Iteration 130, loss = 0.30295992\n",
      "Iteration 131, loss = 0.30211829\n",
      "Iteration 132, loss = 0.30130261\n",
      "Iteration 133, loss = 0.30044609\n",
      "Iteration 134, loss = 0.29965363\n",
      "Iteration 135, loss = 0.29880771\n",
      "Iteration 136, loss = 0.29799657\n",
      "Iteration 137, loss = 0.29718240\n",
      "Iteration 138, loss = 0.29641618\n",
      "Iteration 139, loss = 0.29559961\n",
      "Iteration 140, loss = 0.29477344\n",
      "Iteration 141, loss = 0.29398775\n",
      "Iteration 142, loss = 0.29318545\n",
      "Iteration 143, loss = 0.29239537\n",
      "Iteration 144, loss = 0.29164463\n",
      "Iteration 145, loss = 0.29094492\n",
      "Iteration 146, loss = 0.29013663\n",
      "Iteration 147, loss = 0.28923742\n",
      "Iteration 148, loss = 0.28858881\n",
      "Iteration 149, loss = 0.28782025\n",
      "Iteration 150, loss = 0.28725345\n",
      "Iteration 151, loss = 0.28644943\n",
      "Iteration 152, loss = 0.28569606\n",
      "Iteration 153, loss = 0.28504605\n",
      "Iteration 154, loss = 0.28427338\n",
      "Iteration 155, loss = 0.28353692\n",
      "Iteration 156, loss = 0.28290773\n",
      "Iteration 157, loss = 0.28225722\n",
      "Iteration 158, loss = 0.28132709\n",
      "Iteration 159, loss = 0.28086009\n",
      "Iteration 160, loss = 0.28010330\n",
      "Iteration 161, loss = 0.27946906\n",
      "Iteration 162, loss = 0.27874174\n",
      "Iteration 163, loss = 0.27804797\n",
      "Iteration 164, loss = 0.27741820\n",
      "Iteration 165, loss = 0.27669553\n",
      "Iteration 166, loss = 0.27610540\n",
      "Iteration 167, loss = 0.27532744\n",
      "Iteration 168, loss = 0.27482275\n",
      "Iteration 169, loss = 0.27401776\n",
      "Iteration 170, loss = 0.27348246\n",
      "Iteration 171, loss = 0.27282339\n",
      "Iteration 172, loss = 0.27217923\n",
      "Iteration 173, loss = 0.27151114\n",
      "Iteration 174, loss = 0.27091871\n",
      "Iteration 175, loss = 0.27026453\n",
      "Iteration 176, loss = 0.26961289\n",
      "Iteration 177, loss = 0.26895515\n",
      "Iteration 178, loss = 0.26839519\n",
      "Iteration 179, loss = 0.26768326\n",
      "Iteration 180, loss = 0.26717926\n",
      "Iteration 181, loss = 0.26647023\n",
      "Iteration 182, loss = 0.26592865\n",
      "Iteration 183, loss = 0.26535544\n",
      "Iteration 184, loss = 0.26482817\n",
      "Iteration 185, loss = 0.26413088\n",
      "Iteration 186, loss = 0.26354035\n",
      "Iteration 187, loss = 0.26296056\n",
      "Iteration 188, loss = 0.26238570\n",
      "Iteration 189, loss = 0.26182291\n",
      "Iteration 190, loss = 0.26113533\n",
      "Iteration 191, loss = 0.26063737\n",
      "Iteration 192, loss = 0.26001758\n",
      "Iteration 193, loss = 0.25944682\n",
      "Iteration 194, loss = 0.25894309\n",
      "Iteration 195, loss = 0.25824511\n",
      "Iteration 196, loss = 0.25780128\n",
      "Iteration 197, loss = 0.25730028\n",
      "Iteration 198, loss = 0.25666381\n",
      "Iteration 199, loss = 0.25601338\n",
      "Iteration 200, loss = 0.25541491\n",
      "[CV] END activation=logistic, alpha=0.001, batch_size=100, hidden_layer_sizes=(400, 100), solver=sgd; total time= 8.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.30747519\n",
      "Iteration 2, loss = 2.29351871\n",
      "Iteration 3, loss = 2.28642768\n",
      "Iteration 4, loss = 2.27848752\n",
      "Iteration 5, loss = 2.26914749\n",
      "Iteration 6, loss = 2.25782795\n",
      "Iteration 7, loss = 2.24345954\n",
      "Iteration 8, loss = 2.22458344\n",
      "Iteration 9, loss = 2.19893017\n",
      "Iteration 10, loss = 2.16378791\n",
      "Iteration 11, loss = 2.11471958\n",
      "Iteration 12, loss = 2.04647460\n",
      "Iteration 13, loss = 1.95486960\n",
      "Iteration 14, loss = 1.84046748\n",
      "Iteration 15, loss = 1.70942849\n",
      "Iteration 16, loss = 1.57370934\n",
      "Iteration 17, loss = 1.44472980\n",
      "Iteration 18, loss = 1.32992661\n",
      "Iteration 19, loss = 1.23162587\n",
      "Iteration 20, loss = 1.14896716\n",
      "Iteration 21, loss = 1.07930903\n",
      "Iteration 22, loss = 1.02003000\n",
      "Iteration 23, loss = 0.96832721\n",
      "Iteration 24, loss = 0.92268889\n",
      "Iteration 25, loss = 0.88124021\n",
      "Iteration 26, loss = 0.84362141\n",
      "Iteration 27, loss = 0.80884958\n",
      "Iteration 28, loss = 0.77680141\n",
      "Iteration 29, loss = 0.74702261\n",
      "Iteration 30, loss = 0.71949538\n",
      "Iteration 31, loss = 0.69414158\n",
      "Iteration 32, loss = 0.67085849\n",
      "Iteration 33, loss = 0.64939730\n",
      "Iteration 34, loss = 0.62952434\n",
      "Iteration 35, loss = 0.61158338\n",
      "Iteration 36, loss = 0.59510636\n",
      "Iteration 37, loss = 0.57984365\n",
      "Iteration 38, loss = 0.56594700\n",
      "Iteration 39, loss = 0.55298767\n",
      "Iteration 40, loss = 0.54105830\n",
      "Iteration 41, loss = 0.52987353\n",
      "Iteration 42, loss = 0.51951787\n",
      "Iteration 43, loss = 0.51001384\n",
      "Iteration 44, loss = 0.50085628\n",
      "Iteration 45, loss = 0.49237252\n",
      "Iteration 46, loss = 0.48440150\n",
      "Iteration 47, loss = 0.47703082\n",
      "Iteration 48, loss = 0.46993507\n",
      "Iteration 49, loss = 0.46331469\n",
      "Iteration 50, loss = 0.45707772\n",
      "Iteration 51, loss = 0.45115969\n",
      "Iteration 52, loss = 0.44548851\n",
      "Iteration 53, loss = 0.44013507\n",
      "Iteration 54, loss = 0.43508823\n",
      "Iteration 55, loss = 0.43032752\n",
      "Iteration 56, loss = 0.42575852\n",
      "Iteration 57, loss = 0.42136473\n",
      "Iteration 58, loss = 0.41738492\n",
      "Iteration 59, loss = 0.41332518\n",
      "Iteration 60, loss = 0.40959427\n",
      "Iteration 61, loss = 0.40600159\n",
      "Iteration 62, loss = 0.40262766\n",
      "Iteration 63, loss = 0.39941778\n",
      "Iteration 64, loss = 0.39622391\n",
      "Iteration 65, loss = 0.39326351\n",
      "Iteration 66, loss = 0.39035755\n",
      "Iteration 67, loss = 0.38755316\n",
      "Iteration 68, loss = 0.38491669\n",
      "Iteration 69, loss = 0.38231899\n",
      "Iteration 70, loss = 0.37987260\n",
      "Iteration 71, loss = 0.37752778\n",
      "Iteration 72, loss = 0.37521757\n",
      "Iteration 73, loss = 0.37288672\n",
      "Iteration 74, loss = 0.37081556\n",
      "Iteration 75, loss = 0.36864810\n",
      "Iteration 76, loss = 0.36681060\n",
      "Iteration 77, loss = 0.36476910\n",
      "Iteration 78, loss = 0.36295320\n",
      "Iteration 79, loss = 0.36104834\n",
      "Iteration 80, loss = 0.35927025\n",
      "Iteration 81, loss = 0.35737622\n",
      "Iteration 82, loss = 0.35575305\n",
      "Iteration 83, loss = 0.35415600\n",
      "Iteration 84, loss = 0.35254946\n",
      "Iteration 85, loss = 0.35087206\n",
      "Iteration 86, loss = 0.34943930\n",
      "Iteration 87, loss = 0.34796781\n",
      "Iteration 88, loss = 0.34638915\n",
      "Iteration 89, loss = 0.34493834\n",
      "Iteration 90, loss = 0.34351532\n",
      "Iteration 91, loss = 0.34214707\n",
      "Iteration 92, loss = 0.34088632\n",
      "Iteration 93, loss = 0.33945975\n",
      "Iteration 94, loss = 0.33817236\n",
      "Iteration 95, loss = 0.33687687\n",
      "Iteration 96, loss = 0.33558381\n",
      "Iteration 97, loss = 0.33442956\n",
      "Iteration 98, loss = 0.33314761\n",
      "Iteration 99, loss = 0.33199698\n",
      "Iteration 100, loss = 0.33066328\n",
      "Iteration 101, loss = 0.32956674\n",
      "Iteration 102, loss = 0.32845166\n",
      "Iteration 103, loss = 0.32723894\n",
      "Iteration 104, loss = 0.32621156\n",
      "Iteration 105, loss = 0.32508603\n",
      "Iteration 106, loss = 0.32389601\n",
      "Iteration 107, loss = 0.32291973\n",
      "Iteration 108, loss = 0.32178881\n",
      "Iteration 109, loss = 0.32062942\n",
      "Iteration 110, loss = 0.31972238\n",
      "Iteration 111, loss = 0.31863526\n",
      "Iteration 112, loss = 0.31766576\n",
      "Iteration 113, loss = 0.31672107\n",
      "Iteration 114, loss = 0.31558341\n",
      "Iteration 115, loss = 0.31474388\n",
      "Iteration 116, loss = 0.31374046\n",
      "Iteration 117, loss = 0.31284171\n",
      "Iteration 118, loss = 0.31179334\n",
      "Iteration 119, loss = 0.31087184\n",
      "Iteration 120, loss = 0.30984379\n",
      "Iteration 121, loss = 0.30902868\n",
      "Iteration 122, loss = 0.30816911\n",
      "Iteration 123, loss = 0.30709994\n",
      "Iteration 124, loss = 0.30633111\n",
      "Iteration 125, loss = 0.30535555\n",
      "Iteration 126, loss = 0.30447303\n",
      "Iteration 127, loss = 0.30351030\n",
      "Iteration 128, loss = 0.30268234\n",
      "Iteration 129, loss = 0.30181447\n",
      "Iteration 130, loss = 0.30089001\n",
      "Iteration 131, loss = 0.30015687\n",
      "Iteration 132, loss = 0.29921631\n",
      "Iteration 133, loss = 0.29845506\n",
      "Iteration 134, loss = 0.29765562\n",
      "Iteration 135, loss = 0.29674129\n",
      "Iteration 136, loss = 0.29606395\n",
      "Iteration 137, loss = 0.29528929\n",
      "Iteration 138, loss = 0.29422225\n",
      "Iteration 139, loss = 0.29363879\n",
      "Iteration 140, loss = 0.29279434\n",
      "Iteration 141, loss = 0.29191513\n",
      "Iteration 142, loss = 0.29116976\n",
      "Iteration 143, loss = 0.29052982\n",
      "Iteration 144, loss = 0.28964012\n",
      "Iteration 145, loss = 0.28887497\n",
      "Iteration 146, loss = 0.28815162\n",
      "Iteration 147, loss = 0.28740903\n",
      "Iteration 148, loss = 0.28655460\n",
      "Iteration 149, loss = 0.28584837\n",
      "Iteration 150, loss = 0.28517658\n",
      "Iteration 151, loss = 0.28441136\n",
      "Iteration 152, loss = 0.28360470\n",
      "Iteration 153, loss = 0.28298576\n",
      "Iteration 154, loss = 0.28224332\n",
      "Iteration 155, loss = 0.28150451\n",
      "Iteration 156, loss = 0.28081067\n",
      "Iteration 157, loss = 0.28013650\n",
      "Iteration 158, loss = 0.27942289\n",
      "Iteration 159, loss = 0.27869171\n",
      "Iteration 160, loss = 0.27807324\n",
      "Iteration 161, loss = 0.27737276\n",
      "Iteration 162, loss = 0.27665667\n",
      "Iteration 163, loss = 0.27596573\n",
      "Iteration 164, loss = 0.27532236\n",
      "Iteration 165, loss = 0.27467979\n",
      "Iteration 166, loss = 0.27392650\n",
      "Iteration 167, loss = 0.27334925\n",
      "Iteration 168, loss = 0.27267940\n",
      "Iteration 169, loss = 0.27213299\n",
      "Iteration 170, loss = 0.27150562\n",
      "Iteration 171, loss = 0.27061914\n",
      "Iteration 172, loss = 0.27001592\n",
      "Iteration 173, loss = 0.26962108\n",
      "Iteration 174, loss = 0.26890704\n",
      "Iteration 175, loss = 0.26830861\n",
      "Iteration 176, loss = 0.26763786\n",
      "Iteration 177, loss = 0.26706225\n",
      "Iteration 178, loss = 0.26640112\n",
      "Iteration 179, loss = 0.26584793\n",
      "Iteration 180, loss = 0.26514079\n",
      "Iteration 181, loss = 0.26456660\n",
      "Iteration 182, loss = 0.26403313\n",
      "Iteration 183, loss = 0.26341378\n",
      "Iteration 184, loss = 0.26280519\n",
      "Iteration 185, loss = 0.26228453\n",
      "Iteration 186, loss = 0.26167015\n",
      "Iteration 187, loss = 0.26109024\n",
      "Iteration 188, loss = 0.26053134\n",
      "Iteration 189, loss = 0.25996718\n",
      "Iteration 190, loss = 0.25938855\n",
      "Iteration 191, loss = 0.25882411\n",
      "Iteration 192, loss = 0.25824762\n",
      "Iteration 193, loss = 0.25759633\n",
      "Iteration 194, loss = 0.25716244\n",
      "Iteration 195, loss = 0.25652113\n",
      "Iteration 196, loss = 0.25601317\n",
      "Iteration 197, loss = 0.25548058\n",
      "Iteration 198, loss = 0.25486302\n",
      "Iteration 199, loss = 0.25445274\n",
      "Iteration 200, loss = 0.25376662\n",
      "[CV] END activation=logistic, alpha=0.001, batch_size=100, hidden_layer_sizes=(400, 100), solver=sgd; total time= 8.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.37509982\n",
      "Iteration 2, loss = 0.43620618\n",
      "Iteration 3, loss = 0.28602729\n",
      "Iteration 4, loss = 0.22262815\n",
      "Iteration 5, loss = 0.18038790\n",
      "Iteration 6, loss = 0.15250095\n",
      "Iteration 7, loss = 0.12934353\n",
      "Iteration 8, loss = 0.11293898\n",
      "Iteration 9, loss = 0.09874433\n",
      "Iteration 10, loss = 0.08678303\n",
      "Iteration 11, loss = 0.07774690\n",
      "Iteration 12, loss = 0.06943602\n",
      "Iteration 13, loss = 0.06246812\n",
      "Iteration 14, loss = 0.05713843\n",
      "Iteration 15, loss = 0.05051507\n",
      "Iteration 16, loss = 0.04602862\n",
      "Iteration 17, loss = 0.04186165\n",
      "Iteration 18, loss = 0.03814968\n",
      "Iteration 19, loss = 0.03469192\n",
      "Iteration 20, loss = 0.03264218\n",
      "Iteration 21, loss = 0.03151717\n",
      "Iteration 22, loss = 0.02820094\n",
      "Iteration 23, loss = 0.02463723\n",
      "Iteration 24, loss = 0.02470074\n",
      "Iteration 25, loss = 0.02371549\n",
      "Iteration 26, loss = 0.02294970\n",
      "Iteration 27, loss = 0.02283991\n",
      "Iteration 28, loss = 0.02204015\n",
      "Iteration 29, loss = 0.02462970\n",
      "Iteration 30, loss = 0.02317746\n",
      "Iteration 31, loss = 0.01949983\n",
      "Iteration 32, loss = 0.01792880\n",
      "Iteration 33, loss = 0.01794827\n",
      "Iteration 34, loss = 0.01855075\n",
      "Iteration 35, loss = 0.03083149\n",
      "Iteration 36, loss = 0.01798546\n",
      "Iteration 37, loss = 0.01691602\n",
      "Iteration 38, loss = 0.01635566\n",
      "Iteration 39, loss = 0.01610944\n",
      "Iteration 40, loss = 0.01582731\n",
      "Iteration 41, loss = 0.03501822\n",
      "Iteration 42, loss = 0.01883917\n",
      "Iteration 43, loss = 0.01652558\n",
      "Iteration 44, loss = 0.01583253\n",
      "Iteration 45, loss = 0.01554388\n",
      "Iteration 46, loss = 0.01528037\n",
      "Iteration 47, loss = 0.01503841\n",
      "Iteration 48, loss = 0.01618573\n",
      "Iteration 49, loss = 0.03383791\n",
      "Iteration 50, loss = 0.01817174\n",
      "Iteration 51, loss = 0.01615719\n",
      "Iteration 52, loss = 0.01524916\n",
      "Iteration 53, loss = 0.01497079\n",
      "Iteration 54, loss = 0.01470117\n",
      "Iteration 55, loss = 0.01456635\n",
      "Iteration 56, loss = 0.02180428\n",
      "Iteration 57, loss = 0.02723030\n",
      "Iteration 58, loss = 0.01898697\n",
      "Iteration 59, loss = 0.01531582\n",
      "Iteration 60, loss = 0.01477232\n",
      "Iteration 61, loss = 0.01456547\n",
      "Iteration 62, loss = 0.01440007\n",
      "Iteration 63, loss = 0.01412673\n",
      "Iteration 64, loss = 0.01391554\n",
      "Iteration 65, loss = 0.01377876\n",
      "Iteration 66, loss = 0.02856771\n",
      "Iteration 67, loss = 0.02303329\n",
      "Iteration 68, loss = 0.01535229\n",
      "Iteration 69, loss = 0.01438056\n",
      "Iteration 70, loss = 0.01410115\n",
      "Iteration 71, loss = 0.01389344\n",
      "Iteration 72, loss = 0.01373609\n",
      "Iteration 73, loss = 0.01356419\n",
      "Iteration 74, loss = 0.01339418\n",
      "Iteration 75, loss = 0.02795455\n",
      "Iteration 76, loss = 0.02351739\n",
      "Iteration 77, loss = 0.01466972\n",
      "Iteration 78, loss = 0.01402612\n",
      "Iteration 79, loss = 0.01377903\n",
      "Iteration 80, loss = 0.01359729\n",
      "Iteration 81, loss = 0.01340431\n",
      "Iteration 82, loss = 0.01327413\n",
      "Iteration 83, loss = 0.01315435\n",
      "Iteration 84, loss = 0.02015887\n",
      "Iteration 85, loss = 0.02714641\n",
      "Iteration 86, loss = 0.01577476\n",
      "Iteration 87, loss = 0.01386806\n",
      "Iteration 88, loss = 0.01351281\n",
      "Iteration 89, loss = 0.01332541\n",
      "Iteration 90, loss = 0.01315064\n",
      "Iteration 91, loss = 0.01298055\n",
      "Iteration 92, loss = 0.01284397\n",
      "Iteration 93, loss = 0.01289547\n",
      "Iteration 94, loss = 0.03036560\n",
      "Iteration 95, loss = 0.02023249\n",
      "Iteration 96, loss = 0.01457586\n",
      "Iteration 97, loss = 0.01351857\n",
      "Iteration 98, loss = 0.01325034\n",
      "Iteration 99, loss = 0.01307355\n",
      "Iteration 100, loss = 0.01291097\n",
      "Iteration 101, loss = 0.01277357\n",
      "Iteration 102, loss = 0.01263325\n",
      "Iteration 103, loss = 0.01258293\n",
      "Iteration 104, loss = 0.03037001\n",
      "Iteration 105, loss = 0.02248242\n",
      "Iteration 106, loss = 0.01399159\n",
      "Iteration 107, loss = 0.01333215\n",
      "Iteration 108, loss = 0.01310843\n",
      "Iteration 109, loss = 0.01293705\n",
      "Iteration 110, loss = 0.01278256\n",
      "Iteration 111, loss = 0.01263537\n",
      "Iteration 112, loss = 0.01249968\n",
      "Iteration 113, loss = 0.01244800\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.001, batch_size=100, hidden_layer_sizes=(100, 100, 100), solver=adam; total time= 3.5min\n",
      "Iteration 1, loss = 1.36085659\n",
      "Iteration 2, loss = 0.45344700\n",
      "Iteration 3, loss = 0.29711927\n",
      "Iteration 4, loss = 0.23165619\n",
      "Iteration 5, loss = 0.19112779\n",
      "Iteration 6, loss = 0.16096652\n",
      "Iteration 7, loss = 0.13874119\n",
      "Iteration 8, loss = 0.12054690\n",
      "Iteration 9, loss = 0.10544502\n",
      "Iteration 10, loss = 0.09260732\n",
      "Iteration 11, loss = 0.08395543\n",
      "Iteration 12, loss = 0.07369945\n",
      "Iteration 13, loss = 0.06613006\n",
      "Iteration 14, loss = 0.05923887\n",
      "Iteration 15, loss = 0.05326824\n",
      "Iteration 16, loss = 0.04786627\n",
      "Iteration 17, loss = 0.04466084\n",
      "Iteration 18, loss = 0.04072918\n",
      "Iteration 19, loss = 0.03570438\n",
      "Iteration 20, loss = 0.03265408\n",
      "Iteration 21, loss = 0.03172265\n",
      "Iteration 22, loss = 0.02897945\n",
      "Iteration 23, loss = 0.02802894\n",
      "Iteration 24, loss = 0.02815862\n",
      "Iteration 25, loss = 0.02385940\n",
      "Iteration 26, loss = 0.02330637\n",
      "Iteration 27, loss = 0.02145508\n",
      "Iteration 28, loss = 0.02269560\n",
      "Iteration 29, loss = 0.02222542\n",
      "Iteration 30, loss = 0.02168003\n",
      "Iteration 31, loss = 0.02500355\n",
      "Iteration 32, loss = 0.02112363\n",
      "Iteration 33, loss = 0.02013362\n",
      "Iteration 34, loss = 0.01802250\n",
      "Iteration 35, loss = 0.01729252\n",
      "Iteration 36, loss = 0.01774025\n",
      "Iteration 37, loss = 0.02996236\n",
      "Iteration 38, loss = 0.02305300\n",
      "Iteration 39, loss = 0.01868642\n",
      "Iteration 40, loss = 0.01703322\n",
      "Iteration 41, loss = 0.01654293\n",
      "Iteration 42, loss = 0.01629631\n",
      "Iteration 43, loss = 0.01603194\n",
      "Iteration 44, loss = 0.02657508\n",
      "Iteration 45, loss = 0.02743527\n",
      "Iteration 46, loss = 0.01754346\n",
      "Iteration 47, loss = 0.01622157\n",
      "Iteration 48, loss = 0.01588325\n",
      "Iteration 49, loss = 0.01555642\n",
      "Iteration 50, loss = 0.01526041\n",
      "Iteration 51, loss = 0.01511859\n",
      "Iteration 52, loss = 0.01610182\n",
      "Iteration 53, loss = 0.03459265\n",
      "Iteration 54, loss = 0.01883568\n",
      "Iteration 55, loss = 0.01592561\n",
      "Iteration 56, loss = 0.01530928\n",
      "Iteration 57, loss = 0.01503744\n",
      "Iteration 58, loss = 0.01483519\n",
      "Iteration 59, loss = 0.01461713\n",
      "Iteration 60, loss = 0.01441659\n",
      "Iteration 61, loss = 0.01440886\n",
      "Iteration 62, loss = 0.04198834\n",
      "Iteration 63, loss = 0.01927188\n",
      "Iteration 64, loss = 0.01544109\n",
      "Iteration 65, loss = 0.01490747\n",
      "Iteration 66, loss = 0.01464308\n",
      "Iteration 67, loss = 0.01444832\n",
      "Iteration 68, loss = 0.01427331\n",
      "Iteration 69, loss = 0.01408613\n",
      "Iteration 70, loss = 0.01393308\n",
      "Iteration 71, loss = 0.02346499\n",
      "Iteration 72, loss = 0.02505627\n",
      "Iteration 73, loss = 0.01563471\n",
      "Iteration 74, loss = 0.01445898\n",
      "Iteration 75, loss = 0.01418109\n",
      "Iteration 76, loss = 0.01394890\n",
      "Iteration 77, loss = 0.01378722\n",
      "Iteration 78, loss = 0.01364220\n",
      "Iteration 79, loss = 0.01432041\n",
      "Iteration 80, loss = 0.03751172\n",
      "Iteration 81, loss = 0.01631801\n",
      "Iteration 82, loss = 0.01457809\n",
      "Iteration 83, loss = 0.01409867\n",
      "Iteration 84, loss = 0.01389301\n",
      "Iteration 85, loss = 0.01371541\n",
      "Iteration 86, loss = 0.01354604\n",
      "Iteration 87, loss = 0.01345377\n",
      "Iteration 88, loss = 0.01357245\n",
      "Iteration 89, loss = 0.03320055\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.001, batch_size=100, hidden_layer_sizes=(100, 100, 100), solver=adam; total time= 2.8min\n",
      "Iteration 1, loss = 1.35985721\n",
      "Iteration 2, loss = 0.45926455\n",
      "Iteration 3, loss = 0.29192237\n",
      "Iteration 4, loss = 0.22190321\n",
      "Iteration 5, loss = 0.18102553\n",
      "Iteration 6, loss = 0.15225128\n",
      "Iteration 7, loss = 0.13179395\n",
      "Iteration 8, loss = 0.11371405\n",
      "Iteration 9, loss = 0.10007470\n",
      "Iteration 10, loss = 0.08934751\n",
      "Iteration 11, loss = 0.07885958\n",
      "Iteration 12, loss = 0.06977837\n",
      "Iteration 13, loss = 0.06337844\n",
      "Iteration 14, loss = 0.05668983\n",
      "Iteration 15, loss = 0.05180487\n",
      "Iteration 16, loss = 0.04648864\n",
      "Iteration 17, loss = 0.04249821\n",
      "Iteration 18, loss = 0.03807128\n",
      "Iteration 19, loss = 0.03618622\n",
      "Iteration 20, loss = 0.03309342\n",
      "Iteration 21, loss = 0.03122273\n",
      "Iteration 22, loss = 0.02795775\n",
      "Iteration 23, loss = 0.02749554\n",
      "Iteration 24, loss = 0.02588291\n",
      "Iteration 25, loss = 0.02517078\n",
      "Iteration 26, loss = 0.02252630\n",
      "Iteration 27, loss = 0.02267548\n",
      "Iteration 28, loss = 0.02270443\n",
      "Iteration 29, loss = 0.02173081\n",
      "Iteration 30, loss = 0.02395024\n",
      "Iteration 31, loss = 0.01984141\n",
      "Iteration 32, loss = 0.01895808\n",
      "Iteration 33, loss = 0.01802936\n",
      "Iteration 34, loss = 0.01751162\n",
      "Iteration 35, loss = 0.01975551\n",
      "Iteration 36, loss = 0.03465691\n",
      "Iteration 37, loss = 0.01934848\n",
      "Iteration 38, loss = 0.01757797\n",
      "Iteration 39, loss = 0.01685063\n",
      "Iteration 40, loss = 0.01675685\n",
      "Iteration 41, loss = 0.02145197\n",
      "Iteration 42, loss = 0.02244287\n",
      "Iteration 43, loss = 0.01837544\n",
      "Iteration 44, loss = 0.01699008\n",
      "Iteration 45, loss = 0.01604526\n",
      "Iteration 46, loss = 0.01570756\n",
      "Iteration 47, loss = 0.01555088\n",
      "Iteration 48, loss = 0.03298119\n",
      "Iteration 49, loss = 0.01890047\n",
      "Iteration 50, loss = 0.01620288\n",
      "Iteration 51, loss = 0.01566664\n",
      "Iteration 52, loss = 0.01540962\n",
      "Iteration 53, loss = 0.01511282\n",
      "Iteration 54, loss = 0.01489725\n",
      "Iteration 55, loss = 0.02395117\n",
      "Iteration 56, loss = 0.02876543\n",
      "Iteration 57, loss = 0.01646382\n",
      "Iteration 58, loss = 0.01532252\n",
      "Iteration 59, loss = 0.01502529\n",
      "Iteration 60, loss = 0.01478940\n",
      "Iteration 61, loss = 0.01458870\n",
      "Iteration 62, loss = 0.01443700\n",
      "Iteration 63, loss = 0.01423714\n",
      "Iteration 64, loss = 0.01429196\n",
      "Iteration 65, loss = 0.03635707\n",
      "Iteration 66, loss = 0.01809183\n",
      "Iteration 67, loss = 0.01523801\n",
      "Iteration 68, loss = 0.01465260\n",
      "Iteration 69, loss = 0.01439659\n",
      "Iteration 70, loss = 0.01421654\n",
      "Iteration 71, loss = 0.01401964\n",
      "Iteration 72, loss = 0.01387665\n",
      "Iteration 73, loss = 0.01396721\n",
      "Iteration 74, loss = 0.03254419\n",
      "Iteration 75, loss = 0.02067449\n",
      "Iteration 76, loss = 0.01574335\n",
      "Iteration 77, loss = 0.01438505\n",
      "Iteration 78, loss = 0.01409909\n",
      "Iteration 79, loss = 0.01387123\n",
      "Iteration 80, loss = 0.01372577\n",
      "Iteration 81, loss = 0.01355570\n",
      "Iteration 82, loss = 0.01347688\n",
      "Iteration 83, loss = 0.01353362\n",
      "Iteration 84, loss = 0.03441970\n",
      "Iteration 85, loss = 0.01731981\n",
      "Iteration 86, loss = 0.01484300\n",
      "Iteration 87, loss = 0.01400850\n",
      "Iteration 88, loss = 0.01373657\n",
      "Iteration 89, loss = 0.01355521\n",
      "Iteration 90, loss = 0.01339425\n",
      "Iteration 91, loss = 0.01325903\n",
      "Iteration 92, loss = 0.01315248\n",
      "Iteration 93, loss = 0.02491073\n",
      "Iteration 94, loss = 0.02514128\n",
      "Iteration 95, loss = 0.01700638\n",
      "Iteration 96, loss = 0.01400719\n",
      "Iteration 97, loss = 0.01365626\n",
      "Iteration 98, loss = 0.01346979\n",
      "Iteration 99, loss = 0.01329996\n",
      "Iteration 100, loss = 0.01316437\n",
      "Iteration 101, loss = 0.01301877\n",
      "Iteration 102, loss = 0.01292064\n",
      "Iteration 103, loss = 0.01517954\n",
      "Iteration 104, loss = 0.03138856\n",
      "Iteration 105, loss = 0.01470670\n",
      "Iteration 106, loss = 0.01368883\n",
      "Iteration 107, loss = 0.01333415\n",
      "Iteration 108, loss = 0.01315401\n",
      "Iteration 109, loss = 0.01302010\n",
      "Iteration 110, loss = 0.01287155\n",
      "Iteration 111, loss = 0.01276913\n",
      "Iteration 112, loss = 0.01276259\n",
      "Iteration 113, loss = 0.01891494\n",
      "Iteration 114, loss = 0.02977159\n",
      "Iteration 115, loss = 0.01423441\n",
      "Iteration 116, loss = 0.01342684\n",
      "Iteration 117, loss = 0.01309099\n",
      "Iteration 118, loss = 0.01293057\n",
      "Iteration 119, loss = 0.01280047\n",
      "Iteration 120, loss = 0.01266533\n",
      "Iteration 121, loss = 0.01255302\n",
      "Iteration 122, loss = 0.01256691\n",
      "Iteration 123, loss = 0.03899242\n",
      "Iteration 124, loss = 0.01498912\n",
      "Iteration 125, loss = 0.01344236\n",
      "Iteration 126, loss = 0.01315770\n",
      "Iteration 127, loss = 0.01298421\n",
      "Iteration 128, loss = 0.01282850\n",
      "Iteration 129, loss = 0.01267993\n",
      "Iteration 130, loss = 0.01255588\n",
      "Iteration 131, loss = 0.01244359\n",
      "Iteration 132, loss = 0.01237440\n",
      "Iteration 133, loss = 0.02738652\n",
      "Iteration 134, loss = 0.02153987\n",
      "Iteration 135, loss = 0.01381938\n",
      "Iteration 136, loss = 0.01304913\n",
      "Iteration 137, loss = 0.01284876\n",
      "Iteration 138, loss = 0.01267820\n",
      "Iteration 139, loss = 0.01253261\n",
      "Iteration 140, loss = 0.01244295\n",
      "Iteration 141, loss = 0.01232624\n",
      "Iteration 142, loss = 0.01751259\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.001, batch_size=100, hidden_layer_sizes=(100, 100, 100), solver=adam; total time= 4.5min\n",
      "Iteration 1, loss = 1.38554276\n",
      "Iteration 2, loss = 0.44150981\n",
      "Iteration 3, loss = 0.28195093\n",
      "Iteration 4, loss = 0.21842248\n",
      "Iteration 5, loss = 0.17911412\n",
      "Iteration 6, loss = 0.15182800\n",
      "Iteration 7, loss = 0.13181750\n",
      "Iteration 8, loss = 0.11640992\n",
      "Iteration 9, loss = 0.10353486\n",
      "Iteration 10, loss = 0.09076934\n",
      "Iteration 11, loss = 0.08268591\n",
      "Iteration 12, loss = 0.07467150\n",
      "Iteration 13, loss = 0.06675137\n",
      "Iteration 14, loss = 0.05945406\n",
      "Iteration 15, loss = 0.05404830\n",
      "Iteration 16, loss = 0.04892106\n",
      "Iteration 17, loss = 0.04582304\n",
      "Iteration 18, loss = 0.04200183\n",
      "Iteration 19, loss = 0.03748727\n",
      "Iteration 20, loss = 0.03470075\n",
      "Iteration 21, loss = 0.03243099\n",
      "Iteration 22, loss = 0.03124574\n",
      "Iteration 23, loss = 0.02825990\n",
      "Iteration 24, loss = 0.02692505\n",
      "Iteration 25, loss = 0.02809845\n",
      "Iteration 26, loss = 0.02472170\n",
      "Iteration 27, loss = 0.02229650\n",
      "Iteration 28, loss = 0.02256772\n",
      "Iteration 29, loss = 0.02663509\n",
      "Iteration 30, loss = 0.02185519\n",
      "Iteration 31, loss = 0.02299089\n",
      "Iteration 32, loss = 0.01975886\n",
      "Iteration 33, loss = 0.01856101\n",
      "Iteration 34, loss = 0.01838955\n",
      "Iteration 35, loss = 0.03445853\n",
      "Iteration 36, loss = 0.02268637\n",
      "Iteration 37, loss = 0.01833794\n",
      "Iteration 38, loss = 0.01772082\n",
      "Iteration 39, loss = 0.01723863\n",
      "Iteration 40, loss = 0.01700470\n",
      "Iteration 41, loss = 0.02669242\n",
      "Iteration 42, loss = 0.02624071\n",
      "Iteration 43, loss = 0.01984066\n",
      "Iteration 44, loss = 0.01721251\n",
      "Iteration 45, loss = 0.01667817\n",
      "Iteration 46, loss = 0.01676599\n",
      "Iteration 47, loss = 0.01718629\n",
      "Iteration 48, loss = 0.01622524\n",
      "Iteration 49, loss = 0.01656809\n",
      "Iteration 50, loss = 0.03378644\n",
      "Iteration 51, loss = 0.01823630\n",
      "Iteration 52, loss = 0.01692473\n",
      "Iteration 53, loss = 0.01583292\n",
      "Iteration 54, loss = 0.01552060\n",
      "Iteration 55, loss = 0.01527814\n",
      "Iteration 56, loss = 0.01506179\n",
      "Iteration 57, loss = 0.01500880\n",
      "Iteration 58, loss = 0.01501394\n",
      "Iteration 59, loss = 0.03347923\n",
      "Iteration 60, loss = 0.01973754\n",
      "Iteration 61, loss = 0.01619915\n",
      "Iteration 62, loss = 0.01524171\n",
      "Iteration 63, loss = 0.01485878\n",
      "Iteration 64, loss = 0.01462222\n",
      "Iteration 65, loss = 0.01449315\n",
      "Iteration 66, loss = 0.01435018\n",
      "Iteration 67, loss = 0.01416497\n",
      "Iteration 68, loss = 0.01401766\n",
      "Iteration 69, loss = 0.02984352\n",
      "Iteration 70, loss = 0.02412306\n",
      "Iteration 71, loss = 0.01513624\n",
      "Iteration 72, loss = 0.01449919\n",
      "Iteration 73, loss = 0.01421731\n",
      "Iteration 74, loss = 0.01403425\n",
      "Iteration 75, loss = 0.01387352\n",
      "Iteration 76, loss = 0.01379240\n",
      "Iteration 77, loss = 0.01410475\n",
      "Iteration 78, loss = 0.04054909\n",
      "Iteration 79, loss = 0.01612941\n",
      "Iteration 80, loss = 0.01447093\n",
      "Iteration 81, loss = 0.01417694\n",
      "Iteration 82, loss = 0.01397731\n",
      "Iteration 83, loss = 0.01381373\n",
      "Iteration 84, loss = 0.01367182\n",
      "Iteration 85, loss = 0.01352505\n",
      "Iteration 86, loss = 0.01340082\n",
      "Iteration 87, loss = 0.01344969\n",
      "Iteration 88, loss = 0.03687027\n",
      "Iteration 89, loss = 0.01690176\n",
      "Iteration 90, loss = 0.01436059\n",
      "Iteration 91, loss = 0.01387578\n",
      "Iteration 92, loss = 0.01368742\n",
      "Iteration 93, loss = 0.01351424\n",
      "Iteration 94, loss = 0.01337806\n",
      "Iteration 95, loss = 0.01323608\n",
      "Iteration 96, loss = 0.01312236\n",
      "Iteration 97, loss = 0.01318717\n",
      "Iteration 98, loss = 0.04100880\n",
      "Iteration 99, loss = 0.01566225\n",
      "Iteration 100, loss = 0.01404802\n",
      "Iteration 101, loss = 0.01365172\n",
      "Iteration 102, loss = 0.01347394\n",
      "Iteration 103, loss = 0.01331941\n",
      "Iteration 104, loss = 0.01317279\n",
      "Iteration 105, loss = 0.01303994\n",
      "Iteration 106, loss = 0.01292301\n",
      "Iteration 107, loss = 0.01290181\n",
      "Iteration 108, loss = 0.03536944\n",
      "Iteration 109, loss = 0.01884017\n",
      "Iteration 110, loss = 0.01403881\n",
      "Iteration 111, loss = 0.01350389\n",
      "Iteration 112, loss = 0.01326228\n",
      "Iteration 113, loss = 0.01311171\n",
      "Iteration 114, loss = 0.01296619\n",
      "Iteration 115, loss = 0.01287924\n",
      "Iteration 116, loss = 0.01275801\n",
      "Iteration 117, loss = 0.01284569\n",
      "Iteration 118, loss = 0.03683955\n",
      "Iteration 119, loss = 0.01636012\n",
      "Iteration 120, loss = 0.01370709\n",
      "Iteration 121, loss = 0.01331320\n",
      "Iteration 122, loss = 0.01312677\n",
      "Iteration 123, loss = 0.01299059\n",
      "Iteration 124, loss = 0.01286269\n",
      "Iteration 125, loss = 0.01271881\n",
      "Iteration 126, loss = 0.01262492\n",
      "Iteration 127, loss = 0.01256492\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.001, batch_size=100, hidden_layer_sizes=(100, 100, 100), solver=adam; total time= 4.0min\n",
      "Iteration 1, loss = 1.38511477\n",
      "Iteration 2, loss = 0.42604155\n",
      "Iteration 3, loss = 0.28479295\n",
      "Iteration 4, loss = 0.22331556\n",
      "Iteration 5, loss = 0.18169768\n",
      "Iteration 6, loss = 0.15354751\n",
      "Iteration 7, loss = 0.13123553\n",
      "Iteration 8, loss = 0.11439121\n",
      "Iteration 9, loss = 0.10067491\n",
      "Iteration 10, loss = 0.08857004\n",
      "Iteration 11, loss = 0.07925566\n",
      "Iteration 12, loss = 0.07003768\n",
      "Iteration 13, loss = 0.06358308\n",
      "Iteration 14, loss = 0.05584472\n",
      "Iteration 15, loss = 0.04927776\n",
      "Iteration 16, loss = 0.04542586\n",
      "Iteration 17, loss = 0.04080252\n",
      "Iteration 18, loss = 0.03615583\n",
      "Iteration 19, loss = 0.03351966\n",
      "Iteration 20, loss = 0.03079758\n",
      "Iteration 21, loss = 0.02872381\n",
      "Iteration 22, loss = 0.02760011\n",
      "Iteration 23, loss = 0.02491258\n",
      "Iteration 24, loss = 0.02569610\n",
      "Iteration 25, loss = 0.02311867\n",
      "Iteration 26, loss = 0.02190619\n",
      "Iteration 27, loss = 0.02077639\n",
      "Iteration 28, loss = 0.02026708\n",
      "Iteration 29, loss = 0.01939103\n",
      "Iteration 30, loss = 0.01924388\n",
      "Iteration 31, loss = 0.03450487\n",
      "Iteration 32, loss = 0.02007083\n",
      "Iteration 33, loss = 0.01797363\n",
      "Iteration 34, loss = 0.01733026\n",
      "Iteration 35, loss = 0.01687437\n",
      "Iteration 36, loss = 0.01657033\n",
      "Iteration 37, loss = 0.01627250\n",
      "Iteration 38, loss = 0.02554712\n",
      "Iteration 39, loss = 0.02728564\n",
      "Iteration 40, loss = 0.01976314\n",
      "Iteration 41, loss = 0.01726823\n",
      "Iteration 42, loss = 0.01626553\n",
      "Iteration 43, loss = 0.01597485\n",
      "Iteration 44, loss = 0.01569184\n",
      "Iteration 45, loss = 0.01546612\n",
      "Iteration 46, loss = 0.01537675\n",
      "Iteration 47, loss = 0.03258358\n",
      "Iteration 48, loss = 0.02256914\n",
      "Iteration 49, loss = 0.01713182\n",
      "Iteration 50, loss = 0.01577297\n",
      "Iteration 51, loss = 0.01542967\n",
      "Iteration 52, loss = 0.01518021\n",
      "Iteration 53, loss = 0.01493970\n",
      "Iteration 54, loss = 0.01470898\n",
      "Iteration 55, loss = 0.01479332\n",
      "Iteration 56, loss = 0.03574995\n",
      "Iteration 57, loss = 0.01625546\n",
      "Iteration 58, loss = 0.01512242\n",
      "Iteration 59, loss = 0.01478749\n",
      "Iteration 60, loss = 0.01458336\n",
      "Iteration 61, loss = 0.01437843\n",
      "Iteration 62, loss = 0.01422229\n",
      "Iteration 63, loss = 0.01409964\n",
      "Iteration 64, loss = 0.03019570\n",
      "Iteration 65, loss = 0.02471417\n",
      "Iteration 66, loss = 0.01618729\n",
      "Iteration 67, loss = 0.01476952\n",
      "Iteration 68, loss = 0.01447907\n",
      "Iteration 69, loss = 0.01427347\n",
      "Iteration 70, loss = 0.01409182\n",
      "Iteration 71, loss = 0.01391612\n",
      "Iteration 72, loss = 0.01378299\n",
      "Iteration 73, loss = 0.01369157\n",
      "Iteration 74, loss = 0.04162988\n",
      "Iteration 75, loss = 0.01626458\n",
      "Iteration 76, loss = 0.01467680\n",
      "Iteration 77, loss = 0.01421538\n",
      "Iteration 78, loss = 0.01402444\n",
      "Iteration 79, loss = 0.01383480\n",
      "Iteration 80, loss = 0.01366441\n",
      "Iteration 81, loss = 0.01359744\n",
      "Iteration 82, loss = 0.01348287\n",
      "Iteration 83, loss = 0.03065394\n",
      "Iteration 84, loss = 0.02039211\n",
      "Iteration 85, loss = 0.01466329\n",
      "Iteration 86, loss = 0.01396046\n",
      "Iteration 87, loss = 0.01374757\n",
      "Iteration 88, loss = 0.01358260\n",
      "Iteration 89, loss = 0.01341694\n",
      "Iteration 90, loss = 0.01327825\n",
      "Iteration 91, loss = 0.01314240\n",
      "Iteration 92, loss = 0.02534501\n",
      "Iteration 93, loss = 0.02730200\n",
      "Iteration 94, loss = 0.01457291\n",
      "Iteration 95, loss = 0.01382701\n",
      "Iteration 96, loss = 0.01357277\n",
      "Iteration 97, loss = 0.01339161\n",
      "Iteration 98, loss = 0.01327401\n",
      "Iteration 99, loss = 0.01309796\n",
      "Iteration 100, loss = 0.01297577\n",
      "Iteration 101, loss = 0.01295919\n",
      "Iteration 102, loss = 0.03059163\n",
      "Iteration 103, loss = 0.02076370\n",
      "Iteration 104, loss = 0.01477028\n",
      "Iteration 105, loss = 0.01358140\n",
      "Iteration 106, loss = 0.01326774\n",
      "Iteration 107, loss = 0.01309987\n",
      "Iteration 108, loss = 0.01295960\n",
      "Iteration 109, loss = 0.01284055\n",
      "Iteration 110, loss = 0.01271685\n",
      "Iteration 111, loss = 0.01270224\n",
      "Iteration 112, loss = 0.02678326\n",
      "Iteration 113, loss = 0.02438774\n",
      "Iteration 114, loss = 0.01419323\n",
      "Iteration 115, loss = 0.01332171\n",
      "Iteration 116, loss = 0.01310646\n",
      "Iteration 117, loss = 0.01294507\n",
      "Iteration 118, loss = 0.01279321\n",
      "Iteration 119, loss = 0.01266803\n",
      "Iteration 120, loss = 0.01255216\n",
      "Iteration 121, loss = 0.01300983\n",
      "Iteration 122, loss = 0.03594878\n",
      "Iteration 123, loss = 0.01463675\n",
      "Iteration 124, loss = 0.01331000\n",
      "Iteration 125, loss = 0.01303567\n",
      "Iteration 126, loss = 0.01288690\n",
      "Iteration 127, loss = 0.01272273\n",
      "Iteration 128, loss = 0.01259144\n",
      "Iteration 129, loss = 0.01249216\n",
      "Iteration 130, loss = 0.01243242\n",
      "Iteration 131, loss = 0.01323004\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.001, batch_size=100, hidden_layer_sizes=(100, 100, 100), solver=adam; total time= 4.1min\n",
      "Iteration 1, loss = 2.30556041\n",
      "Iteration 2, loss = 2.30230657\n",
      "Iteration 3, loss = 2.30225582\n",
      "Iteration 4, loss = 2.30202226\n",
      "Iteration 5, loss = 2.30206751\n",
      "Iteration 6, loss = 2.30182086\n",
      "Iteration 7, loss = 2.30194602\n",
      "Iteration 8, loss = 2.30179118\n",
      "Iteration 9, loss = 2.30168513\n",
      "Iteration 10, loss = 2.30168590\n",
      "Iteration 11, loss = 2.30160314\n",
      "Iteration 12, loss = 2.30129739\n",
      "Iteration 13, loss = 2.30130049\n",
      "Iteration 14, loss = 2.30109918\n",
      "Iteration 15, loss = 2.30105280\n",
      "Iteration 16, loss = 2.30087524\n",
      "Iteration 17, loss = 2.30068192\n",
      "Iteration 18, loss = 2.30089987\n",
      "Iteration 19, loss = 2.30064876\n",
      "Iteration 20, loss = 2.30026860\n",
      "Iteration 21, loss = 2.30041277\n",
      "Iteration 22, loss = 2.30019261\n",
      "Iteration 23, loss = 2.30005726\n",
      "Iteration 24, loss = 2.29987942\n",
      "Iteration 25, loss = 2.29942249\n",
      "Iteration 26, loss = 2.29955334\n",
      "Iteration 27, loss = 2.29929327\n",
      "Iteration 28, loss = 2.29892501\n",
      "Iteration 29, loss = 2.29886606\n",
      "Iteration 30, loss = 2.29861692\n",
      "Iteration 31, loss = 2.29857722\n",
      "Iteration 32, loss = 2.29838798\n",
      "Iteration 33, loss = 2.29790057\n",
      "Iteration 34, loss = 2.29744754\n",
      "Iteration 35, loss = 2.29752546\n",
      "Iteration 36, loss = 2.29709612\n",
      "Iteration 37, loss = 2.29666078\n",
      "Iteration 38, loss = 2.29617651\n",
      "Iteration 39, loss = 2.29583610\n",
      "Iteration 40, loss = 2.29531763\n",
      "Iteration 41, loss = 2.29475885\n",
      "Iteration 42, loss = 2.29417968\n",
      "Iteration 43, loss = 2.29363968\n",
      "Iteration 44, loss = 2.29280266\n",
      "Iteration 45, loss = 2.29233339\n",
      "Iteration 46, loss = 2.29121334\n",
      "Iteration 47, loss = 2.29033741\n",
      "Iteration 48, loss = 2.28928236\n",
      "Iteration 49, loss = 2.28778164\n",
      "Iteration 50, loss = 2.28640736\n",
      "Iteration 51, loss = 2.28470073\n",
      "Iteration 52, loss = 2.28279149\n",
      "Iteration 53, loss = 2.28043550\n",
      "Iteration 54, loss = 2.27768376\n",
      "Iteration 55, loss = 2.27453071\n",
      "Iteration 56, loss = 2.27038748\n",
      "Iteration 57, loss = 2.26555953\n",
      "Iteration 58, loss = 2.25919978\n",
      "Iteration 59, loss = 2.25129549\n",
      "Iteration 60, loss = 2.24100983\n",
      "Iteration 61, loss = 2.22740174\n",
      "Iteration 62, loss = 2.20892983\n",
      "Iteration 63, loss = 2.18414561\n",
      "Iteration 64, loss = 2.15041123\n",
      "Iteration 65, loss = 2.10621486\n",
      "Iteration 66, loss = 2.05113114\n",
      "Iteration 67, loss = 1.98908876\n",
      "Iteration 68, loss = 1.92738129\n",
      "Iteration 69, loss = 1.87149963\n",
      "Iteration 70, loss = 1.82446573\n",
      "Iteration 71, loss = 1.78551502\n",
      "Iteration 72, loss = 1.75287710\n",
      "Iteration 73, loss = 1.72430922\n",
      "Iteration 74, loss = 1.69846811\n",
      "Iteration 75, loss = 1.67392108\n",
      "Iteration 76, loss = 1.64895535\n",
      "Iteration 77, loss = 1.62330109\n",
      "Iteration 78, loss = 1.59529041\n",
      "Iteration 79, loss = 1.56417112\n",
      "Iteration 80, loss = 1.52925662\n",
      "Iteration 81, loss = 1.48972377\n",
      "Iteration 82, loss = 1.44549648\n",
      "Iteration 83, loss = 1.39771063\n",
      "Iteration 84, loss = 1.34848316\n",
      "Iteration 85, loss = 1.30024383\n",
      "Iteration 86, loss = 1.25541987\n",
      "Iteration 87, loss = 1.21463608\n",
      "Iteration 88, loss = 1.17787848\n",
      "Iteration 89, loss = 1.14413333\n",
      "Iteration 90, loss = 1.11239753\n",
      "Iteration 91, loss = 1.08172370\n",
      "Iteration 92, loss = 1.05157527\n",
      "Iteration 93, loss = 1.02229255\n",
      "Iteration 94, loss = 0.99363412\n",
      "Iteration 95, loss = 0.96579822\n",
      "Iteration 96, loss = 0.93958041\n",
      "Iteration 97, loss = 0.91496042\n",
      "Iteration 98, loss = 0.89214551\n",
      "Iteration 99, loss = 0.87106003\n",
      "Iteration 100, loss = 0.85164433\n",
      "Iteration 101, loss = 0.83360431\n",
      "Iteration 102, loss = 0.81702007\n",
      "Iteration 103, loss = 0.80174879\n",
      "Iteration 104, loss = 0.78735539\n",
      "Iteration 105, loss = 0.77391376\n",
      "Iteration 106, loss = 0.76134628\n",
      "Iteration 107, loss = 0.74945896\n",
      "Iteration 108, loss = 0.73817775\n",
      "Iteration 109, loss = 0.72754315\n",
      "Iteration 110, loss = 0.71735053\n",
      "Iteration 111, loss = 0.70761555\n",
      "Iteration 112, loss = 0.69827088\n",
      "Iteration 113, loss = 0.68946847\n",
      "Iteration 114, loss = 0.68090659\n",
      "Iteration 115, loss = 0.67266496\n",
      "Iteration 116, loss = 0.66477283\n",
      "Iteration 117, loss = 0.65711926\n",
      "Iteration 118, loss = 0.64984791\n",
      "Iteration 119, loss = 0.64284300\n",
      "Iteration 120, loss = 0.63574509\n",
      "Iteration 121, loss = 0.62901635\n",
      "Iteration 122, loss = 0.62253946\n",
      "Iteration 123, loss = 0.61627559\n",
      "Iteration 124, loss = 0.61038539\n",
      "Iteration 125, loss = 0.60436027\n",
      "Iteration 126, loss = 0.59885686\n",
      "Iteration 127, loss = 0.59327114\n",
      "Iteration 128, loss = 0.58795112\n",
      "Iteration 129, loss = 0.58279823\n",
      "Iteration 130, loss = 0.57780436\n",
      "Iteration 131, loss = 0.57294282\n",
      "Iteration 132, loss = 0.56821844\n",
      "Iteration 133, loss = 0.56365897\n",
      "Iteration 134, loss = 0.55906721\n",
      "Iteration 135, loss = 0.55484612\n",
      "Iteration 136, loss = 0.55062405\n",
      "Iteration 137, loss = 0.54665288\n",
      "Iteration 138, loss = 0.54270105\n",
      "Iteration 139, loss = 0.53877143\n",
      "Iteration 140, loss = 0.53502603\n",
      "Iteration 141, loss = 0.53144034\n",
      "Iteration 142, loss = 0.52770865\n",
      "Iteration 143, loss = 0.52421923\n",
      "Iteration 144, loss = 0.52089673\n",
      "Iteration 145, loss = 0.51758102\n",
      "Iteration 146, loss = 0.51429586\n",
      "Iteration 147, loss = 0.51099573\n",
      "Iteration 148, loss = 0.50800421\n",
      "Iteration 149, loss = 0.50487259\n",
      "Iteration 150, loss = 0.50183842\n",
      "Iteration 151, loss = 0.49895449\n",
      "Iteration 152, loss = 0.49601627\n",
      "Iteration 153, loss = 0.49317240\n",
      "Iteration 154, loss = 0.49028572\n",
      "Iteration 155, loss = 0.48768196\n",
      "Iteration 156, loss = 0.48488063\n",
      "Iteration 157, loss = 0.48227164\n",
      "Iteration 158, loss = 0.47960332\n",
      "Iteration 159, loss = 0.47699195\n",
      "Iteration 160, loss = 0.47447187\n",
      "Iteration 161, loss = 0.47195187\n",
      "Iteration 162, loss = 0.46953640\n",
      "Iteration 163, loss = 0.46694854\n",
      "Iteration 164, loss = 0.46452001\n",
      "Iteration 165, loss = 0.46229167\n",
      "Iteration 166, loss = 0.45980333\n",
      "Iteration 167, loss = 0.45738357\n",
      "Iteration 168, loss = 0.45522232\n",
      "Iteration 169, loss = 0.45271791\n",
      "Iteration 170, loss = 0.45067769\n",
      "Iteration 171, loss = 0.44842816\n",
      "Iteration 172, loss = 0.44617455\n",
      "Iteration 173, loss = 0.44388189\n",
      "Iteration 174, loss = 0.44171161\n",
      "Iteration 175, loss = 0.43928348\n",
      "Iteration 176, loss = 0.43746317\n",
      "Iteration 177, loss = 0.43511928\n",
      "Iteration 178, loss = 0.43305542\n",
      "Iteration 179, loss = 0.43083139\n",
      "Iteration 180, loss = 0.42881878\n",
      "Iteration 181, loss = 0.42673224\n",
      "Iteration 182, loss = 0.42450640\n",
      "Iteration 183, loss = 0.42250571\n",
      "Iteration 184, loss = 0.42041141\n",
      "Iteration 185, loss = 0.41835122\n",
      "Iteration 186, loss = 0.41640176\n",
      "Iteration 187, loss = 0.41414020\n",
      "Iteration 188, loss = 0.41221282\n",
      "Iteration 189, loss = 0.41023311\n",
      "Iteration 190, loss = 0.40818070\n",
      "Iteration 191, loss = 0.40600644\n",
      "Iteration 192, loss = 0.40417350\n",
      "Iteration 193, loss = 0.40218253\n",
      "Iteration 194, loss = 0.40010386\n",
      "Iteration 195, loss = 0.39811106\n",
      "Iteration 196, loss = 0.39623299\n",
      "Iteration 197, loss = 0.39415315\n",
      "Iteration 198, loss = 0.39208332\n",
      "Iteration 199, loss = 0.39026945\n",
      "Iteration 200, loss = 0.38821578\n",
      "[CV] END activation=logistic, alpha=0.001, batch_size=100, hidden_layer_sizes=(100, 100, 100), solver=sgd; total time= 5.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.30417545\n",
      "Iteration 2, loss = 2.30236058\n",
      "Iteration 3, loss = 2.30218640\n",
      "Iteration 4, loss = 2.30204701\n",
      "Iteration 5, loss = 2.30201437\n",
      "Iteration 6, loss = 2.30184927\n",
      "Iteration 7, loss = 2.30177267\n",
      "Iteration 8, loss = 2.30169886\n",
      "Iteration 9, loss = 2.30160892\n",
      "Iteration 10, loss = 2.30148649\n",
      "Iteration 11, loss = 2.30137496\n",
      "Iteration 12, loss = 2.30129558\n",
      "Iteration 13, loss = 2.30140271\n",
      "Iteration 14, loss = 2.30118050\n",
      "Iteration 15, loss = 2.30100702\n",
      "Iteration 16, loss = 2.30081896\n",
      "Iteration 17, loss = 2.30074796\n",
      "Iteration 18, loss = 2.30093485\n",
      "Iteration 19, loss = 2.30072625\n",
      "Iteration 20, loss = 2.30054552\n",
      "Iteration 21, loss = 2.30032395\n",
      "Iteration 22, loss = 2.30030435\n",
      "Iteration 23, loss = 2.30004233\n",
      "Iteration 24, loss = 2.29980561\n",
      "Iteration 25, loss = 2.29976955\n",
      "Iteration 26, loss = 2.29974421\n",
      "Iteration 27, loss = 2.29962256\n",
      "Iteration 28, loss = 2.29947392\n",
      "Iteration 29, loss = 2.29920340\n",
      "Iteration 30, loss = 2.29915510\n",
      "Iteration 31, loss = 2.29887197\n",
      "Iteration 32, loss = 2.29854523\n",
      "Iteration 33, loss = 2.29827157\n",
      "Iteration 34, loss = 2.29827394\n",
      "Iteration 35, loss = 2.29799447\n",
      "Iteration 36, loss = 2.29776231\n",
      "Iteration 37, loss = 2.29751421\n",
      "Iteration 38, loss = 2.29715505\n",
      "Iteration 39, loss = 2.29674836\n",
      "Iteration 40, loss = 2.29646734\n",
      "Iteration 41, loss = 2.29595533\n",
      "Iteration 42, loss = 2.29571166\n",
      "Iteration 43, loss = 2.29529603\n",
      "Iteration 44, loss = 2.29475900\n",
      "Iteration 45, loss = 2.29439595\n",
      "Iteration 46, loss = 2.29371956\n",
      "Iteration 47, loss = 2.29299640\n",
      "Iteration 48, loss = 2.29236852\n",
      "Iteration 49, loss = 2.29178823\n",
      "Iteration 50, loss = 2.29109910\n",
      "Iteration 51, loss = 2.29000764\n",
      "Iteration 52, loss = 2.28920472\n",
      "Iteration 53, loss = 2.28793252\n",
      "Iteration 54, loss = 2.28674999\n",
      "Iteration 55, loss = 2.28518418\n",
      "Iteration 56, loss = 2.28356284\n",
      "Iteration 57, loss = 2.28180638\n",
      "Iteration 58, loss = 2.27940350\n",
      "Iteration 59, loss = 2.27682149\n",
      "Iteration 60, loss = 2.27392758\n",
      "Iteration 61, loss = 2.27028486\n",
      "Iteration 62, loss = 2.26575529\n",
      "Iteration 63, loss = 2.26073558\n",
      "Iteration 64, loss = 2.25405000\n",
      "Iteration 65, loss = 2.24575806\n",
      "Iteration 66, loss = 2.23509850\n",
      "Iteration 67, loss = 2.22133642\n",
      "Iteration 68, loss = 2.20297472\n",
      "Iteration 69, loss = 2.17829191\n",
      "Iteration 70, loss = 2.14539022\n",
      "Iteration 71, loss = 2.10175346\n",
      "Iteration 72, loss = 2.04666803\n",
      "Iteration 73, loss = 1.98183989\n",
      "Iteration 74, loss = 1.91244795\n",
      "Iteration 75, loss = 1.84384285\n",
      "Iteration 76, loss = 1.77854524\n",
      "Iteration 77, loss = 1.71639708\n",
      "Iteration 78, loss = 1.65531928\n",
      "Iteration 79, loss = 1.59350403\n",
      "Iteration 80, loss = 1.53143272\n",
      "Iteration 81, loss = 1.47049724\n",
      "Iteration 82, loss = 1.41386984\n",
      "Iteration 83, loss = 1.36373848\n",
      "Iteration 84, loss = 1.32042387\n",
      "Iteration 85, loss = 1.28291346\n",
      "Iteration 86, loss = 1.25057850\n",
      "Iteration 87, loss = 1.22228602\n",
      "Iteration 88, loss = 1.19675225\n",
      "Iteration 89, loss = 1.17363847\n",
      "Iteration 90, loss = 1.15250031\n",
      "Iteration 91, loss = 1.13273794\n",
      "Iteration 92, loss = 1.11424301\n",
      "Iteration 93, loss = 1.09650500\n",
      "Iteration 94, loss = 1.07976588\n",
      "Iteration 95, loss = 1.06339904\n",
      "Iteration 96, loss = 1.04755589\n",
      "Iteration 97, loss = 1.03175797\n",
      "Iteration 98, loss = 1.01652347\n",
      "Iteration 99, loss = 1.00087157\n",
      "Iteration 100, loss = 0.98536434\n",
      "Iteration 101, loss = 0.96952145\n",
      "Iteration 102, loss = 0.95348172\n",
      "Iteration 103, loss = 0.93719099\n",
      "Iteration 104, loss = 0.92056467\n",
      "Iteration 105, loss = 0.90355894\n",
      "Iteration 106, loss = 0.88636355\n",
      "Iteration 107, loss = 0.86909552\n",
      "Iteration 108, loss = 0.85166017\n",
      "Iteration 109, loss = 0.83464378\n",
      "Iteration 110, loss = 0.81798620\n",
      "Iteration 111, loss = 0.80192945\n",
      "Iteration 112, loss = 0.78657525\n",
      "Iteration 113, loss = 0.77196912\n",
      "Iteration 114, loss = 0.75842405\n",
      "Iteration 115, loss = 0.74531363\n",
      "Iteration 116, loss = 0.73332114\n",
      "Iteration 117, loss = 0.72198623\n",
      "Iteration 118, loss = 0.71143110\n",
      "Iteration 119, loss = 0.70142109\n",
      "Iteration 120, loss = 0.69190951\n",
      "Iteration 121, loss = 0.68306249\n",
      "Iteration 122, loss = 0.67460860\n",
      "Iteration 123, loss = 0.66644374\n",
      "Iteration 124, loss = 0.65879323\n",
      "Iteration 125, loss = 0.65143607\n",
      "Iteration 126, loss = 0.64443348\n",
      "Iteration 127, loss = 0.63763606\n",
      "Iteration 128, loss = 0.63102875\n",
      "Iteration 129, loss = 0.62473523\n",
      "Iteration 130, loss = 0.61854661\n",
      "Iteration 131, loss = 0.61272369\n",
      "Iteration 132, loss = 0.60688364\n",
      "Iteration 133, loss = 0.60122342\n",
      "Iteration 134, loss = 0.59591845\n",
      "Iteration 135, loss = 0.59058124\n",
      "Iteration 136, loss = 0.58542173\n",
      "Iteration 137, loss = 0.58039662\n",
      "Iteration 138, loss = 0.57560254\n",
      "Iteration 139, loss = 0.57076859\n",
      "Iteration 140, loss = 0.56583178\n",
      "Iteration 141, loss = 0.56152867\n",
      "Iteration 142, loss = 0.55686489\n",
      "Iteration 143, loss = 0.55251937\n",
      "Iteration 144, loss = 0.54817896\n",
      "Iteration 145, loss = 0.54400568\n",
      "Iteration 146, loss = 0.53979140\n",
      "Iteration 147, loss = 0.53591229\n",
      "Iteration 148, loss = 0.53176937\n",
      "Iteration 149, loss = 0.52792946\n",
      "Iteration 150, loss = 0.52408096\n",
      "Iteration 151, loss = 0.52024574\n",
      "Iteration 152, loss = 0.51642716\n",
      "Iteration 153, loss = 0.51298708\n",
      "Iteration 154, loss = 0.50923979\n",
      "Iteration 155, loss = 0.50573641\n",
      "Iteration 156, loss = 0.50217531\n",
      "Iteration 157, loss = 0.49881428\n",
      "Iteration 158, loss = 0.49540578\n",
      "Iteration 159, loss = 0.49201284\n",
      "Iteration 160, loss = 0.48877812\n",
      "Iteration 161, loss = 0.48548231\n",
      "Iteration 162, loss = 0.48247749\n",
      "Iteration 163, loss = 0.47921101\n",
      "Iteration 164, loss = 0.47605764\n",
      "Iteration 165, loss = 0.47294034\n",
      "Iteration 166, loss = 0.47011870\n",
      "Iteration 167, loss = 0.46705380\n",
      "Iteration 168, loss = 0.46406261\n",
      "Iteration 169, loss = 0.46124812\n",
      "Iteration 170, loss = 0.45826482\n",
      "Iteration 171, loss = 0.45550976\n",
      "Iteration 172, loss = 0.45269258\n",
      "Iteration 173, loss = 0.44997793\n",
      "Iteration 174, loss = 0.44722528\n",
      "Iteration 175, loss = 0.44430633\n",
      "Iteration 176, loss = 0.44177737\n",
      "Iteration 177, loss = 0.43911022\n",
      "Iteration 178, loss = 0.43666137\n",
      "Iteration 179, loss = 0.43397916\n",
      "Iteration 180, loss = 0.43135155\n",
      "Iteration 181, loss = 0.42879375\n",
      "Iteration 182, loss = 0.42636718\n",
      "Iteration 183, loss = 0.42393323\n",
      "Iteration 184, loss = 0.42141510\n",
      "Iteration 185, loss = 0.41912388\n",
      "Iteration 186, loss = 0.41675035\n",
      "Iteration 187, loss = 0.41416627\n",
      "Iteration 188, loss = 0.41204150\n",
      "Iteration 189, loss = 0.40968233\n",
      "Iteration 190, loss = 0.40739478\n",
      "Iteration 191, loss = 0.40523972\n",
      "Iteration 192, loss = 0.40289022\n",
      "Iteration 193, loss = 0.40070823\n",
      "Iteration 194, loss = 0.39857907\n",
      "Iteration 195, loss = 0.39639818\n",
      "Iteration 196, loss = 0.39427972\n",
      "Iteration 197, loss = 0.39214797\n",
      "Iteration 198, loss = 0.39008114\n",
      "Iteration 199, loss = 0.38795913\n",
      "Iteration 200, loss = 0.38584149\n",
      "[CV] END activation=logistic, alpha=0.001, batch_size=100, hidden_layer_sizes=(100, 100, 100), solver=sgd; total time= 5.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.30472089\n",
      "Iteration 2, loss = 2.30235501\n",
      "Iteration 3, loss = 2.30228684\n",
      "Iteration 4, loss = 2.30195838\n",
      "Iteration 5, loss = 2.30216321\n",
      "Iteration 6, loss = 2.30198970\n",
      "Iteration 7, loss = 2.30194033\n",
      "Iteration 8, loss = 2.30169800\n",
      "Iteration 9, loss = 2.30167579\n",
      "Iteration 10, loss = 2.30154915\n",
      "Iteration 11, loss = 2.30156900\n",
      "Iteration 12, loss = 2.30141519\n",
      "Iteration 13, loss = 2.30107843\n",
      "Iteration 14, loss = 2.30125441\n",
      "Iteration 15, loss = 2.30095095\n",
      "Iteration 16, loss = 2.30085520\n",
      "Iteration 17, loss = 2.30073677\n",
      "Iteration 18, loss = 2.30065318\n",
      "Iteration 19, loss = 2.30051213\n",
      "Iteration 20, loss = 2.30048568\n",
      "Iteration 21, loss = 2.30035345\n",
      "Iteration 22, loss = 2.30008285\n",
      "Iteration 23, loss = 2.29993355\n",
      "Iteration 24, loss = 2.29982261\n",
      "Iteration 25, loss = 2.29957667\n",
      "Iteration 26, loss = 2.29935330\n",
      "Iteration 27, loss = 2.29915536\n",
      "Iteration 28, loss = 2.29903542\n",
      "Iteration 29, loss = 2.29882784\n",
      "Iteration 30, loss = 2.29838157\n",
      "Iteration 31, loss = 2.29806865\n",
      "Iteration 32, loss = 2.29784972\n",
      "Iteration 33, loss = 2.29745699\n",
      "Iteration 34, loss = 2.29722158\n",
      "Iteration 35, loss = 2.29703884\n",
      "Iteration 36, loss = 2.29663621\n",
      "Iteration 37, loss = 2.29612366\n",
      "Iteration 38, loss = 2.29562498\n",
      "Iteration 39, loss = 2.29505404\n",
      "Iteration 40, loss = 2.29467383\n",
      "Iteration 41, loss = 2.29413459\n",
      "Iteration 42, loss = 2.29352516\n",
      "Iteration 43, loss = 2.29277783\n",
      "Iteration 44, loss = 2.29205799\n",
      "Iteration 45, loss = 2.29106947\n",
      "Iteration 46, loss = 2.29013952\n",
      "Iteration 47, loss = 2.28899036\n",
      "Iteration 48, loss = 2.28770099\n",
      "Iteration 49, loss = 2.28590291\n",
      "Iteration 50, loss = 2.28453599\n",
      "Iteration 51, loss = 2.28243050\n",
      "Iteration 52, loss = 2.27996546\n",
      "Iteration 53, loss = 2.27739500\n",
      "Iteration 54, loss = 2.27416807\n",
      "Iteration 55, loss = 2.27007338\n",
      "Iteration 56, loss = 2.26531905\n",
      "Iteration 57, loss = 2.25928322\n",
      "Iteration 58, loss = 2.25192324\n",
      "Iteration 59, loss = 2.24235783\n",
      "Iteration 60, loss = 2.22982081\n",
      "Iteration 61, loss = 2.21388812\n",
      "Iteration 62, loss = 2.19274877\n",
      "Iteration 63, loss = 2.16477171\n",
      "Iteration 64, loss = 2.12853581\n",
      "Iteration 65, loss = 2.08304654\n",
      "Iteration 66, loss = 2.02938370\n",
      "Iteration 67, loss = 1.97049182\n",
      "Iteration 68, loss = 1.90975278\n",
      "Iteration 69, loss = 1.84866297\n",
      "Iteration 70, loss = 1.78655480\n",
      "Iteration 71, loss = 1.72249897\n",
      "Iteration 72, loss = 1.65483063\n",
      "Iteration 73, loss = 1.58461544\n",
      "Iteration 74, loss = 1.51543724\n",
      "Iteration 75, loss = 1.45193094\n",
      "Iteration 76, loss = 1.39671161\n",
      "Iteration 77, loss = 1.35042674\n",
      "Iteration 78, loss = 1.31172595\n",
      "Iteration 79, loss = 1.27888449\n",
      "Iteration 80, loss = 1.25099881\n",
      "Iteration 81, loss = 1.22625994\n",
      "Iteration 82, loss = 1.20417993\n",
      "Iteration 83, loss = 1.18387984\n",
      "Iteration 84, loss = 1.16564817\n",
      "Iteration 85, loss = 1.14865895\n",
      "Iteration 86, loss = 1.13249091\n",
      "Iteration 87, loss = 1.11758210\n",
      "Iteration 88, loss = 1.10361801\n",
      "Iteration 89, loss = 1.09015172\n",
      "Iteration 90, loss = 1.07737284\n",
      "Iteration 91, loss = 1.06505067\n",
      "Iteration 92, loss = 1.05306261\n",
      "Iteration 93, loss = 1.04170389\n",
      "Iteration 94, loss = 1.03057826\n",
      "Iteration 95, loss = 1.01945269\n",
      "Iteration 96, loss = 1.00898930\n",
      "Iteration 97, loss = 0.99835391\n",
      "Iteration 98, loss = 0.98810171\n",
      "Iteration 99, loss = 0.97786466\n",
      "Iteration 100, loss = 0.96819457\n",
      "Iteration 101, loss = 0.95838321\n",
      "Iteration 102, loss = 0.94842298\n",
      "Iteration 103, loss = 0.93878643\n",
      "Iteration 104, loss = 0.92933541\n",
      "Iteration 105, loss = 0.92000667\n",
      "Iteration 106, loss = 0.91046814\n",
      "Iteration 107, loss = 0.90104057\n",
      "Iteration 108, loss = 0.89148769\n",
      "Iteration 109, loss = 0.88200685\n",
      "Iteration 110, loss = 0.87230057\n",
      "Iteration 111, loss = 0.86249458\n",
      "Iteration 112, loss = 0.85265099\n",
      "Iteration 113, loss = 0.84248178\n",
      "Iteration 114, loss = 0.83226694\n",
      "Iteration 115, loss = 0.82180559\n",
      "Iteration 116, loss = 0.81065812\n",
      "Iteration 117, loss = 0.79927382\n",
      "Iteration 118, loss = 0.78763688\n",
      "Iteration 119, loss = 0.77537928\n",
      "Iteration 120, loss = 0.76261159\n",
      "Iteration 121, loss = 0.74971963\n",
      "Iteration 122, loss = 0.73663409\n",
      "Iteration 123, loss = 0.72311601\n",
      "Iteration 124, loss = 0.70979732\n",
      "Iteration 125, loss = 0.69663663\n",
      "Iteration 126, loss = 0.68367472\n",
      "Iteration 127, loss = 0.67160639\n",
      "Iteration 128, loss = 0.65986040\n",
      "Iteration 129, loss = 0.64891961\n",
      "Iteration 130, loss = 0.63852033\n",
      "Iteration 131, loss = 0.62895062\n",
      "Iteration 132, loss = 0.61994740\n",
      "Iteration 133, loss = 0.61159890\n",
      "Iteration 134, loss = 0.60380248\n",
      "Iteration 135, loss = 0.59634439\n",
      "Iteration 136, loss = 0.58944181\n",
      "Iteration 137, loss = 0.58274599\n",
      "Iteration 138, loss = 0.57666769\n",
      "Iteration 139, loss = 0.57070165\n",
      "Iteration 140, loss = 0.56509032\n",
      "Iteration 141, loss = 0.55973490\n",
      "Iteration 142, loss = 0.55469781\n",
      "Iteration 143, loss = 0.54974373\n",
      "Iteration 144, loss = 0.54503686\n",
      "Iteration 145, loss = 0.54045675\n",
      "Iteration 146, loss = 0.53594893\n",
      "Iteration 147, loss = 0.53177771\n",
      "Iteration 148, loss = 0.52768276\n",
      "Iteration 149, loss = 0.52352622\n",
      "Iteration 150, loss = 0.51986249\n",
      "Iteration 151, loss = 0.51593249\n",
      "Iteration 152, loss = 0.51231627\n",
      "Iteration 153, loss = 0.50868466\n",
      "Iteration 154, loss = 0.50518095\n",
      "Iteration 155, loss = 0.50174703\n",
      "Iteration 156, loss = 0.49859236\n",
      "Iteration 157, loss = 0.49537124\n",
      "Iteration 158, loss = 0.49207352\n",
      "Iteration 159, loss = 0.48903902\n",
      "Iteration 160, loss = 0.48584945\n",
      "Iteration 161, loss = 0.48306573\n",
      "Iteration 162, loss = 0.48017316\n",
      "Iteration 163, loss = 0.47730298\n",
      "Iteration 164, loss = 0.47438899\n",
      "Iteration 165, loss = 0.47145934\n",
      "Iteration 166, loss = 0.46889573\n",
      "Iteration 167, loss = 0.46620854\n",
      "Iteration 168, loss = 0.46349759\n",
      "Iteration 169, loss = 0.46096589\n",
      "Iteration 170, loss = 0.45829002\n",
      "Iteration 171, loss = 0.45590476\n",
      "Iteration 172, loss = 0.45332146\n",
      "Iteration 173, loss = 0.45086971\n",
      "Iteration 174, loss = 0.44842909\n",
      "Iteration 175, loss = 0.44601300\n",
      "Iteration 176, loss = 0.44360579\n",
      "Iteration 177, loss = 0.44123404\n",
      "Iteration 178, loss = 0.43881428\n",
      "Iteration 179, loss = 0.43668560\n",
      "Iteration 180, loss = 0.43432766\n",
      "Iteration 181, loss = 0.43207165\n",
      "Iteration 182, loss = 0.42982391\n",
      "Iteration 183, loss = 0.42756605\n",
      "Iteration 184, loss = 0.42530931\n",
      "Iteration 185, loss = 0.42317567\n",
      "Iteration 186, loss = 0.42097960\n",
      "Iteration 187, loss = 0.41893425\n",
      "Iteration 188, loss = 0.41677132\n",
      "Iteration 189, loss = 0.41454758\n",
      "Iteration 190, loss = 0.41259585\n",
      "Iteration 191, loss = 0.41050301\n",
      "Iteration 192, loss = 0.40829409\n",
      "Iteration 193, loss = 0.40628775\n",
      "Iteration 194, loss = 0.40416084\n",
      "Iteration 195, loss = 0.40198281\n",
      "Iteration 196, loss = 0.40005758\n",
      "Iteration 197, loss = 0.39800490\n",
      "Iteration 198, loss = 0.39592060\n",
      "Iteration 199, loss = 0.39399543\n",
      "Iteration 200, loss = 0.39192169\n",
      "[CV] END activation=logistic, alpha=0.001, batch_size=100, hidden_layer_sizes=(100, 100, 100), solver=sgd; total time= 5.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.30764086\n",
      "Iteration 2, loss = 2.30255695\n",
      "Iteration 3, loss = 2.30220630\n",
      "Iteration 4, loss = 2.30218313\n",
      "Iteration 5, loss = 2.30214746\n",
      "Iteration 6, loss = 2.30209855\n",
      "Iteration 7, loss = 2.30211395\n",
      "Iteration 8, loss = 2.30193785\n",
      "Iteration 9, loss = 2.30177789\n",
      "Iteration 10, loss = 2.30182239\n",
      "Iteration 11, loss = 2.30167250\n",
      "Iteration 12, loss = 2.30159140\n",
      "Iteration 13, loss = 2.30150684\n",
      "Iteration 14, loss = 2.30145907\n",
      "Iteration 15, loss = 2.30136213\n",
      "Iteration 16, loss = 2.30116141\n",
      "Iteration 17, loss = 2.30115909\n",
      "Iteration 18, loss = 2.30096235\n",
      "Iteration 19, loss = 2.30077677\n",
      "Iteration 20, loss = 2.30079637\n",
      "Iteration 21, loss = 2.30075367\n",
      "Iteration 22, loss = 2.30042586\n",
      "Iteration 23, loss = 2.30013495\n",
      "Iteration 24, loss = 2.30013691\n",
      "Iteration 25, loss = 2.29997554\n",
      "Iteration 26, loss = 2.30007850\n",
      "Iteration 27, loss = 2.29967569\n",
      "Iteration 28, loss = 2.29939402\n",
      "Iteration 29, loss = 2.29935940\n",
      "Iteration 30, loss = 2.29904467\n",
      "Iteration 31, loss = 2.29880233\n",
      "Iteration 32, loss = 2.29866884\n",
      "Iteration 33, loss = 2.29852319\n",
      "Iteration 34, loss = 2.29823766\n",
      "Iteration 35, loss = 2.29812645\n",
      "Iteration 36, loss = 2.29769528\n",
      "Iteration 37, loss = 2.29746586\n",
      "Iteration 38, loss = 2.29724338\n",
      "Iteration 39, loss = 2.29688350\n",
      "Iteration 40, loss = 2.29639000\n",
      "Iteration 41, loss = 2.29611291\n",
      "Iteration 42, loss = 2.29553504\n",
      "Iteration 43, loss = 2.29512084\n",
      "Iteration 44, loss = 2.29465201\n",
      "Iteration 45, loss = 2.29402603\n",
      "Iteration 46, loss = 2.29333767\n",
      "Iteration 47, loss = 2.29251308\n",
      "Iteration 48, loss = 2.29168632\n",
      "Iteration 49, loss = 2.29079241\n",
      "Iteration 50, loss = 2.29012019\n",
      "Iteration 51, loss = 2.28875958\n",
      "Iteration 52, loss = 2.28771921\n",
      "Iteration 53, loss = 2.28627160\n",
      "Iteration 54, loss = 2.28471882\n",
      "Iteration 55, loss = 2.28284307\n",
      "Iteration 56, loss = 2.28045481\n",
      "Iteration 57, loss = 2.27800311\n",
      "Iteration 58, loss = 2.27499936\n",
      "Iteration 59, loss = 2.27165231\n",
      "Iteration 60, loss = 2.26708660\n",
      "Iteration 61, loss = 2.26197406\n",
      "Iteration 62, loss = 2.25532305\n",
      "Iteration 63, loss = 2.24712908\n",
      "Iteration 64, loss = 2.23670561\n",
      "Iteration 65, loss = 2.22336226\n",
      "Iteration 66, loss = 2.20589658\n",
      "Iteration 67, loss = 2.18324209\n",
      "Iteration 68, loss = 2.15360100\n",
      "Iteration 69, loss = 2.11552759\n",
      "Iteration 70, loss = 2.06892820\n",
      "Iteration 71, loss = 2.01515967\n",
      "Iteration 72, loss = 1.95685264\n",
      "Iteration 73, loss = 1.89649840\n",
      "Iteration 74, loss = 1.83528208\n",
      "Iteration 75, loss = 1.77245036\n",
      "Iteration 76, loss = 1.70697778\n",
      "Iteration 77, loss = 1.63909388\n",
      "Iteration 78, loss = 1.57183461\n",
      "Iteration 79, loss = 1.50936393\n",
      "Iteration 80, loss = 1.45543074\n",
      "Iteration 81, loss = 1.41066945\n",
      "Iteration 82, loss = 1.37390172\n",
      "Iteration 83, loss = 1.34401331\n",
      "Iteration 84, loss = 1.31850974\n",
      "Iteration 85, loss = 1.29660525\n",
      "Iteration 86, loss = 1.27706112\n",
      "Iteration 87, loss = 1.25910171\n",
      "Iteration 88, loss = 1.24278031\n",
      "Iteration 89, loss = 1.22723501\n",
      "Iteration 90, loss = 1.21207146\n",
      "Iteration 91, loss = 1.19725859\n",
      "Iteration 92, loss = 1.18268157\n",
      "Iteration 93, loss = 1.16813683\n",
      "Iteration 94, loss = 1.15341900\n",
      "Iteration 95, loss = 1.13840268\n",
      "Iteration 96, loss = 1.12301451\n",
      "Iteration 97, loss = 1.10669506\n",
      "Iteration 98, loss = 1.08984104\n",
      "Iteration 99, loss = 1.07159983\n",
      "Iteration 100, loss = 1.05217783\n",
      "Iteration 101, loss = 1.03123706\n",
      "Iteration 102, loss = 1.00849077\n",
      "Iteration 103, loss = 0.98441559\n",
      "Iteration 104, loss = 0.95857227\n",
      "Iteration 105, loss = 0.93228040\n",
      "Iteration 106, loss = 0.90592111\n",
      "Iteration 107, loss = 0.88050667\n",
      "Iteration 108, loss = 0.85629789\n",
      "Iteration 109, loss = 0.83434046\n",
      "Iteration 110, loss = 0.81416573\n",
      "Iteration 111, loss = 0.79625900\n",
      "Iteration 112, loss = 0.77998772\n",
      "Iteration 113, loss = 0.76533170\n",
      "Iteration 114, loss = 0.75198100\n",
      "Iteration 115, loss = 0.73992771\n",
      "Iteration 116, loss = 0.72864637\n",
      "Iteration 117, loss = 0.71819213\n",
      "Iteration 118, loss = 0.70828911\n",
      "Iteration 119, loss = 0.69907851\n",
      "Iteration 120, loss = 0.69053529\n",
      "Iteration 121, loss = 0.68207240\n",
      "Iteration 122, loss = 0.67411813\n",
      "Iteration 123, loss = 0.66641419\n",
      "Iteration 124, loss = 0.65916418\n",
      "Iteration 125, loss = 0.65196647\n",
      "Iteration 126, loss = 0.64513490\n",
      "Iteration 127, loss = 0.63826925\n",
      "Iteration 128, loss = 0.63193051\n",
      "Iteration 129, loss = 0.62570742\n",
      "Iteration 130, loss = 0.61945440\n",
      "Iteration 131, loss = 0.61344386\n",
      "Iteration 132, loss = 0.60753486\n",
      "Iteration 133, loss = 0.60182682\n",
      "Iteration 134, loss = 0.59616338\n",
      "Iteration 135, loss = 0.59076865\n",
      "Iteration 136, loss = 0.58548874\n",
      "Iteration 137, loss = 0.58025019\n",
      "Iteration 138, loss = 0.57517587\n",
      "Iteration 139, loss = 0.57016354\n",
      "Iteration 140, loss = 0.56525876\n",
      "Iteration 141, loss = 0.56059289\n",
      "Iteration 142, loss = 0.55599911\n",
      "Iteration 143, loss = 0.55133663\n",
      "Iteration 144, loss = 0.54696782\n",
      "Iteration 145, loss = 0.54256146\n",
      "Iteration 146, loss = 0.53829010\n",
      "Iteration 147, loss = 0.53419817\n",
      "Iteration 148, loss = 0.53003502\n",
      "Iteration 149, loss = 0.52617579\n",
      "Iteration 150, loss = 0.52214457\n",
      "Iteration 151, loss = 0.51823325\n",
      "Iteration 152, loss = 0.51453012\n",
      "Iteration 153, loss = 0.51074423\n",
      "Iteration 154, loss = 0.50713802\n",
      "Iteration 155, loss = 0.50367756\n",
      "Iteration 156, loss = 0.50006791\n",
      "Iteration 157, loss = 0.49671566\n",
      "Iteration 158, loss = 0.49328805\n",
      "Iteration 159, loss = 0.48998927\n",
      "Iteration 160, loss = 0.48677111\n",
      "Iteration 161, loss = 0.48345580\n",
      "Iteration 162, loss = 0.48024086\n",
      "Iteration 163, loss = 0.47730712\n",
      "Iteration 164, loss = 0.47409171\n",
      "Iteration 165, loss = 0.47114389\n",
      "Iteration 166, loss = 0.46803164\n",
      "Iteration 167, loss = 0.46517034\n",
      "Iteration 168, loss = 0.46230522\n",
      "Iteration 169, loss = 0.45948151\n",
      "Iteration 170, loss = 0.45671152\n",
      "Iteration 171, loss = 0.45382700\n",
      "Iteration 172, loss = 0.45104749\n",
      "Iteration 173, loss = 0.44830891\n",
      "Iteration 174, loss = 0.44571158\n",
      "Iteration 175, loss = 0.44307387\n",
      "Iteration 176, loss = 0.44035145\n",
      "Iteration 177, loss = 0.43777200\n",
      "Iteration 178, loss = 0.43506396\n",
      "Iteration 179, loss = 0.43242459\n",
      "Iteration 180, loss = 0.43002756\n",
      "Iteration 181, loss = 0.42763301\n",
      "Iteration 182, loss = 0.42502826\n",
      "Iteration 183, loss = 0.42263868\n",
      "Iteration 184, loss = 0.42005376\n",
      "Iteration 185, loss = 0.41776271\n",
      "Iteration 186, loss = 0.41533162\n",
      "Iteration 187, loss = 0.41303691\n",
      "Iteration 188, loss = 0.41065942\n",
      "Iteration 189, loss = 0.40823757\n",
      "Iteration 190, loss = 0.40585035\n",
      "Iteration 191, loss = 0.40352973\n",
      "Iteration 192, loss = 0.40129056\n",
      "Iteration 193, loss = 0.39913474\n",
      "Iteration 194, loss = 0.39674827\n",
      "Iteration 195, loss = 0.39447654\n",
      "Iteration 196, loss = 0.39226160\n",
      "Iteration 197, loss = 0.39017055\n",
      "Iteration 198, loss = 0.38803212\n",
      "Iteration 199, loss = 0.38584600\n",
      "Iteration 200, loss = 0.38373072\n",
      "[CV] END activation=logistic, alpha=0.001, batch_size=100, hidden_layer_sizes=(100, 100, 100), solver=sgd; total time= 5.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.30796780\n",
      "Iteration 2, loss = 2.30259501\n",
      "Iteration 3, loss = 2.30247586\n",
      "Iteration 4, loss = 2.30222985\n",
      "Iteration 5, loss = 2.30216764\n",
      "Iteration 6, loss = 2.30209721\n",
      "Iteration 7, loss = 2.30170942\n",
      "Iteration 8, loss = 2.30191313\n",
      "Iteration 9, loss = 2.30182568\n",
      "Iteration 10, loss = 2.30175373\n",
      "Iteration 11, loss = 2.30173651\n",
      "Iteration 12, loss = 2.30156666\n",
      "Iteration 13, loss = 2.30160635\n",
      "Iteration 14, loss = 2.30137957\n",
      "Iteration 15, loss = 2.30132052\n",
      "Iteration 16, loss = 2.30105274\n",
      "Iteration 17, loss = 2.30117743\n",
      "Iteration 18, loss = 2.30076987\n",
      "Iteration 19, loss = 2.30067417\n",
      "Iteration 20, loss = 2.30064443\n",
      "Iteration 21, loss = 2.30058236\n",
      "Iteration 22, loss = 2.30030071\n",
      "Iteration 23, loss = 2.30029042\n",
      "Iteration 24, loss = 2.30002589\n",
      "Iteration 25, loss = 2.29975081\n",
      "Iteration 26, loss = 2.29968463\n",
      "Iteration 27, loss = 2.29960596\n",
      "Iteration 28, loss = 2.29922915\n",
      "Iteration 29, loss = 2.29899005\n",
      "Iteration 30, loss = 2.29883929\n",
      "Iteration 31, loss = 2.29860464\n",
      "Iteration 32, loss = 2.29827538\n",
      "Iteration 33, loss = 2.29813426\n",
      "Iteration 34, loss = 2.29788615\n",
      "Iteration 35, loss = 2.29740636\n",
      "Iteration 36, loss = 2.29713653\n",
      "Iteration 37, loss = 2.29667501\n",
      "Iteration 38, loss = 2.29640093\n",
      "Iteration 39, loss = 2.29606962\n",
      "Iteration 40, loss = 2.29546935\n",
      "Iteration 41, loss = 2.29480861\n",
      "Iteration 42, loss = 2.29436939\n",
      "Iteration 43, loss = 2.29366237\n",
      "Iteration 44, loss = 2.29288700\n",
      "Iteration 45, loss = 2.29212245\n",
      "Iteration 46, loss = 2.29119530\n",
      "Iteration 47, loss = 2.28997628\n",
      "Iteration 48, loss = 2.28876493\n",
      "Iteration 49, loss = 2.28738445\n",
      "Iteration 50, loss = 2.28558872\n",
      "Iteration 51, loss = 2.28369738\n",
      "Iteration 52, loss = 2.28154441\n",
      "Iteration 53, loss = 2.27862164\n",
      "Iteration 54, loss = 2.27532218\n",
      "Iteration 55, loss = 2.27140149\n",
      "Iteration 56, loss = 2.26609610\n",
      "Iteration 57, loss = 2.25962462\n",
      "Iteration 58, loss = 2.25116889\n",
      "Iteration 59, loss = 2.24028555\n",
      "Iteration 60, loss = 2.22584683\n",
      "Iteration 61, loss = 2.20626068\n",
      "Iteration 62, loss = 2.17970001\n",
      "Iteration 63, loss = 2.14402265\n",
      "Iteration 64, loss = 2.09774574\n",
      "Iteration 65, loss = 2.04154376\n",
      "Iteration 66, loss = 1.98003339\n",
      "Iteration 67, loss = 1.91910668\n",
      "Iteration 68, loss = 1.86447580\n",
      "Iteration 69, loss = 1.81751745\n",
      "Iteration 70, loss = 1.77831153\n",
      "Iteration 71, loss = 1.74475380\n",
      "Iteration 72, loss = 1.71527915\n",
      "Iteration 73, loss = 1.68830727\n",
      "Iteration 74, loss = 1.66260287\n",
      "Iteration 75, loss = 1.63745708\n",
      "Iteration 76, loss = 1.61161886\n",
      "Iteration 77, loss = 1.58485886\n",
      "Iteration 78, loss = 1.55685820\n",
      "Iteration 79, loss = 1.52781364\n",
      "Iteration 80, loss = 1.49786565\n",
      "Iteration 81, loss = 1.46741393\n",
      "Iteration 82, loss = 1.43698207\n",
      "Iteration 83, loss = 1.40708577\n",
      "Iteration 84, loss = 1.37766355\n",
      "Iteration 85, loss = 1.34940860\n",
      "Iteration 86, loss = 1.32227378\n",
      "Iteration 87, loss = 1.29614650\n",
      "Iteration 88, loss = 1.27142851\n",
      "Iteration 89, loss = 1.24812665\n",
      "Iteration 90, loss = 1.22620269\n",
      "Iteration 91, loss = 1.20542942\n",
      "Iteration 92, loss = 1.18576216\n",
      "Iteration 93, loss = 1.16713416\n",
      "Iteration 94, loss = 1.14925309\n",
      "Iteration 95, loss = 1.13207098\n",
      "Iteration 96, loss = 1.11561108\n",
      "Iteration 97, loss = 1.09972942\n",
      "Iteration 98, loss = 1.08414099\n",
      "Iteration 99, loss = 1.06924258\n",
      "Iteration 100, loss = 1.05483659\n",
      "Iteration 101, loss = 1.04030309\n",
      "Iteration 102, loss = 1.02639784\n",
      "Iteration 103, loss = 1.01247016\n",
      "Iteration 104, loss = 0.99861807\n",
      "Iteration 105, loss = 0.98485956\n",
      "Iteration 106, loss = 0.97095469\n",
      "Iteration 107, loss = 0.95689771\n",
      "Iteration 108, loss = 0.94255006\n",
      "Iteration 109, loss = 0.92783030\n",
      "Iteration 110, loss = 0.91253801\n",
      "Iteration 111, loss = 0.89685512\n",
      "Iteration 112, loss = 0.88050226\n",
      "Iteration 113, loss = 0.86361079\n",
      "Iteration 114, loss = 0.84606860\n",
      "Iteration 115, loss = 0.82818545\n",
      "Iteration 116, loss = 0.81011661\n",
      "Iteration 117, loss = 0.79193806\n",
      "Iteration 118, loss = 0.77445371\n",
      "Iteration 119, loss = 0.75758994\n",
      "Iteration 120, loss = 0.74161080\n",
      "Iteration 121, loss = 0.72640309\n",
      "Iteration 122, loss = 0.71257443\n",
      "Iteration 123, loss = 0.69962900\n",
      "Iteration 124, loss = 0.68782502\n",
      "Iteration 125, loss = 0.67676766\n",
      "Iteration 126, loss = 0.66661050\n",
      "Iteration 127, loss = 0.65709644\n",
      "Iteration 128, loss = 0.64827354\n",
      "Iteration 129, loss = 0.63990222\n",
      "Iteration 130, loss = 0.63216362\n",
      "Iteration 131, loss = 0.62468278\n",
      "Iteration 132, loss = 0.61781306\n",
      "Iteration 133, loss = 0.61114390\n",
      "Iteration 134, loss = 0.60475306\n",
      "Iteration 135, loss = 0.59878753\n",
      "Iteration 136, loss = 0.59294206\n",
      "Iteration 137, loss = 0.58715793\n",
      "Iteration 138, loss = 0.58176321\n",
      "Iteration 139, loss = 0.57663704\n",
      "Iteration 140, loss = 0.57151313\n",
      "Iteration 141, loss = 0.56660231\n",
      "Iteration 142, loss = 0.56171037\n",
      "Iteration 143, loss = 0.55732921\n",
      "Iteration 144, loss = 0.55269470\n",
      "Iteration 145, loss = 0.54824458\n",
      "Iteration 146, loss = 0.54410574\n",
      "Iteration 147, loss = 0.53997129\n",
      "Iteration 148, loss = 0.53580635\n",
      "Iteration 149, loss = 0.53194807\n",
      "Iteration 150, loss = 0.52805094\n",
      "Iteration 151, loss = 0.52430631\n",
      "Iteration 152, loss = 0.52052901\n",
      "Iteration 153, loss = 0.51692940\n",
      "Iteration 154, loss = 0.51340782\n",
      "Iteration 155, loss = 0.51000701\n",
      "Iteration 156, loss = 0.50663487\n",
      "Iteration 157, loss = 0.50322156\n",
      "Iteration 158, loss = 0.49992805\n",
      "Iteration 159, loss = 0.49683872\n",
      "Iteration 160, loss = 0.49359044\n",
      "Iteration 161, loss = 0.49053902\n",
      "Iteration 162, loss = 0.48740129\n",
      "Iteration 163, loss = 0.48454935\n",
      "Iteration 164, loss = 0.48148621\n",
      "Iteration 165, loss = 0.47851649\n",
      "Iteration 166, loss = 0.47575005\n",
      "Iteration 167, loss = 0.47267647\n",
      "Iteration 168, loss = 0.46990921\n",
      "Iteration 169, loss = 0.46743176\n",
      "Iteration 170, loss = 0.46438229\n",
      "Iteration 171, loss = 0.46179833\n",
      "Iteration 172, loss = 0.45911426\n",
      "Iteration 173, loss = 0.45653099\n",
      "Iteration 174, loss = 0.45389214\n",
      "Iteration 175, loss = 0.45125423\n",
      "Iteration 176, loss = 0.44868138\n",
      "Iteration 177, loss = 0.44606164\n",
      "Iteration 178, loss = 0.44372601\n",
      "Iteration 179, loss = 0.44119994\n",
      "Iteration 180, loss = 0.43877596\n",
      "Iteration 181, loss = 0.43629963\n",
      "Iteration 182, loss = 0.43406795\n",
      "Iteration 183, loss = 0.43156722\n",
      "Iteration 184, loss = 0.42916357\n",
      "Iteration 185, loss = 0.42688950\n",
      "Iteration 186, loss = 0.42442385\n",
      "Iteration 187, loss = 0.42220818\n",
      "Iteration 188, loss = 0.41994580\n",
      "Iteration 189, loss = 0.41758114\n",
      "Iteration 190, loss = 0.41537339\n",
      "Iteration 191, loss = 0.41301631\n",
      "Iteration 192, loss = 0.41082856\n",
      "Iteration 193, loss = 0.40878426\n",
      "Iteration 194, loss = 0.40649566\n",
      "Iteration 195, loss = 0.40422999\n",
      "Iteration 196, loss = 0.40206258\n",
      "Iteration 197, loss = 0.39984843\n",
      "Iteration 198, loss = 0.39781367\n",
      "Iteration 199, loss = 0.39568359\n",
      "Iteration 200, loss = 0.39353404\n",
      "[CV] END activation=logistic, alpha=0.001, batch_size=100, hidden_layer_sizes=(100, 100, 100), solver=sgd; total time= 5.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.13300647\n",
      "Iteration 2, loss = 0.44622310\n",
      "Iteration 3, loss = 0.33023898\n",
      "Iteration 4, loss = 0.28066525\n",
      "Iteration 5, loss = 0.24869856\n",
      "Iteration 6, loss = 0.22487570\n",
      "Iteration 7, loss = 0.20581441\n",
      "Iteration 8, loss = 0.18975616\n",
      "Iteration 9, loss = 0.17683221\n",
      "Iteration 10, loss = 0.16527866\n",
      "Iteration 11, loss = 0.15459677\n",
      "Iteration 12, loss = 0.14506439\n",
      "Iteration 13, loss = 0.13715169\n",
      "Iteration 14, loss = 0.12937324\n",
      "Iteration 15, loss = 0.12237154\n",
      "Iteration 16, loss = 0.11575153\n",
      "Iteration 17, loss = 0.10991912\n",
      "Iteration 18, loss = 0.10416218\n",
      "Iteration 19, loss = 0.09881593\n",
      "Iteration 20, loss = 0.09423751\n",
      "Iteration 21, loss = 0.08942803\n",
      "Iteration 22, loss = 0.08522296\n",
      "Iteration 23, loss = 0.08130613\n",
      "Iteration 24, loss = 0.07712207\n",
      "Iteration 25, loss = 0.07362439\n",
      "Iteration 26, loss = 0.07026867\n",
      "Iteration 27, loss = 0.06716847\n",
      "Iteration 28, loss = 0.06407330\n",
      "Iteration 29, loss = 0.06154146\n",
      "Iteration 30, loss = 0.05861195\n",
      "Iteration 31, loss = 0.05590709\n",
      "Iteration 32, loss = 0.05379150\n",
      "Iteration 33, loss = 0.05136879\n",
      "Iteration 34, loss = 0.04928624\n",
      "Iteration 35, loss = 0.04694626\n",
      "Iteration 36, loss = 0.04520207\n",
      "Iteration 37, loss = 0.04332546\n",
      "Iteration 38, loss = 0.04187313\n",
      "Iteration 39, loss = 0.03997502\n",
      "Iteration 40, loss = 0.03854095\n",
      "Iteration 41, loss = 0.03715708\n",
      "Iteration 42, loss = 0.03537639\n",
      "Iteration 43, loss = 0.03415113\n",
      "Iteration 44, loss = 0.03277099\n",
      "Iteration 45, loss = 0.03156731\n",
      "Iteration 46, loss = 0.03037636\n",
      "Iteration 47, loss = 0.02940150\n",
      "Iteration 48, loss = 0.02840731\n",
      "Iteration 49, loss = 0.02748838\n",
      "Iteration 50, loss = 0.02652210\n",
      "Iteration 51, loss = 0.02572311\n",
      "Iteration 52, loss = 0.02485267\n",
      "Iteration 53, loss = 0.02408840\n",
      "Iteration 54, loss = 0.02363909\n",
      "Iteration 55, loss = 0.02275912\n",
      "Iteration 56, loss = 0.02216289\n",
      "Iteration 57, loss = 0.02164590\n",
      "Iteration 58, loss = 0.02089608\n",
      "Iteration 59, loss = 0.02053147\n",
      "Iteration 60, loss = 0.01998578\n",
      "Iteration 61, loss = 0.01948189\n",
      "Iteration 62, loss = 0.01903151\n",
      "Iteration 63, loss = 0.01855554\n",
      "Iteration 64, loss = 0.01824176\n",
      "Iteration 65, loss = 0.01781946\n",
      "Iteration 66, loss = 0.01746454\n",
      "Iteration 67, loss = 0.01722698\n",
      "Iteration 68, loss = 0.01680231\n",
      "Iteration 69, loss = 0.01649185\n",
      "Iteration 70, loss = 0.01633336\n",
      "Iteration 71, loss = 0.01598630\n",
      "Iteration 72, loss = 0.01573254\n",
      "Iteration 73, loss = 0.01542534\n",
      "Iteration 74, loss = 0.01522311\n",
      "Iteration 75, loss = 0.01506974\n",
      "Iteration 76, loss = 0.01482561\n",
      "Iteration 77, loss = 0.01461241\n",
      "Iteration 78, loss = 0.01444277\n",
      "Iteration 79, loss = 0.01427679\n",
      "Iteration 80, loss = 0.01407371\n",
      "Iteration 81, loss = 0.01392033\n",
      "Iteration 82, loss = 0.01377728\n",
      "Iteration 83, loss = 0.01368573\n",
      "Iteration 84, loss = 0.01342880\n",
      "Iteration 85, loss = 0.01332554\n",
      "Iteration 86, loss = 0.01315534\n",
      "Iteration 87, loss = 0.01304075\n",
      "Iteration 88, loss = 0.01299347\n",
      "Iteration 89, loss = 0.01285644\n",
      "Iteration 90, loss = 0.01273909\n",
      "Iteration 91, loss = 0.01256512\n",
      "Iteration 92, loss = 0.01251461\n",
      "Iteration 93, loss = 0.01245088\n",
      "Iteration 94, loss = 0.01233797\n",
      "Iteration 95, loss = 0.01221941\n",
      "Iteration 96, loss = 0.01217322\n",
      "Iteration 97, loss = 0.01201216\n",
      "Iteration 98, loss = 0.01200031\n",
      "Iteration 99, loss = 0.01183631\n",
      "Iteration 100, loss = 0.01179263\n",
      "Iteration 101, loss = 0.01170912\n",
      "Iteration 102, loss = 0.01168366\n",
      "Iteration 103, loss = 0.01167697\n",
      "Iteration 104, loss = 0.01157293\n",
      "Iteration 105, loss = 0.01152171\n",
      "Iteration 106, loss = 0.01145725\n",
      "Iteration 107, loss = 0.01145982\n",
      "Iteration 108, loss = 0.01133342\n",
      "Iteration 109, loss = 0.01128634\n",
      "Iteration 110, loss = 0.01118603\n",
      "Iteration 111, loss = 0.01116471\n",
      "Iteration 112, loss = 0.01116071\n",
      "Iteration 113, loss = 0.01107585\n",
      "Iteration 114, loss = 0.01103108\n",
      "Iteration 115, loss = 0.01096319\n",
      "Iteration 116, loss = 0.01096967\n",
      "Iteration 117, loss = 0.01096878\n",
      "Iteration 118, loss = 0.01083755\n",
      "Iteration 119, loss = 0.01088079\n",
      "Iteration 120, loss = 0.01088395\n",
      "Iteration 121, loss = 0.01082102\n",
      "Iteration 122, loss = 0.01077465\n",
      "Iteration 123, loss = 0.01063844\n",
      "Iteration 124, loss = 0.01063459\n",
      "Iteration 125, loss = 0.01066930\n",
      "Iteration 126, loss = 0.01061510\n",
      "Iteration 127, loss = 0.01079646\n",
      "Iteration 128, loss = 0.01058451\n",
      "Iteration 129, loss = 0.01053929\n",
      "Iteration 130, loss = 0.01046658\n",
      "Iteration 131, loss = 0.01045659\n",
      "Iteration 132, loss = 0.01045998\n",
      "Iteration 133, loss = 0.01052210\n",
      "Iteration 134, loss = 0.01050936\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.001, batch_size=200, hidden_layer_sizes=(100,), solver=adam; total time= 1.4min\n",
      "Iteration 1, loss = 1.09444670\n",
      "Iteration 2, loss = 0.43891736\n",
      "Iteration 3, loss = 0.32908778\n",
      "Iteration 4, loss = 0.28157254\n",
      "Iteration 5, loss = 0.25133385\n",
      "Iteration 6, loss = 0.22834297\n",
      "Iteration 7, loss = 0.20921389\n",
      "Iteration 8, loss = 0.19324232\n",
      "Iteration 9, loss = 0.17936348\n",
      "Iteration 10, loss = 0.16761284\n",
      "Iteration 11, loss = 0.15718292\n",
      "Iteration 12, loss = 0.14708335\n",
      "Iteration 13, loss = 0.13837955\n",
      "Iteration 14, loss = 0.13028525\n",
      "Iteration 15, loss = 0.12337009\n",
      "Iteration 16, loss = 0.11699630\n",
      "Iteration 17, loss = 0.11042246\n",
      "Iteration 18, loss = 0.10424743\n",
      "Iteration 19, loss = 0.09878048\n",
      "Iteration 20, loss = 0.09366135\n",
      "Iteration 21, loss = 0.08922683\n",
      "Iteration 22, loss = 0.08493128\n",
      "Iteration 23, loss = 0.08058452\n",
      "Iteration 24, loss = 0.07667878\n",
      "Iteration 25, loss = 0.07313069\n",
      "Iteration 26, loss = 0.06958372\n",
      "Iteration 27, loss = 0.06639715\n",
      "Iteration 28, loss = 0.06327145\n",
      "Iteration 29, loss = 0.06036625\n",
      "Iteration 30, loss = 0.05789064\n",
      "Iteration 31, loss = 0.05498055\n",
      "Iteration 32, loss = 0.05275970\n",
      "Iteration 33, loss = 0.05052645\n",
      "Iteration 34, loss = 0.04838176\n",
      "Iteration 35, loss = 0.04626151\n",
      "Iteration 36, loss = 0.04430761\n",
      "Iteration 37, loss = 0.04240278\n",
      "Iteration 38, loss = 0.04063501\n",
      "Iteration 39, loss = 0.03906179\n",
      "Iteration 40, loss = 0.03743405\n",
      "Iteration 41, loss = 0.03613213\n",
      "Iteration 42, loss = 0.03450886\n",
      "Iteration 43, loss = 0.03328706\n",
      "Iteration 44, loss = 0.03201337\n",
      "Iteration 45, loss = 0.03103699\n",
      "Iteration 46, loss = 0.02984453\n",
      "Iteration 47, loss = 0.02878499\n",
      "Iteration 48, loss = 0.02790888\n",
      "Iteration 49, loss = 0.02687656\n",
      "Iteration 50, loss = 0.02613813\n",
      "Iteration 51, loss = 0.02521794\n",
      "Iteration 52, loss = 0.02456931\n",
      "Iteration 53, loss = 0.02350966\n",
      "Iteration 54, loss = 0.02303115\n",
      "Iteration 55, loss = 0.02238886\n",
      "Iteration 56, loss = 0.02169602\n",
      "Iteration 57, loss = 0.02130751\n",
      "Iteration 58, loss = 0.02064432\n",
      "Iteration 59, loss = 0.02007971\n",
      "Iteration 60, loss = 0.01967853\n",
      "Iteration 61, loss = 0.01912960\n",
      "Iteration 62, loss = 0.01884118\n",
      "Iteration 63, loss = 0.01835778\n",
      "Iteration 64, loss = 0.01798612\n",
      "Iteration 65, loss = 0.01760271\n",
      "Iteration 66, loss = 0.01729990\n",
      "Iteration 67, loss = 0.01688012\n",
      "Iteration 68, loss = 0.01657576\n",
      "Iteration 69, loss = 0.01628071\n",
      "Iteration 70, loss = 0.01602844\n",
      "Iteration 71, loss = 0.01579625\n",
      "Iteration 72, loss = 0.01566328\n",
      "Iteration 73, loss = 0.01525993\n",
      "Iteration 74, loss = 0.01501803\n",
      "Iteration 75, loss = 0.01483967\n",
      "Iteration 76, loss = 0.01467743\n",
      "Iteration 77, loss = 0.01446763\n",
      "Iteration 78, loss = 0.01425040\n",
      "Iteration 79, loss = 0.01409012\n",
      "Iteration 80, loss = 0.01388442\n",
      "Iteration 81, loss = 0.01374788\n",
      "Iteration 82, loss = 0.01359547\n",
      "Iteration 83, loss = 0.01349055\n",
      "Iteration 84, loss = 0.01333642\n",
      "Iteration 85, loss = 0.01309681\n",
      "Iteration 86, loss = 0.01303183\n",
      "Iteration 87, loss = 0.01289103\n",
      "Iteration 88, loss = 0.01280917\n",
      "Iteration 89, loss = 0.01268076\n",
      "Iteration 90, loss = 0.01255674\n",
      "Iteration 91, loss = 0.01245901\n",
      "Iteration 92, loss = 0.01246272\n",
      "Iteration 93, loss = 0.01223320\n",
      "Iteration 94, loss = 0.01221049\n",
      "Iteration 95, loss = 0.01209371\n",
      "Iteration 96, loss = 0.01199406\n",
      "Iteration 97, loss = 0.01193123\n",
      "Iteration 98, loss = 0.01190531\n",
      "Iteration 99, loss = 0.01179007\n",
      "Iteration 100, loss = 0.01166768\n",
      "Iteration 101, loss = 0.01165644\n",
      "Iteration 102, loss = 0.01153004\n",
      "Iteration 103, loss = 0.01151748\n",
      "Iteration 104, loss = 0.01145921\n",
      "Iteration 105, loss = 0.01133358\n",
      "Iteration 106, loss = 0.01133373\n",
      "Iteration 107, loss = 0.01130562\n",
      "Iteration 108, loss = 0.01132081\n",
      "Iteration 109, loss = 0.01115864\n",
      "Iteration 110, loss = 0.01111171\n",
      "Iteration 111, loss = 0.01106502\n",
      "Iteration 112, loss = 0.01099952\n",
      "Iteration 113, loss = 0.01098045\n",
      "Iteration 114, loss = 0.01100621\n",
      "Iteration 115, loss = 0.01093118\n",
      "Iteration 116, loss = 0.01089156\n",
      "Iteration 117, loss = 0.01079960\n",
      "Iteration 118, loss = 0.01075905\n",
      "Iteration 119, loss = 0.01074120\n",
      "Iteration 120, loss = 0.01073421\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.001, batch_size=200, hidden_layer_sizes=(100,), solver=adam; total time= 1.3min\n",
      "Iteration 1, loss = 1.15476180\n",
      "Iteration 2, loss = 0.45406458\n",
      "Iteration 3, loss = 0.33177954\n",
      "Iteration 4, loss = 0.28069805\n",
      "Iteration 5, loss = 0.24850259\n",
      "Iteration 6, loss = 0.22495069\n",
      "Iteration 7, loss = 0.20677737\n",
      "Iteration 8, loss = 0.19100420\n",
      "Iteration 9, loss = 0.17758841\n",
      "Iteration 10, loss = 0.16602039\n",
      "Iteration 11, loss = 0.15540545\n",
      "Iteration 12, loss = 0.14603393\n",
      "Iteration 13, loss = 0.13765472\n",
      "Iteration 14, loss = 0.13036100\n",
      "Iteration 15, loss = 0.12272568\n",
      "Iteration 16, loss = 0.11647885\n",
      "Iteration 17, loss = 0.11014528\n",
      "Iteration 18, loss = 0.10488752\n",
      "Iteration 19, loss = 0.09949063\n",
      "Iteration 20, loss = 0.09499297\n",
      "Iteration 21, loss = 0.09013399\n",
      "Iteration 22, loss = 0.08556087\n",
      "Iteration 23, loss = 0.08110344\n",
      "Iteration 24, loss = 0.07771205\n",
      "Iteration 25, loss = 0.07393180\n",
      "Iteration 26, loss = 0.07069783\n",
      "Iteration 27, loss = 0.06733273\n",
      "Iteration 28, loss = 0.06437255\n",
      "Iteration 29, loss = 0.06153474\n",
      "Iteration 30, loss = 0.05899050\n",
      "Iteration 31, loss = 0.05619762\n",
      "Iteration 32, loss = 0.05366428\n",
      "Iteration 33, loss = 0.05147668\n",
      "Iteration 34, loss = 0.04918125\n",
      "Iteration 35, loss = 0.04726229\n",
      "Iteration 36, loss = 0.04521687\n",
      "Iteration 37, loss = 0.04345342\n",
      "Iteration 38, loss = 0.04160154\n",
      "Iteration 39, loss = 0.04013320\n",
      "Iteration 40, loss = 0.03833031\n",
      "Iteration 41, loss = 0.03676257\n",
      "Iteration 42, loss = 0.03543491\n",
      "Iteration 43, loss = 0.03432541\n",
      "Iteration 44, loss = 0.03290016\n",
      "Iteration 45, loss = 0.03157140\n",
      "Iteration 46, loss = 0.03048716\n",
      "Iteration 47, loss = 0.02949560\n",
      "Iteration 48, loss = 0.02848834\n",
      "Iteration 49, loss = 0.02752935\n",
      "Iteration 50, loss = 0.02665618\n",
      "Iteration 51, loss = 0.02564826\n",
      "Iteration 52, loss = 0.02492377\n",
      "Iteration 53, loss = 0.02427087\n",
      "Iteration 54, loss = 0.02356525\n",
      "Iteration 55, loss = 0.02270693\n",
      "Iteration 56, loss = 0.02214341\n",
      "Iteration 57, loss = 0.02153221\n",
      "Iteration 58, loss = 0.02107583\n",
      "Iteration 59, loss = 0.02043734\n",
      "Iteration 60, loss = 0.02014180\n",
      "Iteration 61, loss = 0.01955673\n",
      "Iteration 62, loss = 0.01906464\n",
      "Iteration 63, loss = 0.01866299\n",
      "Iteration 64, loss = 0.01835163\n",
      "Iteration 65, loss = 0.01790821\n",
      "Iteration 66, loss = 0.01747698\n",
      "Iteration 67, loss = 0.01707849\n",
      "Iteration 68, loss = 0.01689429\n",
      "Iteration 69, loss = 0.01660967\n",
      "Iteration 70, loss = 0.01627631\n",
      "Iteration 71, loss = 0.01597667\n",
      "Iteration 72, loss = 0.01567575\n",
      "Iteration 73, loss = 0.01544221\n",
      "Iteration 74, loss = 0.01518711\n",
      "Iteration 75, loss = 0.01503957\n",
      "Iteration 76, loss = 0.01481149\n",
      "Iteration 77, loss = 0.01458411\n",
      "Iteration 78, loss = 0.01436831\n",
      "Iteration 79, loss = 0.01417627\n",
      "Iteration 80, loss = 0.01396767\n",
      "Iteration 81, loss = 0.01385203\n",
      "Iteration 82, loss = 0.01371878\n",
      "Iteration 83, loss = 0.01354737\n",
      "Iteration 84, loss = 0.01337959\n",
      "Iteration 85, loss = 0.01323609\n",
      "Iteration 86, loss = 0.01306976\n",
      "Iteration 87, loss = 0.01298239\n",
      "Iteration 88, loss = 0.01286342\n",
      "Iteration 89, loss = 0.01280978\n",
      "Iteration 90, loss = 0.01263347\n",
      "Iteration 91, loss = 0.01251049\n",
      "Iteration 92, loss = 0.01237316\n",
      "Iteration 93, loss = 0.01242824\n",
      "Iteration 94, loss = 0.01234591\n",
      "Iteration 95, loss = 0.01217639\n",
      "Iteration 96, loss = 0.01204600\n",
      "Iteration 97, loss = 0.01201476\n",
      "Iteration 98, loss = 0.01187883\n",
      "Iteration 99, loss = 0.01180946\n",
      "Iteration 100, loss = 0.01191448\n",
      "Iteration 101, loss = 0.01168290\n",
      "Iteration 102, loss = 0.01159931\n",
      "Iteration 103, loss = 0.01154101\n",
      "Iteration 104, loss = 0.01142808\n",
      "Iteration 105, loss = 0.01140443\n",
      "Iteration 106, loss = 0.01134044\n",
      "Iteration 107, loss = 0.01126667\n",
      "Iteration 108, loss = 0.01139986\n",
      "Iteration 109, loss = 0.01117870\n",
      "Iteration 110, loss = 0.01112666\n",
      "Iteration 111, loss = 0.01109745\n",
      "Iteration 112, loss = 0.01112723\n",
      "Iteration 113, loss = 0.01100790\n",
      "Iteration 114, loss = 0.01090858\n",
      "Iteration 115, loss = 0.01093044\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.001, batch_size=200, hidden_layer_sizes=(100,), solver=adam; total time= 1.2min\n",
      "Iteration 1, loss = 1.14458418\n",
      "Iteration 2, loss = 0.44932694\n",
      "Iteration 3, loss = 0.33327401\n",
      "Iteration 4, loss = 0.28402954\n",
      "Iteration 5, loss = 0.25293601\n",
      "Iteration 6, loss = 0.23016250\n",
      "Iteration 7, loss = 0.21138605\n",
      "Iteration 8, loss = 0.19546251\n",
      "Iteration 9, loss = 0.18166217\n",
      "Iteration 10, loss = 0.16963604\n",
      "Iteration 11, loss = 0.15923240\n",
      "Iteration 12, loss = 0.14962799\n",
      "Iteration 13, loss = 0.14121358\n",
      "Iteration 14, loss = 0.13346971\n",
      "Iteration 15, loss = 0.12610415\n",
      "Iteration 16, loss = 0.11979179\n",
      "Iteration 17, loss = 0.11332907\n",
      "Iteration 18, loss = 0.10783330\n",
      "Iteration 19, loss = 0.10260201\n",
      "Iteration 20, loss = 0.09764288\n",
      "Iteration 21, loss = 0.09310366\n",
      "Iteration 22, loss = 0.08879176\n",
      "Iteration 23, loss = 0.08471817\n",
      "Iteration 24, loss = 0.08078962\n",
      "Iteration 25, loss = 0.07742115\n",
      "Iteration 26, loss = 0.07397958\n",
      "Iteration 27, loss = 0.07111154\n",
      "Iteration 28, loss = 0.06767304\n",
      "Iteration 29, loss = 0.06505160\n",
      "Iteration 30, loss = 0.06228250\n",
      "Iteration 31, loss = 0.05951429\n",
      "Iteration 32, loss = 0.05708116\n",
      "Iteration 33, loss = 0.05485284\n",
      "Iteration 34, loss = 0.05264049\n",
      "Iteration 35, loss = 0.05049899\n",
      "Iteration 36, loss = 0.04856340\n",
      "Iteration 37, loss = 0.04677677\n",
      "Iteration 38, loss = 0.04480398\n",
      "Iteration 39, loss = 0.04331500\n",
      "Iteration 40, loss = 0.04170591\n",
      "Iteration 41, loss = 0.04014722\n",
      "Iteration 42, loss = 0.03874104\n",
      "Iteration 43, loss = 0.03712545\n",
      "Iteration 44, loss = 0.03591294\n",
      "Iteration 45, loss = 0.03455721\n",
      "Iteration 46, loss = 0.03347627\n",
      "Iteration 47, loss = 0.03229117\n",
      "Iteration 48, loss = 0.03124825\n",
      "Iteration 49, loss = 0.03028385\n",
      "Iteration 50, loss = 0.02924707\n",
      "Iteration 51, loss = 0.02826456\n",
      "Iteration 52, loss = 0.02733672\n",
      "Iteration 53, loss = 0.02662889\n",
      "Iteration 54, loss = 0.02583445\n",
      "Iteration 55, loss = 0.02518336\n",
      "Iteration 56, loss = 0.02429599\n",
      "Iteration 57, loss = 0.02366995\n",
      "Iteration 58, loss = 0.02287701\n",
      "Iteration 59, loss = 0.02234537\n",
      "Iteration 60, loss = 0.02177054\n",
      "Iteration 61, loss = 0.02118976\n",
      "Iteration 62, loss = 0.02072024\n",
      "Iteration 63, loss = 0.02013580\n",
      "Iteration 64, loss = 0.01971901\n",
      "Iteration 65, loss = 0.01939834\n",
      "Iteration 66, loss = 0.01884575\n",
      "Iteration 67, loss = 0.01854591\n",
      "Iteration 68, loss = 0.01813874\n",
      "Iteration 69, loss = 0.01780563\n",
      "Iteration 70, loss = 0.01749353\n",
      "Iteration 71, loss = 0.01709473\n",
      "Iteration 72, loss = 0.01682104\n",
      "Iteration 73, loss = 0.01650642\n",
      "Iteration 74, loss = 0.01623472\n",
      "Iteration 75, loss = 0.01608349\n",
      "Iteration 76, loss = 0.01577316\n",
      "Iteration 77, loss = 0.01559486\n",
      "Iteration 78, loss = 0.01530528\n",
      "Iteration 79, loss = 0.01510166\n",
      "Iteration 80, loss = 0.01491224\n",
      "Iteration 81, loss = 0.01472760\n",
      "Iteration 82, loss = 0.01452623\n",
      "Iteration 83, loss = 0.01426809\n",
      "Iteration 84, loss = 0.01419203\n",
      "Iteration 85, loss = 0.01402315\n",
      "Iteration 86, loss = 0.01394948\n",
      "Iteration 87, loss = 0.01368312\n",
      "Iteration 88, loss = 0.01355235\n",
      "Iteration 89, loss = 0.01343185\n",
      "Iteration 90, loss = 0.01329419\n",
      "Iteration 91, loss = 0.01323168\n",
      "Iteration 92, loss = 0.01310728\n",
      "Iteration 93, loss = 0.01302015\n",
      "Iteration 94, loss = 0.01286321\n",
      "Iteration 95, loss = 0.01273063\n",
      "Iteration 96, loss = 0.01257376\n",
      "Iteration 97, loss = 0.01249502\n",
      "Iteration 98, loss = 0.01241868\n",
      "Iteration 99, loss = 0.01233543\n",
      "Iteration 100, loss = 0.01230245\n",
      "Iteration 101, loss = 0.01222719\n",
      "Iteration 102, loss = 0.01213112\n",
      "Iteration 103, loss = 0.01201551\n",
      "Iteration 104, loss = 0.01192078\n",
      "Iteration 105, loss = 0.01181615\n",
      "Iteration 106, loss = 0.01179454\n",
      "Iteration 107, loss = 0.01171605\n",
      "Iteration 108, loss = 0.01167307\n",
      "Iteration 109, loss = 0.01166207\n",
      "Iteration 110, loss = 0.01158146\n",
      "Iteration 111, loss = 0.01148965\n",
      "Iteration 112, loss = 0.01140920\n",
      "Iteration 113, loss = 0.01137697\n",
      "Iteration 114, loss = 0.01130740\n",
      "Iteration 115, loss = 0.01126943\n",
      "Iteration 116, loss = 0.01126597\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.001, batch_size=200, hidden_layer_sizes=(100,), solver=adam; total time= 1.2min\n",
      "Iteration 1, loss = 1.16710907\n",
      "Iteration 2, loss = 0.46162413\n",
      "Iteration 3, loss = 0.33937561\n",
      "Iteration 4, loss = 0.28850241\n",
      "Iteration 5, loss = 0.25574806\n",
      "Iteration 6, loss = 0.23174629\n",
      "Iteration 7, loss = 0.21303592\n",
      "Iteration 8, loss = 0.19735009\n",
      "Iteration 9, loss = 0.18336063\n",
      "Iteration 10, loss = 0.17167467\n",
      "Iteration 11, loss = 0.16102986\n",
      "Iteration 12, loss = 0.15164181\n",
      "Iteration 13, loss = 0.14241241\n",
      "Iteration 14, loss = 0.13500283\n",
      "Iteration 15, loss = 0.12771623\n",
      "Iteration 16, loss = 0.12045781\n",
      "Iteration 17, loss = 0.11455862\n",
      "Iteration 18, loss = 0.10867849\n",
      "Iteration 19, loss = 0.10310465\n",
      "Iteration 20, loss = 0.09815106\n",
      "Iteration 21, loss = 0.09357914\n",
      "Iteration 22, loss = 0.08908513\n",
      "Iteration 23, loss = 0.08479957\n",
      "Iteration 24, loss = 0.08086900\n",
      "Iteration 25, loss = 0.07690469\n",
      "Iteration 26, loss = 0.07333033\n",
      "Iteration 27, loss = 0.06995576\n",
      "Iteration 28, loss = 0.06701184\n",
      "Iteration 29, loss = 0.06407947\n",
      "Iteration 30, loss = 0.06137002\n",
      "Iteration 31, loss = 0.05856394\n",
      "Iteration 32, loss = 0.05612843\n",
      "Iteration 33, loss = 0.05372159\n",
      "Iteration 34, loss = 0.05139454\n",
      "Iteration 35, loss = 0.04904509\n",
      "Iteration 36, loss = 0.04704645\n",
      "Iteration 37, loss = 0.04540059\n",
      "Iteration 38, loss = 0.04333617\n",
      "Iteration 39, loss = 0.04175782\n",
      "Iteration 40, loss = 0.04005053\n",
      "Iteration 41, loss = 0.03842820\n",
      "Iteration 42, loss = 0.03701294\n",
      "Iteration 43, loss = 0.03564558\n",
      "Iteration 44, loss = 0.03435365\n",
      "Iteration 45, loss = 0.03311470\n",
      "Iteration 46, loss = 0.03190296\n",
      "Iteration 47, loss = 0.03082029\n",
      "Iteration 48, loss = 0.02978096\n",
      "Iteration 49, loss = 0.02898556\n",
      "Iteration 50, loss = 0.02772316\n",
      "Iteration 51, loss = 0.02693257\n",
      "Iteration 52, loss = 0.02608629\n",
      "Iteration 53, loss = 0.02523984\n",
      "Iteration 54, loss = 0.02464185\n",
      "Iteration 55, loss = 0.02378100\n",
      "Iteration 56, loss = 0.02306751\n",
      "Iteration 57, loss = 0.02258486\n",
      "Iteration 58, loss = 0.02190338\n",
      "Iteration 59, loss = 0.02130196\n",
      "Iteration 60, loss = 0.02081039\n",
      "Iteration 61, loss = 0.02015705\n",
      "Iteration 62, loss = 0.01973577\n",
      "Iteration 63, loss = 0.01928633\n",
      "Iteration 64, loss = 0.01888408\n",
      "Iteration 65, loss = 0.01852156\n",
      "Iteration 66, loss = 0.01810950\n",
      "Iteration 67, loss = 0.01778017\n",
      "Iteration 68, loss = 0.01736248\n",
      "Iteration 69, loss = 0.01699147\n",
      "Iteration 70, loss = 0.01682359\n",
      "Iteration 71, loss = 0.01653927\n",
      "Iteration 72, loss = 0.01621155\n",
      "Iteration 73, loss = 0.01598346\n",
      "Iteration 74, loss = 0.01575528\n",
      "Iteration 75, loss = 0.01550091\n",
      "Iteration 76, loss = 0.01529094\n",
      "Iteration 77, loss = 0.01506842\n",
      "Iteration 78, loss = 0.01489029\n",
      "Iteration 79, loss = 0.01466432\n",
      "Iteration 80, loss = 0.01443235\n",
      "Iteration 81, loss = 0.01425285\n",
      "Iteration 82, loss = 0.01411971\n",
      "Iteration 83, loss = 0.01392965\n",
      "Iteration 84, loss = 0.01378854\n",
      "Iteration 85, loss = 0.01364243\n",
      "Iteration 86, loss = 0.01350520\n",
      "Iteration 87, loss = 0.01338477\n",
      "Iteration 88, loss = 0.01325751\n",
      "Iteration 89, loss = 0.01308595\n",
      "Iteration 90, loss = 0.01292721\n",
      "Iteration 91, loss = 0.01292380\n",
      "Iteration 92, loss = 0.01281669\n",
      "Iteration 93, loss = 0.01268611\n",
      "Iteration 94, loss = 0.01255349\n",
      "Iteration 95, loss = 0.01246210\n",
      "Iteration 96, loss = 0.01234998\n",
      "Iteration 97, loss = 0.01229836\n",
      "Iteration 98, loss = 0.01215602\n",
      "Iteration 99, loss = 0.01207608\n",
      "Iteration 100, loss = 0.01207928\n",
      "Iteration 101, loss = 0.01197390\n",
      "Iteration 102, loss = 0.01189801\n",
      "Iteration 103, loss = 0.01175474\n",
      "Iteration 104, loss = 0.01172189\n",
      "Iteration 105, loss = 0.01163219\n",
      "Iteration 106, loss = 0.01161873\n",
      "Iteration 107, loss = 0.01152354\n",
      "Iteration 108, loss = 0.01162261\n",
      "Iteration 109, loss = 0.01141054\n",
      "Iteration 110, loss = 0.01134159\n",
      "Iteration 111, loss = 0.01133635\n",
      "Iteration 112, loss = 0.01128660\n",
      "Iteration 113, loss = 0.01119165\n",
      "Iteration 114, loss = 0.01113108\n",
      "Iteration 115, loss = 0.01115759\n",
      "Iteration 116, loss = 0.01107550\n",
      "Iteration 117, loss = 0.01107067\n",
      "Iteration 118, loss = 0.01098592\n",
      "Iteration 119, loss = 0.01090989\n",
      "Iteration 120, loss = 0.01092537\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.001, batch_size=200, hidden_layer_sizes=(100,), solver=adam; total time= 1.3min\n",
      "Iteration 1, loss = 2.26593606\n",
      "Iteration 2, loss = 2.18271390\n",
      "Iteration 3, loss = 2.09475345\n",
      "Iteration 4, loss = 1.99641664\n",
      "Iteration 5, loss = 1.88697997\n",
      "Iteration 6, loss = 1.76862041\n",
      "Iteration 7, loss = 1.64654161\n",
      "Iteration 8, loss = 1.52630068\n",
      "Iteration 9, loss = 1.41297685\n",
      "Iteration 10, loss = 1.30961883\n",
      "Iteration 11, loss = 1.21717138\n",
      "Iteration 12, loss = 1.13570194\n",
      "Iteration 13, loss = 1.06408650\n",
      "Iteration 14, loss = 1.00140563\n",
      "Iteration 15, loss = 0.94638174\n",
      "Iteration 16, loss = 0.89796149\n",
      "Iteration 17, loss = 0.85517380\n",
      "Iteration 18, loss = 0.81733723\n",
      "Iteration 19, loss = 0.78345255\n",
      "Iteration 20, loss = 0.75329012\n",
      "Iteration 21, loss = 0.72612390\n",
      "Iteration 22, loss = 0.70159686\n",
      "Iteration 23, loss = 0.67932718\n",
      "Iteration 24, loss = 0.65913817\n",
      "Iteration 25, loss = 0.64063182\n",
      "Iteration 26, loss = 0.62368955\n",
      "Iteration 27, loss = 0.60810312\n",
      "Iteration 28, loss = 0.59370089\n",
      "Iteration 29, loss = 0.58038695\n",
      "Iteration 30, loss = 0.56802409\n",
      "Iteration 31, loss = 0.55653783\n",
      "Iteration 32, loss = 0.54579065\n",
      "Iteration 33, loss = 0.53580078\n",
      "Iteration 34, loss = 0.52642310\n",
      "Iteration 35, loss = 0.51765114\n",
      "Iteration 36, loss = 0.50932423\n",
      "Iteration 37, loss = 0.50157133\n",
      "Iteration 38, loss = 0.49415410\n",
      "Iteration 39, loss = 0.48721337\n",
      "Iteration 40, loss = 0.48063712\n",
      "Iteration 41, loss = 0.47440307\n",
      "Iteration 42, loss = 0.46848584\n",
      "Iteration 43, loss = 0.46284972\n",
      "Iteration 44, loss = 0.45749994\n",
      "Iteration 45, loss = 0.45237693\n",
      "Iteration 46, loss = 0.44748635\n",
      "Iteration 47, loss = 0.44291373\n",
      "Iteration 48, loss = 0.43842471\n",
      "Iteration 49, loss = 0.43419293\n",
      "Iteration 50, loss = 0.43017019\n",
      "Iteration 51, loss = 0.42621243\n",
      "Iteration 52, loss = 0.42250415\n",
      "Iteration 53, loss = 0.41890559\n",
      "Iteration 54, loss = 0.41546216\n",
      "Iteration 55, loss = 0.41213334\n",
      "Iteration 56, loss = 0.40892814\n",
      "Iteration 57, loss = 0.40587007\n",
      "Iteration 58, loss = 0.40287711\n",
      "Iteration 59, loss = 0.39999584\n",
      "Iteration 60, loss = 0.39725528\n",
      "Iteration 61, loss = 0.39457501\n",
      "Iteration 62, loss = 0.39200774\n",
      "Iteration 63, loss = 0.38950697\n",
      "Iteration 64, loss = 0.38705357\n",
      "Iteration 65, loss = 0.38472316\n",
      "Iteration 66, loss = 0.38243485\n",
      "Iteration 67, loss = 0.38024751\n",
      "Iteration 68, loss = 0.37807867\n",
      "Iteration 69, loss = 0.37597834\n",
      "Iteration 70, loss = 0.37396397\n",
      "Iteration 71, loss = 0.37201857\n",
      "Iteration 72, loss = 0.37010310\n",
      "Iteration 73, loss = 0.36827761\n",
      "Iteration 74, loss = 0.36644089\n",
      "Iteration 75, loss = 0.36467201\n",
      "Iteration 76, loss = 0.36291539\n",
      "Iteration 77, loss = 0.36127388\n",
      "Iteration 78, loss = 0.35962888\n",
      "Iteration 79, loss = 0.35800321\n",
      "Iteration 80, loss = 0.35646569\n",
      "Iteration 81, loss = 0.35494807\n",
      "Iteration 82, loss = 0.35346000\n",
      "Iteration 83, loss = 0.35197533\n",
      "Iteration 84, loss = 0.35056213\n",
      "Iteration 85, loss = 0.34917767\n",
      "Iteration 86, loss = 0.34780645\n",
      "Iteration 87, loss = 0.34646999\n",
      "Iteration 88, loss = 0.34516332\n",
      "Iteration 89, loss = 0.34392057\n",
      "Iteration 90, loss = 0.34261459\n",
      "Iteration 91, loss = 0.34138116\n",
      "Iteration 92, loss = 0.34021687\n",
      "Iteration 93, loss = 0.33901219\n",
      "Iteration 94, loss = 0.33780993\n",
      "Iteration 95, loss = 0.33669731\n",
      "Iteration 96, loss = 0.33556931\n",
      "Iteration 97, loss = 0.33449567\n",
      "Iteration 98, loss = 0.33339254\n",
      "Iteration 99, loss = 0.33235303\n",
      "Iteration 100, loss = 0.33127088\n",
      "Iteration 101, loss = 0.33027050\n",
      "Iteration 102, loss = 0.32925777\n",
      "Iteration 103, loss = 0.32824574\n",
      "Iteration 104, loss = 0.32731075\n",
      "Iteration 105, loss = 0.32633054\n",
      "Iteration 106, loss = 0.32537431\n",
      "Iteration 107, loss = 0.32443565\n",
      "Iteration 108, loss = 0.32351147\n",
      "Iteration 109, loss = 0.32262699\n",
      "Iteration 110, loss = 0.32172912\n",
      "Iteration 111, loss = 0.32082498\n",
      "Iteration 112, loss = 0.31998888\n",
      "Iteration 113, loss = 0.31913885\n",
      "Iteration 114, loss = 0.31827041\n",
      "Iteration 115, loss = 0.31744978\n",
      "Iteration 116, loss = 0.31661665\n",
      "Iteration 117, loss = 0.31580908\n",
      "Iteration 118, loss = 0.31502754\n",
      "Iteration 119, loss = 0.31426375\n",
      "Iteration 120, loss = 0.31347189\n",
      "Iteration 121, loss = 0.31269115\n",
      "Iteration 122, loss = 0.31194566\n",
      "Iteration 123, loss = 0.31120129\n",
      "Iteration 124, loss = 0.31046076\n",
      "Iteration 125, loss = 0.30971278\n",
      "Iteration 126, loss = 0.30901538\n",
      "Iteration 127, loss = 0.30829650\n",
      "Iteration 128, loss = 0.30757454\n",
      "Iteration 129, loss = 0.30688144\n",
      "Iteration 130, loss = 0.30617608\n",
      "Iteration 131, loss = 0.30552646\n",
      "Iteration 132, loss = 0.30481727\n",
      "Iteration 133, loss = 0.30416100\n",
      "Iteration 134, loss = 0.30347698\n",
      "Iteration 135, loss = 0.30284901\n",
      "Iteration 136, loss = 0.30221274\n",
      "Iteration 137, loss = 0.30155800\n",
      "Iteration 138, loss = 0.30091886\n",
      "Iteration 139, loss = 0.30028661\n",
      "Iteration 140, loss = 0.29964523\n",
      "Iteration 141, loss = 0.29906843\n",
      "Iteration 142, loss = 0.29842586\n",
      "Iteration 143, loss = 0.29783836\n",
      "Iteration 144, loss = 0.29721999\n",
      "Iteration 145, loss = 0.29663441\n",
      "Iteration 146, loss = 0.29602861\n",
      "Iteration 147, loss = 0.29549406\n",
      "Iteration 148, loss = 0.29488876\n",
      "Iteration 149, loss = 0.29431696\n",
      "Iteration 150, loss = 0.29373243\n",
      "Iteration 151, loss = 0.29315299\n",
      "Iteration 152, loss = 0.29261668\n",
      "Iteration 153, loss = 0.29205429\n",
      "Iteration 154, loss = 0.29153071\n",
      "Iteration 155, loss = 0.29095148\n",
      "Iteration 156, loss = 0.29041677\n",
      "Iteration 157, loss = 0.28987199\n",
      "Iteration 158, loss = 0.28935080\n",
      "Iteration 159, loss = 0.28882170\n",
      "Iteration 160, loss = 0.28826595\n",
      "Iteration 161, loss = 0.28779970\n",
      "Iteration 162, loss = 0.28724413\n",
      "Iteration 163, loss = 0.28674356\n",
      "Iteration 164, loss = 0.28622941\n",
      "Iteration 165, loss = 0.28571846\n",
      "Iteration 166, loss = 0.28520797\n",
      "Iteration 167, loss = 0.28471715\n",
      "Iteration 168, loss = 0.28423856\n",
      "Iteration 169, loss = 0.28372363\n",
      "Iteration 170, loss = 0.28323798\n",
      "Iteration 171, loss = 0.28276085\n",
      "Iteration 172, loss = 0.28228314\n",
      "Iteration 173, loss = 0.28180801\n",
      "Iteration 174, loss = 0.28129942\n",
      "Iteration 175, loss = 0.28083634\n",
      "Iteration 176, loss = 0.28037355\n",
      "Iteration 177, loss = 0.27986852\n",
      "Iteration 178, loss = 0.27939997\n",
      "Iteration 179, loss = 0.27897167\n",
      "Iteration 180, loss = 0.27848851\n",
      "Iteration 181, loss = 0.27804921\n",
      "Iteration 182, loss = 0.27759965\n",
      "Iteration 183, loss = 0.27710670\n",
      "Iteration 184, loss = 0.27668163\n",
      "Iteration 185, loss = 0.27623052\n",
      "Iteration 186, loss = 0.27578110\n",
      "Iteration 187, loss = 0.27532424\n",
      "Iteration 188, loss = 0.27491550\n",
      "Iteration 189, loss = 0.27444148\n",
      "Iteration 190, loss = 0.27403308\n",
      "Iteration 191, loss = 0.27359863\n",
      "Iteration 192, loss = 0.27314130\n",
      "Iteration 193, loss = 0.27271870\n",
      "Iteration 194, loss = 0.27228757\n",
      "Iteration 195, loss = 0.27186975\n",
      "Iteration 196, loss = 0.27144739\n",
      "Iteration 197, loss = 0.27103316\n",
      "Iteration 198, loss = 0.27060156\n",
      "Iteration 199, loss = 0.27017996\n",
      "Iteration 200, loss = 0.26978870\n",
      "[CV] END activation=logistic, alpha=0.001, batch_size=200, hidden_layer_sizes=(100,), solver=sgd; total time= 1.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.26137917\n",
      "Iteration 2, loss = 2.17750136\n",
      "Iteration 3, loss = 2.09319950\n",
      "Iteration 4, loss = 1.99762136\n",
      "Iteration 5, loss = 1.89073273\n",
      "Iteration 6, loss = 1.77405897\n",
      "Iteration 7, loss = 1.65341555\n",
      "Iteration 8, loss = 1.53421076\n",
      "Iteration 9, loss = 1.42150829\n",
      "Iteration 10, loss = 1.31814644\n",
      "Iteration 11, loss = 1.22560274\n",
      "Iteration 12, loss = 1.14367071\n",
      "Iteration 13, loss = 1.07165145\n",
      "Iteration 14, loss = 1.00865284\n",
      "Iteration 15, loss = 0.95329668\n",
      "Iteration 16, loss = 0.90471442\n",
      "Iteration 17, loss = 0.86168909\n",
      "Iteration 18, loss = 0.82368546\n",
      "Iteration 19, loss = 0.78977401\n",
      "Iteration 20, loss = 0.75939514\n",
      "Iteration 21, loss = 0.73216247\n",
      "Iteration 22, loss = 0.70752532\n",
      "Iteration 23, loss = 0.68522087\n",
      "Iteration 24, loss = 0.66487673\n",
      "Iteration 25, loss = 0.64635723\n",
      "Iteration 26, loss = 0.62928116\n",
      "Iteration 27, loss = 0.61363170\n",
      "Iteration 28, loss = 0.59915437\n",
      "Iteration 29, loss = 0.58570546\n",
      "Iteration 30, loss = 0.57328543\n",
      "Iteration 31, loss = 0.56174193\n",
      "Iteration 32, loss = 0.55089533\n",
      "Iteration 33, loss = 0.54076928\n",
      "Iteration 34, loss = 0.53126033\n",
      "Iteration 35, loss = 0.52235189\n",
      "Iteration 36, loss = 0.51402013\n",
      "Iteration 37, loss = 0.50607380\n",
      "Iteration 38, loss = 0.49861849\n",
      "Iteration 39, loss = 0.49153862\n",
      "Iteration 40, loss = 0.48485463\n",
      "Iteration 41, loss = 0.47850620\n",
      "Iteration 42, loss = 0.47254944\n",
      "Iteration 43, loss = 0.46673726\n",
      "Iteration 44, loss = 0.46133558\n",
      "Iteration 45, loss = 0.45615469\n",
      "Iteration 46, loss = 0.45116409\n",
      "Iteration 47, loss = 0.44647978\n",
      "Iteration 48, loss = 0.44190855\n",
      "Iteration 49, loss = 0.43762115\n",
      "Iteration 50, loss = 0.43343206\n",
      "Iteration 51, loss = 0.42945745\n",
      "Iteration 52, loss = 0.42561864\n",
      "Iteration 53, loss = 0.42196499\n",
      "Iteration 54, loss = 0.41841531\n",
      "Iteration 55, loss = 0.41502589\n",
      "Iteration 56, loss = 0.41175590\n",
      "Iteration 57, loss = 0.40862305\n",
      "Iteration 58, loss = 0.40559367\n",
      "Iteration 59, loss = 0.40260754\n",
      "Iteration 60, loss = 0.39981397\n",
      "Iteration 61, loss = 0.39704818\n",
      "Iteration 62, loss = 0.39440517\n",
      "Iteration 63, loss = 0.39186509\n",
      "Iteration 64, loss = 0.38936855\n",
      "Iteration 65, loss = 0.38695262\n",
      "Iteration 66, loss = 0.38463551\n",
      "Iteration 67, loss = 0.38236232\n",
      "Iteration 68, loss = 0.38018738\n",
      "Iteration 69, loss = 0.37802069\n",
      "Iteration 70, loss = 0.37595425\n",
      "Iteration 71, loss = 0.37395596\n",
      "Iteration 72, loss = 0.37198095\n",
      "Iteration 73, loss = 0.37007335\n",
      "Iteration 74, loss = 0.36824022\n",
      "Iteration 75, loss = 0.36637979\n",
      "Iteration 76, loss = 0.36463804\n",
      "Iteration 77, loss = 0.36292404\n",
      "Iteration 78, loss = 0.36123177\n",
      "Iteration 79, loss = 0.35957692\n",
      "Iteration 80, loss = 0.35798926\n",
      "Iteration 81, loss = 0.35641021\n",
      "Iteration 82, loss = 0.35487582\n",
      "Iteration 83, loss = 0.35338439\n",
      "Iteration 84, loss = 0.35191805\n",
      "Iteration 85, loss = 0.35050763\n",
      "Iteration 86, loss = 0.34909494\n",
      "Iteration 87, loss = 0.34773032\n",
      "Iteration 88, loss = 0.34640060\n",
      "Iteration 89, loss = 0.34507372\n",
      "Iteration 90, loss = 0.34376299\n",
      "Iteration 91, loss = 0.34251847\n",
      "Iteration 92, loss = 0.34125206\n",
      "Iteration 93, loss = 0.34006041\n",
      "Iteration 94, loss = 0.33887508\n",
      "Iteration 95, loss = 0.33770494\n",
      "Iteration 96, loss = 0.33652303\n",
      "Iteration 97, loss = 0.33543237\n",
      "Iteration 98, loss = 0.33429287\n",
      "Iteration 99, loss = 0.33320108\n",
      "Iteration 100, loss = 0.33216109\n",
      "Iteration 101, loss = 0.33109020\n",
      "Iteration 102, loss = 0.33004224\n",
      "Iteration 103, loss = 0.32901715\n",
      "Iteration 104, loss = 0.32801192\n",
      "Iteration 105, loss = 0.32703587\n",
      "Iteration 106, loss = 0.32611122\n",
      "Iteration 107, loss = 0.32510638\n",
      "Iteration 108, loss = 0.32412867\n",
      "Iteration 109, loss = 0.32324071\n",
      "Iteration 110, loss = 0.32230123\n",
      "Iteration 111, loss = 0.32141751\n",
      "Iteration 112, loss = 0.32051955\n",
      "Iteration 113, loss = 0.31963629\n",
      "Iteration 114, loss = 0.31880238\n",
      "Iteration 115, loss = 0.31791683\n",
      "Iteration 116, loss = 0.31707834\n",
      "Iteration 117, loss = 0.31626167\n",
      "Iteration 118, loss = 0.31543892\n",
      "Iteration 119, loss = 0.31462545\n",
      "Iteration 120, loss = 0.31383575\n",
      "Iteration 121, loss = 0.31301219\n",
      "Iteration 122, loss = 0.31226617\n",
      "Iteration 123, loss = 0.31144140\n",
      "Iteration 124, loss = 0.31069778\n",
      "Iteration 125, loss = 0.30996389\n",
      "Iteration 126, loss = 0.30921964\n",
      "Iteration 127, loss = 0.30848051\n",
      "Iteration 128, loss = 0.30773192\n",
      "Iteration 129, loss = 0.30704754\n",
      "Iteration 130, loss = 0.30632511\n",
      "Iteration 131, loss = 0.30562315\n",
      "Iteration 132, loss = 0.30493935\n",
      "Iteration 133, loss = 0.30421948\n",
      "Iteration 134, loss = 0.30359071\n",
      "Iteration 135, loss = 0.30288027\n",
      "Iteration 136, loss = 0.30220392\n",
      "Iteration 137, loss = 0.30158134\n",
      "Iteration 138, loss = 0.30089442\n",
      "Iteration 139, loss = 0.30024146\n",
      "Iteration 140, loss = 0.29960679\n",
      "Iteration 141, loss = 0.29895926\n",
      "Iteration 142, loss = 0.29835451\n",
      "Iteration 143, loss = 0.29772854\n",
      "Iteration 144, loss = 0.29709315\n",
      "Iteration 145, loss = 0.29647231\n",
      "Iteration 146, loss = 0.29587980\n",
      "Iteration 147, loss = 0.29526542\n",
      "Iteration 148, loss = 0.29472040\n",
      "Iteration 149, loss = 0.29409854\n",
      "Iteration 150, loss = 0.29349061\n",
      "Iteration 151, loss = 0.29291765\n",
      "Iteration 152, loss = 0.29233245\n",
      "Iteration 153, loss = 0.29176773\n",
      "Iteration 154, loss = 0.29116541\n",
      "Iteration 155, loss = 0.29063486\n",
      "Iteration 156, loss = 0.29006290\n",
      "Iteration 157, loss = 0.28951992\n",
      "Iteration 158, loss = 0.28895782\n",
      "Iteration 159, loss = 0.28841370\n",
      "Iteration 160, loss = 0.28787313\n",
      "Iteration 161, loss = 0.28734268\n",
      "Iteration 162, loss = 0.28680143\n",
      "Iteration 163, loss = 0.28627316\n",
      "Iteration 164, loss = 0.28574461\n",
      "Iteration 165, loss = 0.28520932\n",
      "Iteration 166, loss = 0.28472518\n",
      "Iteration 167, loss = 0.28417805\n",
      "Iteration 168, loss = 0.28369294\n",
      "Iteration 169, loss = 0.28317015\n",
      "Iteration 170, loss = 0.28264519\n",
      "Iteration 171, loss = 0.28217372\n",
      "Iteration 172, loss = 0.28165230\n",
      "Iteration 173, loss = 0.28116682\n",
      "Iteration 174, loss = 0.28066392\n",
      "Iteration 175, loss = 0.28015670\n",
      "Iteration 176, loss = 0.27969910\n",
      "Iteration 177, loss = 0.27918597\n",
      "Iteration 178, loss = 0.27871376\n",
      "Iteration 179, loss = 0.27825004\n",
      "Iteration 180, loss = 0.27776197\n",
      "Iteration 181, loss = 0.27732079\n",
      "Iteration 182, loss = 0.27680026\n",
      "Iteration 183, loss = 0.27634498\n",
      "Iteration 184, loss = 0.27588154\n",
      "Iteration 185, loss = 0.27542803\n",
      "Iteration 186, loss = 0.27497241\n",
      "Iteration 187, loss = 0.27448126\n",
      "Iteration 188, loss = 0.27405722\n",
      "Iteration 189, loss = 0.27357273\n",
      "Iteration 190, loss = 0.27314505\n",
      "Iteration 191, loss = 0.27268390\n",
      "Iteration 192, loss = 0.27225355\n",
      "Iteration 193, loss = 0.27181946\n",
      "Iteration 194, loss = 0.27137331\n",
      "Iteration 195, loss = 0.27092679\n",
      "Iteration 196, loss = 0.27048303\n",
      "Iteration 197, loss = 0.27006697\n",
      "Iteration 198, loss = 0.26961290\n",
      "Iteration 199, loss = 0.26920141\n",
      "Iteration 200, loss = 0.26876361\n",
      "[CV] END activation=logistic, alpha=0.001, batch_size=200, hidden_layer_sizes=(100,), solver=sgd; total time= 1.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.27505048\n",
      "Iteration 2, loss = 2.18630434\n",
      "Iteration 3, loss = 2.10336855\n",
      "Iteration 4, loss = 2.01068734\n",
      "Iteration 5, loss = 1.90744541\n",
      "Iteration 6, loss = 1.79509431\n",
      "Iteration 7, loss = 1.67809978\n",
      "Iteration 8, loss = 1.56127330\n",
      "Iteration 9, loss = 1.44942709\n",
      "Iteration 10, loss = 1.34568543\n",
      "Iteration 11, loss = 1.25180744\n",
      "Iteration 12, loss = 1.16804408\n",
      "Iteration 13, loss = 1.09403125\n",
      "Iteration 14, loss = 1.02890986\n",
      "Iteration 15, loss = 0.97172214\n",
      "Iteration 16, loss = 0.92119049\n",
      "Iteration 17, loss = 0.87660238\n",
      "Iteration 18, loss = 0.83701093\n",
      "Iteration 19, loss = 0.80177748\n",
      "Iteration 20, loss = 0.77014488\n",
      "Iteration 21, loss = 0.74171277\n",
      "Iteration 22, loss = 0.71607979\n",
      "Iteration 23, loss = 0.69281951\n",
      "Iteration 24, loss = 0.67162462\n",
      "Iteration 25, loss = 0.65225243\n",
      "Iteration 26, loss = 0.63445754\n",
      "Iteration 27, loss = 0.61811976\n",
      "Iteration 28, loss = 0.60297802\n",
      "Iteration 29, loss = 0.58901713\n",
      "Iteration 30, loss = 0.57600947\n",
      "Iteration 31, loss = 0.56396172\n",
      "Iteration 32, loss = 0.55267724\n",
      "Iteration 33, loss = 0.54219166\n",
      "Iteration 34, loss = 0.53230487\n",
      "Iteration 35, loss = 0.52309168\n",
      "Iteration 36, loss = 0.51438313\n",
      "Iteration 37, loss = 0.50619725\n",
      "Iteration 38, loss = 0.49847781\n",
      "Iteration 39, loss = 0.49115302\n",
      "Iteration 40, loss = 0.48427767\n",
      "Iteration 41, loss = 0.47773822\n",
      "Iteration 42, loss = 0.47152691\n",
      "Iteration 43, loss = 0.46565337\n",
      "Iteration 44, loss = 0.46003444\n",
      "Iteration 45, loss = 0.45470144\n",
      "Iteration 46, loss = 0.44963110\n",
      "Iteration 47, loss = 0.44476812\n",
      "Iteration 48, loss = 0.44013125\n",
      "Iteration 49, loss = 0.43571284\n",
      "Iteration 50, loss = 0.43151804\n",
      "Iteration 51, loss = 0.42739987\n",
      "Iteration 52, loss = 0.42353999\n",
      "Iteration 53, loss = 0.41981178\n",
      "Iteration 54, loss = 0.41618105\n",
      "Iteration 55, loss = 0.41275300\n",
      "Iteration 56, loss = 0.40942761\n",
      "Iteration 57, loss = 0.40624952\n",
      "Iteration 58, loss = 0.40317475\n",
      "Iteration 59, loss = 0.40022071\n",
      "Iteration 60, loss = 0.39733340\n",
      "Iteration 61, loss = 0.39455206\n",
      "Iteration 62, loss = 0.39187006\n",
      "Iteration 63, loss = 0.38929831\n",
      "Iteration 64, loss = 0.38677013\n",
      "Iteration 65, loss = 0.38437486\n",
      "Iteration 66, loss = 0.38200663\n",
      "Iteration 67, loss = 0.37976242\n",
      "Iteration 68, loss = 0.37752236\n",
      "Iteration 69, loss = 0.37538006\n",
      "Iteration 70, loss = 0.37327955\n",
      "Iteration 71, loss = 0.37125615\n",
      "Iteration 72, loss = 0.36926891\n",
      "Iteration 73, loss = 0.36738317\n",
      "Iteration 74, loss = 0.36549153\n",
      "Iteration 75, loss = 0.36370181\n",
      "Iteration 76, loss = 0.36191502\n",
      "Iteration 77, loss = 0.36023229\n",
      "Iteration 78, loss = 0.35852275\n",
      "Iteration 79, loss = 0.35687040\n",
      "Iteration 80, loss = 0.35530480\n",
      "Iteration 81, loss = 0.35371943\n",
      "Iteration 82, loss = 0.35222069\n",
      "Iteration 83, loss = 0.35067855\n",
      "Iteration 84, loss = 0.34926109\n",
      "Iteration 85, loss = 0.34784229\n",
      "Iteration 86, loss = 0.34646616\n",
      "Iteration 87, loss = 0.34508715\n",
      "Iteration 88, loss = 0.34374448\n",
      "Iteration 89, loss = 0.34245796\n",
      "Iteration 90, loss = 0.34117068\n",
      "Iteration 91, loss = 0.33992773\n",
      "Iteration 92, loss = 0.33868812\n",
      "Iteration 93, loss = 0.33748274\n",
      "Iteration 94, loss = 0.33630843\n",
      "Iteration 95, loss = 0.33512670\n",
      "Iteration 96, loss = 0.33401094\n",
      "Iteration 97, loss = 0.33285446\n",
      "Iteration 98, loss = 0.33180407\n",
      "Iteration 99, loss = 0.33071936\n",
      "Iteration 100, loss = 0.32964776\n",
      "Iteration 101, loss = 0.32859887\n",
      "Iteration 102, loss = 0.32760653\n",
      "Iteration 103, loss = 0.32656817\n",
      "Iteration 104, loss = 0.32558560\n",
      "Iteration 105, loss = 0.32463270\n",
      "Iteration 106, loss = 0.32364751\n",
      "Iteration 107, loss = 0.32273348\n",
      "Iteration 108, loss = 0.32182473\n",
      "Iteration 109, loss = 0.32089828\n",
      "Iteration 110, loss = 0.31998568\n",
      "Iteration 111, loss = 0.31910579\n",
      "Iteration 112, loss = 0.31820754\n",
      "Iteration 113, loss = 0.31736258\n",
      "Iteration 114, loss = 0.31653215\n",
      "Iteration 115, loss = 0.31567069\n",
      "Iteration 116, loss = 0.31483490\n",
      "Iteration 117, loss = 0.31405921\n",
      "Iteration 118, loss = 0.31322838\n",
      "Iteration 119, loss = 0.31244365\n",
      "Iteration 120, loss = 0.31169601\n",
      "Iteration 121, loss = 0.31086474\n",
      "Iteration 122, loss = 0.31010124\n",
      "Iteration 123, loss = 0.30935901\n",
      "Iteration 124, loss = 0.30862957\n",
      "Iteration 125, loss = 0.30788904\n",
      "Iteration 126, loss = 0.30718846\n",
      "Iteration 127, loss = 0.30643550\n",
      "Iteration 128, loss = 0.30572501\n",
      "Iteration 129, loss = 0.30502269\n",
      "Iteration 130, loss = 0.30433384\n",
      "Iteration 131, loss = 0.30366934\n",
      "Iteration 132, loss = 0.30295735\n",
      "Iteration 133, loss = 0.30231821\n",
      "Iteration 134, loss = 0.30162259\n",
      "Iteration 135, loss = 0.30099584\n",
      "Iteration 136, loss = 0.30032258\n",
      "Iteration 137, loss = 0.29968969\n",
      "Iteration 138, loss = 0.29906049\n",
      "Iteration 139, loss = 0.29842323\n",
      "Iteration 140, loss = 0.29781873\n",
      "Iteration 141, loss = 0.29716841\n",
      "Iteration 142, loss = 0.29656642\n",
      "Iteration 143, loss = 0.29596483\n",
      "Iteration 144, loss = 0.29536633\n",
      "Iteration 145, loss = 0.29476588\n",
      "Iteration 146, loss = 0.29419318\n",
      "Iteration 147, loss = 0.29359891\n",
      "Iteration 148, loss = 0.29300600\n",
      "Iteration 149, loss = 0.29245730\n",
      "Iteration 150, loss = 0.29190420\n",
      "Iteration 151, loss = 0.29131219\n",
      "Iteration 152, loss = 0.29078011\n",
      "Iteration 153, loss = 0.29020478\n",
      "Iteration 154, loss = 0.28963314\n",
      "Iteration 155, loss = 0.28910205\n",
      "Iteration 156, loss = 0.28856990\n",
      "Iteration 157, loss = 0.28802367\n",
      "Iteration 158, loss = 0.28747676\n",
      "Iteration 159, loss = 0.28699087\n",
      "Iteration 160, loss = 0.28642760\n",
      "Iteration 161, loss = 0.28592256\n",
      "Iteration 162, loss = 0.28538179\n",
      "Iteration 163, loss = 0.28485671\n",
      "Iteration 164, loss = 0.28437890\n",
      "Iteration 165, loss = 0.28383731\n",
      "Iteration 166, loss = 0.28334403\n",
      "Iteration 167, loss = 0.28285721\n",
      "Iteration 168, loss = 0.28237837\n",
      "Iteration 169, loss = 0.28185501\n",
      "Iteration 170, loss = 0.28138373\n",
      "Iteration 171, loss = 0.28087195\n",
      "Iteration 172, loss = 0.28042353\n",
      "Iteration 173, loss = 0.27990118\n",
      "Iteration 174, loss = 0.27943478\n",
      "Iteration 175, loss = 0.27895399\n",
      "Iteration 176, loss = 0.27848870\n",
      "Iteration 177, loss = 0.27800850\n",
      "Iteration 178, loss = 0.27755128\n",
      "Iteration 179, loss = 0.27708624\n",
      "Iteration 180, loss = 0.27663696\n",
      "Iteration 181, loss = 0.27616179\n",
      "Iteration 182, loss = 0.27570543\n",
      "Iteration 183, loss = 0.27526324\n",
      "Iteration 184, loss = 0.27479167\n",
      "Iteration 185, loss = 0.27434240\n",
      "Iteration 186, loss = 0.27389682\n",
      "Iteration 187, loss = 0.27343754\n",
      "Iteration 188, loss = 0.27297993\n",
      "Iteration 189, loss = 0.27256946\n",
      "Iteration 190, loss = 0.27213455\n",
      "Iteration 191, loss = 0.27169576\n",
      "Iteration 192, loss = 0.27124654\n",
      "Iteration 193, loss = 0.27083575\n",
      "Iteration 194, loss = 0.27040650\n",
      "Iteration 195, loss = 0.26997497\n",
      "Iteration 196, loss = 0.26952925\n",
      "Iteration 197, loss = 0.26911157\n",
      "Iteration 198, loss = 0.26869558\n",
      "Iteration 199, loss = 0.26828676\n",
      "Iteration 200, loss = 0.26785535\n",
      "[CV] END activation=logistic, alpha=0.001, batch_size=200, hidden_layer_sizes=(100,), solver=sgd; total time= 1.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.28483541\n",
      "Iteration 2, loss = 2.20038148\n",
      "Iteration 3, loss = 2.11610736\n",
      "Iteration 4, loss = 2.02222580\n",
      "Iteration 5, loss = 1.91655725\n",
      "Iteration 6, loss = 1.80092347\n",
      "Iteration 7, loss = 1.67961705\n",
      "Iteration 8, loss = 1.55863121\n",
      "Iteration 9, loss = 1.44321718\n",
      "Iteration 10, loss = 1.33698446\n",
      "Iteration 11, loss = 1.24183516\n",
      "Iteration 12, loss = 1.15782167\n",
      "Iteration 13, loss = 1.08417842\n",
      "Iteration 14, loss = 1.01975232\n",
      "Iteration 15, loss = 0.96340903\n",
      "Iteration 16, loss = 0.91390406\n",
      "Iteration 17, loss = 0.87021919\n",
      "Iteration 18, loss = 0.83152769\n",
      "Iteration 19, loss = 0.79714502\n",
      "Iteration 20, loss = 0.76637127\n",
      "Iteration 21, loss = 0.73862244\n",
      "Iteration 22, loss = 0.71359549\n",
      "Iteration 23, loss = 0.69089322\n",
      "Iteration 24, loss = 0.67022476\n",
      "Iteration 25, loss = 0.65131757\n",
      "Iteration 26, loss = 0.63396028\n",
      "Iteration 27, loss = 0.61797361\n",
      "Iteration 28, loss = 0.60321718\n",
      "Iteration 29, loss = 0.58956842\n",
      "Iteration 30, loss = 0.57690660\n",
      "Iteration 31, loss = 0.56501049\n",
      "Iteration 32, loss = 0.55397964\n",
      "Iteration 33, loss = 0.54369571\n",
      "Iteration 34, loss = 0.53399003\n",
      "Iteration 35, loss = 0.52491369\n",
      "Iteration 36, loss = 0.51642500\n",
      "Iteration 37, loss = 0.50832347\n",
      "Iteration 38, loss = 0.50074010\n",
      "Iteration 39, loss = 0.49354711\n",
      "Iteration 40, loss = 0.48676842\n",
      "Iteration 41, loss = 0.48031079\n",
      "Iteration 42, loss = 0.47420505\n",
      "Iteration 43, loss = 0.46839591\n",
      "Iteration 44, loss = 0.46288533\n",
      "Iteration 45, loss = 0.45762863\n",
      "Iteration 46, loss = 0.45261664\n",
      "Iteration 47, loss = 0.44783197\n",
      "Iteration 48, loss = 0.44328063\n",
      "Iteration 49, loss = 0.43890704\n",
      "Iteration 50, loss = 0.43469907\n",
      "Iteration 51, loss = 0.43069696\n",
      "Iteration 52, loss = 0.42687490\n",
      "Iteration 53, loss = 0.42317013\n",
      "Iteration 54, loss = 0.41962475\n",
      "Iteration 55, loss = 0.41621543\n",
      "Iteration 56, loss = 0.41294711\n",
      "Iteration 57, loss = 0.40977330\n",
      "Iteration 58, loss = 0.40673159\n",
      "Iteration 59, loss = 0.40377137\n",
      "Iteration 60, loss = 0.40094952\n",
      "Iteration 61, loss = 0.39818214\n",
      "Iteration 62, loss = 0.39553251\n",
      "Iteration 63, loss = 0.39297914\n",
      "Iteration 64, loss = 0.39056547\n",
      "Iteration 65, loss = 0.38811998\n",
      "Iteration 66, loss = 0.38579184\n",
      "Iteration 67, loss = 0.38355585\n",
      "Iteration 68, loss = 0.38134421\n",
      "Iteration 69, loss = 0.37923071\n",
      "Iteration 70, loss = 0.37716744\n",
      "Iteration 71, loss = 0.37513758\n",
      "Iteration 72, loss = 0.37321502\n",
      "Iteration 73, loss = 0.37130407\n",
      "Iteration 74, loss = 0.36947796\n",
      "Iteration 75, loss = 0.36765142\n",
      "Iteration 76, loss = 0.36589898\n",
      "Iteration 77, loss = 0.36418773\n",
      "Iteration 78, loss = 0.36249986\n",
      "Iteration 79, loss = 0.36089807\n",
      "Iteration 80, loss = 0.35927728\n",
      "Iteration 81, loss = 0.35774359\n",
      "Iteration 82, loss = 0.35621379\n",
      "Iteration 83, loss = 0.35475849\n",
      "Iteration 84, loss = 0.35333473\n",
      "Iteration 85, loss = 0.35185398\n",
      "Iteration 86, loss = 0.35047767\n",
      "Iteration 87, loss = 0.34912012\n",
      "Iteration 88, loss = 0.34781739\n",
      "Iteration 89, loss = 0.34650047\n",
      "Iteration 90, loss = 0.34520432\n",
      "Iteration 91, loss = 0.34396233\n",
      "Iteration 92, loss = 0.34270666\n",
      "Iteration 93, loss = 0.34152235\n",
      "Iteration 94, loss = 0.34034994\n",
      "Iteration 95, loss = 0.33920148\n",
      "Iteration 96, loss = 0.33805128\n",
      "Iteration 97, loss = 0.33691692\n",
      "Iteration 98, loss = 0.33586534\n",
      "Iteration 99, loss = 0.33475845\n",
      "Iteration 100, loss = 0.33369657\n",
      "Iteration 101, loss = 0.33264086\n",
      "Iteration 102, loss = 0.33161803\n",
      "Iteration 103, loss = 0.33063032\n",
      "Iteration 104, loss = 0.32960640\n",
      "Iteration 105, loss = 0.32863200\n",
      "Iteration 106, loss = 0.32768262\n",
      "Iteration 107, loss = 0.32673754\n",
      "Iteration 108, loss = 0.32581935\n",
      "Iteration 109, loss = 0.32486459\n",
      "Iteration 110, loss = 0.32395935\n",
      "Iteration 111, loss = 0.32306461\n",
      "Iteration 112, loss = 0.32218997\n",
      "Iteration 113, loss = 0.32131864\n",
      "Iteration 114, loss = 0.32048424\n",
      "Iteration 115, loss = 0.31963779\n",
      "Iteration 116, loss = 0.31878156\n",
      "Iteration 117, loss = 0.31796336\n",
      "Iteration 118, loss = 0.31717978\n",
      "Iteration 119, loss = 0.31635710\n",
      "Iteration 120, loss = 0.31557231\n",
      "Iteration 121, loss = 0.31480514\n",
      "Iteration 122, loss = 0.31402770\n",
      "Iteration 123, loss = 0.31323874\n",
      "Iteration 124, loss = 0.31252025\n",
      "Iteration 125, loss = 0.31175052\n",
      "Iteration 126, loss = 0.31100562\n",
      "Iteration 127, loss = 0.31029535\n",
      "Iteration 128, loss = 0.30958086\n",
      "Iteration 129, loss = 0.30886346\n",
      "Iteration 130, loss = 0.30814764\n",
      "Iteration 131, loss = 0.30746073\n",
      "Iteration 132, loss = 0.30678712\n",
      "Iteration 133, loss = 0.30608735\n",
      "Iteration 134, loss = 0.30541061\n",
      "Iteration 135, loss = 0.30476473\n",
      "Iteration 136, loss = 0.30407396\n",
      "Iteration 137, loss = 0.30348440\n",
      "Iteration 138, loss = 0.30280236\n",
      "Iteration 139, loss = 0.30216488\n",
      "Iteration 140, loss = 0.30151686\n",
      "Iteration 141, loss = 0.30089422\n",
      "Iteration 142, loss = 0.30026783\n",
      "Iteration 143, loss = 0.29963383\n",
      "Iteration 144, loss = 0.29903921\n",
      "Iteration 145, loss = 0.29841863\n",
      "Iteration 146, loss = 0.29780518\n",
      "Iteration 147, loss = 0.29721751\n",
      "Iteration 148, loss = 0.29663947\n",
      "Iteration 149, loss = 0.29603303\n",
      "Iteration 150, loss = 0.29547707\n",
      "Iteration 151, loss = 0.29486532\n",
      "Iteration 152, loss = 0.29430180\n",
      "Iteration 153, loss = 0.29377394\n",
      "Iteration 154, loss = 0.29317659\n",
      "Iteration 155, loss = 0.29259714\n",
      "Iteration 156, loss = 0.29206816\n",
      "Iteration 157, loss = 0.29148971\n",
      "Iteration 158, loss = 0.29096832\n",
      "Iteration 159, loss = 0.29042957\n",
      "Iteration 160, loss = 0.28989210\n",
      "Iteration 161, loss = 0.28934233\n",
      "Iteration 162, loss = 0.28882814\n",
      "Iteration 163, loss = 0.28830439\n",
      "Iteration 164, loss = 0.28777115\n",
      "Iteration 165, loss = 0.28726718\n",
      "Iteration 166, loss = 0.28674496\n",
      "Iteration 167, loss = 0.28621792\n",
      "Iteration 168, loss = 0.28571541\n",
      "Iteration 169, loss = 0.28519620\n",
      "Iteration 170, loss = 0.28470204\n",
      "Iteration 171, loss = 0.28420099\n",
      "Iteration 172, loss = 0.28369406\n",
      "Iteration 173, loss = 0.28321152\n",
      "Iteration 174, loss = 0.28272170\n",
      "Iteration 175, loss = 0.28221906\n",
      "Iteration 176, loss = 0.28172906\n",
      "Iteration 177, loss = 0.28123694\n",
      "Iteration 178, loss = 0.28077084\n",
      "Iteration 179, loss = 0.28031590\n",
      "Iteration 180, loss = 0.27981072\n",
      "Iteration 181, loss = 0.27936965\n",
      "Iteration 182, loss = 0.27887156\n",
      "Iteration 183, loss = 0.27840697\n",
      "Iteration 184, loss = 0.27796639\n",
      "Iteration 185, loss = 0.27749651\n",
      "Iteration 186, loss = 0.27702287\n",
      "Iteration 187, loss = 0.27656956\n",
      "Iteration 188, loss = 0.27610448\n",
      "Iteration 189, loss = 0.27565224\n",
      "Iteration 190, loss = 0.27522875\n",
      "Iteration 191, loss = 0.27476288\n",
      "Iteration 192, loss = 0.27431747\n",
      "Iteration 193, loss = 0.27386255\n",
      "Iteration 194, loss = 0.27343494\n",
      "Iteration 195, loss = 0.27299586\n",
      "Iteration 196, loss = 0.27255592\n",
      "Iteration 197, loss = 0.27210877\n",
      "Iteration 198, loss = 0.27168148\n",
      "Iteration 199, loss = 0.27123674\n",
      "Iteration 200, loss = 0.27080746\n",
      "[CV] END activation=logistic, alpha=0.001, batch_size=200, hidden_layer_sizes=(100,), solver=sgd; total time= 1.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.27251426\n",
      "Iteration 2, loss = 2.19287494\n",
      "Iteration 3, loss = 2.11304902\n",
      "Iteration 4, loss = 2.02320885\n",
      "Iteration 5, loss = 1.92133594\n",
      "Iteration 6, loss = 1.80913784\n",
      "Iteration 7, loss = 1.69066713\n",
      "Iteration 8, loss = 1.57168695\n",
      "Iteration 9, loss = 1.45732446\n",
      "Iteration 10, loss = 1.35133978\n",
      "Iteration 11, loss = 1.25540501\n",
      "Iteration 12, loss = 1.17013160\n",
      "Iteration 13, loss = 1.09497230\n",
      "Iteration 14, loss = 1.02902817\n",
      "Iteration 15, loss = 0.97119058\n",
      "Iteration 16, loss = 0.92039468\n",
      "Iteration 17, loss = 0.87574847\n",
      "Iteration 18, loss = 0.83616177\n",
      "Iteration 19, loss = 0.80108115\n",
      "Iteration 20, loss = 0.76975889\n",
      "Iteration 21, loss = 0.74164013\n",
      "Iteration 22, loss = 0.71641871\n",
      "Iteration 23, loss = 0.69348070\n",
      "Iteration 24, loss = 0.67274741\n",
      "Iteration 25, loss = 0.65378062\n",
      "Iteration 26, loss = 0.63638845\n",
      "Iteration 27, loss = 0.62048035\n",
      "Iteration 28, loss = 0.60575632\n",
      "Iteration 29, loss = 0.59209535\n",
      "Iteration 30, loss = 0.57941126\n",
      "Iteration 31, loss = 0.56773829\n",
      "Iteration 32, loss = 0.55670892\n",
      "Iteration 33, loss = 0.54646419\n",
      "Iteration 34, loss = 0.53682255\n",
      "Iteration 35, loss = 0.52777703\n",
      "Iteration 36, loss = 0.51926409\n",
      "Iteration 37, loss = 0.51124865\n",
      "Iteration 38, loss = 0.50369272\n",
      "Iteration 39, loss = 0.49649090\n",
      "Iteration 40, loss = 0.48969382\n",
      "Iteration 41, loss = 0.48327520\n",
      "Iteration 42, loss = 0.47715692\n",
      "Iteration 43, loss = 0.47137419\n",
      "Iteration 44, loss = 0.46585939\n",
      "Iteration 45, loss = 0.46054220\n",
      "Iteration 46, loss = 0.45550950\n",
      "Iteration 47, loss = 0.45071980\n",
      "Iteration 48, loss = 0.44613691\n",
      "Iteration 49, loss = 0.44172248\n",
      "Iteration 50, loss = 0.43750603\n",
      "Iteration 51, loss = 0.43349576\n",
      "Iteration 52, loss = 0.42963309\n",
      "Iteration 53, loss = 0.42585847\n",
      "Iteration 54, loss = 0.42230331\n",
      "Iteration 55, loss = 0.41887480\n",
      "Iteration 56, loss = 0.41556249\n",
      "Iteration 57, loss = 0.41235271\n",
      "Iteration 58, loss = 0.40931031\n",
      "Iteration 59, loss = 0.40631298\n",
      "Iteration 60, loss = 0.40344918\n",
      "Iteration 61, loss = 0.40066572\n",
      "Iteration 62, loss = 0.39796885\n",
      "Iteration 63, loss = 0.39538889\n",
      "Iteration 64, loss = 0.39285665\n",
      "Iteration 65, loss = 0.39048091\n",
      "Iteration 66, loss = 0.38808796\n",
      "Iteration 67, loss = 0.38576887\n",
      "Iteration 68, loss = 0.38361285\n",
      "Iteration 69, loss = 0.38143469\n",
      "Iteration 70, loss = 0.37930324\n",
      "Iteration 71, loss = 0.37727141\n",
      "Iteration 72, loss = 0.37527096\n",
      "Iteration 73, loss = 0.37338938\n",
      "Iteration 74, loss = 0.37147800\n",
      "Iteration 75, loss = 0.36966763\n",
      "Iteration 76, loss = 0.36788291\n",
      "Iteration 77, loss = 0.36616027\n",
      "Iteration 78, loss = 0.36447368\n",
      "Iteration 79, loss = 0.36279508\n",
      "Iteration 80, loss = 0.36115810\n",
      "Iteration 81, loss = 0.35962381\n",
      "Iteration 82, loss = 0.35805134\n",
      "Iteration 83, loss = 0.35653831\n",
      "Iteration 84, loss = 0.35505819\n",
      "Iteration 85, loss = 0.35361375\n",
      "Iteration 86, loss = 0.35222636\n",
      "Iteration 87, loss = 0.35082881\n",
      "Iteration 88, loss = 0.34948606\n",
      "Iteration 89, loss = 0.34815705\n",
      "Iteration 90, loss = 0.34684556\n",
      "Iteration 91, loss = 0.34556824\n",
      "Iteration 92, loss = 0.34432621\n",
      "Iteration 93, loss = 0.34312928\n",
      "Iteration 94, loss = 0.34190225\n",
      "Iteration 95, loss = 0.34072032\n",
      "Iteration 96, loss = 0.33957576\n",
      "Iteration 97, loss = 0.33843267\n",
      "Iteration 98, loss = 0.33733545\n",
      "Iteration 99, loss = 0.33622096\n",
      "Iteration 100, loss = 0.33510951\n",
      "Iteration 101, loss = 0.33412155\n",
      "Iteration 102, loss = 0.33307555\n",
      "Iteration 103, loss = 0.33202652\n",
      "Iteration 104, loss = 0.33101635\n",
      "Iteration 105, loss = 0.33000850\n",
      "Iteration 106, loss = 0.32904724\n",
      "Iteration 107, loss = 0.32809204\n",
      "Iteration 108, loss = 0.32711693\n",
      "Iteration 109, loss = 0.32619652\n",
      "Iteration 110, loss = 0.32526684\n",
      "Iteration 111, loss = 0.32436927\n",
      "Iteration 112, loss = 0.32347812\n",
      "Iteration 113, loss = 0.32260980\n",
      "Iteration 114, loss = 0.32177469\n",
      "Iteration 115, loss = 0.32087363\n",
      "Iteration 116, loss = 0.32002268\n",
      "Iteration 117, loss = 0.31921583\n",
      "Iteration 118, loss = 0.31837190\n",
      "Iteration 119, loss = 0.31757605\n",
      "Iteration 120, loss = 0.31675269\n",
      "Iteration 121, loss = 0.31598351\n",
      "Iteration 122, loss = 0.31517489\n",
      "Iteration 123, loss = 0.31443104\n",
      "Iteration 124, loss = 0.31366855\n",
      "Iteration 125, loss = 0.31292294\n",
      "Iteration 126, loss = 0.31217980\n",
      "Iteration 127, loss = 0.31143911\n",
      "Iteration 128, loss = 0.31075303\n",
      "Iteration 129, loss = 0.31000917\n",
      "Iteration 130, loss = 0.30928165\n",
      "Iteration 131, loss = 0.30858472\n",
      "Iteration 132, loss = 0.30786464\n",
      "Iteration 133, loss = 0.30720590\n",
      "Iteration 134, loss = 0.30650733\n",
      "Iteration 135, loss = 0.30583961\n",
      "Iteration 136, loss = 0.30519651\n",
      "Iteration 137, loss = 0.30453260\n",
      "Iteration 138, loss = 0.30387797\n",
      "Iteration 139, loss = 0.30324705\n",
      "Iteration 140, loss = 0.30263175\n",
      "Iteration 141, loss = 0.30197844\n",
      "Iteration 142, loss = 0.30133624\n",
      "Iteration 143, loss = 0.30071978\n",
      "Iteration 144, loss = 0.30008250\n",
      "Iteration 145, loss = 0.29951287\n",
      "Iteration 146, loss = 0.29887406\n",
      "Iteration 147, loss = 0.29829192\n",
      "Iteration 148, loss = 0.29770911\n",
      "Iteration 149, loss = 0.29713861\n",
      "Iteration 150, loss = 0.29654089\n",
      "Iteration 151, loss = 0.29596159\n",
      "Iteration 152, loss = 0.29537833\n",
      "Iteration 153, loss = 0.29484641\n",
      "Iteration 154, loss = 0.29424075\n",
      "Iteration 155, loss = 0.29369422\n",
      "Iteration 156, loss = 0.29314258\n",
      "Iteration 157, loss = 0.29258728\n",
      "Iteration 158, loss = 0.29203693\n",
      "Iteration 159, loss = 0.29150194\n",
      "Iteration 160, loss = 0.29097045\n",
      "Iteration 161, loss = 0.29042429\n",
      "Iteration 162, loss = 0.28993856\n",
      "Iteration 163, loss = 0.28939366\n",
      "Iteration 164, loss = 0.28888310\n",
      "Iteration 165, loss = 0.28834930\n",
      "Iteration 166, loss = 0.28783941\n",
      "Iteration 167, loss = 0.28731838\n",
      "Iteration 168, loss = 0.28679753\n",
      "Iteration 169, loss = 0.28633834\n",
      "Iteration 170, loss = 0.28581167\n",
      "Iteration 171, loss = 0.28531665\n",
      "Iteration 172, loss = 0.28481470\n",
      "Iteration 173, loss = 0.28432745\n",
      "Iteration 174, loss = 0.28382736\n",
      "Iteration 175, loss = 0.28339376\n",
      "Iteration 176, loss = 0.28290001\n",
      "Iteration 177, loss = 0.28240510\n",
      "Iteration 178, loss = 0.28195293\n",
      "Iteration 179, loss = 0.28145482\n",
      "Iteration 180, loss = 0.28102135\n",
      "Iteration 181, loss = 0.28052866\n",
      "Iteration 182, loss = 0.28006355\n",
      "Iteration 183, loss = 0.27960308\n",
      "Iteration 184, loss = 0.27914233\n",
      "Iteration 185, loss = 0.27868009\n",
      "Iteration 186, loss = 0.27823981\n",
      "Iteration 187, loss = 0.27778054\n",
      "Iteration 188, loss = 0.27733863\n",
      "Iteration 189, loss = 0.27691034\n",
      "Iteration 190, loss = 0.27650162\n",
      "Iteration 191, loss = 0.27601772\n",
      "Iteration 192, loss = 0.27557148\n",
      "Iteration 193, loss = 0.27512953\n",
      "Iteration 194, loss = 0.27470346\n",
      "Iteration 195, loss = 0.27426543\n",
      "Iteration 196, loss = 0.27384178\n",
      "Iteration 197, loss = 0.27339405\n",
      "Iteration 198, loss = 0.27300552\n",
      "Iteration 199, loss = 0.27257071\n",
      "Iteration 200, loss = 0.27217738\n",
      "[CV] END activation=logistic, alpha=0.001, batch_size=200, hidden_layer_sizes=(100,), solver=sgd; total time= 1.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.13842514\n",
      "Iteration 2, loss = 0.37030496\n",
      "Iteration 3, loss = 0.26991451\n",
      "Iteration 4, loss = 0.22290166\n",
      "Iteration 5, loss = 0.18992757\n",
      "Iteration 6, loss = 0.16517144\n",
      "Iteration 7, loss = 0.14591554\n",
      "Iteration 8, loss = 0.12846698\n",
      "Iteration 9, loss = 0.11430538\n",
      "Iteration 10, loss = 0.10176179\n",
      "Iteration 11, loss = 0.09016085\n",
      "Iteration 12, loss = 0.08058491\n",
      "Iteration 13, loss = 0.07321526\n",
      "Iteration 14, loss = 0.06674535\n",
      "Iteration 15, loss = 0.05952077\n",
      "Iteration 16, loss = 0.05296964\n",
      "Iteration 17, loss = 0.04701629\n",
      "Iteration 18, loss = 0.04363634\n",
      "Iteration 19, loss = 0.03967663\n",
      "Iteration 20, loss = 0.03635545\n",
      "Iteration 21, loss = 0.03328244\n",
      "Iteration 22, loss = 0.03012469\n",
      "Iteration 23, loss = 0.02729054\n",
      "Iteration 24, loss = 0.02514448\n",
      "Iteration 25, loss = 0.02390532\n",
      "Iteration 26, loss = 0.02216788\n",
      "Iteration 27, loss = 0.02103323\n",
      "Iteration 28, loss = 0.01955103\n",
      "Iteration 29, loss = 0.01879325\n",
      "Iteration 30, loss = 0.01774625\n",
      "Iteration 31, loss = 0.01782027\n",
      "Iteration 32, loss = 0.01630032\n",
      "Iteration 33, loss = 0.01546185\n",
      "Iteration 34, loss = 0.01517979\n",
      "Iteration 35, loss = 0.01445652\n",
      "Iteration 36, loss = 0.01448550\n",
      "Iteration 37, loss = 0.01367257\n",
      "Iteration 38, loss = 0.01361137\n",
      "Iteration 39, loss = 0.01345636\n",
      "Iteration 40, loss = 0.01319497\n",
      "Iteration 41, loss = 0.01344185\n",
      "Iteration 42, loss = 0.01780936\n",
      "Iteration 43, loss = 0.01711580\n",
      "Iteration 44, loss = 0.01286858\n",
      "Iteration 45, loss = 0.01218110\n",
      "Iteration 46, loss = 0.01183381\n",
      "Iteration 47, loss = 0.01162155\n",
      "Iteration 48, loss = 0.01136195\n",
      "Iteration 49, loss = 0.01129474\n",
      "Iteration 50, loss = 0.01129456\n",
      "Iteration 51, loss = 0.01102618\n",
      "Iteration 52, loss = 0.01105642\n",
      "Iteration 53, loss = 0.02501125\n",
      "Iteration 54, loss = 0.01964155\n",
      "Iteration 55, loss = 0.01256861\n",
      "Iteration 56, loss = 0.01165265\n",
      "Iteration 57, loss = 0.01123288\n",
      "Iteration 58, loss = 0.01101802\n",
      "Iteration 59, loss = 0.01083820\n",
      "Iteration 60, loss = 0.01065567\n",
      "Iteration 61, loss = 0.01049299\n",
      "Iteration 62, loss = 0.01033682\n",
      "Iteration 63, loss = 0.01017050\n",
      "Iteration 64, loss = 0.01002423\n",
      "Iteration 65, loss = 0.00989368\n",
      "Iteration 66, loss = 0.00995338\n",
      "Iteration 67, loss = 0.01199376\n",
      "Iteration 68, loss = 0.02666258\n",
      "Iteration 69, loss = 0.01283919\n",
      "Iteration 70, loss = 0.01085118\n",
      "Iteration 71, loss = 0.01040041\n",
      "Iteration 72, loss = 0.01020411\n",
      "Iteration 73, loss = 0.01005189\n",
      "Iteration 74, loss = 0.00990336\n",
      "Iteration 75, loss = 0.00975655\n",
      "Iteration 76, loss = 0.00963455\n",
      "Iteration 77, loss = 0.00951172\n",
      "Iteration 78, loss = 0.00939374\n",
      "Iteration 79, loss = 0.00927188\n",
      "Iteration 80, loss = 0.00917956\n",
      "Iteration 81, loss = 0.00906994\n",
      "Iteration 82, loss = 0.00913153\n",
      "Iteration 83, loss = 0.01255354\n",
      "Iteration 84, loss = 0.02697267\n",
      "Iteration 85, loss = 0.01156435\n",
      "Iteration 86, loss = 0.00997120\n",
      "Iteration 87, loss = 0.00964253\n",
      "Iteration 88, loss = 0.00949147\n",
      "Iteration 89, loss = 0.00936549\n",
      "Iteration 90, loss = 0.00925390\n",
      "Iteration 91, loss = 0.00912056\n",
      "Iteration 92, loss = 0.00901336\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.001, batch_size=200, hidden_layer_sizes=(400, 100), solver=adam; total time= 3.5min\n",
      "Iteration 1, loss = 1.12209185\n",
      "Iteration 2, loss = 0.35322397\n",
      "Iteration 3, loss = 0.26057448\n",
      "Iteration 4, loss = 0.21501667\n",
      "Iteration 5, loss = 0.18523248\n",
      "Iteration 6, loss = 0.15960270\n",
      "Iteration 7, loss = 0.14109149\n",
      "Iteration 8, loss = 0.12449218\n",
      "Iteration 9, loss = 0.10994915\n",
      "Iteration 10, loss = 0.09860482\n",
      "Iteration 11, loss = 0.08723160\n",
      "Iteration 12, loss = 0.07883780\n",
      "Iteration 13, loss = 0.07058310\n",
      "Iteration 14, loss = 0.06275072\n",
      "Iteration 15, loss = 0.05705936\n",
      "Iteration 16, loss = 0.05152914\n",
      "Iteration 17, loss = 0.04579872\n",
      "Iteration 18, loss = 0.04191237\n",
      "Iteration 19, loss = 0.03789460\n",
      "Iteration 20, loss = 0.03397004\n",
      "Iteration 21, loss = 0.03149379\n",
      "Iteration 22, loss = 0.02880574\n",
      "Iteration 23, loss = 0.02670075\n",
      "Iteration 24, loss = 0.02476539\n",
      "Iteration 25, loss = 0.02292767\n",
      "Iteration 26, loss = 0.02115082\n",
      "Iteration 27, loss = 0.02029244\n",
      "Iteration 28, loss = 0.01893124\n",
      "Iteration 29, loss = 0.01804448\n",
      "Iteration 30, loss = 0.01707538\n",
      "Iteration 31, loss = 0.01650442\n",
      "Iteration 32, loss = 0.01601948\n",
      "Iteration 33, loss = 0.01513154\n",
      "Iteration 34, loss = 0.01482147\n",
      "Iteration 35, loss = 0.01428358\n",
      "Iteration 36, loss = 0.01416946\n",
      "Iteration 37, loss = 0.01357569\n",
      "Iteration 38, loss = 0.01331162\n",
      "Iteration 39, loss = 0.01311608\n",
      "Iteration 40, loss = 0.01824547\n",
      "Iteration 41, loss = 0.02749813\n",
      "Iteration 42, loss = 0.01444276\n",
      "Iteration 43, loss = 0.01302409\n",
      "Iteration 44, loss = 0.01257345\n",
      "Iteration 45, loss = 0.01230844\n",
      "Iteration 46, loss = 0.01209212\n",
      "Iteration 47, loss = 0.01183929\n",
      "Iteration 48, loss = 0.01163830\n",
      "Iteration 49, loss = 0.01151690\n",
      "Iteration 50, loss = 0.01131479\n",
      "Iteration 51, loss = 0.01116527\n",
      "Iteration 52, loss = 0.01105102\n",
      "Iteration 53, loss = 0.01107787\n",
      "Iteration 54, loss = 0.02745600\n",
      "Iteration 55, loss = 0.01558461\n",
      "Iteration 56, loss = 0.01199688\n",
      "Iteration 57, loss = 0.01135570\n",
      "Iteration 58, loss = 0.01112421\n",
      "Iteration 59, loss = 0.01093609\n",
      "Iteration 60, loss = 0.01075637\n",
      "Iteration 61, loss = 0.01058181\n",
      "Iteration 62, loss = 0.01043943\n",
      "Iteration 63, loss = 0.01028705\n",
      "Iteration 64, loss = 0.01014411\n",
      "Iteration 65, loss = 0.00999877\n",
      "Iteration 66, loss = 0.00990187\n",
      "Iteration 67, loss = 0.00979168\n",
      "Iteration 68, loss = 0.01982085\n",
      "Iteration 69, loss = 0.02451617\n",
      "Iteration 70, loss = 0.01218111\n",
      "Iteration 71, loss = 0.01079844\n",
      "Iteration 72, loss = 0.01051770\n",
      "Iteration 73, loss = 0.01032668\n",
      "Iteration 74, loss = 0.01017057\n",
      "Iteration 75, loss = 0.01001746\n",
      "Iteration 76, loss = 0.00986692\n",
      "Iteration 77, loss = 0.00973036\n",
      "Iteration 78, loss = 0.00960095\n",
      "Iteration 79, loss = 0.00948157\n",
      "Iteration 80, loss = 0.00936192\n",
      "Iteration 81, loss = 0.00930038\n",
      "Iteration 82, loss = 0.00922092\n",
      "Iteration 83, loss = 0.00920574\n",
      "Iteration 84, loss = 0.03163176\n",
      "Iteration 85, loss = 0.01475020\n",
      "Iteration 86, loss = 0.01052246\n",
      "Iteration 87, loss = 0.01006882\n",
      "Iteration 88, loss = 0.00988426\n",
      "Iteration 89, loss = 0.00972630\n",
      "Iteration 90, loss = 0.00959257\n",
      "Iteration 91, loss = 0.00947418\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.001, batch_size=200, hidden_layer_sizes=(400, 100), solver=adam; total time= 3.4min\n",
      "Iteration 1, loss = 1.11803296\n",
      "Iteration 2, loss = 0.35810397\n",
      "Iteration 3, loss = 0.26205929\n",
      "Iteration 4, loss = 0.21566559\n",
      "Iteration 5, loss = 0.18435144\n",
      "Iteration 6, loss = 0.15882888\n",
      "Iteration 7, loss = 0.13930352\n",
      "Iteration 8, loss = 0.12384205\n",
      "Iteration 9, loss = 0.10952848\n",
      "Iteration 10, loss = 0.09742440\n",
      "Iteration 11, loss = 0.08645852\n",
      "Iteration 12, loss = 0.07817717\n",
      "Iteration 13, loss = 0.06993233\n",
      "Iteration 14, loss = 0.06399930\n",
      "Iteration 15, loss = 0.05600028\n",
      "Iteration 16, loss = 0.05050608\n",
      "Iteration 17, loss = 0.04689555\n",
      "Iteration 18, loss = 0.04151812\n",
      "Iteration 19, loss = 0.03809831\n",
      "Iteration 20, loss = 0.03625246\n",
      "Iteration 21, loss = 0.03198731\n",
      "Iteration 22, loss = 0.02931557\n",
      "Iteration 23, loss = 0.02686760\n",
      "Iteration 24, loss = 0.02496408\n",
      "Iteration 25, loss = 0.02362576\n",
      "Iteration 26, loss = 0.02160947\n",
      "Iteration 27, loss = 0.02070404\n",
      "Iteration 28, loss = 0.01936595\n",
      "Iteration 29, loss = 0.01822547\n",
      "Iteration 30, loss = 0.01761039\n",
      "Iteration 31, loss = 0.01642467\n",
      "Iteration 32, loss = 0.01587529\n",
      "Iteration 33, loss = 0.01541182\n",
      "Iteration 34, loss = 0.01510984\n",
      "Iteration 35, loss = 0.01453568\n",
      "Iteration 36, loss = 0.01397621\n",
      "Iteration 37, loss = 0.01386625\n",
      "Iteration 38, loss = 0.01334525\n",
      "Iteration 39, loss = 0.01333950\n",
      "Iteration 40, loss = 0.01283212\n",
      "Iteration 41, loss = 0.01276843\n",
      "Iteration 42, loss = 0.01259345\n",
      "Iteration 43, loss = 0.02285781\n",
      "Iteration 44, loss = 0.01473068\n",
      "Iteration 45, loss = 0.01258325\n",
      "Iteration 46, loss = 0.01203569\n",
      "Iteration 47, loss = 0.01173647\n",
      "Iteration 48, loss = 0.01144283\n",
      "Iteration 49, loss = 0.01125676\n",
      "Iteration 50, loss = 0.01108208\n",
      "Iteration 51, loss = 0.01088948\n",
      "Iteration 52, loss = 0.01079153\n",
      "Iteration 53, loss = 0.01085836\n",
      "Iteration 54, loss = 0.02568098\n",
      "Iteration 55, loss = 0.02158662\n",
      "Iteration 56, loss = 0.01293125\n",
      "Iteration 57, loss = 0.01158235\n",
      "Iteration 58, loss = 0.01130598\n",
      "Iteration 59, loss = 0.01111513\n",
      "Iteration 60, loss = 0.01091156\n",
      "Iteration 61, loss = 0.01073330\n",
      "Iteration 62, loss = 0.01058452\n",
      "Iteration 63, loss = 0.01043782\n",
      "Iteration 64, loss = 0.01026367\n",
      "Iteration 65, loss = 0.01016062\n",
      "Iteration 66, loss = 0.01000312\n",
      "Iteration 67, loss = 0.00990478\n",
      "Iteration 68, loss = 0.00982804\n",
      "Iteration 69, loss = 0.00968567\n",
      "Iteration 70, loss = 0.00955627\n",
      "Iteration 71, loss = 0.02493881\n",
      "Iteration 72, loss = 0.02271092\n",
      "Iteration 73, loss = 0.01332400\n",
      "Iteration 74, loss = 0.01082357\n",
      "Iteration 75, loss = 0.01044149\n",
      "Iteration 76, loss = 0.01025110\n",
      "Iteration 77, loss = 0.01009196\n",
      "Iteration 78, loss = 0.00995620\n",
      "Iteration 79, loss = 0.00981000\n",
      "Iteration 80, loss = 0.00967411\n",
      "Iteration 81, loss = 0.00954450\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.001, batch_size=200, hidden_layer_sizes=(400, 100), solver=adam; total time= 3.1min\n",
      "Iteration 1, loss = 1.10638955\n",
      "Iteration 2, loss = 0.35969056\n",
      "Iteration 3, loss = 0.26289700\n",
      "Iteration 4, loss = 0.21732176\n",
      "Iteration 5, loss = 0.18522433\n",
      "Iteration 6, loss = 0.16184438\n",
      "Iteration 7, loss = 0.14210364\n",
      "Iteration 8, loss = 0.12557324\n",
      "Iteration 9, loss = 0.11117613\n",
      "Iteration 10, loss = 0.09969089\n",
      "Iteration 11, loss = 0.08921012\n",
      "Iteration 12, loss = 0.07932708\n",
      "Iteration 13, loss = 0.07178668\n",
      "Iteration 14, loss = 0.06575354\n",
      "Iteration 15, loss = 0.05805020\n",
      "Iteration 16, loss = 0.05235954\n",
      "Iteration 17, loss = 0.04746119\n",
      "Iteration 18, loss = 0.04311251\n",
      "Iteration 19, loss = 0.03898512\n",
      "Iteration 20, loss = 0.03587183\n",
      "Iteration 21, loss = 0.03220323\n",
      "Iteration 22, loss = 0.02948059\n",
      "Iteration 23, loss = 0.02759781\n",
      "Iteration 24, loss = 0.02505669\n",
      "Iteration 25, loss = 0.02400935\n",
      "Iteration 26, loss = 0.02209165\n",
      "Iteration 27, loss = 0.02056038\n",
      "Iteration 28, loss = 0.01979278\n",
      "Iteration 29, loss = 0.01835937\n",
      "Iteration 30, loss = 0.01736325\n",
      "Iteration 31, loss = 0.01667913\n",
      "Iteration 32, loss = 0.01613065\n",
      "Iteration 33, loss = 0.01568689\n",
      "Iteration 34, loss = 0.01526447\n",
      "Iteration 35, loss = 0.01513486\n",
      "Iteration 36, loss = 0.01445203\n",
      "Iteration 37, loss = 0.01376151\n",
      "Iteration 38, loss = 0.01348855\n",
      "Iteration 39, loss = 0.01342129\n",
      "Iteration 40, loss = 0.01334121\n",
      "Iteration 41, loss = 0.01562110\n",
      "Iteration 42, loss = 0.01766116\n",
      "Iteration 43, loss = 0.01439100\n",
      "Iteration 44, loss = 0.01396208\n",
      "Iteration 45, loss = 0.01226877\n",
      "Iteration 46, loss = 0.01187470\n",
      "Iteration 47, loss = 0.01158542\n",
      "Iteration 48, loss = 0.01133208\n",
      "Iteration 49, loss = 0.01114236\n",
      "Iteration 50, loss = 0.01102831\n",
      "Iteration 51, loss = 0.01114455\n",
      "Iteration 52, loss = 0.01510075\n",
      "Iteration 53, loss = 0.02838672\n",
      "Iteration 54, loss = 0.01340382\n",
      "Iteration 55, loss = 0.01174500\n",
      "Iteration 56, loss = 0.01132904\n",
      "Iteration 57, loss = 0.01110859\n",
      "Iteration 58, loss = 0.01092646\n",
      "Iteration 59, loss = 0.01073907\n",
      "Iteration 60, loss = 0.01057828\n",
      "Iteration 61, loss = 0.01041856\n",
      "Iteration 62, loss = 0.01025631\n",
      "Iteration 63, loss = 0.01012730\n",
      "Iteration 64, loss = 0.01000622\n",
      "Iteration 65, loss = 0.00991767\n",
      "Iteration 66, loss = 0.02757711\n",
      "Iteration 67, loss = 0.01683958\n",
      "Iteration 68, loss = 0.01183724\n",
      "Iteration 69, loss = 0.01074779\n",
      "Iteration 70, loss = 0.01048090\n",
      "Iteration 71, loss = 0.01031902\n",
      "Iteration 72, loss = 0.01016398\n",
      "Iteration 73, loss = 0.01002198\n",
      "Iteration 74, loss = 0.00988440\n",
      "Iteration 75, loss = 0.00977213\n",
      "Iteration 76, loss = 0.00965978\n",
      "Iteration 77, loss = 0.00952768\n",
      "Iteration 78, loss = 0.00940970\n",
      "Iteration 79, loss = 0.00929857\n",
      "Iteration 80, loss = 0.00924818\n",
      "Iteration 81, loss = 0.00916007\n",
      "Iteration 82, loss = 0.02565316\n",
      "Iteration 83, loss = 0.01961540\n",
      "Iteration 84, loss = 0.01209381\n",
      "Iteration 85, loss = 0.01037293\n",
      "Iteration 86, loss = 0.00999974\n",
      "Iteration 87, loss = 0.00981483\n",
      "Iteration 88, loss = 0.00967753\n",
      "Iteration 89, loss = 0.00954328\n",
      "Iteration 90, loss = 0.00941861\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.001, batch_size=200, hidden_layer_sizes=(400, 100), solver=adam; total time= 3.4min\n",
      "Iteration 1, loss = 1.12244185\n",
      "Iteration 2, loss = 0.36172321\n",
      "Iteration 3, loss = 0.26299657\n",
      "Iteration 4, loss = 0.21834795\n",
      "Iteration 5, loss = 0.18566226\n",
      "Iteration 6, loss = 0.16158783\n",
      "Iteration 7, loss = 0.14204018\n",
      "Iteration 8, loss = 0.12482709\n",
      "Iteration 9, loss = 0.11001888\n",
      "Iteration 10, loss = 0.09870737\n",
      "Iteration 11, loss = 0.08812572\n",
      "Iteration 12, loss = 0.07846825\n",
      "Iteration 13, loss = 0.07054237\n",
      "Iteration 14, loss = 0.06370779\n",
      "Iteration 15, loss = 0.05718442\n",
      "Iteration 16, loss = 0.05188376\n",
      "Iteration 17, loss = 0.04639427\n",
      "Iteration 18, loss = 0.04147398\n",
      "Iteration 19, loss = 0.03756969\n",
      "Iteration 20, loss = 0.03427094\n",
      "Iteration 21, loss = 0.03159605\n",
      "Iteration 22, loss = 0.02916151\n",
      "Iteration 23, loss = 0.02631330\n",
      "Iteration 24, loss = 0.02395252\n",
      "Iteration 25, loss = 0.02256151\n",
      "Iteration 26, loss = 0.02116570\n",
      "Iteration 27, loss = 0.01996984\n",
      "Iteration 28, loss = 0.01891142\n",
      "Iteration 29, loss = 0.01819923\n",
      "Iteration 30, loss = 0.01727136\n",
      "Iteration 31, loss = 0.01653854\n",
      "Iteration 32, loss = 0.01567246\n",
      "Iteration 33, loss = 0.01524212\n",
      "Iteration 34, loss = 0.01481332\n",
      "Iteration 35, loss = 0.01452273\n",
      "Iteration 36, loss = 0.01400376\n",
      "Iteration 37, loss = 0.01393483\n",
      "Iteration 38, loss = 0.01363778\n",
      "Iteration 39, loss = 0.01374470\n",
      "Iteration 40, loss = 0.01768440\n",
      "Iteration 41, loss = 0.01561456\n",
      "Iteration 42, loss = 0.01329630\n",
      "Iteration 43, loss = 0.01231871\n",
      "Iteration 44, loss = 0.01194956\n",
      "Iteration 45, loss = 0.01170350\n",
      "Iteration 46, loss = 0.01153292\n",
      "Iteration 47, loss = 0.01133450\n",
      "Iteration 48, loss = 0.01118823\n",
      "Iteration 49, loss = 0.01124190\n",
      "Iteration 50, loss = 0.02910384\n",
      "Iteration 51, loss = 0.01813823\n",
      "Iteration 52, loss = 0.01294002\n",
      "Iteration 53, loss = 0.01167761\n",
      "Iteration 54, loss = 0.01138759\n",
      "Iteration 55, loss = 0.01119515\n",
      "Iteration 56, loss = 0.01103400\n",
      "Iteration 57, loss = 0.01085266\n",
      "Iteration 58, loss = 0.01069901\n",
      "Iteration 59, loss = 0.01051967\n",
      "Iteration 60, loss = 0.01036731\n",
      "Iteration 61, loss = 0.01024788\n",
      "Iteration 62, loss = 0.01015167\n",
      "Iteration 63, loss = 0.01021705\n",
      "Iteration 64, loss = 0.00993929\n",
      "Iteration 65, loss = 0.00986194\n",
      "Iteration 66, loss = 0.00987208\n",
      "Iteration 67, loss = 0.01315272\n",
      "Iteration 68, loss = 0.03283662\n",
      "Iteration 69, loss = 0.01252197\n",
      "Iteration 70, loss = 0.01081548\n",
      "Iteration 71, loss = 0.01047504\n",
      "Iteration 72, loss = 0.01029632\n",
      "Iteration 73, loss = 0.01013613\n",
      "Iteration 74, loss = 0.00998545\n",
      "Iteration 75, loss = 0.00985169\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.001, batch_size=200, hidden_layer_sizes=(400, 100), solver=adam; total time= 2.8min\n",
      "Iteration 1, loss = 2.30530458\n",
      "Iteration 2, loss = 2.29790730\n",
      "Iteration 3, loss = 2.29430948\n",
      "Iteration 4, loss = 2.29060950\n",
      "Iteration 5, loss = 2.28679809\n",
      "Iteration 6, loss = 2.28273293\n",
      "Iteration 7, loss = 2.27842369\n",
      "Iteration 8, loss = 2.27373334\n",
      "Iteration 9, loss = 2.26861521\n",
      "Iteration 10, loss = 2.26292837\n",
      "Iteration 11, loss = 2.25656837\n",
      "Iteration 12, loss = 2.24952528\n",
      "Iteration 13, loss = 2.24128217\n",
      "Iteration 14, loss = 2.23213312\n",
      "Iteration 15, loss = 2.22142069\n",
      "Iteration 16, loss = 2.20894232\n",
      "Iteration 17, loss = 2.19435104\n",
      "Iteration 18, loss = 2.17702873\n",
      "Iteration 19, loss = 2.15670731\n",
      "Iteration 20, loss = 2.13252351\n",
      "Iteration 21, loss = 2.10395422\n",
      "Iteration 22, loss = 2.07002980\n",
      "Iteration 23, loss = 2.03039051\n",
      "Iteration 24, loss = 1.98426045\n",
      "Iteration 25, loss = 1.93203572\n",
      "Iteration 26, loss = 1.87410386\n",
      "Iteration 27, loss = 1.81159062\n",
      "Iteration 28, loss = 1.74620920\n",
      "Iteration 29, loss = 1.67993943\n",
      "Iteration 30, loss = 1.61438237\n",
      "Iteration 31, loss = 1.55122897\n",
      "Iteration 32, loss = 1.49106177\n",
      "Iteration 33, loss = 1.43439778\n",
      "Iteration 34, loss = 1.38100084\n",
      "Iteration 35, loss = 1.33090560\n",
      "Iteration 36, loss = 1.28377710\n",
      "Iteration 37, loss = 1.23972555\n",
      "Iteration 38, loss = 1.19810960\n",
      "Iteration 39, loss = 1.15940804\n",
      "Iteration 40, loss = 1.12329805\n",
      "Iteration 41, loss = 1.08958758\n",
      "Iteration 42, loss = 1.05827864\n",
      "Iteration 43, loss = 1.02921934\n",
      "Iteration 44, loss = 1.00224057\n",
      "Iteration 45, loss = 0.97698569\n",
      "Iteration 46, loss = 0.95356233\n",
      "Iteration 47, loss = 0.93165163\n",
      "Iteration 48, loss = 0.91102872\n",
      "Iteration 49, loss = 0.89168098\n",
      "Iteration 50, loss = 0.87326979\n",
      "Iteration 51, loss = 0.85576458\n",
      "Iteration 52, loss = 0.83923919\n",
      "Iteration 53, loss = 0.82348537\n",
      "Iteration 54, loss = 0.80829253\n",
      "Iteration 55, loss = 0.79368995\n",
      "Iteration 56, loss = 0.77970659\n",
      "Iteration 57, loss = 0.76615174\n",
      "Iteration 58, loss = 0.75321187\n",
      "Iteration 59, loss = 0.74064717\n",
      "Iteration 60, loss = 0.72841034\n",
      "Iteration 61, loss = 0.71658122\n",
      "Iteration 62, loss = 0.70516913\n",
      "Iteration 63, loss = 0.69400830\n",
      "Iteration 64, loss = 0.68336895\n",
      "Iteration 65, loss = 0.67298581\n",
      "Iteration 66, loss = 0.66292268\n",
      "Iteration 67, loss = 0.65319568\n",
      "Iteration 68, loss = 0.64378317\n",
      "Iteration 69, loss = 0.63480798\n",
      "Iteration 70, loss = 0.62597845\n",
      "Iteration 71, loss = 0.61760528\n",
      "Iteration 72, loss = 0.60928321\n",
      "Iteration 73, loss = 0.60138772\n",
      "Iteration 74, loss = 0.59383410\n",
      "Iteration 75, loss = 0.58637742\n",
      "Iteration 76, loss = 0.57927057\n",
      "Iteration 77, loss = 0.57238403\n",
      "Iteration 78, loss = 0.56572786\n",
      "Iteration 79, loss = 0.55933942\n",
      "Iteration 80, loss = 0.55306320\n",
      "Iteration 81, loss = 0.54711814\n",
      "Iteration 82, loss = 0.54123411\n",
      "Iteration 83, loss = 0.53571325\n",
      "Iteration 84, loss = 0.53016949\n",
      "Iteration 85, loss = 0.52505431\n",
      "Iteration 86, loss = 0.51986439\n",
      "Iteration 87, loss = 0.51492722\n",
      "Iteration 88, loss = 0.51017208\n",
      "Iteration 89, loss = 0.50544879\n",
      "Iteration 90, loss = 0.50099817\n",
      "Iteration 91, loss = 0.49659802\n",
      "Iteration 92, loss = 0.49231126\n",
      "Iteration 93, loss = 0.48827836\n",
      "Iteration 94, loss = 0.48428136\n",
      "Iteration 95, loss = 0.48038919\n",
      "Iteration 96, loss = 0.47661270\n",
      "Iteration 97, loss = 0.47296479\n",
      "Iteration 98, loss = 0.46941823\n",
      "Iteration 99, loss = 0.46595125\n",
      "Iteration 100, loss = 0.46265113\n",
      "Iteration 101, loss = 0.45934711\n",
      "Iteration 102, loss = 0.45616918\n",
      "Iteration 103, loss = 0.45304301\n",
      "Iteration 104, loss = 0.45002532\n",
      "Iteration 105, loss = 0.44711531\n",
      "Iteration 106, loss = 0.44432072\n",
      "Iteration 107, loss = 0.44154387\n",
      "Iteration 108, loss = 0.43889928\n",
      "Iteration 109, loss = 0.43615426\n",
      "Iteration 110, loss = 0.43366181\n",
      "Iteration 111, loss = 0.43118414\n",
      "Iteration 112, loss = 0.42868046\n",
      "Iteration 113, loss = 0.42643203\n",
      "Iteration 114, loss = 0.42411270\n",
      "Iteration 115, loss = 0.42184984\n",
      "Iteration 116, loss = 0.41972063\n",
      "Iteration 117, loss = 0.41752550\n",
      "Iteration 118, loss = 0.41548359\n",
      "Iteration 119, loss = 0.41351972\n",
      "Iteration 120, loss = 0.41148660\n",
      "Iteration 121, loss = 0.40953128\n",
      "Iteration 122, loss = 0.40771978\n",
      "Iteration 123, loss = 0.40583409\n",
      "Iteration 124, loss = 0.40400963\n",
      "Iteration 125, loss = 0.40232587\n",
      "Iteration 126, loss = 0.40054187\n",
      "Iteration 127, loss = 0.39894744\n",
      "Iteration 128, loss = 0.39724895\n",
      "Iteration 129, loss = 0.39569972\n",
      "Iteration 130, loss = 0.39418777\n",
      "Iteration 131, loss = 0.39263104\n",
      "Iteration 132, loss = 0.39110403\n",
      "Iteration 133, loss = 0.38964222\n",
      "Iteration 134, loss = 0.38822493\n",
      "Iteration 135, loss = 0.38682000\n",
      "Iteration 136, loss = 0.38544232\n",
      "Iteration 137, loss = 0.38411912\n",
      "Iteration 138, loss = 0.38281215\n",
      "Iteration 139, loss = 0.38148610\n",
      "Iteration 140, loss = 0.38019682\n",
      "Iteration 141, loss = 0.37902636\n",
      "Iteration 142, loss = 0.37780322\n",
      "Iteration 143, loss = 0.37656242\n",
      "Iteration 144, loss = 0.37545107\n",
      "Iteration 145, loss = 0.37428130\n",
      "Iteration 146, loss = 0.37315818\n",
      "Iteration 147, loss = 0.37200540\n",
      "Iteration 148, loss = 0.37086751\n",
      "Iteration 149, loss = 0.36987924\n",
      "Iteration 150, loss = 0.36880435\n",
      "Iteration 151, loss = 0.36776168\n",
      "Iteration 152, loss = 0.36672707\n",
      "Iteration 153, loss = 0.36571816\n",
      "Iteration 154, loss = 0.36470589\n",
      "Iteration 155, loss = 0.36384026\n",
      "Iteration 156, loss = 0.36276358\n",
      "Iteration 157, loss = 0.36183218\n",
      "Iteration 158, loss = 0.36093346\n",
      "Iteration 159, loss = 0.36005938\n",
      "Iteration 160, loss = 0.35908968\n",
      "Iteration 161, loss = 0.35820980\n",
      "Iteration 162, loss = 0.35729144\n",
      "Iteration 163, loss = 0.35649226\n",
      "Iteration 164, loss = 0.35561320\n",
      "Iteration 165, loss = 0.35478969\n",
      "Iteration 166, loss = 0.35384788\n",
      "Iteration 167, loss = 0.35310740\n",
      "Iteration 168, loss = 0.35230487\n",
      "Iteration 169, loss = 0.35147029\n",
      "Iteration 170, loss = 0.35073356\n",
      "Iteration 171, loss = 0.34989237\n",
      "Iteration 172, loss = 0.34911928\n",
      "Iteration 173, loss = 0.34841270\n",
      "Iteration 174, loss = 0.34762815\n",
      "Iteration 175, loss = 0.34692402\n",
      "Iteration 176, loss = 0.34620688\n",
      "Iteration 177, loss = 0.34537331\n",
      "Iteration 178, loss = 0.34469783\n",
      "Iteration 179, loss = 0.34393529\n",
      "Iteration 180, loss = 0.34334319\n",
      "Iteration 181, loss = 0.34253629\n",
      "Iteration 182, loss = 0.34191160\n",
      "Iteration 183, loss = 0.34123620\n",
      "Iteration 184, loss = 0.34050773\n",
      "Iteration 185, loss = 0.33987261\n",
      "Iteration 186, loss = 0.33914584\n",
      "Iteration 187, loss = 0.33857128\n",
      "Iteration 188, loss = 0.33789452\n",
      "Iteration 189, loss = 0.33722504\n",
      "Iteration 190, loss = 0.33663334\n",
      "Iteration 191, loss = 0.33593768\n",
      "Iteration 192, loss = 0.33529011\n",
      "Iteration 193, loss = 0.33472600\n",
      "Iteration 194, loss = 0.33406790\n",
      "Iteration 195, loss = 0.33346216\n",
      "Iteration 196, loss = 0.33287759\n",
      "Iteration 197, loss = 0.33231641\n",
      "Iteration 198, loss = 0.33170534\n",
      "Iteration 199, loss = 0.33101552\n",
      "Iteration 200, loss = 0.33053799\n",
      "[CV] END activation=logistic, alpha=0.001, batch_size=200, hidden_layer_sizes=(400, 100), solver=sgd; total time= 6.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.30802677\n",
      "Iteration 2, loss = 2.29763633\n",
      "Iteration 3, loss = 2.29422378\n",
      "Iteration 4, loss = 2.29048390\n",
      "Iteration 5, loss = 2.28662988\n",
      "Iteration 6, loss = 2.28276670\n",
      "Iteration 7, loss = 2.27835300\n",
      "Iteration 8, loss = 2.27365998\n",
      "Iteration 9, loss = 2.26855047\n",
      "Iteration 10, loss = 2.26275571\n",
      "Iteration 11, loss = 2.25624256\n",
      "Iteration 12, loss = 2.24910768\n",
      "Iteration 13, loss = 2.24060312\n",
      "Iteration 14, loss = 2.23111821\n",
      "Iteration 15, loss = 2.21983430\n",
      "Iteration 16, loss = 2.20676431\n",
      "Iteration 17, loss = 2.19126547\n",
      "Iteration 18, loss = 2.17317675\n",
      "Iteration 19, loss = 2.15196421\n",
      "Iteration 20, loss = 2.12690247\n",
      "Iteration 21, loss = 2.09770619\n",
      "Iteration 22, loss = 2.06435789\n",
      "Iteration 23, loss = 2.02671095\n",
      "Iteration 24, loss = 1.98513547\n",
      "Iteration 25, loss = 1.94032602\n",
      "Iteration 26, loss = 1.89274687\n",
      "Iteration 27, loss = 1.84310962\n",
      "Iteration 28, loss = 1.79187104\n",
      "Iteration 29, loss = 1.73923813\n",
      "Iteration 30, loss = 1.68520481\n",
      "Iteration 31, loss = 1.62929424\n",
      "Iteration 32, loss = 1.57229700\n",
      "Iteration 33, loss = 1.51414048\n",
      "Iteration 34, loss = 1.45583754\n",
      "Iteration 35, loss = 1.39817941\n",
      "Iteration 36, loss = 1.34212208\n",
      "Iteration 37, loss = 1.28877761\n",
      "Iteration 38, loss = 1.23832461\n",
      "Iteration 39, loss = 1.19150741\n",
      "Iteration 40, loss = 1.14822870\n",
      "Iteration 41, loss = 1.10844866\n",
      "Iteration 42, loss = 1.07190234\n",
      "Iteration 43, loss = 1.03824332\n",
      "Iteration 44, loss = 1.00734555\n",
      "Iteration 45, loss = 0.97861493\n",
      "Iteration 46, loss = 0.95202051\n",
      "Iteration 47, loss = 0.92727083\n",
      "Iteration 48, loss = 0.90402144\n",
      "Iteration 49, loss = 0.88220804\n",
      "Iteration 50, loss = 0.86183977\n",
      "Iteration 51, loss = 0.84243438\n",
      "Iteration 52, loss = 0.82422013\n",
      "Iteration 53, loss = 0.80671806\n",
      "Iteration 54, loss = 0.79014176\n",
      "Iteration 55, loss = 0.77438611\n",
      "Iteration 56, loss = 0.75942528\n",
      "Iteration 57, loss = 0.74505455\n",
      "Iteration 58, loss = 0.73134490\n",
      "Iteration 59, loss = 0.71829723\n",
      "Iteration 60, loss = 0.70582242\n",
      "Iteration 61, loss = 0.69388218\n",
      "Iteration 62, loss = 0.68247703\n",
      "Iteration 63, loss = 0.67153822\n",
      "Iteration 64, loss = 0.66104054\n",
      "Iteration 65, loss = 0.65108215\n",
      "Iteration 66, loss = 0.64135798\n",
      "Iteration 67, loss = 0.63220942\n",
      "Iteration 68, loss = 0.62350055\n",
      "Iteration 69, loss = 0.61514203\n",
      "Iteration 70, loss = 0.60701542\n",
      "Iteration 71, loss = 0.59938066\n",
      "Iteration 72, loss = 0.59188155\n",
      "Iteration 73, loss = 0.58477799\n",
      "Iteration 74, loss = 0.57781559\n",
      "Iteration 75, loss = 0.57140423\n",
      "Iteration 76, loss = 0.56502419\n",
      "Iteration 77, loss = 0.55897118\n",
      "Iteration 78, loss = 0.55308831\n",
      "Iteration 79, loss = 0.54738198\n",
      "Iteration 80, loss = 0.54194221\n",
      "Iteration 81, loss = 0.53676971\n",
      "Iteration 82, loss = 0.53158160\n",
      "Iteration 83, loss = 0.52659889\n",
      "Iteration 84, loss = 0.52189496\n",
      "Iteration 85, loss = 0.51720653\n",
      "Iteration 86, loss = 0.51282183\n",
      "Iteration 87, loss = 0.50845580\n",
      "Iteration 88, loss = 0.50433728\n",
      "Iteration 89, loss = 0.50022335\n",
      "Iteration 90, loss = 0.49629999\n",
      "Iteration 91, loss = 0.49243767\n",
      "Iteration 92, loss = 0.48877093\n",
      "Iteration 93, loss = 0.48513149\n",
      "Iteration 94, loss = 0.48167173\n",
      "Iteration 95, loss = 0.47823155\n",
      "Iteration 96, loss = 0.47489739\n",
      "Iteration 97, loss = 0.47174886\n",
      "Iteration 98, loss = 0.46855579\n",
      "Iteration 99, loss = 0.46549795\n",
      "Iteration 100, loss = 0.46253116\n",
      "Iteration 101, loss = 0.45968862\n",
      "Iteration 102, loss = 0.45685917\n",
      "Iteration 103, loss = 0.45409432\n",
      "Iteration 104, loss = 0.45155043\n",
      "Iteration 105, loss = 0.44890533\n",
      "Iteration 106, loss = 0.44637156\n",
      "Iteration 107, loss = 0.44391777\n",
      "Iteration 108, loss = 0.44154309\n",
      "Iteration 109, loss = 0.43913787\n",
      "Iteration 110, loss = 0.43695434\n",
      "Iteration 111, loss = 0.43467529\n",
      "Iteration 112, loss = 0.43247570\n",
      "Iteration 113, loss = 0.43035335\n",
      "Iteration 114, loss = 0.42823587\n",
      "Iteration 115, loss = 0.42620744\n",
      "Iteration 116, loss = 0.42430813\n",
      "Iteration 117, loss = 0.42229356\n",
      "Iteration 118, loss = 0.42038589\n",
      "Iteration 119, loss = 0.41855935\n",
      "Iteration 120, loss = 0.41676468\n",
      "Iteration 121, loss = 0.41500185\n",
      "Iteration 122, loss = 0.41324558\n",
      "Iteration 123, loss = 0.41153687\n",
      "Iteration 124, loss = 0.40979455\n",
      "Iteration 125, loss = 0.40821418\n",
      "Iteration 126, loss = 0.40663346\n",
      "Iteration 127, loss = 0.40508182\n",
      "Iteration 128, loss = 0.40354162\n",
      "Iteration 129, loss = 0.40195935\n",
      "Iteration 130, loss = 0.40050416\n",
      "Iteration 131, loss = 0.39904217\n",
      "Iteration 132, loss = 0.39762545\n",
      "Iteration 133, loss = 0.39618885\n",
      "Iteration 134, loss = 0.39487445\n",
      "Iteration 135, loss = 0.39349076\n",
      "Iteration 136, loss = 0.39222222\n",
      "Iteration 137, loss = 0.39083357\n",
      "Iteration 138, loss = 0.38959194\n",
      "Iteration 139, loss = 0.38823288\n",
      "Iteration 140, loss = 0.38701723\n",
      "Iteration 141, loss = 0.38577421\n",
      "Iteration 142, loss = 0.38455864\n",
      "Iteration 143, loss = 0.38341644\n",
      "Iteration 144, loss = 0.38223636\n",
      "Iteration 145, loss = 0.38105364\n",
      "Iteration 146, loss = 0.37990590\n",
      "Iteration 147, loss = 0.37883338\n",
      "Iteration 148, loss = 0.37768856\n",
      "Iteration 149, loss = 0.37659804\n",
      "Iteration 150, loss = 0.37550680\n",
      "Iteration 151, loss = 0.37447784\n",
      "Iteration 152, loss = 0.37347282\n",
      "Iteration 153, loss = 0.37247088\n",
      "Iteration 154, loss = 0.37145163\n",
      "Iteration 155, loss = 0.37042119\n",
      "Iteration 156, loss = 0.36943206\n",
      "Iteration 157, loss = 0.36847947\n",
      "Iteration 158, loss = 0.36742725\n",
      "Iteration 159, loss = 0.36663412\n",
      "Iteration 160, loss = 0.36553261\n",
      "Iteration 161, loss = 0.36467683\n",
      "Iteration 162, loss = 0.36375203\n",
      "Iteration 163, loss = 0.36283175\n",
      "Iteration 164, loss = 0.36198101\n",
      "Iteration 165, loss = 0.36110696\n",
      "Iteration 166, loss = 0.36015505\n",
      "Iteration 167, loss = 0.35932830\n",
      "Iteration 168, loss = 0.35849959\n",
      "Iteration 169, loss = 0.35770160\n",
      "Iteration 170, loss = 0.35675641\n",
      "Iteration 171, loss = 0.35599112\n",
      "Iteration 172, loss = 0.35517530\n",
      "Iteration 173, loss = 0.35439260\n",
      "Iteration 174, loss = 0.35361342\n",
      "Iteration 175, loss = 0.35278996\n",
      "Iteration 176, loss = 0.35204810\n",
      "Iteration 177, loss = 0.35125903\n",
      "Iteration 178, loss = 0.35051680\n",
      "Iteration 179, loss = 0.34972204\n",
      "Iteration 180, loss = 0.34898843\n",
      "Iteration 181, loss = 0.34823496\n",
      "Iteration 182, loss = 0.34752447\n",
      "Iteration 183, loss = 0.34683164\n",
      "Iteration 184, loss = 0.34604725\n",
      "Iteration 185, loss = 0.34533990\n",
      "Iteration 186, loss = 0.34459460\n",
      "Iteration 187, loss = 0.34394531\n",
      "Iteration 188, loss = 0.34320510\n",
      "Iteration 189, loss = 0.34256106\n",
      "Iteration 190, loss = 0.34180513\n",
      "Iteration 191, loss = 0.34122936\n",
      "Iteration 192, loss = 0.34046966\n",
      "Iteration 193, loss = 0.33984376\n",
      "Iteration 194, loss = 0.33915277\n",
      "Iteration 195, loss = 0.33855030\n",
      "Iteration 196, loss = 0.33790526\n",
      "Iteration 197, loss = 0.33720065\n",
      "Iteration 198, loss = 0.33666731\n",
      "Iteration 199, loss = 0.33597107\n",
      "Iteration 200, loss = 0.33534729\n",
      "[CV] END activation=logistic, alpha=0.001, batch_size=200, hidden_layer_sizes=(400, 100), solver=sgd; total time= 6.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.31909309\n",
      "Iteration 2, loss = 2.29800784\n",
      "Iteration 3, loss = 2.29436894\n",
      "Iteration 4, loss = 2.29070741\n",
      "Iteration 5, loss = 2.28690543\n",
      "Iteration 6, loss = 2.28281976\n",
      "Iteration 7, loss = 2.27849073\n",
      "Iteration 8, loss = 2.27373166\n",
      "Iteration 9, loss = 2.26866346\n",
      "Iteration 10, loss = 2.26289727\n",
      "Iteration 11, loss = 2.25653821\n",
      "Iteration 12, loss = 2.24933745\n",
      "Iteration 13, loss = 2.24116902\n",
      "Iteration 14, loss = 2.23179723\n",
      "Iteration 15, loss = 2.22087309\n",
      "Iteration 16, loss = 2.20815500\n",
      "Iteration 17, loss = 2.19318882\n",
      "Iteration 18, loss = 2.17559364\n",
      "Iteration 19, loss = 2.15473939\n",
      "Iteration 20, loss = 2.12998829\n",
      "Iteration 21, loss = 2.10057123\n",
      "Iteration 22, loss = 2.06572645\n",
      "Iteration 23, loss = 2.02480378\n",
      "Iteration 24, loss = 1.97729358\n",
      "Iteration 25, loss = 1.92302929\n",
      "Iteration 26, loss = 1.86241962\n",
      "Iteration 27, loss = 1.79628522\n",
      "Iteration 28, loss = 1.72629248\n",
      "Iteration 29, loss = 1.65386539\n",
      "Iteration 30, loss = 1.58106378\n",
      "Iteration 31, loss = 1.51002836\n",
      "Iteration 32, loss = 1.44220145\n",
      "Iteration 33, loss = 1.37844862\n",
      "Iteration 34, loss = 1.31993825\n",
      "Iteration 35, loss = 1.26660224\n",
      "Iteration 36, loss = 1.21828858\n",
      "Iteration 37, loss = 1.17490326\n",
      "Iteration 38, loss = 1.13579634\n",
      "Iteration 39, loss = 1.10059294\n",
      "Iteration 40, loss = 1.06876514\n",
      "Iteration 41, loss = 1.03991703\n",
      "Iteration 42, loss = 1.01356418\n",
      "Iteration 43, loss = 0.98954053\n",
      "Iteration 44, loss = 0.96731468\n",
      "Iteration 45, loss = 0.94681290\n",
      "Iteration 46, loss = 0.92761001\n",
      "Iteration 47, loss = 0.90986727\n",
      "Iteration 48, loss = 0.89297675\n",
      "Iteration 49, loss = 0.87698878\n",
      "Iteration 50, loss = 0.86190236\n",
      "Iteration 51, loss = 0.84760660\n",
      "Iteration 52, loss = 0.83375693\n",
      "Iteration 53, loss = 0.82053478\n",
      "Iteration 54, loss = 0.80769286\n",
      "Iteration 55, loss = 0.79545851\n",
      "Iteration 56, loss = 0.78354468\n",
      "Iteration 57, loss = 0.77192961\n",
      "Iteration 58, loss = 0.76066600\n",
      "Iteration 59, loss = 0.74973141\n",
      "Iteration 60, loss = 0.73906670\n",
      "Iteration 61, loss = 0.72865754\n",
      "Iteration 62, loss = 0.71845297\n",
      "Iteration 63, loss = 0.70856889\n",
      "Iteration 64, loss = 0.69897335\n",
      "Iteration 65, loss = 0.68951480\n",
      "Iteration 66, loss = 0.68037941\n",
      "Iteration 67, loss = 0.67138903\n",
      "Iteration 68, loss = 0.66267290\n",
      "Iteration 69, loss = 0.65405727\n",
      "Iteration 70, loss = 0.64590241\n",
      "Iteration 71, loss = 0.63776257\n",
      "Iteration 72, loss = 0.62988852\n",
      "Iteration 73, loss = 0.62217976\n",
      "Iteration 74, loss = 0.61472391\n",
      "Iteration 75, loss = 0.60741209\n",
      "Iteration 76, loss = 0.60034161\n",
      "Iteration 77, loss = 0.59339395\n",
      "Iteration 78, loss = 0.58664562\n",
      "Iteration 79, loss = 0.58013726\n",
      "Iteration 80, loss = 0.57361976\n",
      "Iteration 81, loss = 0.56745318\n",
      "Iteration 82, loss = 0.56145347\n",
      "Iteration 83, loss = 0.55556091\n",
      "Iteration 84, loss = 0.54975438\n",
      "Iteration 85, loss = 0.54406849\n",
      "Iteration 86, loss = 0.53868247\n",
      "Iteration 87, loss = 0.53337325\n",
      "Iteration 88, loss = 0.52820155\n",
      "Iteration 89, loss = 0.52312849\n",
      "Iteration 90, loss = 0.51823172\n",
      "Iteration 91, loss = 0.51345901\n",
      "Iteration 92, loss = 0.50878296\n",
      "Iteration 93, loss = 0.50420323\n",
      "Iteration 94, loss = 0.49972811\n",
      "Iteration 95, loss = 0.49550010\n",
      "Iteration 96, loss = 0.49129426\n",
      "Iteration 97, loss = 0.48720944\n",
      "Iteration 98, loss = 0.48313378\n",
      "Iteration 99, loss = 0.47927484\n",
      "Iteration 100, loss = 0.47562400\n",
      "Iteration 101, loss = 0.47181785\n",
      "Iteration 102, loss = 0.46824219\n",
      "Iteration 103, loss = 0.46475213\n",
      "Iteration 104, loss = 0.46133826\n",
      "Iteration 105, loss = 0.45812087\n",
      "Iteration 106, loss = 0.45493876\n",
      "Iteration 107, loss = 0.45179560\n",
      "Iteration 108, loss = 0.44872262\n",
      "Iteration 109, loss = 0.44578349\n",
      "Iteration 110, loss = 0.44290258\n",
      "Iteration 111, loss = 0.44008857\n",
      "Iteration 112, loss = 0.43726383\n",
      "Iteration 113, loss = 0.43472337\n",
      "Iteration 114, loss = 0.43212668\n",
      "Iteration 115, loss = 0.42951006\n",
      "Iteration 116, loss = 0.42714504\n",
      "Iteration 117, loss = 0.42475382\n",
      "Iteration 118, loss = 0.42244388\n",
      "Iteration 119, loss = 0.42016163\n",
      "Iteration 120, loss = 0.41792617\n",
      "Iteration 121, loss = 0.41578523\n",
      "Iteration 122, loss = 0.41364243\n",
      "Iteration 123, loss = 0.41161223\n",
      "Iteration 124, loss = 0.40963728\n",
      "Iteration 125, loss = 0.40766058\n",
      "Iteration 126, loss = 0.40581620\n",
      "Iteration 127, loss = 0.40389987\n",
      "Iteration 128, loss = 0.40208628\n",
      "Iteration 129, loss = 0.40038256\n",
      "Iteration 130, loss = 0.39858624\n",
      "Iteration 131, loss = 0.39689274\n",
      "Iteration 132, loss = 0.39525101\n",
      "Iteration 133, loss = 0.39366026\n",
      "Iteration 134, loss = 0.39197676\n",
      "Iteration 135, loss = 0.39050103\n",
      "Iteration 136, loss = 0.38898217\n",
      "Iteration 137, loss = 0.38754139\n",
      "Iteration 138, loss = 0.38604080\n",
      "Iteration 139, loss = 0.38466259\n",
      "Iteration 140, loss = 0.38321152\n",
      "Iteration 141, loss = 0.38190824\n",
      "Iteration 142, loss = 0.38051850\n",
      "Iteration 143, loss = 0.37923648\n",
      "Iteration 144, loss = 0.37792035\n",
      "Iteration 145, loss = 0.37665319\n",
      "Iteration 146, loss = 0.37542982\n",
      "Iteration 147, loss = 0.37427629\n",
      "Iteration 148, loss = 0.37305707\n",
      "Iteration 149, loss = 0.37184117\n",
      "Iteration 150, loss = 0.37067988\n",
      "Iteration 151, loss = 0.36953247\n",
      "Iteration 152, loss = 0.36846415\n",
      "Iteration 153, loss = 0.36732861\n",
      "Iteration 154, loss = 0.36635161\n",
      "Iteration 155, loss = 0.36516758\n",
      "Iteration 156, loss = 0.36421970\n",
      "Iteration 157, loss = 0.36313676\n",
      "Iteration 158, loss = 0.36217815\n",
      "Iteration 159, loss = 0.36108603\n",
      "Iteration 160, loss = 0.36012785\n",
      "Iteration 161, loss = 0.35928485\n",
      "Iteration 162, loss = 0.35820090\n",
      "Iteration 163, loss = 0.35741247\n",
      "Iteration 164, loss = 0.35643653\n",
      "Iteration 165, loss = 0.35560291\n",
      "Iteration 166, loss = 0.35460805\n",
      "Iteration 167, loss = 0.35379397\n",
      "Iteration 168, loss = 0.35289397\n",
      "Iteration 169, loss = 0.35208619\n",
      "Iteration 170, loss = 0.35115389\n",
      "Iteration 171, loss = 0.35036573\n",
      "Iteration 172, loss = 0.34952739\n",
      "Iteration 173, loss = 0.34875427\n",
      "Iteration 174, loss = 0.34792187\n",
      "Iteration 175, loss = 0.34711018\n",
      "Iteration 176, loss = 0.34637108\n",
      "Iteration 177, loss = 0.34555086\n",
      "Iteration 178, loss = 0.34482620\n",
      "Iteration 179, loss = 0.34412940\n",
      "Iteration 180, loss = 0.34331882\n",
      "Iteration 181, loss = 0.34262837\n",
      "Iteration 182, loss = 0.34185151\n",
      "Iteration 183, loss = 0.34116852\n",
      "Iteration 184, loss = 0.34046258\n",
      "Iteration 185, loss = 0.33969668\n",
      "Iteration 186, loss = 0.33903611\n",
      "Iteration 187, loss = 0.33835661\n",
      "Iteration 188, loss = 0.33759552\n",
      "Iteration 189, loss = 0.33701459\n",
      "Iteration 190, loss = 0.33626521\n",
      "Iteration 191, loss = 0.33569287\n",
      "Iteration 192, loss = 0.33506581\n",
      "Iteration 193, loss = 0.33432560\n",
      "Iteration 194, loss = 0.33373038\n",
      "Iteration 195, loss = 0.33310636\n",
      "Iteration 196, loss = 0.33247661\n",
      "Iteration 197, loss = 0.33185147\n",
      "Iteration 198, loss = 0.33120177\n",
      "Iteration 199, loss = 0.33062643\n",
      "Iteration 200, loss = 0.32996615\n",
      "[CV] END activation=logistic, alpha=0.001, batch_size=200, hidden_layer_sizes=(400, 100), solver=sgd; total time= 6.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.30517121\n",
      "Iteration 2, loss = 2.29810535\n",
      "Iteration 3, loss = 2.29478153\n",
      "Iteration 4, loss = 2.29123352\n",
      "Iteration 5, loss = 2.28753892\n",
      "Iteration 6, loss = 2.28360087\n",
      "Iteration 7, loss = 2.27962389\n",
      "Iteration 8, loss = 2.27522251\n",
      "Iteration 9, loss = 2.27055532\n",
      "Iteration 10, loss = 2.26510351\n",
      "Iteration 11, loss = 2.25936528\n",
      "Iteration 12, loss = 2.25261184\n",
      "Iteration 13, loss = 2.24531288\n",
      "Iteration 14, loss = 2.23667145\n",
      "Iteration 15, loss = 2.22690234\n",
      "Iteration 16, loss = 2.21540681\n",
      "Iteration 17, loss = 2.20216801\n",
      "Iteration 18, loss = 2.18646457\n",
      "Iteration 19, loss = 2.16796144\n",
      "Iteration 20, loss = 2.14601354\n",
      "Iteration 21, loss = 2.12010881\n",
      "Iteration 22, loss = 2.08975834\n",
      "Iteration 23, loss = 2.05371108\n",
      "Iteration 24, loss = 2.01239215\n",
      "Iteration 25, loss = 1.96491180\n",
      "Iteration 26, loss = 1.91207520\n",
      "Iteration 27, loss = 1.85449345\n",
      "Iteration 28, loss = 1.79311078\n",
      "Iteration 29, loss = 1.72976065\n",
      "Iteration 30, loss = 1.66571984\n",
      "Iteration 31, loss = 1.60254282\n",
      "Iteration 32, loss = 1.54137948\n",
      "Iteration 33, loss = 1.48291001\n",
      "Iteration 34, loss = 1.42752913\n",
      "Iteration 35, loss = 1.37518189\n",
      "Iteration 36, loss = 1.32578835\n",
      "Iteration 37, loss = 1.27907151\n",
      "Iteration 38, loss = 1.23493754\n",
      "Iteration 39, loss = 1.19318452\n",
      "Iteration 40, loss = 1.15371277\n",
      "Iteration 41, loss = 1.11652818\n",
      "Iteration 42, loss = 1.08146473\n",
      "Iteration 43, loss = 1.04864008\n",
      "Iteration 44, loss = 1.01787806\n",
      "Iteration 45, loss = 0.98901926\n",
      "Iteration 46, loss = 0.96185996\n",
      "Iteration 47, loss = 0.93653300\n",
      "Iteration 48, loss = 0.91265077\n",
      "Iteration 49, loss = 0.89013915\n",
      "Iteration 50, loss = 0.86889541\n",
      "Iteration 51, loss = 0.84881065\n",
      "Iteration 52, loss = 0.82975971\n",
      "Iteration 53, loss = 0.81150892\n",
      "Iteration 54, loss = 0.79426890\n",
      "Iteration 55, loss = 0.77785315\n",
      "Iteration 56, loss = 0.76200415\n",
      "Iteration 57, loss = 0.74690668\n",
      "Iteration 58, loss = 0.73239378\n",
      "Iteration 59, loss = 0.71839847\n",
      "Iteration 60, loss = 0.70515970\n",
      "Iteration 61, loss = 0.69238231\n",
      "Iteration 62, loss = 0.68002692\n",
      "Iteration 63, loss = 0.66818456\n",
      "Iteration 64, loss = 0.65696401\n",
      "Iteration 65, loss = 0.64622214\n",
      "Iteration 66, loss = 0.63585681\n",
      "Iteration 67, loss = 0.62606499\n",
      "Iteration 68, loss = 0.61653282\n",
      "Iteration 69, loss = 0.60740872\n",
      "Iteration 70, loss = 0.59863030\n",
      "Iteration 71, loss = 0.59034942\n",
      "Iteration 72, loss = 0.58238622\n",
      "Iteration 73, loss = 0.57476935\n",
      "Iteration 74, loss = 0.56740467\n",
      "Iteration 75, loss = 0.56042233\n",
      "Iteration 76, loss = 0.55370187\n",
      "Iteration 77, loss = 0.54722144\n",
      "Iteration 78, loss = 0.54099630\n",
      "Iteration 79, loss = 0.53503561\n",
      "Iteration 80, loss = 0.52931328\n",
      "Iteration 81, loss = 0.52376204\n",
      "Iteration 82, loss = 0.51849560\n",
      "Iteration 83, loss = 0.51343632\n",
      "Iteration 84, loss = 0.50849659\n",
      "Iteration 85, loss = 0.50374961\n",
      "Iteration 86, loss = 0.49919488\n",
      "Iteration 87, loss = 0.49468146\n",
      "Iteration 88, loss = 0.49053984\n",
      "Iteration 89, loss = 0.48633050\n",
      "Iteration 90, loss = 0.48236841\n",
      "Iteration 91, loss = 0.47850078\n",
      "Iteration 92, loss = 0.47470184\n",
      "Iteration 93, loss = 0.47116274\n",
      "Iteration 94, loss = 0.46760701\n",
      "Iteration 95, loss = 0.46428243\n",
      "Iteration 96, loss = 0.46102239\n",
      "Iteration 97, loss = 0.45783635\n",
      "Iteration 98, loss = 0.45468059\n",
      "Iteration 99, loss = 0.45168836\n",
      "Iteration 100, loss = 0.44874543\n",
      "Iteration 101, loss = 0.44597804\n",
      "Iteration 102, loss = 0.44321013\n",
      "Iteration 103, loss = 0.44058813\n",
      "Iteration 104, loss = 0.43795240\n",
      "Iteration 105, loss = 0.43542334\n",
      "Iteration 106, loss = 0.43300166\n",
      "Iteration 107, loss = 0.43066427\n",
      "Iteration 108, loss = 0.42828361\n",
      "Iteration 109, loss = 0.42604466\n",
      "Iteration 110, loss = 0.42389671\n",
      "Iteration 111, loss = 0.42171396\n",
      "Iteration 112, loss = 0.41963391\n",
      "Iteration 113, loss = 0.41760275\n",
      "Iteration 114, loss = 0.41565126\n",
      "Iteration 115, loss = 0.41371227\n",
      "Iteration 116, loss = 0.41181311\n",
      "Iteration 117, loss = 0.40988830\n",
      "Iteration 118, loss = 0.40822377\n",
      "Iteration 119, loss = 0.40645330\n",
      "Iteration 120, loss = 0.40473992\n",
      "Iteration 121, loss = 0.40314615\n",
      "Iteration 122, loss = 0.40151191\n",
      "Iteration 123, loss = 0.39990513\n",
      "Iteration 124, loss = 0.39829058\n",
      "Iteration 125, loss = 0.39680381\n",
      "Iteration 126, loss = 0.39537204\n",
      "Iteration 127, loss = 0.39384416\n",
      "Iteration 128, loss = 0.39237073\n",
      "Iteration 129, loss = 0.39100070\n",
      "Iteration 130, loss = 0.38971033\n",
      "Iteration 131, loss = 0.38831883\n",
      "Iteration 132, loss = 0.38708572\n",
      "Iteration 133, loss = 0.38574457\n",
      "Iteration 134, loss = 0.38440299\n",
      "Iteration 135, loss = 0.38319045\n",
      "Iteration 136, loss = 0.38202347\n",
      "Iteration 137, loss = 0.38083568\n",
      "Iteration 138, loss = 0.37964172\n",
      "Iteration 139, loss = 0.37842766\n",
      "Iteration 140, loss = 0.37740817\n",
      "Iteration 141, loss = 0.37624670\n",
      "Iteration 142, loss = 0.37518006\n",
      "Iteration 143, loss = 0.37401576\n",
      "Iteration 144, loss = 0.37304013\n",
      "Iteration 145, loss = 0.37198628\n",
      "Iteration 146, loss = 0.37102298\n",
      "Iteration 147, loss = 0.36995743\n",
      "Iteration 148, loss = 0.36901169\n",
      "Iteration 149, loss = 0.36802702\n",
      "Iteration 150, loss = 0.36707343\n",
      "Iteration 151, loss = 0.36611926\n",
      "Iteration 152, loss = 0.36517712\n",
      "Iteration 153, loss = 0.36430720\n",
      "Iteration 154, loss = 0.36338180\n",
      "Iteration 155, loss = 0.36238651\n",
      "Iteration 156, loss = 0.36161221\n",
      "Iteration 157, loss = 0.36079062\n",
      "Iteration 158, loss = 0.35979260\n",
      "Iteration 159, loss = 0.35907536\n",
      "Iteration 160, loss = 0.35810634\n",
      "Iteration 161, loss = 0.35738597\n",
      "Iteration 162, loss = 0.35654538\n",
      "Iteration 163, loss = 0.35570390\n",
      "Iteration 164, loss = 0.35500460\n",
      "Iteration 165, loss = 0.35416012\n",
      "Iteration 166, loss = 0.35338116\n",
      "Iteration 167, loss = 0.35267930\n",
      "Iteration 168, loss = 0.35184748\n",
      "Iteration 169, loss = 0.35109470\n",
      "Iteration 170, loss = 0.35035168\n",
      "Iteration 171, loss = 0.34968301\n",
      "Iteration 172, loss = 0.34890356\n",
      "Iteration 173, loss = 0.34820300\n",
      "Iteration 174, loss = 0.34751561\n",
      "Iteration 175, loss = 0.34678812\n",
      "Iteration 176, loss = 0.34598410\n",
      "Iteration 177, loss = 0.34544792\n",
      "Iteration 178, loss = 0.34470572\n",
      "Iteration 179, loss = 0.34401467\n",
      "Iteration 180, loss = 0.34334700\n",
      "Iteration 181, loss = 0.34269459\n",
      "Iteration 182, loss = 0.34202726\n",
      "Iteration 183, loss = 0.34137002\n",
      "Iteration 184, loss = 0.34074155\n",
      "Iteration 185, loss = 0.34006798\n",
      "Iteration 186, loss = 0.33944921\n",
      "Iteration 187, loss = 0.33878274\n",
      "Iteration 188, loss = 0.33819965\n",
      "Iteration 189, loss = 0.33763328\n",
      "Iteration 190, loss = 0.33696524\n",
      "Iteration 191, loss = 0.33634745\n",
      "Iteration 192, loss = 0.33579714\n",
      "Iteration 193, loss = 0.33518810\n",
      "Iteration 194, loss = 0.33453211\n",
      "Iteration 195, loss = 0.33399669\n",
      "Iteration 196, loss = 0.33342347\n",
      "Iteration 197, loss = 0.33282234\n",
      "Iteration 198, loss = 0.33226014\n",
      "Iteration 199, loss = 0.33164249\n",
      "Iteration 200, loss = 0.33108241\n",
      "[CV] END activation=logistic, alpha=0.001, batch_size=200, hidden_layer_sizes=(400, 100), solver=sgd; total time= 6.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.30655642\n",
      "Iteration 2, loss = 2.29709679\n",
      "Iteration 3, loss = 2.29389111\n",
      "Iteration 4, loss = 2.29076022\n",
      "Iteration 5, loss = 2.28717913\n",
      "Iteration 6, loss = 2.28377209\n",
      "Iteration 7, loss = 2.27988152\n",
      "Iteration 8, loss = 2.27578807\n",
      "Iteration 9, loss = 2.27134755\n",
      "Iteration 10, loss = 2.26634371\n",
      "Iteration 11, loss = 2.26107519\n",
      "Iteration 12, loss = 2.25509054\n",
      "Iteration 13, loss = 2.24814832\n",
      "Iteration 14, loss = 2.24036749\n",
      "Iteration 15, loss = 2.23157873\n",
      "Iteration 16, loss = 2.22125982\n",
      "Iteration 17, loss = 2.20949466\n",
      "Iteration 18, loss = 2.19561454\n",
      "Iteration 19, loss = 2.17951075\n",
      "Iteration 20, loss = 2.16027897\n",
      "Iteration 21, loss = 2.13761773\n",
      "Iteration 22, loss = 2.11078114\n",
      "Iteration 23, loss = 2.07906854\n",
      "Iteration 24, loss = 2.04180623\n",
      "Iteration 25, loss = 1.99829190\n",
      "Iteration 26, loss = 1.94839090\n",
      "Iteration 27, loss = 1.89212446\n",
      "Iteration 28, loss = 1.83002865\n",
      "Iteration 29, loss = 1.76302180\n",
      "Iteration 30, loss = 1.69302657\n",
      "Iteration 31, loss = 1.62127915\n",
      "Iteration 32, loss = 1.54975729\n",
      "Iteration 33, loss = 1.47993367\n",
      "Iteration 34, loss = 1.41330840\n",
      "Iteration 35, loss = 1.35084999\n",
      "Iteration 36, loss = 1.29303653\n",
      "Iteration 37, loss = 1.24012244\n",
      "Iteration 38, loss = 1.19217932\n",
      "Iteration 39, loss = 1.14871622\n",
      "Iteration 40, loss = 1.10952145\n",
      "Iteration 41, loss = 1.07390022\n",
      "Iteration 42, loss = 1.04181237\n",
      "Iteration 43, loss = 1.01247214\n",
      "Iteration 44, loss = 0.98571176\n",
      "Iteration 45, loss = 0.96093200\n",
      "Iteration 46, loss = 0.93812076\n",
      "Iteration 47, loss = 0.91684645\n",
      "Iteration 48, loss = 0.89696043\n",
      "Iteration 49, loss = 0.87820397\n",
      "Iteration 50, loss = 0.86045758\n",
      "Iteration 51, loss = 0.84368930\n",
      "Iteration 52, loss = 0.82763354\n",
      "Iteration 53, loss = 0.81230787\n",
      "Iteration 54, loss = 0.79763708\n",
      "Iteration 55, loss = 0.78339367\n",
      "Iteration 56, loss = 0.76970622\n",
      "Iteration 57, loss = 0.75655607\n",
      "Iteration 58, loss = 0.74373097\n",
      "Iteration 59, loss = 0.73141773\n",
      "Iteration 60, loss = 0.71948431\n",
      "Iteration 61, loss = 0.70783967\n",
      "Iteration 62, loss = 0.69660007\n",
      "Iteration 63, loss = 0.68577617\n",
      "Iteration 64, loss = 0.67518450\n",
      "Iteration 65, loss = 0.66494146\n",
      "Iteration 66, loss = 0.65501727\n",
      "Iteration 67, loss = 0.64544535\n",
      "Iteration 68, loss = 0.63619626\n",
      "Iteration 69, loss = 0.62731563\n",
      "Iteration 70, loss = 0.61851629\n",
      "Iteration 71, loss = 0.61023465\n",
      "Iteration 72, loss = 0.60216219\n",
      "Iteration 73, loss = 0.59435236\n",
      "Iteration 74, loss = 0.58674949\n",
      "Iteration 75, loss = 0.57946933\n",
      "Iteration 76, loss = 0.57241000\n",
      "Iteration 77, loss = 0.56559528\n",
      "Iteration 78, loss = 0.55913321\n",
      "Iteration 79, loss = 0.55268926\n",
      "Iteration 80, loss = 0.54667483\n",
      "Iteration 81, loss = 0.54079818\n",
      "Iteration 82, loss = 0.53504706\n",
      "Iteration 83, loss = 0.52956310\n",
      "Iteration 84, loss = 0.52417677\n",
      "Iteration 85, loss = 0.51910390\n",
      "Iteration 86, loss = 0.51408160\n",
      "Iteration 87, loss = 0.50931393\n",
      "Iteration 88, loss = 0.50466863\n",
      "Iteration 89, loss = 0.50017879\n",
      "Iteration 90, loss = 0.49572579\n",
      "Iteration 91, loss = 0.49154234\n",
      "Iteration 92, loss = 0.48757151\n",
      "Iteration 93, loss = 0.48350274\n",
      "Iteration 94, loss = 0.47964011\n",
      "Iteration 95, loss = 0.47597952\n",
      "Iteration 96, loss = 0.47235162\n",
      "Iteration 97, loss = 0.46891120\n",
      "Iteration 98, loss = 0.46547551\n",
      "Iteration 99, loss = 0.46216456\n",
      "Iteration 100, loss = 0.45890607\n",
      "Iteration 101, loss = 0.45579904\n",
      "Iteration 102, loss = 0.45283409\n",
      "Iteration 103, loss = 0.44988755\n",
      "Iteration 104, loss = 0.44699684\n",
      "Iteration 105, loss = 0.44420571\n",
      "Iteration 106, loss = 0.44146271\n",
      "Iteration 107, loss = 0.43893038\n",
      "Iteration 108, loss = 0.43630513\n",
      "Iteration 109, loss = 0.43381091\n",
      "Iteration 110, loss = 0.43135548\n",
      "Iteration 111, loss = 0.42900727\n",
      "Iteration 112, loss = 0.42673006\n",
      "Iteration 113, loss = 0.42438861\n",
      "Iteration 114, loss = 0.42225991\n",
      "Iteration 115, loss = 0.42016804\n",
      "Iteration 116, loss = 0.41802726\n",
      "Iteration 117, loss = 0.41603049\n",
      "Iteration 118, loss = 0.41410529\n",
      "Iteration 119, loss = 0.41213725\n",
      "Iteration 120, loss = 0.41018152\n",
      "Iteration 121, loss = 0.40828317\n",
      "Iteration 122, loss = 0.40653252\n",
      "Iteration 123, loss = 0.40484077\n",
      "Iteration 124, loss = 0.40306116\n",
      "Iteration 125, loss = 0.40139617\n",
      "Iteration 126, loss = 0.39979602\n",
      "Iteration 127, loss = 0.39808980\n",
      "Iteration 128, loss = 0.39656829\n",
      "Iteration 129, loss = 0.39507149\n",
      "Iteration 130, loss = 0.39353179\n",
      "Iteration 131, loss = 0.39207406\n",
      "Iteration 132, loss = 0.39061597\n",
      "Iteration 133, loss = 0.38918070\n",
      "Iteration 134, loss = 0.38777846\n",
      "Iteration 135, loss = 0.38642063\n",
      "Iteration 136, loss = 0.38509063\n",
      "Iteration 137, loss = 0.38380080\n",
      "Iteration 138, loss = 0.38249029\n",
      "Iteration 139, loss = 0.38129829\n",
      "Iteration 140, loss = 0.37989610\n",
      "Iteration 141, loss = 0.37884663\n",
      "Iteration 142, loss = 0.37755631\n",
      "Iteration 143, loss = 0.37638678\n",
      "Iteration 144, loss = 0.37522821\n",
      "Iteration 145, loss = 0.37413872\n",
      "Iteration 146, loss = 0.37301218\n",
      "Iteration 147, loss = 0.37189118\n",
      "Iteration 148, loss = 0.37078486\n",
      "Iteration 149, loss = 0.36975594\n",
      "Iteration 150, loss = 0.36870040\n",
      "Iteration 151, loss = 0.36769971\n",
      "Iteration 152, loss = 0.36666633\n",
      "Iteration 153, loss = 0.36569771\n",
      "Iteration 154, loss = 0.36476411\n",
      "Iteration 155, loss = 0.36374860\n",
      "Iteration 156, loss = 0.36274850\n",
      "Iteration 157, loss = 0.36178649\n",
      "Iteration 158, loss = 0.36087509\n",
      "Iteration 159, loss = 0.36003411\n",
      "Iteration 160, loss = 0.35907748\n",
      "Iteration 161, loss = 0.35826916\n",
      "Iteration 162, loss = 0.35735041\n",
      "Iteration 163, loss = 0.35649195\n",
      "Iteration 164, loss = 0.35559755\n",
      "Iteration 165, loss = 0.35482218\n",
      "Iteration 166, loss = 0.35386686\n",
      "Iteration 167, loss = 0.35316563\n",
      "Iteration 168, loss = 0.35232595\n",
      "Iteration 169, loss = 0.35156535\n",
      "Iteration 170, loss = 0.35073333\n",
      "Iteration 171, loss = 0.34987280\n",
      "Iteration 172, loss = 0.34915544\n",
      "Iteration 173, loss = 0.34842411\n",
      "Iteration 174, loss = 0.34759382\n",
      "Iteration 175, loss = 0.34688117\n",
      "Iteration 176, loss = 0.34612274\n",
      "Iteration 177, loss = 0.34540380\n",
      "Iteration 178, loss = 0.34471060\n",
      "Iteration 179, loss = 0.34396229\n",
      "Iteration 180, loss = 0.34329196\n",
      "Iteration 181, loss = 0.34258448\n",
      "Iteration 182, loss = 0.34185395\n",
      "Iteration 183, loss = 0.34116163\n",
      "Iteration 184, loss = 0.34046409\n",
      "Iteration 185, loss = 0.33983746\n",
      "Iteration 186, loss = 0.33911776\n",
      "Iteration 187, loss = 0.33850572\n",
      "Iteration 188, loss = 0.33785664\n",
      "Iteration 189, loss = 0.33719198\n",
      "Iteration 190, loss = 0.33660088\n",
      "Iteration 191, loss = 0.33590369\n",
      "Iteration 192, loss = 0.33524469\n",
      "Iteration 193, loss = 0.33462090\n",
      "Iteration 194, loss = 0.33401326\n",
      "Iteration 195, loss = 0.33336376\n",
      "Iteration 196, loss = 0.33279833\n",
      "Iteration 197, loss = 0.33213319\n",
      "Iteration 198, loss = 0.33154354\n",
      "Iteration 199, loss = 0.33100281\n",
      "Iteration 200, loss = 0.33040786\n",
      "[CV] END activation=logistic, alpha=0.001, batch_size=200, hidden_layer_sizes=(400, 100), solver=sgd; total time= 6.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.69245684\n",
      "Iteration 2, loss = 0.67353362\n",
      "Iteration 3, loss = 0.42259150\n",
      "Iteration 4, loss = 0.31487394\n",
      "Iteration 5, loss = 0.25152864\n",
      "Iteration 6, loss = 0.21016625\n",
      "Iteration 7, loss = 0.18035868\n",
      "Iteration 8, loss = 0.15742850\n",
      "Iteration 9, loss = 0.13797522\n",
      "Iteration 10, loss = 0.12347645\n",
      "Iteration 11, loss = 0.11060203\n",
      "Iteration 12, loss = 0.10068633\n",
      "Iteration 13, loss = 0.09027451\n",
      "Iteration 14, loss = 0.08210942\n",
      "Iteration 15, loss = 0.07446451\n",
      "Iteration 16, loss = 0.06953358\n",
      "Iteration 17, loss = 0.06344770\n",
      "Iteration 18, loss = 0.05625954\n",
      "Iteration 19, loss = 0.05150847\n",
      "Iteration 20, loss = 0.04687226\n",
      "Iteration 21, loss = 0.04291316\n",
      "Iteration 22, loss = 0.03999010\n",
      "Iteration 23, loss = 0.03535093\n",
      "Iteration 24, loss = 0.03264239\n",
      "Iteration 25, loss = 0.03001673\n",
      "Iteration 26, loss = 0.02710622\n",
      "Iteration 27, loss = 0.02672196\n",
      "Iteration 28, loss = 0.02360336\n",
      "Iteration 29, loss = 0.02147939\n",
      "Iteration 30, loss = 0.01989178\n",
      "Iteration 31, loss = 0.01842003\n",
      "Iteration 32, loss = 0.01697332\n",
      "Iteration 33, loss = 0.01591673\n",
      "Iteration 34, loss = 0.01499142\n",
      "Iteration 35, loss = 0.01462348\n",
      "Iteration 36, loss = 0.01383968\n",
      "Iteration 37, loss = 0.01325040\n",
      "Iteration 38, loss = 0.01229770\n",
      "Iteration 39, loss = 0.01223337\n",
      "Iteration 40, loss = 0.01192835\n",
      "Iteration 41, loss = 0.01153377\n",
      "Iteration 42, loss = 0.01114325\n",
      "Iteration 43, loss = 0.01064719\n",
      "Iteration 44, loss = 0.01039506\n",
      "Iteration 45, loss = 0.01024638\n",
      "Iteration 46, loss = 0.01004390\n",
      "Iteration 47, loss = 0.00975999\n",
      "Iteration 48, loss = 0.00964339\n",
      "Iteration 49, loss = 0.00956393\n",
      "Iteration 50, loss = 0.00954735\n",
      "Iteration 51, loss = 0.00932153\n",
      "Iteration 52, loss = 0.00960021\n",
      "Iteration 53, loss = 0.03569239\n",
      "Iteration 54, loss = 0.01357063\n",
      "Iteration 55, loss = 0.00990685\n",
      "Iteration 56, loss = 0.00932981\n",
      "Iteration 57, loss = 0.00915634\n",
      "Iteration 58, loss = 0.00906396\n",
      "Iteration 59, loss = 0.00898778\n",
      "Iteration 60, loss = 0.00889524\n",
      "Iteration 61, loss = 0.00882242\n",
      "Iteration 62, loss = 0.00876496\n",
      "Iteration 63, loss = 0.00868145\n",
      "Iteration 64, loss = 0.00860132\n",
      "Iteration 65, loss = 0.00853593\n",
      "Iteration 66, loss = 0.00846717\n",
      "Iteration 67, loss = 0.00840459\n",
      "Iteration 68, loss = 0.00832517\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.001, batch_size=200, hidden_layer_sizes=(100, 100, 100), solver=adam; total time= 1.4min\n",
      "Iteration 1, loss = 1.75237627\n",
      "Iteration 2, loss = 0.63381129\n",
      "Iteration 3, loss = 0.37327865\n",
      "Iteration 4, loss = 0.28725122\n",
      "Iteration 5, loss = 0.23337132\n",
      "Iteration 6, loss = 0.19763382\n",
      "Iteration 7, loss = 0.16781495\n",
      "Iteration 8, loss = 0.14686975\n",
      "Iteration 9, loss = 0.12893316\n",
      "Iteration 10, loss = 0.11479496\n",
      "Iteration 11, loss = 0.10252540\n",
      "Iteration 12, loss = 0.09216360\n",
      "Iteration 13, loss = 0.08278219\n",
      "Iteration 14, loss = 0.07483424\n",
      "Iteration 15, loss = 0.06813482\n",
      "Iteration 16, loss = 0.06061784\n",
      "Iteration 17, loss = 0.05581664\n",
      "Iteration 18, loss = 0.05088533\n",
      "Iteration 19, loss = 0.04579200\n",
      "Iteration 20, loss = 0.04146305\n",
      "Iteration 21, loss = 0.03736358\n",
      "Iteration 22, loss = 0.03458653\n",
      "Iteration 23, loss = 0.03220327\n",
      "Iteration 24, loss = 0.02846228\n",
      "Iteration 25, loss = 0.02660666\n",
      "Iteration 26, loss = 0.02363625\n",
      "Iteration 27, loss = 0.02148679\n",
      "Iteration 28, loss = 0.01952110\n",
      "Iteration 29, loss = 0.01809487\n",
      "Iteration 30, loss = 0.01680138\n",
      "Iteration 31, loss = 0.01585040\n",
      "Iteration 32, loss = 0.01457717\n",
      "Iteration 33, loss = 0.01396460\n",
      "Iteration 34, loss = 0.01355917\n",
      "Iteration 35, loss = 0.01253958\n",
      "Iteration 36, loss = 0.01218225\n",
      "Iteration 37, loss = 0.01203528\n",
      "Iteration 38, loss = 0.01156481\n",
      "Iteration 39, loss = 0.01153753\n",
      "Iteration 40, loss = 0.01141869\n",
      "Iteration 41, loss = 0.01066666\n",
      "Iteration 42, loss = 0.01038260\n",
      "Iteration 43, loss = 0.01055276\n",
      "Iteration 44, loss = 0.01011200\n",
      "Iteration 45, loss = 0.00981630\n",
      "Iteration 46, loss = 0.00965973\n",
      "Iteration 47, loss = 0.01636101\n",
      "Iteration 48, loss = 0.02360951\n",
      "Iteration 49, loss = 0.01128411\n",
      "Iteration 50, loss = 0.00988074\n",
      "Iteration 51, loss = 0.00964792\n",
      "Iteration 52, loss = 0.00948164\n",
      "Iteration 53, loss = 0.00936676\n",
      "Iteration 54, loss = 0.00929856\n",
      "Iteration 55, loss = 0.00924197\n",
      "Iteration 56, loss = 0.00911676\n",
      "Iteration 57, loss = 0.00896300\n",
      "Iteration 58, loss = 0.00903364\n",
      "Iteration 59, loss = 0.00889669\n",
      "Iteration 60, loss = 0.00879271\n",
      "Iteration 61, loss = 0.00871698\n",
      "Iteration 62, loss = 0.00854072\n",
      "Iteration 63, loss = 0.00845148\n",
      "Iteration 64, loss = 0.00831845\n",
      "Iteration 65, loss = 0.00826391\n",
      "Iteration 66, loss = 0.00818681\n",
      "Iteration 67, loss = 0.00988853\n",
      "Iteration 68, loss = 0.03033656\n",
      "Iteration 69, loss = 0.01076915\n",
      "Iteration 70, loss = 0.00880294\n",
      "Iteration 71, loss = 0.00852647\n",
      "Iteration 72, loss = 0.00839835\n",
      "Iteration 73, loss = 0.00832178\n",
      "Iteration 74, loss = 0.00826164\n",
      "Iteration 75, loss = 0.00819340\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.001, batch_size=200, hidden_layer_sizes=(100, 100, 100), solver=adam; total time= 1.5min\n",
      "Iteration 1, loss = 1.76233892\n",
      "Iteration 2, loss = 0.69683499\n",
      "Iteration 3, loss = 0.42245715\n",
      "Iteration 4, loss = 0.31738029\n",
      "Iteration 5, loss = 0.25585106\n",
      "Iteration 6, loss = 0.21271697\n",
      "Iteration 7, loss = 0.18133042\n",
      "Iteration 8, loss = 0.15792760\n",
      "Iteration 9, loss = 0.13843199\n",
      "Iteration 10, loss = 0.12425301\n",
      "Iteration 11, loss = 0.11084999\n",
      "Iteration 12, loss = 0.09916916\n",
      "Iteration 13, loss = 0.08966843\n",
      "Iteration 14, loss = 0.08163371\n",
      "Iteration 15, loss = 0.07472601\n",
      "Iteration 16, loss = 0.06758394\n",
      "Iteration 17, loss = 0.06165085\n",
      "Iteration 18, loss = 0.05623500\n",
      "Iteration 19, loss = 0.05273168\n",
      "Iteration 20, loss = 0.04833076\n",
      "Iteration 21, loss = 0.04415510\n",
      "Iteration 22, loss = 0.03986827\n",
      "Iteration 23, loss = 0.03678638\n",
      "Iteration 24, loss = 0.03393018\n",
      "Iteration 25, loss = 0.03189964\n",
      "Iteration 26, loss = 0.02910177\n",
      "Iteration 27, loss = 0.02624767\n",
      "Iteration 28, loss = 0.02469895\n",
      "Iteration 29, loss = 0.02351051\n",
      "Iteration 30, loss = 0.02155233\n",
      "Iteration 31, loss = 0.02032817\n",
      "Iteration 32, loss = 0.01881234\n",
      "Iteration 33, loss = 0.01776790\n",
      "Iteration 34, loss = 0.01701405\n",
      "Iteration 35, loss = 0.01599800\n",
      "Iteration 36, loss = 0.01490085\n",
      "Iteration 37, loss = 0.01432670\n",
      "Iteration 38, loss = 0.01376768\n",
      "Iteration 39, loss = 0.01309424\n",
      "Iteration 40, loss = 0.01235041\n",
      "Iteration 41, loss = 0.01170889\n",
      "Iteration 42, loss = 0.01132406\n",
      "Iteration 43, loss = 0.01088359\n",
      "Iteration 44, loss = 0.01254644\n",
      "Iteration 45, loss = 0.01159938\n",
      "Iteration 46, loss = 0.01285563\n",
      "Iteration 47, loss = 0.01839682\n",
      "Iteration 48, loss = 0.01516678\n",
      "Iteration 49, loss = 0.01074118\n",
      "Iteration 50, loss = 0.01005478\n",
      "Iteration 51, loss = 0.00978507\n",
      "Iteration 52, loss = 0.00964836\n",
      "Iteration 53, loss = 0.00952389\n",
      "Iteration 54, loss = 0.00939603\n",
      "Iteration 55, loss = 0.00932277\n",
      "Iteration 56, loss = 0.00921554\n",
      "Iteration 57, loss = 0.00911476\n",
      "Iteration 58, loss = 0.00902702\n",
      "Iteration 59, loss = 0.00891462\n",
      "Iteration 60, loss = 0.00883383\n",
      "Iteration 61, loss = 0.00871247\n",
      "Iteration 62, loss = 0.00861931\n",
      "Iteration 63, loss = 0.00853660\n",
      "Iteration 64, loss = 0.00847358\n",
      "Iteration 65, loss = 0.00841907\n",
      "Iteration 66, loss = 0.03558923\n",
      "Iteration 67, loss = 0.01440376\n",
      "Iteration 68, loss = 0.00951562\n",
      "Iteration 69, loss = 0.00893114\n",
      "Iteration 70, loss = 0.00873892\n",
      "Iteration 71, loss = 0.00863260\n",
      "Iteration 72, loss = 0.00856208\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.001, batch_size=200, hidden_layer_sizes=(100, 100, 100), solver=adam; total time= 1.5min\n",
      "Iteration 1, loss = 1.76284168\n",
      "Iteration 2, loss = 0.69778082\n",
      "Iteration 3, loss = 0.40418887\n",
      "Iteration 4, loss = 0.30477437\n",
      "Iteration 5, loss = 0.24814896\n",
      "Iteration 6, loss = 0.21147384\n",
      "Iteration 7, loss = 0.18484225\n",
      "Iteration 8, loss = 0.16282995\n",
      "Iteration 9, loss = 0.14596594\n",
      "Iteration 10, loss = 0.13062100\n",
      "Iteration 11, loss = 0.11705989\n",
      "Iteration 12, loss = 0.10513702\n",
      "Iteration 13, loss = 0.09560100\n",
      "Iteration 14, loss = 0.08803604\n",
      "Iteration 15, loss = 0.07863007\n",
      "Iteration 16, loss = 0.07046127\n",
      "Iteration 17, loss = 0.06586066\n",
      "Iteration 18, loss = 0.05896898\n",
      "Iteration 19, loss = 0.05459713\n",
      "Iteration 20, loss = 0.04890858\n",
      "Iteration 21, loss = 0.04478431\n",
      "Iteration 22, loss = 0.04088797\n",
      "Iteration 23, loss = 0.03689321\n",
      "Iteration 24, loss = 0.03376783\n",
      "Iteration 25, loss = 0.03054790\n",
      "Iteration 26, loss = 0.02772537\n",
      "Iteration 27, loss = 0.02509006\n",
      "Iteration 28, loss = 0.02333434\n",
      "Iteration 29, loss = 0.02156696\n",
      "Iteration 30, loss = 0.02017928\n",
      "Iteration 31, loss = 0.01843945\n",
      "Iteration 32, loss = 0.01722339\n",
      "Iteration 33, loss = 0.01656607\n",
      "Iteration 34, loss = 0.01528468\n",
      "Iteration 35, loss = 0.01425590\n",
      "Iteration 36, loss = 0.01385931\n",
      "Iteration 37, loss = 0.01314585\n",
      "Iteration 38, loss = 0.01275713\n",
      "Iteration 39, loss = 0.01187190\n",
      "Iteration 40, loss = 0.01141040\n",
      "Iteration 41, loss = 0.01124820\n",
      "Iteration 42, loss = 0.01125197\n",
      "Iteration 43, loss = 0.01060037\n",
      "Iteration 44, loss = 0.01047140\n",
      "Iteration 45, loss = 0.01028028\n",
      "Iteration 46, loss = 0.01009680\n",
      "Iteration 47, loss = 0.00974956\n",
      "Iteration 48, loss = 0.00950902\n",
      "Iteration 49, loss = 0.00954995\n",
      "Iteration 50, loss = 0.01112896\n",
      "Iteration 51, loss = 0.03953316\n",
      "Iteration 52, loss = 0.01161477\n",
      "Iteration 53, loss = 0.00985607\n",
      "Iteration 54, loss = 0.00955807\n",
      "Iteration 55, loss = 0.00936387\n",
      "Iteration 56, loss = 0.00926194\n",
      "Iteration 57, loss = 0.00916154\n",
      "Iteration 58, loss = 0.00908164\n",
      "Iteration 59, loss = 0.00899048\n",
      "Iteration 60, loss = 0.00892850\n",
      "Iteration 61, loss = 0.00885480\n",
      "Iteration 62, loss = 0.00876315\n",
      "Iteration 63, loss = 0.00869154\n",
      "Iteration 64, loss = 0.00861395\n",
      "Iteration 65, loss = 0.00855072\n",
      "Iteration 66, loss = 0.00848244\n",
      "Iteration 67, loss = 0.00837771\n",
      "Iteration 68, loss = 0.00830960\n",
      "Iteration 69, loss = 0.00824594\n",
      "Iteration 70, loss = 0.00815921\n",
      "Iteration 71, loss = 0.00809827\n",
      "Iteration 72, loss = 0.00813417\n",
      "Iteration 73, loss = 0.03507733\n",
      "Iteration 74, loss = 0.01256248\n",
      "Iteration 75, loss = 0.00917620\n",
      "Iteration 76, loss = 0.00854534\n",
      "Iteration 77, loss = 0.00839669\n",
      "Iteration 78, loss = 0.00832530\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.001, batch_size=200, hidden_layer_sizes=(100, 100, 100), solver=adam; total time= 1.5min\n",
      "Iteration 1, loss = 1.77068818\n",
      "Iteration 2, loss = 0.73089087\n",
      "Iteration 3, loss = 0.45476071\n",
      "Iteration 4, loss = 0.32906306\n",
      "Iteration 5, loss = 0.26242966\n",
      "Iteration 6, loss = 0.22285314\n",
      "Iteration 7, loss = 0.19221737\n",
      "Iteration 8, loss = 0.16870420\n",
      "Iteration 9, loss = 0.14881320\n",
      "Iteration 10, loss = 0.13224120\n",
      "Iteration 11, loss = 0.12021389\n",
      "Iteration 12, loss = 0.10751696\n",
      "Iteration 13, loss = 0.09677794\n",
      "Iteration 14, loss = 0.08859093\n",
      "Iteration 15, loss = 0.08024533\n",
      "Iteration 16, loss = 0.07289958\n",
      "Iteration 17, loss = 0.06639605\n",
      "Iteration 18, loss = 0.06147130\n",
      "Iteration 19, loss = 0.05477206\n",
      "Iteration 20, loss = 0.05078018\n",
      "Iteration 21, loss = 0.04645015\n",
      "Iteration 22, loss = 0.04245181\n",
      "Iteration 23, loss = 0.03942176\n",
      "Iteration 24, loss = 0.03541762\n",
      "Iteration 25, loss = 0.03317510\n",
      "Iteration 26, loss = 0.03117597\n",
      "Iteration 27, loss = 0.02873424\n",
      "Iteration 28, loss = 0.02601916\n",
      "Iteration 29, loss = 0.02349329\n",
      "Iteration 30, loss = 0.02189585\n",
      "Iteration 31, loss = 0.02019707\n",
      "Iteration 32, loss = 0.01905805\n",
      "Iteration 33, loss = 0.01775040\n",
      "Iteration 34, loss = 0.01683083\n",
      "Iteration 35, loss = 0.01647285\n",
      "Iteration 36, loss = 0.01459523\n",
      "Iteration 37, loss = 0.01402462\n",
      "Iteration 38, loss = 0.01341308\n",
      "Iteration 39, loss = 0.01326722\n",
      "Iteration 40, loss = 0.01253219\n",
      "Iteration 41, loss = 0.01211735\n",
      "Iteration 42, loss = 0.01168661\n",
      "Iteration 43, loss = 0.01215546\n",
      "Iteration 44, loss = 0.01189553\n",
      "Iteration 45, loss = 0.01167174\n",
      "Iteration 46, loss = 0.01119407\n",
      "Iteration 47, loss = 0.01503581\n",
      "Iteration 48, loss = 0.02067955\n",
      "Iteration 49, loss = 0.01221843\n",
      "Iteration 50, loss = 0.01044984\n",
      "Iteration 51, loss = 0.01023327\n",
      "Iteration 52, loss = 0.01006222\n",
      "Iteration 53, loss = 0.00992636\n",
      "Iteration 54, loss = 0.00982885\n",
      "Iteration 55, loss = 0.00990770\n",
      "Iteration 56, loss = 0.00986384\n",
      "Iteration 57, loss = 0.01963333\n",
      "Iteration 58, loss = 0.01816985\n",
      "Iteration 59, loss = 0.01051705\n",
      "Iteration 60, loss = 0.00984588\n",
      "Iteration 61, loss = 0.00961143\n",
      "Iteration 62, loss = 0.00950020\n",
      "Iteration 63, loss = 0.00939546\n",
      "Iteration 64, loss = 0.00928607\n",
      "Iteration 65, loss = 0.00917637\n",
      "Iteration 66, loss = 0.00908958\n",
      "Iteration 67, loss = 0.00901531\n",
      "Iteration 68, loss = 0.00892928\n",
      "Iteration 69, loss = 0.00883050\n",
      "Iteration 70, loss = 0.00873675\n",
      "Iteration 71, loss = 0.00866461\n",
      "Iteration 72, loss = 0.00858021\n",
      "Iteration 73, loss = 0.00847924\n",
      "Iteration 74, loss = 0.00840035\n",
      "Iteration 75, loss = 0.00833255\n",
      "Iteration 76, loss = 0.00830747\n",
      "Iteration 77, loss = 0.03221665\n",
      "Iteration 78, loss = 0.01470248\n",
      "Iteration 79, loss = 0.00981212\n",
      "Iteration 80, loss = 0.00886707\n",
      "Iteration 81, loss = 0.00867464\n",
      "Iteration 82, loss = 0.00857775\n",
      "Iteration 83, loss = 0.00850516\n",
      "Iteration 84, loss = 0.00843603\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.001, batch_size=200, hidden_layer_sizes=(100, 100, 100), solver=adam; total time= 1.7min\n",
      "Iteration 1, loss = 2.30459288\n",
      "Iteration 2, loss = 2.30179955\n",
      "Iteration 3, loss = 2.30178210\n",
      "Iteration 4, loss = 2.30176298\n",
      "Iteration 5, loss = 2.30165550\n",
      "Iteration 6, loss = 2.30163514\n",
      "Iteration 7, loss = 2.30161403\n",
      "Iteration 8, loss = 2.30146888\n",
      "Iteration 9, loss = 2.30154890\n",
      "Iteration 10, loss = 2.30140838\n",
      "Iteration 11, loss = 2.30140506\n",
      "Iteration 12, loss = 2.30144742\n",
      "Iteration 13, loss = 2.30135781\n",
      "Iteration 14, loss = 2.30133619\n",
      "Iteration 15, loss = 2.30117276\n",
      "Iteration 16, loss = 2.30120934\n",
      "Iteration 17, loss = 2.30119986\n",
      "Iteration 18, loss = 2.30105057\n",
      "Iteration 19, loss = 2.30090864\n",
      "Iteration 20, loss = 2.30100349\n",
      "Iteration 21, loss = 2.30094691\n",
      "Iteration 22, loss = 2.30090264\n",
      "Iteration 23, loss = 2.30082815\n",
      "Iteration 24, loss = 2.30075271\n",
      "Iteration 25, loss = 2.30075668\n",
      "Iteration 26, loss = 2.30069908\n",
      "Iteration 27, loss = 2.30061036\n",
      "Iteration 28, loss = 2.30053011\n",
      "Iteration 29, loss = 2.30049245\n",
      "Iteration 30, loss = 2.30039892\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.001, batch_size=200, hidden_layer_sizes=(100, 100, 100), solver=sgd; total time=  26.9s\n",
      "Iteration 1, loss = 2.31059214\n",
      "Iteration 2, loss = 2.30174706\n",
      "Iteration 3, loss = 2.30178177\n",
      "Iteration 4, loss = 2.30169812\n",
      "Iteration 5, loss = 2.30158529\n",
      "Iteration 6, loss = 2.30161188\n",
      "Iteration 7, loss = 2.30156411\n",
      "Iteration 8, loss = 2.30155019\n",
      "Iteration 9, loss = 2.30145546\n",
      "Iteration 10, loss = 2.30142612\n",
      "Iteration 11, loss = 2.30130839\n",
      "Iteration 12, loss = 2.30123510\n",
      "Iteration 13, loss = 2.30129375\n",
      "Iteration 14, loss = 2.30115413\n",
      "Iteration 15, loss = 2.30124848\n",
      "Iteration 16, loss = 2.30115455\n",
      "Iteration 17, loss = 2.30110515\n",
      "Iteration 18, loss = 2.30096620\n",
      "Iteration 19, loss = 2.30100914\n",
      "Iteration 20, loss = 2.30094425\n",
      "Iteration 21, loss = 2.30081317\n",
      "Iteration 22, loss = 2.30083842\n",
      "Iteration 23, loss = 2.30071319\n",
      "Iteration 24, loss = 2.30065343\n",
      "Iteration 25, loss = 2.30059620\n",
      "Iteration 26, loss = 2.30062020\n",
      "Iteration 27, loss = 2.30055272\n",
      "Iteration 28, loss = 2.30042267\n",
      "Iteration 29, loss = 2.30036928\n",
      "Iteration 30, loss = 2.30036719\n",
      "Iteration 31, loss = 2.30030119\n",
      "Iteration 32, loss = 2.30013353\n",
      "Iteration 33, loss = 2.30006715\n",
      "Iteration 34, loss = 2.29998762\n",
      "Iteration 35, loss = 2.30002006\n",
      "Iteration 36, loss = 2.29992570\n",
      "Iteration 37, loss = 2.29991997\n",
      "Iteration 38, loss = 2.29983173\n",
      "Iteration 39, loss = 2.29971589\n",
      "Iteration 40, loss = 2.29973129\n",
      "Iteration 41, loss = 2.29960772\n",
      "Iteration 42, loss = 2.29954993\n",
      "Iteration 43, loss = 2.29951885\n",
      "Iteration 44, loss = 2.29922777\n",
      "Iteration 45, loss = 2.29933384\n",
      "Iteration 46, loss = 2.29923823\n",
      "Iteration 47, loss = 2.29910907\n",
      "Iteration 48, loss = 2.29903016\n",
      "Iteration 49, loss = 2.29895817\n",
      "Iteration 50, loss = 2.29876278\n",
      "Iteration 51, loss = 2.29870841\n",
      "Iteration 52, loss = 2.29861030\n",
      "Iteration 53, loss = 2.29856907\n",
      "Iteration 54, loss = 2.29854226\n",
      "Iteration 55, loss = 2.29829336\n",
      "Iteration 56, loss = 2.29820430\n",
      "Iteration 57, loss = 2.29811544\n",
      "Iteration 58, loss = 2.29799411\n",
      "Iteration 59, loss = 2.29790036\n",
      "Iteration 60, loss = 2.29781270\n",
      "Iteration 61, loss = 2.29770340\n",
      "Iteration 62, loss = 2.29751477\n",
      "Iteration 63, loss = 2.29739695\n",
      "Iteration 64, loss = 2.29720269\n",
      "Iteration 65, loss = 2.29701558\n",
      "Iteration 66, loss = 2.29691178\n",
      "Iteration 67, loss = 2.29672626\n",
      "Iteration 68, loss = 2.29651628\n",
      "Iteration 69, loss = 2.29640473\n",
      "Iteration 70, loss = 2.29620401\n",
      "Iteration 71, loss = 2.29601399\n",
      "Iteration 72, loss = 2.29584395\n",
      "Iteration 73, loss = 2.29567364\n",
      "Iteration 74, loss = 2.29544375\n",
      "Iteration 75, loss = 2.29522959\n",
      "Iteration 76, loss = 2.29496027\n",
      "Iteration 77, loss = 2.29466964\n",
      "Iteration 78, loss = 2.29452037\n",
      "Iteration 79, loss = 2.29426681\n",
      "Iteration 80, loss = 2.29399919\n",
      "Iteration 81, loss = 2.29372634\n",
      "Iteration 82, loss = 2.29335784\n",
      "Iteration 83, loss = 2.29304906\n",
      "Iteration 84, loss = 2.29279195\n",
      "Iteration 85, loss = 2.29227805\n",
      "Iteration 86, loss = 2.29191970\n",
      "Iteration 87, loss = 2.29159830\n",
      "Iteration 88, loss = 2.29109379\n",
      "Iteration 89, loss = 2.29066972\n",
      "Iteration 90, loss = 2.29020608\n",
      "Iteration 91, loss = 2.28967000\n",
      "Iteration 92, loss = 2.28913891\n",
      "Iteration 93, loss = 2.28867587\n",
      "Iteration 94, loss = 2.28805985\n",
      "Iteration 95, loss = 2.28745152\n",
      "Iteration 96, loss = 2.28669915\n",
      "Iteration 97, loss = 2.28602235\n",
      "Iteration 98, loss = 2.28528868\n",
      "Iteration 99, loss = 2.28432935\n",
      "Iteration 100, loss = 2.28348420\n",
      "Iteration 101, loss = 2.28255409\n",
      "Iteration 102, loss = 2.28149975\n",
      "Iteration 103, loss = 2.28037749\n",
      "Iteration 104, loss = 2.27916182\n",
      "Iteration 105, loss = 2.27780589\n",
      "Iteration 106, loss = 2.27643722\n",
      "Iteration 107, loss = 2.27466042\n",
      "Iteration 108, loss = 2.27302466\n",
      "Iteration 109, loss = 2.27112022\n",
      "Iteration 110, loss = 2.26908107\n",
      "Iteration 111, loss = 2.26674438\n",
      "Iteration 112, loss = 2.26414094\n",
      "Iteration 113, loss = 2.26132647\n",
      "Iteration 114, loss = 2.25812920\n",
      "Iteration 115, loss = 2.25454164\n",
      "Iteration 116, loss = 2.25058893\n",
      "Iteration 117, loss = 2.24599661\n",
      "Iteration 118, loss = 2.24099916\n",
      "Iteration 119, loss = 2.23509744\n",
      "Iteration 120, loss = 2.22853180\n",
      "Iteration 121, loss = 2.22091407\n",
      "Iteration 122, loss = 2.21222805\n",
      "Iteration 123, loss = 2.20228206\n",
      "Iteration 124, loss = 2.19063655\n",
      "Iteration 125, loss = 2.17735998\n",
      "Iteration 126, loss = 2.16203576\n",
      "Iteration 127, loss = 2.14432228\n",
      "Iteration 128, loss = 2.12408823\n",
      "Iteration 129, loss = 2.10119568\n",
      "Iteration 130, loss = 2.07556581\n",
      "Iteration 131, loss = 2.04720087\n",
      "Iteration 132, loss = 2.01694594\n",
      "Iteration 133, loss = 1.98458109\n",
      "Iteration 134, loss = 1.95123164\n",
      "Iteration 135, loss = 1.91731257\n",
      "Iteration 136, loss = 1.88326840\n",
      "Iteration 137, loss = 1.84934354\n",
      "Iteration 138, loss = 1.81580031\n",
      "Iteration 139, loss = 1.78270311\n",
      "Iteration 140, loss = 1.74997411\n",
      "Iteration 141, loss = 1.71763453\n",
      "Iteration 142, loss = 1.68567862\n",
      "Iteration 143, loss = 1.65422082\n",
      "Iteration 144, loss = 1.62360667\n",
      "Iteration 145, loss = 1.59395792\n",
      "Iteration 146, loss = 1.56526227\n",
      "Iteration 147, loss = 1.53822774\n",
      "Iteration 148, loss = 1.51284839\n",
      "Iteration 149, loss = 1.48895112\n",
      "Iteration 150, loss = 1.46675511\n",
      "Iteration 151, loss = 1.44616134\n",
      "Iteration 152, loss = 1.42714829\n",
      "Iteration 153, loss = 1.40946722\n",
      "Iteration 154, loss = 1.39295072\n",
      "Iteration 155, loss = 1.37754363\n",
      "Iteration 156, loss = 1.36327382\n",
      "Iteration 157, loss = 1.34972660\n",
      "Iteration 158, loss = 1.33694209\n",
      "Iteration 159, loss = 1.32483416\n",
      "Iteration 160, loss = 1.31333213\n",
      "Iteration 161, loss = 1.30230641\n",
      "Iteration 162, loss = 1.29161130\n",
      "Iteration 163, loss = 1.28138115\n",
      "Iteration 164, loss = 1.27152261\n",
      "Iteration 165, loss = 1.26192879\n",
      "Iteration 166, loss = 1.25247236\n",
      "Iteration 167, loss = 1.24328672\n",
      "Iteration 168, loss = 1.23436676\n",
      "Iteration 169, loss = 1.22555359\n",
      "Iteration 170, loss = 1.21695657\n",
      "Iteration 171, loss = 1.20852635\n",
      "Iteration 172, loss = 1.20011213\n",
      "Iteration 173, loss = 1.19194801\n",
      "Iteration 174, loss = 1.18389765\n",
      "Iteration 175, loss = 1.17582628\n",
      "Iteration 176, loss = 1.16793769\n",
      "Iteration 177, loss = 1.16011225\n",
      "Iteration 178, loss = 1.15244374\n",
      "Iteration 179, loss = 1.14483661\n",
      "Iteration 180, loss = 1.13718221\n",
      "Iteration 181, loss = 1.12967679\n",
      "Iteration 182, loss = 1.12234183\n",
      "Iteration 183, loss = 1.11496071\n",
      "Iteration 184, loss = 1.10766509\n",
      "Iteration 185, loss = 1.10038386\n",
      "Iteration 186, loss = 1.09315739\n",
      "Iteration 187, loss = 1.08590497\n",
      "Iteration 188, loss = 1.07867233\n",
      "Iteration 189, loss = 1.07147543\n",
      "Iteration 190, loss = 1.06443308\n",
      "Iteration 191, loss = 1.05721975\n",
      "Iteration 192, loss = 1.05008991\n",
      "Iteration 193, loss = 1.04289766\n",
      "Iteration 194, loss = 1.03590281\n",
      "Iteration 195, loss = 1.02878283\n",
      "Iteration 196, loss = 1.02171360\n",
      "Iteration 197, loss = 1.01455818\n",
      "Iteration 198, loss = 1.00754946\n",
      "Iteration 199, loss = 1.00052596\n",
      "Iteration 200, loss = 0.99343988\n",
      "[CV] END activation=logistic, alpha=0.001, batch_size=200, hidden_layer_sizes=(100, 100, 100), solver=sgd; total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.30895313\n",
      "Iteration 2, loss = 2.30190550\n",
      "Iteration 3, loss = 2.30178759\n",
      "Iteration 4, loss = 2.30172292\n",
      "Iteration 5, loss = 2.30168378\n",
      "Iteration 6, loss = 2.30171638\n",
      "Iteration 7, loss = 2.30164787\n",
      "Iteration 8, loss = 2.30157511\n",
      "Iteration 9, loss = 2.30153091\n",
      "Iteration 10, loss = 2.30148058\n",
      "Iteration 11, loss = 2.30153996\n",
      "Iteration 12, loss = 2.30134685\n",
      "Iteration 13, loss = 2.30143441\n",
      "Iteration 14, loss = 2.30126838\n",
      "Iteration 15, loss = 2.30124311\n",
      "Iteration 16, loss = 2.30118115\n",
      "Iteration 17, loss = 2.30108795\n",
      "Iteration 18, loss = 2.30106507\n",
      "Iteration 19, loss = 2.30108195\n",
      "Iteration 20, loss = 2.30110004\n",
      "Iteration 21, loss = 2.30096601\n",
      "Iteration 22, loss = 2.30088362\n",
      "Iteration 23, loss = 2.30099164\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.001, batch_size=200, hidden_layer_sizes=(100, 100, 100), solver=sgd; total time=  21.8s\n",
      "Iteration 1, loss = 2.31087416\n",
      "Iteration 2, loss = 2.30190924\n",
      "Iteration 3, loss = 2.30173668\n",
      "Iteration 4, loss = 2.30167314\n",
      "Iteration 5, loss = 2.30177287\n",
      "Iteration 6, loss = 2.30167656\n",
      "Iteration 7, loss = 2.30160336\n",
      "Iteration 8, loss = 2.30152231\n",
      "Iteration 9, loss = 2.30149755\n",
      "Iteration 10, loss = 2.30150619\n",
      "Iteration 11, loss = 2.30137110\n",
      "Iteration 12, loss = 2.30139681\n",
      "Iteration 13, loss = 2.30137905\n",
      "Iteration 14, loss = 2.30138800\n",
      "Iteration 15, loss = 2.30124395\n",
      "Iteration 16, loss = 2.30115405\n",
      "Iteration 17, loss = 2.30123791\n",
      "Iteration 18, loss = 2.30108152\n",
      "Iteration 19, loss = 2.30110018\n",
      "Iteration 20, loss = 2.30098878\n",
      "Iteration 21, loss = 2.30089796\n",
      "Iteration 22, loss = 2.30078345\n",
      "Iteration 23, loss = 2.30074565\n",
      "Iteration 24, loss = 2.30076274\n",
      "Iteration 25, loss = 2.30069767\n",
      "Iteration 26, loss = 2.30063018\n",
      "Iteration 27, loss = 2.30061036\n",
      "Iteration 28, loss = 2.30060890\n",
      "Iteration 29, loss = 2.30049453\n",
      "Iteration 30, loss = 2.30039110\n",
      "Iteration 31, loss = 2.30035556\n",
      "Iteration 32, loss = 2.30029874\n",
      "Iteration 33, loss = 2.30028094\n",
      "Iteration 34, loss = 2.30016666\n",
      "Iteration 35, loss = 2.30014240\n",
      "Iteration 36, loss = 2.30006440\n",
      "Iteration 37, loss = 2.29988077\n",
      "Iteration 38, loss = 2.30007044\n",
      "Iteration 39, loss = 2.29992314\n",
      "Iteration 40, loss = 2.29978177\n",
      "Iteration 41, loss = 2.29973129\n",
      "Iteration 42, loss = 2.29961779\n",
      "Iteration 43, loss = 2.29947726\n",
      "Iteration 44, loss = 2.29945571\n",
      "Iteration 45, loss = 2.29930681\n",
      "Iteration 46, loss = 2.29944278\n",
      "Iteration 47, loss = 2.29917493\n",
      "Iteration 48, loss = 2.29906419\n",
      "Iteration 49, loss = 2.29899926\n",
      "Iteration 50, loss = 2.29899033\n",
      "Iteration 51, loss = 2.29885157\n",
      "Iteration 52, loss = 2.29877577\n",
      "Iteration 53, loss = 2.29865757\n",
      "Iteration 54, loss = 2.29853856\n",
      "Iteration 55, loss = 2.29855651\n",
      "Iteration 56, loss = 2.29831774\n",
      "Iteration 57, loss = 2.29823087\n",
      "Iteration 58, loss = 2.29810081\n",
      "Iteration 59, loss = 2.29798042\n",
      "Iteration 60, loss = 2.29788401\n",
      "Iteration 61, loss = 2.29768835\n",
      "Iteration 62, loss = 2.29749530\n",
      "Iteration 63, loss = 2.29750281\n",
      "Iteration 64, loss = 2.29725637\n",
      "Iteration 65, loss = 2.29723576\n",
      "Iteration 66, loss = 2.29704240\n",
      "Iteration 67, loss = 2.29688474\n",
      "Iteration 68, loss = 2.29665531\n",
      "Iteration 69, loss = 2.29645129\n",
      "Iteration 70, loss = 2.29640579\n",
      "Iteration 71, loss = 2.29613727\n",
      "Iteration 72, loss = 2.29600673\n",
      "Iteration 73, loss = 2.29584970\n",
      "Iteration 74, loss = 2.29550077\n",
      "Iteration 75, loss = 2.29533191\n",
      "Iteration 76, loss = 2.29498959\n",
      "Iteration 77, loss = 2.29482782\n",
      "Iteration 78, loss = 2.29467790\n",
      "Iteration 79, loss = 2.29436747\n",
      "Iteration 80, loss = 2.29404426\n",
      "Iteration 81, loss = 2.29361425\n",
      "Iteration 82, loss = 2.29337813\n",
      "Iteration 83, loss = 2.29302214\n",
      "Iteration 84, loss = 2.29280615\n",
      "Iteration 85, loss = 2.29233734\n",
      "Iteration 86, loss = 2.29204185\n",
      "Iteration 87, loss = 2.29164580\n",
      "Iteration 88, loss = 2.29119986\n",
      "Iteration 89, loss = 2.29068071\n",
      "Iteration 90, loss = 2.29024254\n",
      "Iteration 91, loss = 2.28975213\n",
      "Iteration 92, loss = 2.28923287\n",
      "Iteration 93, loss = 2.28870167\n",
      "Iteration 94, loss = 2.28806890\n",
      "Iteration 95, loss = 2.28740954\n",
      "Iteration 96, loss = 2.28665730\n",
      "Iteration 97, loss = 2.28592294\n",
      "Iteration 98, loss = 2.28519834\n",
      "Iteration 99, loss = 2.28437221\n",
      "Iteration 100, loss = 2.28336332\n",
      "Iteration 101, loss = 2.28260133\n",
      "Iteration 102, loss = 2.28132536\n",
      "Iteration 103, loss = 2.28020312\n",
      "Iteration 104, loss = 2.27906473\n",
      "Iteration 105, loss = 2.27762531\n",
      "Iteration 106, loss = 2.27614357\n",
      "Iteration 107, loss = 2.27453180\n",
      "Iteration 108, loss = 2.27270876\n",
      "Iteration 109, loss = 2.27089309\n",
      "Iteration 110, loss = 2.26878976\n",
      "Iteration 111, loss = 2.26643136\n",
      "Iteration 112, loss = 2.26361899\n",
      "Iteration 113, loss = 2.26095442\n",
      "Iteration 114, loss = 2.25779109\n",
      "Iteration 115, loss = 2.25419561\n",
      "Iteration 116, loss = 2.25018188\n",
      "Iteration 117, loss = 2.24552363\n",
      "Iteration 118, loss = 2.24045788\n",
      "Iteration 119, loss = 2.23461105\n",
      "Iteration 120, loss = 2.22794659\n",
      "Iteration 121, loss = 2.22031435\n",
      "Iteration 122, loss = 2.21162246\n",
      "Iteration 123, loss = 2.20162774\n",
      "Iteration 124, loss = 2.18994002\n",
      "Iteration 125, loss = 2.17649937\n",
      "Iteration 126, loss = 2.16101174\n",
      "Iteration 127, loss = 2.14298178\n",
      "Iteration 128, loss = 2.12246780\n",
      "Iteration 129, loss = 2.09879622\n",
      "Iteration 130, loss = 2.07195220\n",
      "Iteration 131, loss = 2.04201326\n",
      "Iteration 132, loss = 2.00920692\n",
      "Iteration 133, loss = 1.97374856\n",
      "Iteration 134, loss = 1.93634781\n",
      "Iteration 135, loss = 1.89766789\n",
      "Iteration 136, loss = 1.85831164\n",
      "Iteration 137, loss = 1.81891522\n",
      "Iteration 138, loss = 1.77983704\n",
      "Iteration 139, loss = 1.74133308\n",
      "Iteration 140, loss = 1.70370793\n",
      "Iteration 141, loss = 1.66697945\n",
      "Iteration 142, loss = 1.63141442\n",
      "Iteration 143, loss = 1.59729262\n",
      "Iteration 144, loss = 1.56490247\n",
      "Iteration 145, loss = 1.53445168\n",
      "Iteration 146, loss = 1.50589361\n",
      "Iteration 147, loss = 1.47967687\n",
      "Iteration 148, loss = 1.45554320\n",
      "Iteration 149, loss = 1.43353408\n",
      "Iteration 150, loss = 1.41325854\n",
      "Iteration 151, loss = 1.39486293\n",
      "Iteration 152, loss = 1.37800175\n",
      "Iteration 153, loss = 1.36233327\n",
      "Iteration 154, loss = 1.34801228\n",
      "Iteration 155, loss = 1.33445943\n",
      "Iteration 156, loss = 1.32198733\n",
      "Iteration 157, loss = 1.31015875\n",
      "Iteration 158, loss = 1.29910284\n",
      "Iteration 159, loss = 1.28852742\n",
      "Iteration 160, loss = 1.27844597\n",
      "Iteration 161, loss = 1.26880518\n",
      "Iteration 162, loss = 1.25953998\n",
      "Iteration 163, loss = 1.25071581\n",
      "Iteration 164, loss = 1.24206017\n",
      "Iteration 165, loss = 1.23365751\n",
      "Iteration 166, loss = 1.22550838\n",
      "Iteration 167, loss = 1.21769740\n",
      "Iteration 168, loss = 1.20987428\n",
      "Iteration 169, loss = 1.20246112\n",
      "Iteration 170, loss = 1.19521000\n",
      "Iteration 171, loss = 1.18804033\n",
      "Iteration 172, loss = 1.18099408\n",
      "Iteration 173, loss = 1.17417799\n",
      "Iteration 174, loss = 1.16745134\n",
      "Iteration 175, loss = 1.16082749\n",
      "Iteration 176, loss = 1.15422705\n",
      "Iteration 177, loss = 1.14804041\n",
      "Iteration 178, loss = 1.14162753\n",
      "Iteration 179, loss = 1.13550172\n",
      "Iteration 180, loss = 1.12926078\n",
      "Iteration 181, loss = 1.12312407\n",
      "Iteration 182, loss = 1.11714296\n",
      "Iteration 183, loss = 1.11124310\n",
      "Iteration 184, loss = 1.10519089\n",
      "Iteration 185, loss = 1.09924233\n",
      "Iteration 186, loss = 1.09337488\n",
      "Iteration 187, loss = 1.08748825\n",
      "Iteration 188, loss = 1.08153328\n",
      "Iteration 189, loss = 1.07568385\n",
      "Iteration 190, loss = 1.06985709\n",
      "Iteration 191, loss = 1.06392635\n",
      "Iteration 192, loss = 1.05806546\n",
      "Iteration 193, loss = 1.05223366\n",
      "Iteration 194, loss = 1.04645168\n",
      "Iteration 195, loss = 1.04052613\n",
      "Iteration 196, loss = 1.03461186\n",
      "Iteration 197, loss = 1.02879099\n",
      "Iteration 198, loss = 1.02306454\n",
      "Iteration 199, loss = 1.01715871\n",
      "Iteration 200, loss = 1.01129672\n",
      "[CV] END activation=logistic, alpha=0.001, batch_size=200, hidden_layer_sizes=(100, 100, 100), solver=sgd; total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.31764538\n",
      "Iteration 2, loss = 2.30202063\n",
      "Iteration 3, loss = 2.30197367\n",
      "Iteration 4, loss = 2.30197396\n",
      "Iteration 5, loss = 2.30196265\n",
      "Iteration 6, loss = 2.30181717\n",
      "Iteration 7, loss = 2.30183524\n",
      "Iteration 8, loss = 2.30188830\n",
      "Iteration 9, loss = 2.30175734\n",
      "Iteration 10, loss = 2.30154635\n",
      "Iteration 11, loss = 2.30173353\n",
      "Iteration 12, loss = 2.30165502\n",
      "Iteration 13, loss = 2.30157283\n",
      "Iteration 14, loss = 2.30143582\n",
      "Iteration 15, loss = 2.30158558\n",
      "Iteration 16, loss = 2.30152625\n",
      "Iteration 17, loss = 2.30145184\n",
      "Iteration 18, loss = 2.30143660\n",
      "Iteration 19, loss = 2.30136609\n",
      "Iteration 20, loss = 2.30138934\n",
      "Iteration 21, loss = 2.30131859\n",
      "Iteration 22, loss = 2.30122002\n",
      "Iteration 23, loss = 2.30124083\n",
      "Iteration 24, loss = 2.30114151\n",
      "Iteration 25, loss = 2.30108715\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.001, batch_size=200, hidden_layer_sizes=(100, 100, 100), solver=sgd; total time=  22.0s\n",
      "Iteration 1, loss = 0.70360484\n",
      "Iteration 2, loss = 0.34046665\n",
      "Iteration 3, loss = 0.29107166\n",
      "Iteration 4, loss = 0.26203187\n",
      "Iteration 5, loss = 0.24268445\n",
      "Iteration 6, loss = 0.22727053\n",
      "Iteration 7, loss = 0.21571400\n",
      "Iteration 8, loss = 0.20702178\n",
      "Iteration 9, loss = 0.19906991\n",
      "Iteration 10, loss = 0.19356240\n",
      "Iteration 11, loss = 0.18729449\n",
      "Iteration 12, loss = 0.18431059\n",
      "Iteration 13, loss = 0.17986459\n",
      "Iteration 14, loss = 0.17761850\n",
      "Iteration 15, loss = 0.17496536\n",
      "Iteration 16, loss = 0.17278947\n",
      "Iteration 17, loss = 0.17055993\n",
      "Iteration 18, loss = 0.16934983\n",
      "Iteration 19, loss = 0.16832917\n",
      "Iteration 20, loss = 0.16613987\n",
      "Iteration 21, loss = 0.16513478\n",
      "Iteration 22, loss = 0.16416124\n",
      "Iteration 23, loss = 0.16363672\n",
      "Iteration 24, loss = 0.16301664\n",
      "Iteration 25, loss = 0.16243131\n",
      "Iteration 26, loss = 0.16128208\n",
      "Iteration 27, loss = 0.16090992\n",
      "Iteration 28, loss = 0.15988885\n",
      "Iteration 29, loss = 0.15985713\n",
      "Iteration 30, loss = 0.15958126\n",
      "Iteration 31, loss = 0.15883401\n",
      "Iteration 32, loss = 0.15806358\n",
      "Iteration 33, loss = 0.15829164\n",
      "Iteration 34, loss = 0.15769607\n",
      "Iteration 35, loss = 0.15753958\n",
      "Iteration 36, loss = 0.15687158\n",
      "Iteration 37, loss = 0.15700028\n",
      "Iteration 38, loss = 0.15659632\n",
      "Iteration 39, loss = 0.15643304\n",
      "Iteration 40, loss = 0.15584344\n",
      "Iteration 41, loss = 0.15614600\n",
      "Iteration 42, loss = 0.15563655\n",
      "Iteration 43, loss = 0.15521170\n",
      "Iteration 44, loss = 0.15596109\n",
      "Iteration 45, loss = 0.15539020\n",
      "Iteration 46, loss = 0.15479563\n",
      "Iteration 47, loss = 0.15480708\n",
      "Iteration 48, loss = 0.15453427\n",
      "Iteration 49, loss = 0.15441314\n",
      "Iteration 50, loss = 0.15444514\n",
      "Iteration 51, loss = 0.15429749\n",
      "Iteration 52, loss = 0.15418054\n",
      "Iteration 53, loss = 0.15392230\n",
      "Iteration 54, loss = 0.15383625\n",
      "Iteration 55, loss = 0.15329992\n",
      "Iteration 56, loss = 0.15372486\n",
      "Iteration 57, loss = 0.15382386\n",
      "Iteration 58, loss = 0.15346019\n",
      "Iteration 59, loss = 0.15425455\n",
      "Iteration 60, loss = 0.15277688\n",
      "Iteration 61, loss = 0.15316308\n",
      "Iteration 62, loss = 0.15311938\n",
      "Iteration 63, loss = 0.15247012\n",
      "Iteration 64, loss = 0.15266364\n",
      "Iteration 65, loss = 0.15302373\n",
      "Iteration 66, loss = 0.15260027\n",
      "Iteration 67, loss = 0.15270008\n",
      "Iteration 68, loss = 0.15262603\n",
      "Iteration 69, loss = 0.15246839\n",
      "Iteration 70, loss = 0.15253659\n",
      "Iteration 71, loss = 0.15246180\n",
      "Iteration 72, loss = 0.15235576\n",
      "Iteration 73, loss = 0.15194161\n",
      "Iteration 74, loss = 0.15188155\n",
      "Iteration 75, loss = 0.15173836\n",
      "Iteration 76, loss = 0.15159939\n",
      "Iteration 77, loss = 0.15128501\n",
      "Iteration 78, loss = 0.15136627\n",
      "Iteration 79, loss = 0.15191329\n",
      "Iteration 80, loss = 0.15116588\n",
      "Iteration 81, loss = 0.15171854\n",
      "Iteration 82, loss = 0.15152613\n",
      "Iteration 83, loss = 0.15159780\n",
      "Iteration 84, loss = 0.15155503\n",
      "Iteration 85, loss = 0.15173938\n",
      "Iteration 86, loss = 0.15103489\n",
      "Iteration 87, loss = 0.15085762\n",
      "Iteration 88, loss = 0.15078233\n",
      "Iteration 89, loss = 0.15110962\n",
      "Iteration 90, loss = 0.15091462\n",
      "Iteration 91, loss = 0.15066952\n",
      "Iteration 92, loss = 0.15061670\n",
      "Iteration 93, loss = 0.15058797\n",
      "Iteration 94, loss = 0.15056538\n",
      "Iteration 95, loss = 0.15110596\n",
      "Iteration 96, loss = 0.15039593\n",
      "Iteration 97, loss = 0.15087619\n",
      "Iteration 98, loss = 0.15057193\n",
      "Iteration 99, loss = 0.15020584\n",
      "Iteration 100, loss = 0.15028992\n",
      "Iteration 101, loss = 0.14987927\n",
      "Iteration 102, loss = 0.15059140\n",
      "Iteration 103, loss = 0.15024494\n",
      "Iteration 104, loss = 0.15056522\n",
      "Iteration 105, loss = 0.14998398\n",
      "Iteration 106, loss = 0.14983678\n",
      "Iteration 107, loss = 0.14964567\n",
      "Iteration 108, loss = 0.15003029\n",
      "Iteration 109, loss = 0.15003639\n",
      "Iteration 110, loss = 0.15084464\n",
      "Iteration 111, loss = 0.14973313\n",
      "Iteration 112, loss = 0.15009297\n",
      "Iteration 113, loss = 0.14983159\n",
      "Iteration 114, loss = 0.14970179\n",
      "Iteration 115, loss = 0.14990065\n",
      "Iteration 116, loss = 0.14938059\n",
      "Iteration 117, loss = 0.14966676\n",
      "Iteration 118, loss = 0.14970156\n",
      "Iteration 119, loss = 0.14998906\n",
      "Iteration 120, loss = 0.14923163\n",
      "Iteration 121, loss = 0.15001019\n",
      "Iteration 122, loss = 0.14912257\n",
      "Iteration 123, loss = 0.14919077\n",
      "Iteration 124, loss = 0.14928998\n",
      "Iteration 125, loss = 0.14880683\n",
      "Iteration 126, loss = 0.14943373\n",
      "Iteration 127, loss = 0.14984295\n",
      "Iteration 128, loss = 0.14948854\n",
      "Iteration 129, loss = 0.14952638\n",
      "Iteration 130, loss = 0.14942470\n",
      "Iteration 131, loss = 0.14945793\n",
      "Iteration 132, loss = 0.14947459\n",
      "Iteration 133, loss = 0.14904524\n",
      "Iteration 134, loss = 0.14898324\n",
      "Iteration 135, loss = 0.14904665\n",
      "Iteration 136, loss = 0.14897748\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.01, batch_size=50, hidden_layer_sizes=(100,), solver=adam; total time= 4.9min\n",
      "Iteration 1, loss = 0.69699894\n",
      "Iteration 2, loss = 0.34173954\n",
      "Iteration 3, loss = 0.29446730\n",
      "Iteration 4, loss = 0.26790925\n",
      "Iteration 5, loss = 0.24797001\n",
      "Iteration 6, loss = 0.23304506\n",
      "Iteration 7, loss = 0.22203221\n",
      "Iteration 8, loss = 0.21295807\n",
      "Iteration 9, loss = 0.20523062\n",
      "Iteration 10, loss = 0.19885922\n",
      "Iteration 11, loss = 0.19390858\n",
      "Iteration 12, loss = 0.18978748\n",
      "Iteration 13, loss = 0.18506173\n",
      "Iteration 14, loss = 0.18186864\n",
      "Iteration 15, loss = 0.17870482\n",
      "Iteration 16, loss = 0.17752023\n",
      "Iteration 17, loss = 0.17490777\n",
      "Iteration 18, loss = 0.17257309\n",
      "Iteration 19, loss = 0.17090674\n",
      "Iteration 20, loss = 0.16983762\n",
      "Iteration 21, loss = 0.16843608\n",
      "Iteration 22, loss = 0.16767325\n",
      "Iteration 23, loss = 0.16731239\n",
      "Iteration 24, loss = 0.16564286\n",
      "Iteration 25, loss = 0.16485211\n",
      "Iteration 26, loss = 0.16386558\n",
      "Iteration 27, loss = 0.16362888\n",
      "Iteration 28, loss = 0.16277099\n",
      "Iteration 29, loss = 0.16281534\n",
      "Iteration 30, loss = 0.16152618\n",
      "Iteration 31, loss = 0.16245491\n",
      "Iteration 32, loss = 0.16128287\n",
      "Iteration 33, loss = 0.16031753\n",
      "Iteration 34, loss = 0.15943524\n",
      "Iteration 35, loss = 0.15970655\n",
      "Iteration 36, loss = 0.15937826\n",
      "Iteration 37, loss = 0.15880802\n",
      "Iteration 38, loss = 0.15880738\n",
      "Iteration 39, loss = 0.15933677\n",
      "Iteration 40, loss = 0.15870234\n",
      "Iteration 41, loss = 0.15806264\n",
      "Iteration 42, loss = 0.15753263\n",
      "Iteration 43, loss = 0.15772608\n",
      "Iteration 44, loss = 0.15767960\n",
      "Iteration 45, loss = 0.15680950\n",
      "Iteration 46, loss = 0.15711363\n",
      "Iteration 47, loss = 0.15737173\n",
      "Iteration 48, loss = 0.15702700\n",
      "Iteration 49, loss = 0.15617678\n",
      "Iteration 50, loss = 0.15619781\n",
      "Iteration 51, loss = 0.15602243\n",
      "Iteration 52, loss = 0.15611278\n",
      "Iteration 53, loss = 0.15608253\n",
      "Iteration 54, loss = 0.15580825\n",
      "Iteration 55, loss = 0.15560213\n",
      "Iteration 56, loss = 0.15573672\n",
      "Iteration 57, loss = 0.15548932\n",
      "Iteration 58, loss = 0.15485992\n",
      "Iteration 59, loss = 0.15505457\n",
      "Iteration 60, loss = 0.15507546\n",
      "Iteration 61, loss = 0.15434520\n",
      "Iteration 62, loss = 0.15505596\n",
      "Iteration 63, loss = 0.15466755\n",
      "Iteration 64, loss = 0.15524321\n",
      "Iteration 65, loss = 0.15493482\n",
      "Iteration 66, loss = 0.15421429\n",
      "Iteration 67, loss = 0.15435135\n",
      "Iteration 68, loss = 0.15359647\n",
      "Iteration 69, loss = 0.15392277\n",
      "Iteration 70, loss = 0.15371758\n",
      "Iteration 71, loss = 0.15342275\n",
      "Iteration 72, loss = 0.15337919\n",
      "Iteration 73, loss = 0.15363450\n",
      "Iteration 74, loss = 0.15294841\n",
      "Iteration 75, loss = 0.15345642\n",
      "Iteration 76, loss = 0.15318396\n",
      "Iteration 77, loss = 0.15348011\n",
      "Iteration 78, loss = 0.15276310\n",
      "Iteration 79, loss = 0.15310002\n",
      "Iteration 80, loss = 0.15289789\n",
      "Iteration 81, loss = 0.15269795\n",
      "Iteration 82, loss = 0.15273132\n",
      "Iteration 83, loss = 0.15289619\n",
      "Iteration 84, loss = 0.15272956\n",
      "Iteration 85, loss = 0.15185845\n",
      "Iteration 86, loss = 0.15304405\n",
      "Iteration 87, loss = 0.15248141\n",
      "Iteration 88, loss = 0.15233549\n",
      "Iteration 89, loss = 0.15259197\n",
      "Iteration 90, loss = 0.15241448\n",
      "Iteration 91, loss = 0.15200327\n",
      "Iteration 92, loss = 0.15152263\n",
      "Iteration 93, loss = 0.15204991\n",
      "Iteration 94, loss = 0.15170117\n",
      "Iteration 95, loss = 0.15207517\n",
      "Iteration 96, loss = 0.15220871\n",
      "Iteration 97, loss = 0.15188024\n",
      "Iteration 98, loss = 0.15184214\n",
      "Iteration 99, loss = 0.15177833\n",
      "Iteration 100, loss = 0.15203049\n",
      "Iteration 101, loss = 0.15150988\n",
      "Iteration 102, loss = 0.15183106\n",
      "Iteration 103, loss = 0.15160250\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.01, batch_size=50, hidden_layer_sizes=(100,), solver=adam; total time= 3.6min\n",
      "Iteration 1, loss = 0.70239734\n",
      "Iteration 2, loss = 0.33906839\n",
      "Iteration 3, loss = 0.29153913\n",
      "Iteration 4, loss = 0.26451982\n",
      "Iteration 5, loss = 0.24444920\n",
      "Iteration 6, loss = 0.23021804\n",
      "Iteration 7, loss = 0.21929676\n",
      "Iteration 8, loss = 0.21008639\n",
      "Iteration 9, loss = 0.20222344\n",
      "Iteration 10, loss = 0.19643847\n",
      "Iteration 11, loss = 0.19074801\n",
      "Iteration 12, loss = 0.18616910\n",
      "Iteration 13, loss = 0.18264451\n",
      "Iteration 14, loss = 0.17918941\n",
      "Iteration 15, loss = 0.17675334\n",
      "Iteration 16, loss = 0.17448496\n",
      "Iteration 17, loss = 0.17249589\n",
      "Iteration 18, loss = 0.17108080\n",
      "Iteration 19, loss = 0.16931659\n",
      "Iteration 20, loss = 0.16757990\n",
      "Iteration 21, loss = 0.16688858\n",
      "Iteration 22, loss = 0.16594663\n",
      "Iteration 23, loss = 0.16440498\n",
      "Iteration 24, loss = 0.16373842\n",
      "Iteration 25, loss = 0.16262264\n",
      "Iteration 26, loss = 0.16218259\n",
      "Iteration 27, loss = 0.16148286\n",
      "Iteration 28, loss = 0.16027183\n",
      "Iteration 29, loss = 0.16023188\n",
      "Iteration 30, loss = 0.16046140\n",
      "Iteration 31, loss = 0.15905490\n",
      "Iteration 32, loss = 0.15913055\n",
      "Iteration 33, loss = 0.15839408\n",
      "Iteration 34, loss = 0.15803726\n",
      "Iteration 35, loss = 0.15817575\n",
      "Iteration 36, loss = 0.15704225\n",
      "Iteration 37, loss = 0.15737340\n",
      "Iteration 38, loss = 0.15717420\n",
      "Iteration 39, loss = 0.15697301\n",
      "Iteration 40, loss = 0.15631893\n",
      "Iteration 41, loss = 0.15582311\n",
      "Iteration 42, loss = 0.15583961\n",
      "Iteration 43, loss = 0.15594225\n",
      "Iteration 44, loss = 0.15583610\n",
      "Iteration 45, loss = 0.15524184\n",
      "Iteration 46, loss = 0.15449692\n",
      "Iteration 47, loss = 0.15501483\n",
      "Iteration 48, loss = 0.15525924\n",
      "Iteration 49, loss = 0.15467895\n",
      "Iteration 50, loss = 0.15413368\n",
      "Iteration 51, loss = 0.15449294\n",
      "Iteration 52, loss = 0.15407393\n",
      "Iteration 53, loss = 0.15319396\n",
      "Iteration 54, loss = 0.15296429\n",
      "Iteration 55, loss = 0.15379319\n",
      "Iteration 56, loss = 0.15402617\n",
      "Iteration 57, loss = 0.15366675\n",
      "Iteration 58, loss = 0.15320876\n",
      "Iteration 59, loss = 0.15291850\n",
      "Iteration 60, loss = 0.15259688\n",
      "Iteration 61, loss = 0.15258410\n",
      "Iteration 62, loss = 0.15252871\n",
      "Iteration 63, loss = 0.15229235\n",
      "Iteration 64, loss = 0.15216964\n",
      "Iteration 65, loss = 0.15231627\n",
      "Iteration 66, loss = 0.15266187\n",
      "Iteration 67, loss = 0.15241809\n",
      "Iteration 68, loss = 0.15255157\n",
      "Iteration 69, loss = 0.15168350\n",
      "Iteration 70, loss = 0.15183964\n",
      "Iteration 71, loss = 0.15190196\n",
      "Iteration 72, loss = 0.15164478\n",
      "Iteration 73, loss = 0.15167917\n",
      "Iteration 74, loss = 0.15153119\n",
      "Iteration 75, loss = 0.15080501\n",
      "Iteration 76, loss = 0.15155841\n",
      "Iteration 77, loss = 0.15163415\n",
      "Iteration 78, loss = 0.15103254\n",
      "Iteration 79, loss = 0.15156136\n",
      "Iteration 80, loss = 0.15075889\n",
      "Iteration 81, loss = 0.15129240\n",
      "Iteration 82, loss = 0.15143098\n",
      "Iteration 83, loss = 0.15084934\n",
      "Iteration 84, loss = 0.15067942\n",
      "Iteration 85, loss = 0.15022444\n",
      "Iteration 86, loss = 0.15102519\n",
      "Iteration 87, loss = 0.15056224\n",
      "Iteration 88, loss = 0.15071181\n",
      "Iteration 89, loss = 0.15036982\n",
      "Iteration 90, loss = 0.15048799\n",
      "Iteration 91, loss = 0.14980457\n",
      "Iteration 92, loss = 0.15070103\n",
      "Iteration 93, loss = 0.15079032\n",
      "Iteration 94, loss = 0.15056994\n",
      "Iteration 95, loss = 0.14976241\n",
      "Iteration 96, loss = 0.14976565\n",
      "Iteration 97, loss = 0.15072812\n",
      "Iteration 98, loss = 0.15008009\n",
      "Iteration 99, loss = 0.14982977\n",
      "Iteration 100, loss = 0.14977997\n",
      "Iteration 101, loss = 0.15015479\n",
      "Iteration 102, loss = 0.14943918\n",
      "Iteration 103, loss = 0.14977433\n",
      "Iteration 104, loss = 0.14943949\n",
      "Iteration 105, loss = 0.15026840\n",
      "Iteration 106, loss = 0.14894216\n",
      "Iteration 107, loss = 0.14922650\n",
      "Iteration 108, loss = 0.15000165\n",
      "Iteration 109, loss = 0.14958588\n",
      "Iteration 110, loss = 0.14925799\n",
      "Iteration 111, loss = 0.14990115\n",
      "Iteration 112, loss = 0.14935835\n",
      "Iteration 113, loss = 0.14937742\n",
      "Iteration 114, loss = 0.14891971\n",
      "Iteration 115, loss = 0.14858884\n",
      "Iteration 116, loss = 0.14942885\n",
      "Iteration 117, loss = 0.14912181\n",
      "Iteration 118, loss = 0.14855531\n",
      "Iteration 119, loss = 0.14908951\n",
      "Iteration 120, loss = 0.14888619\n",
      "Iteration 121, loss = 0.14917163\n",
      "Iteration 122, loss = 0.14881023\n",
      "Iteration 123, loss = 0.14834344\n",
      "Iteration 124, loss = 0.14895082\n",
      "Iteration 125, loss = 0.14858683\n",
      "Iteration 126, loss = 0.14852513\n",
      "Iteration 127, loss = 0.14845845\n",
      "Iteration 128, loss = 0.14875381\n",
      "Iteration 129, loss = 0.14895847\n",
      "Iteration 130, loss = 0.14862464\n",
      "Iteration 131, loss = 0.14854205\n",
      "Iteration 132, loss = 0.14873438\n",
      "Iteration 133, loss = 0.14858068\n",
      "Iteration 134, loss = 0.14810795\n",
      "Iteration 135, loss = 0.14795666\n",
      "Iteration 136, loss = 0.14843648\n",
      "Iteration 137, loss = 0.14841036\n",
      "Iteration 138, loss = 0.14801157\n",
      "Iteration 139, loss = 0.14822219\n",
      "Iteration 140, loss = 0.14801177\n",
      "Iteration 141, loss = 0.14853111\n",
      "Iteration 142, loss = 0.14799469\n",
      "Iteration 143, loss = 0.14785511\n",
      "Iteration 144, loss = 0.14796629\n",
      "Iteration 145, loss = 0.14829184\n",
      "Iteration 146, loss = 0.14840652\n",
      "Iteration 147, loss = 0.14792955\n",
      "Iteration 148, loss = 0.14822290\n",
      "Iteration 149, loss = 0.14789541\n",
      "Iteration 150, loss = 0.14775262\n",
      "Iteration 151, loss = 0.14830226\n",
      "Iteration 152, loss = 0.14830400\n",
      "Iteration 153, loss = 0.14777067\n",
      "Iteration 154, loss = 0.14790268\n",
      "Iteration 155, loss = 0.14740181\n",
      "Iteration 156, loss = 0.14813956\n",
      "Iteration 157, loss = 0.14768322\n",
      "Iteration 158, loss = 0.14785307\n",
      "Iteration 159, loss = 0.14714820\n",
      "Iteration 160, loss = 0.14818708\n",
      "Iteration 161, loss = 0.14832939\n",
      "Iteration 162, loss = 0.14828470\n",
      "Iteration 163, loss = 0.14729593\n",
      "Iteration 164, loss = 0.14756357\n",
      "Iteration 165, loss = 0.14750459\n",
      "Iteration 166, loss = 0.14824312\n",
      "Iteration 167, loss = 0.14782943\n",
      "Iteration 168, loss = 0.14695356\n",
      "Iteration 169, loss = 0.14855897\n",
      "Iteration 170, loss = 0.14812927\n",
      "Iteration 171, loss = 0.14755846\n",
      "Iteration 172, loss = 0.14747241\n",
      "Iteration 173, loss = 0.14771585\n",
      "Iteration 174, loss = 0.14730827\n",
      "Iteration 175, loss = 0.14773483\n",
      "Iteration 176, loss = 0.14721741\n",
      "Iteration 177, loss = 0.14776814\n",
      "Iteration 178, loss = 0.14727163\n",
      "Iteration 179, loss = 0.14789410\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.01, batch_size=50, hidden_layer_sizes=(100,), solver=adam; total time= 6.5min\n",
      "Iteration 1, loss = 0.69458424\n",
      "Iteration 2, loss = 0.33919172\n",
      "Iteration 3, loss = 0.29217369\n",
      "Iteration 4, loss = 0.26328418\n",
      "Iteration 5, loss = 0.24328712\n",
      "Iteration 6, loss = 0.22957791\n",
      "Iteration 7, loss = 0.21725993\n",
      "Iteration 8, loss = 0.20793288\n",
      "Iteration 9, loss = 0.20112137\n",
      "Iteration 10, loss = 0.19426635\n",
      "Iteration 11, loss = 0.18954010\n",
      "Iteration 12, loss = 0.18514132\n",
      "Iteration 13, loss = 0.18165474\n",
      "Iteration 14, loss = 0.17852468\n",
      "Iteration 15, loss = 0.17594004\n",
      "Iteration 16, loss = 0.17355570\n",
      "Iteration 17, loss = 0.17162293\n",
      "Iteration 18, loss = 0.17035554\n",
      "Iteration 19, loss = 0.16846965\n",
      "Iteration 20, loss = 0.16762310\n",
      "Iteration 21, loss = 0.16621076\n",
      "Iteration 22, loss = 0.16474865\n",
      "Iteration 23, loss = 0.16390593\n",
      "Iteration 24, loss = 0.16305717\n",
      "Iteration 25, loss = 0.16270245\n",
      "Iteration 26, loss = 0.16155568\n",
      "Iteration 27, loss = 0.16116485\n",
      "Iteration 28, loss = 0.16067343\n",
      "Iteration 29, loss = 0.15990433\n",
      "Iteration 30, loss = 0.15983789\n",
      "Iteration 31, loss = 0.15926825\n",
      "Iteration 32, loss = 0.15871923\n",
      "Iteration 33, loss = 0.15838351\n",
      "Iteration 34, loss = 0.15843722\n",
      "Iteration 35, loss = 0.15817028\n",
      "Iteration 36, loss = 0.15777251\n",
      "Iteration 37, loss = 0.15750787\n",
      "Iteration 38, loss = 0.15700290\n",
      "Iteration 39, loss = 0.15694344\n",
      "Iteration 40, loss = 0.15661630\n",
      "Iteration 41, loss = 0.15669575\n",
      "Iteration 42, loss = 0.15612676\n",
      "Iteration 43, loss = 0.15593706\n",
      "Iteration 44, loss = 0.15596631\n",
      "Iteration 45, loss = 0.15604476\n",
      "Iteration 46, loss = 0.15566954\n",
      "Iteration 47, loss = 0.15522500\n",
      "Iteration 48, loss = 0.15532324\n",
      "Iteration 49, loss = 0.15523736\n",
      "Iteration 50, loss = 0.15481874\n",
      "Iteration 51, loss = 0.15486344\n",
      "Iteration 52, loss = 0.15502451\n",
      "Iteration 53, loss = 0.15382969\n",
      "Iteration 54, loss = 0.15496880\n",
      "Iteration 55, loss = 0.15477337\n",
      "Iteration 56, loss = 0.15473975\n",
      "Iteration 57, loss = 0.15421147\n",
      "Iteration 58, loss = 0.15404288\n",
      "Iteration 59, loss = 0.15344046\n",
      "Iteration 60, loss = 0.15348018\n",
      "Iteration 61, loss = 0.15387955\n",
      "Iteration 62, loss = 0.15395418\n",
      "Iteration 63, loss = 0.15315216\n",
      "Iteration 64, loss = 0.15332771\n",
      "Iteration 65, loss = 0.15299721\n",
      "Iteration 66, loss = 0.15375085\n",
      "Iteration 67, loss = 0.15339515\n",
      "Iteration 68, loss = 0.15331803\n",
      "Iteration 69, loss = 0.15321215\n",
      "Iteration 70, loss = 0.15278523\n",
      "Iteration 71, loss = 0.15267289\n",
      "Iteration 72, loss = 0.15237212\n",
      "Iteration 73, loss = 0.15265711\n",
      "Iteration 74, loss = 0.15275483\n",
      "Iteration 75, loss = 0.15295089\n",
      "Iteration 76, loss = 0.15242072\n",
      "Iteration 77, loss = 0.15252882\n",
      "Iteration 78, loss = 0.15270197\n",
      "Iteration 79, loss = 0.15237235\n",
      "Iteration 80, loss = 0.15225368\n",
      "Iteration 81, loss = 0.15166796\n",
      "Iteration 82, loss = 0.15217104\n",
      "Iteration 83, loss = 0.15168748\n",
      "Iteration 84, loss = 0.15230063\n",
      "Iteration 85, loss = 0.15228750\n",
      "Iteration 86, loss = 0.15167078\n",
      "Iteration 87, loss = 0.15104142\n",
      "Iteration 88, loss = 0.15140501\n",
      "Iteration 89, loss = 0.15202398\n",
      "Iteration 90, loss = 0.15139821\n",
      "Iteration 91, loss = 0.15155634\n",
      "Iteration 92, loss = 0.15124250\n",
      "Iteration 93, loss = 0.15179032\n",
      "Iteration 94, loss = 0.15142312\n",
      "Iteration 95, loss = 0.15117885\n",
      "Iteration 96, loss = 0.15150151\n",
      "Iteration 97, loss = 0.15148032\n",
      "Iteration 98, loss = 0.15084713\n",
      "Iteration 99, loss = 0.15153810\n",
      "Iteration 100, loss = 0.15134239\n",
      "Iteration 101, loss = 0.15125392\n",
      "Iteration 102, loss = 0.15078944\n",
      "Iteration 103, loss = 0.15110763\n",
      "Iteration 104, loss = 0.15044282\n",
      "Iteration 105, loss = 0.15062243\n",
      "Iteration 106, loss = 0.15085820\n",
      "Iteration 107, loss = 0.15077983\n",
      "Iteration 108, loss = 0.15064753\n",
      "Iteration 109, loss = 0.15053219\n",
      "Iteration 110, loss = 0.15135023\n",
      "Iteration 111, loss = 0.15031828\n",
      "Iteration 112, loss = 0.15065923\n",
      "Iteration 113, loss = 0.15011099\n",
      "Iteration 114, loss = 0.15070420\n",
      "Iteration 115, loss = 0.15047934\n",
      "Iteration 116, loss = 0.15023404\n",
      "Iteration 117, loss = 0.15021727\n",
      "Iteration 118, loss = 0.15089500\n",
      "Iteration 119, loss = 0.15056021\n",
      "Iteration 120, loss = 0.15016301\n",
      "Iteration 121, loss = 0.15026280\n",
      "Iteration 122, loss = 0.15004675\n",
      "Iteration 123, loss = 0.14996855\n",
      "Iteration 124, loss = 0.15026717\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.01, batch_size=50, hidden_layer_sizes=(100,), solver=adam; total time= 4.4min\n",
      "Iteration 1, loss = 0.71135627\n",
      "Iteration 2, loss = 0.34251361\n",
      "Iteration 3, loss = 0.29383107\n",
      "Iteration 4, loss = 0.26659852\n",
      "Iteration 5, loss = 0.24715341\n",
      "Iteration 6, loss = 0.23286087\n",
      "Iteration 7, loss = 0.22258347\n",
      "Iteration 8, loss = 0.21229251\n",
      "Iteration 9, loss = 0.20527727\n",
      "Iteration 10, loss = 0.19875041\n",
      "Iteration 11, loss = 0.19405699\n",
      "Iteration 12, loss = 0.18911603\n",
      "Iteration 13, loss = 0.18570265\n",
      "Iteration 14, loss = 0.18233999\n",
      "Iteration 15, loss = 0.18030518\n",
      "Iteration 16, loss = 0.17779786\n",
      "Iteration 17, loss = 0.17494444\n",
      "Iteration 18, loss = 0.17327542\n",
      "Iteration 19, loss = 0.17220543\n",
      "Iteration 20, loss = 0.17118874\n",
      "Iteration 21, loss = 0.16939601\n",
      "Iteration 22, loss = 0.16764533\n",
      "Iteration 23, loss = 0.16662067\n",
      "Iteration 24, loss = 0.16634508\n",
      "Iteration 25, loss = 0.16594349\n",
      "Iteration 26, loss = 0.16443108\n",
      "Iteration 27, loss = 0.16378569\n",
      "Iteration 28, loss = 0.16303123\n",
      "Iteration 29, loss = 0.16310431\n",
      "Iteration 30, loss = 0.16186973\n",
      "Iteration 31, loss = 0.16151291\n",
      "Iteration 32, loss = 0.16070411\n",
      "Iteration 33, loss = 0.16056854\n",
      "Iteration 34, loss = 0.16050218\n",
      "Iteration 35, loss = 0.16032722\n",
      "Iteration 36, loss = 0.15895241\n",
      "Iteration 37, loss = 0.15961042\n",
      "Iteration 38, loss = 0.15892932\n",
      "Iteration 39, loss = 0.15769384\n",
      "Iteration 40, loss = 0.15811946\n",
      "Iteration 41, loss = 0.15845279\n",
      "Iteration 42, loss = 0.15792243\n",
      "Iteration 43, loss = 0.15689282\n",
      "Iteration 44, loss = 0.15706821\n",
      "Iteration 45, loss = 0.15758018\n",
      "Iteration 46, loss = 0.15643103\n",
      "Iteration 47, loss = 0.15652083\n",
      "Iteration 48, loss = 0.15670195\n",
      "Iteration 49, loss = 0.15699571\n",
      "Iteration 50, loss = 0.15608996\n",
      "Iteration 51, loss = 0.15619105\n",
      "Iteration 52, loss = 0.15568357\n",
      "Iteration 53, loss = 0.15629019\n",
      "Iteration 54, loss = 0.15549682\n",
      "Iteration 55, loss = 0.15522451\n",
      "Iteration 56, loss = 0.15583782\n",
      "Iteration 57, loss = 0.15545995\n",
      "Iteration 58, loss = 0.15498457\n",
      "Iteration 59, loss = 0.15536489\n",
      "Iteration 60, loss = 0.15544275\n",
      "Iteration 61, loss = 0.15438155\n",
      "Iteration 62, loss = 0.15429679\n",
      "Iteration 63, loss = 0.15502048\n",
      "Iteration 64, loss = 0.15428352\n",
      "Iteration 65, loss = 0.15450307\n",
      "Iteration 66, loss = 0.15414606\n",
      "Iteration 67, loss = 0.15370892\n",
      "Iteration 68, loss = 0.15405948\n",
      "Iteration 69, loss = 0.15430593\n",
      "Iteration 70, loss = 0.15411826\n",
      "Iteration 71, loss = 0.15450900\n",
      "Iteration 72, loss = 0.15347375\n",
      "Iteration 73, loss = 0.15365202\n",
      "Iteration 74, loss = 0.15370417\n",
      "Iteration 75, loss = 0.15299235\n",
      "Iteration 76, loss = 0.15337981\n",
      "Iteration 77, loss = 0.15295831\n",
      "Iteration 78, loss = 0.15301243\n",
      "Iteration 79, loss = 0.15319260\n",
      "Iteration 80, loss = 0.15256209\n",
      "Iteration 81, loss = 0.15295595\n",
      "Iteration 82, loss = 0.15321706\n",
      "Iteration 83, loss = 0.15260518\n",
      "Iteration 84, loss = 0.15285104\n",
      "Iteration 85, loss = 0.15270759\n",
      "Iteration 86, loss = 0.15235246\n",
      "Iteration 87, loss = 0.15290324\n",
      "Iteration 88, loss = 0.15281757\n",
      "Iteration 89, loss = 0.15247652\n",
      "Iteration 90, loss = 0.15207426\n",
      "Iteration 91, loss = 0.15252216\n",
      "Iteration 92, loss = 0.15226562\n",
      "Iteration 93, loss = 0.15251554\n",
      "Iteration 94, loss = 0.15196296\n",
      "Iteration 95, loss = 0.15180118\n",
      "Iteration 96, loss = 0.15178530\n",
      "Iteration 97, loss = 0.15154925\n",
      "Iteration 98, loss = 0.15133895\n",
      "Iteration 99, loss = 0.15214739\n",
      "Iteration 100, loss = 0.15207110\n",
      "Iteration 101, loss = 0.15148900\n",
      "Iteration 102, loss = 0.15131185\n",
      "Iteration 103, loss = 0.15144947\n",
      "Iteration 104, loss = 0.15124041\n",
      "Iteration 105, loss = 0.15151987\n",
      "Iteration 106, loss = 0.15135600\n",
      "Iteration 107, loss = 0.15165472\n",
      "Iteration 108, loss = 0.15088712\n",
      "Iteration 109, loss = 0.15069574\n",
      "Iteration 110, loss = 0.15093258\n",
      "Iteration 111, loss = 0.15113442\n",
      "Iteration 112, loss = 0.15108636\n",
      "Iteration 113, loss = 0.15119849\n",
      "Iteration 114, loss = 0.15096432\n",
      "Iteration 115, loss = 0.15102907\n",
      "Iteration 116, loss = 0.15070130\n",
      "Iteration 117, loss = 0.15131260\n",
      "Iteration 118, loss = 0.15101451\n",
      "Iteration 119, loss = 0.15110502\n",
      "Iteration 120, loss = 0.15037400\n",
      "Iteration 121, loss = 0.15095210\n",
      "Iteration 122, loss = 0.15030712\n",
      "Iteration 123, loss = 0.15052482\n",
      "Iteration 124, loss = 0.14983002\n",
      "Iteration 125, loss = 0.15048978\n",
      "Iteration 126, loss = 0.15044979\n",
      "Iteration 127, loss = 0.15065781\n",
      "Iteration 128, loss = 0.15051538\n",
      "Iteration 129, loss = 0.15058052\n",
      "Iteration 130, loss = 0.15019878\n",
      "Iteration 131, loss = 0.14961520\n",
      "Iteration 132, loss = 0.15059307\n",
      "Iteration 133, loss = 0.15042218\n",
      "Iteration 134, loss = 0.15045851\n",
      "Iteration 135, loss = 0.14989064\n",
      "Iteration 136, loss = 0.14997553\n",
      "Iteration 137, loss = 0.15007076\n",
      "Iteration 138, loss = 0.15021713\n",
      "Iteration 139, loss = 0.15081403\n",
      "Iteration 140, loss = 0.15002065\n",
      "Iteration 141, loss = 0.15013961\n",
      "Iteration 142, loss = 0.15041236\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.01, batch_size=50, hidden_layer_sizes=(100,), solver=adam; total time= 5.1min\n",
      "Iteration 1, loss = 2.14906123\n",
      "Iteration 2, loss = 1.72204317\n",
      "Iteration 3, loss = 1.27864978\n",
      "Iteration 4, loss = 0.98842095\n",
      "Iteration 5, loss = 0.81761523\n",
      "Iteration 6, loss = 0.71039897\n",
      "Iteration 7, loss = 0.63748047\n",
      "Iteration 8, loss = 0.58479767\n",
      "Iteration 9, loss = 0.54501404\n",
      "Iteration 10, loss = 0.51406643\n",
      "Iteration 11, loss = 0.48925136\n",
      "Iteration 12, loss = 0.46887571\n",
      "Iteration 13, loss = 0.45203467\n",
      "Iteration 14, loss = 0.43772021\n",
      "Iteration 15, loss = 0.42562497\n",
      "Iteration 16, loss = 0.41508259\n",
      "Iteration 17, loss = 0.40590467\n",
      "Iteration 18, loss = 0.39772522\n",
      "Iteration 19, loss = 0.39056534\n",
      "Iteration 20, loss = 0.38400282\n",
      "Iteration 21, loss = 0.37823644\n",
      "Iteration 22, loss = 0.37267092\n",
      "Iteration 23, loss = 0.36801491\n",
      "Iteration 24, loss = 0.36349108\n",
      "Iteration 25, loss = 0.35934093\n",
      "Iteration 26, loss = 0.35538147\n",
      "Iteration 27, loss = 0.35178800\n",
      "Iteration 28, loss = 0.34850539\n",
      "Iteration 29, loss = 0.34525597\n",
      "Iteration 30, loss = 0.34225319\n",
      "Iteration 31, loss = 0.33940897\n",
      "Iteration 32, loss = 0.33671961\n",
      "Iteration 33, loss = 0.33425537\n",
      "Iteration 34, loss = 0.33172384\n",
      "Iteration 35, loss = 0.32935565\n",
      "Iteration 36, loss = 0.32714421\n",
      "Iteration 37, loss = 0.32505422\n",
      "Iteration 38, loss = 0.32289671\n",
      "Iteration 39, loss = 0.32093489\n",
      "Iteration 40, loss = 0.31886274\n",
      "Iteration 41, loss = 0.31707433\n",
      "Iteration 42, loss = 0.31511550\n",
      "Iteration 43, loss = 0.31350134\n",
      "Iteration 44, loss = 0.31178995\n",
      "Iteration 45, loss = 0.31015594\n",
      "Iteration 46, loss = 0.30839842\n",
      "Iteration 47, loss = 0.30688599\n",
      "Iteration 48, loss = 0.30542139\n",
      "Iteration 49, loss = 0.30390681\n",
      "Iteration 50, loss = 0.30245215\n",
      "Iteration 51, loss = 0.30095004\n",
      "Iteration 52, loss = 0.29947862\n",
      "Iteration 53, loss = 0.29816905\n",
      "Iteration 54, loss = 0.29673936\n",
      "Iteration 55, loss = 0.29546911\n",
      "Iteration 56, loss = 0.29404465\n",
      "Iteration 57, loss = 0.29278354\n",
      "Iteration 58, loss = 0.29164607\n",
      "Iteration 59, loss = 0.29031595\n",
      "Iteration 60, loss = 0.28907672\n",
      "Iteration 61, loss = 0.28783368\n",
      "Iteration 62, loss = 0.28662166\n",
      "Iteration 63, loss = 0.28542560\n",
      "Iteration 64, loss = 0.28440070\n",
      "Iteration 65, loss = 0.28310598\n",
      "Iteration 66, loss = 0.28202059\n",
      "Iteration 67, loss = 0.28099052\n",
      "Iteration 68, loss = 0.27985102\n",
      "Iteration 69, loss = 0.27880335\n",
      "Iteration 70, loss = 0.27771172\n",
      "Iteration 71, loss = 0.27666504\n",
      "Iteration 72, loss = 0.27560501\n",
      "Iteration 73, loss = 0.27461948\n",
      "Iteration 74, loss = 0.27361028\n",
      "Iteration 75, loss = 0.27262103\n",
      "Iteration 76, loss = 0.27164942\n",
      "Iteration 77, loss = 0.27059074\n",
      "Iteration 78, loss = 0.26974259\n",
      "Iteration 79, loss = 0.26874115\n",
      "Iteration 80, loss = 0.26773826\n",
      "Iteration 81, loss = 0.26683421\n",
      "Iteration 82, loss = 0.26594670\n",
      "Iteration 83, loss = 0.26507715\n",
      "Iteration 84, loss = 0.26414236\n",
      "Iteration 85, loss = 0.26322330\n",
      "Iteration 86, loss = 0.26236153\n",
      "Iteration 87, loss = 0.26148451\n",
      "Iteration 88, loss = 0.26061489\n",
      "Iteration 89, loss = 0.25978692\n",
      "Iteration 90, loss = 0.25889346\n",
      "Iteration 91, loss = 0.25806266\n",
      "Iteration 92, loss = 0.25733238\n",
      "Iteration 93, loss = 0.25650553\n",
      "Iteration 94, loss = 0.25561581\n",
      "Iteration 95, loss = 0.25484218\n",
      "Iteration 96, loss = 0.25397523\n",
      "Iteration 97, loss = 0.25329324\n",
      "Iteration 98, loss = 0.25250956\n",
      "Iteration 99, loss = 0.25173781\n",
      "Iteration 100, loss = 0.25093776\n",
      "Iteration 101, loss = 0.25025152\n",
      "Iteration 102, loss = 0.24953802\n",
      "Iteration 103, loss = 0.24878195\n",
      "Iteration 104, loss = 0.24801896\n",
      "Iteration 105, loss = 0.24735679\n",
      "Iteration 106, loss = 0.24664328\n",
      "Iteration 107, loss = 0.24591876\n",
      "Iteration 108, loss = 0.24516330\n",
      "Iteration 109, loss = 0.24450558\n",
      "Iteration 110, loss = 0.24384425\n",
      "Iteration 111, loss = 0.24315726\n",
      "Iteration 112, loss = 0.24243214\n",
      "Iteration 113, loss = 0.24181458\n",
      "Iteration 114, loss = 0.24114557\n",
      "Iteration 115, loss = 0.24050610\n",
      "Iteration 116, loss = 0.23990748\n",
      "Iteration 117, loss = 0.23927963\n",
      "Iteration 118, loss = 0.23861821\n",
      "Iteration 119, loss = 0.23795815\n",
      "Iteration 120, loss = 0.23741573\n",
      "Iteration 121, loss = 0.23672538\n",
      "Iteration 122, loss = 0.23618063\n",
      "Iteration 123, loss = 0.23562416\n",
      "Iteration 124, loss = 0.23494188\n",
      "Iteration 125, loss = 0.23433851\n",
      "Iteration 126, loss = 0.23380155\n",
      "Iteration 127, loss = 0.23312812\n",
      "Iteration 128, loss = 0.23259654\n",
      "Iteration 129, loss = 0.23208389\n",
      "Iteration 130, loss = 0.23150110\n",
      "Iteration 131, loss = 0.23098380\n",
      "Iteration 132, loss = 0.23039803\n",
      "Iteration 133, loss = 0.22987145\n",
      "Iteration 134, loss = 0.22934479\n",
      "Iteration 135, loss = 0.22881716\n",
      "Iteration 136, loss = 0.22833633\n",
      "Iteration 137, loss = 0.22772750\n",
      "Iteration 138, loss = 0.22723970\n",
      "Iteration 139, loss = 0.22677510\n",
      "Iteration 140, loss = 0.22615228\n",
      "Iteration 141, loss = 0.22565386\n",
      "Iteration 142, loss = 0.22513587\n",
      "Iteration 143, loss = 0.22467068\n",
      "Iteration 144, loss = 0.22419423\n",
      "Iteration 145, loss = 0.22373891\n",
      "Iteration 146, loss = 0.22317919\n",
      "Iteration 147, loss = 0.22276266\n",
      "Iteration 148, loss = 0.22225520\n",
      "Iteration 149, loss = 0.22178950\n",
      "Iteration 150, loss = 0.22131740\n",
      "Iteration 151, loss = 0.22085674\n",
      "Iteration 152, loss = 0.22036709\n",
      "Iteration 153, loss = 0.21995804\n",
      "Iteration 154, loss = 0.21954132\n",
      "Iteration 155, loss = 0.21898987\n",
      "Iteration 156, loss = 0.21865304\n",
      "Iteration 157, loss = 0.21817858\n",
      "Iteration 158, loss = 0.21776168\n",
      "Iteration 159, loss = 0.21734943\n",
      "Iteration 160, loss = 0.21688979\n",
      "Iteration 161, loss = 0.21646241\n",
      "Iteration 162, loss = 0.21603298\n",
      "Iteration 163, loss = 0.21568203\n",
      "Iteration 164, loss = 0.21523047\n",
      "Iteration 165, loss = 0.21490038\n",
      "Iteration 166, loss = 0.21439552\n",
      "Iteration 167, loss = 0.21403187\n",
      "Iteration 168, loss = 0.21364299\n",
      "Iteration 169, loss = 0.21321819\n",
      "Iteration 170, loss = 0.21283572\n",
      "Iteration 171, loss = 0.21242977\n",
      "Iteration 172, loss = 0.21197875\n",
      "Iteration 173, loss = 0.21169704\n",
      "Iteration 174, loss = 0.21131561\n",
      "Iteration 175, loss = 0.21088267\n",
      "Iteration 176, loss = 0.21058242\n",
      "Iteration 177, loss = 0.21018263\n",
      "Iteration 178, loss = 0.20982778\n",
      "Iteration 179, loss = 0.20946424\n",
      "Iteration 180, loss = 0.20911220\n",
      "Iteration 181, loss = 0.20873101\n",
      "Iteration 182, loss = 0.20840022\n",
      "Iteration 183, loss = 0.20804530\n",
      "Iteration 184, loss = 0.20765989\n",
      "Iteration 185, loss = 0.20729622\n",
      "Iteration 186, loss = 0.20699842\n",
      "Iteration 187, loss = 0.20664491\n",
      "Iteration 188, loss = 0.20626620\n",
      "Iteration 189, loss = 0.20596121\n",
      "Iteration 190, loss = 0.20564477\n",
      "Iteration 191, loss = 0.20532389\n",
      "Iteration 192, loss = 0.20497506\n",
      "Iteration 193, loss = 0.20466756\n",
      "Iteration 194, loss = 0.20438323\n",
      "Iteration 195, loss = 0.20404556\n",
      "Iteration 196, loss = 0.20373891\n",
      "Iteration 197, loss = 0.20338034\n",
      "Iteration 198, loss = 0.20300983\n",
      "Iteration 199, loss = 0.20278046\n",
      "Iteration 200, loss = 0.20242469\n",
      "[CV] END activation=logistic, alpha=0.01, batch_size=50, hidden_layer_sizes=(100,), solver=sgd; total time= 4.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.14710607\n",
      "Iteration 2, loss = 1.72920399\n",
      "Iteration 3, loss = 1.28778409\n",
      "Iteration 4, loss = 0.99106891\n",
      "Iteration 5, loss = 0.81537028\n",
      "Iteration 6, loss = 0.70578771\n",
      "Iteration 7, loss = 0.63213257\n",
      "Iteration 8, loss = 0.57939612\n",
      "Iteration 9, loss = 0.53996602\n",
      "Iteration 10, loss = 0.50938215\n",
      "Iteration 11, loss = 0.48521011\n",
      "Iteration 12, loss = 0.46533468\n",
      "Iteration 13, loss = 0.44899469\n",
      "Iteration 14, loss = 0.43514450\n",
      "Iteration 15, loss = 0.42331982\n",
      "Iteration 16, loss = 0.41304913\n",
      "Iteration 17, loss = 0.40418831\n",
      "Iteration 18, loss = 0.39644552\n",
      "Iteration 19, loss = 0.38931838\n",
      "Iteration 20, loss = 0.38299522\n",
      "Iteration 21, loss = 0.37732873\n",
      "Iteration 22, loss = 0.37207264\n",
      "Iteration 23, loss = 0.36738728\n",
      "Iteration 24, loss = 0.36297423\n",
      "Iteration 25, loss = 0.35897908\n",
      "Iteration 26, loss = 0.35516159\n",
      "Iteration 27, loss = 0.35168814\n",
      "Iteration 28, loss = 0.34837327\n",
      "Iteration 29, loss = 0.34525187\n",
      "Iteration 30, loss = 0.34233317\n",
      "Iteration 31, loss = 0.33950798\n",
      "Iteration 32, loss = 0.33699765\n",
      "Iteration 33, loss = 0.33454122\n",
      "Iteration 34, loss = 0.33204908\n",
      "Iteration 35, loss = 0.32979644\n",
      "Iteration 36, loss = 0.32760015\n",
      "Iteration 37, loss = 0.32558922\n",
      "Iteration 38, loss = 0.32345051\n",
      "Iteration 39, loss = 0.32158759\n",
      "Iteration 40, loss = 0.31973583\n",
      "Iteration 41, loss = 0.31793402\n",
      "Iteration 42, loss = 0.31616993\n",
      "Iteration 43, loss = 0.31447031\n",
      "Iteration 44, loss = 0.31283117\n",
      "Iteration 45, loss = 0.31107251\n",
      "Iteration 46, loss = 0.30962410\n",
      "Iteration 47, loss = 0.30812950\n",
      "Iteration 48, loss = 0.30661876\n",
      "Iteration 49, loss = 0.30516607\n",
      "Iteration 50, loss = 0.30368324\n",
      "Iteration 51, loss = 0.30239802\n",
      "Iteration 52, loss = 0.30089633\n",
      "Iteration 53, loss = 0.29965995\n",
      "Iteration 54, loss = 0.29824993\n",
      "Iteration 55, loss = 0.29695322\n",
      "Iteration 56, loss = 0.29572635\n",
      "Iteration 57, loss = 0.29442180\n",
      "Iteration 58, loss = 0.29323836\n",
      "Iteration 59, loss = 0.29195799\n",
      "Iteration 60, loss = 0.29079273\n",
      "Iteration 61, loss = 0.28965867\n",
      "Iteration 62, loss = 0.28849781\n",
      "Iteration 63, loss = 0.28732452\n",
      "Iteration 64, loss = 0.28615376\n",
      "Iteration 65, loss = 0.28513497\n",
      "Iteration 66, loss = 0.28395671\n",
      "Iteration 67, loss = 0.28288064\n",
      "Iteration 68, loss = 0.28174954\n",
      "Iteration 69, loss = 0.28073528\n",
      "Iteration 70, loss = 0.27974084\n",
      "Iteration 71, loss = 0.27865969\n",
      "Iteration 72, loss = 0.27767635\n",
      "Iteration 73, loss = 0.27659885\n",
      "Iteration 74, loss = 0.27547463\n",
      "Iteration 75, loss = 0.27469855\n",
      "Iteration 76, loss = 0.27352803\n",
      "Iteration 77, loss = 0.27266157\n",
      "Iteration 78, loss = 0.27171465\n",
      "Iteration 79, loss = 0.27075613\n",
      "Iteration 80, loss = 0.26974380\n",
      "Iteration 81, loss = 0.26880025\n",
      "Iteration 82, loss = 0.26789463\n",
      "Iteration 83, loss = 0.26707124\n",
      "Iteration 84, loss = 0.26595498\n",
      "Iteration 85, loss = 0.26521586\n",
      "Iteration 86, loss = 0.26437040\n",
      "Iteration 87, loss = 0.26351200\n",
      "Iteration 88, loss = 0.26253987\n",
      "Iteration 89, loss = 0.26173150\n",
      "Iteration 90, loss = 0.26067489\n",
      "Iteration 91, loss = 0.26003649\n",
      "Iteration 92, loss = 0.25922525\n",
      "Iteration 93, loss = 0.25838046\n",
      "Iteration 94, loss = 0.25755895\n",
      "Iteration 95, loss = 0.25675775\n",
      "Iteration 96, loss = 0.25598204\n",
      "Iteration 97, loss = 0.25505389\n",
      "Iteration 98, loss = 0.25437229\n",
      "Iteration 99, loss = 0.25362584\n",
      "Iteration 100, loss = 0.25277300\n",
      "Iteration 101, loss = 0.25200116\n",
      "Iteration 102, loss = 0.25129825\n",
      "Iteration 103, loss = 0.25050320\n",
      "Iteration 104, loss = 0.24980910\n",
      "Iteration 105, loss = 0.24914864\n",
      "Iteration 106, loss = 0.24834789\n",
      "Iteration 107, loss = 0.24758349\n",
      "Iteration 108, loss = 0.24690005\n",
      "Iteration 109, loss = 0.24624817\n",
      "Iteration 110, loss = 0.24547195\n",
      "Iteration 111, loss = 0.24486293\n",
      "Iteration 112, loss = 0.24418197\n",
      "Iteration 113, loss = 0.24352317\n",
      "Iteration 114, loss = 0.24282794\n",
      "Iteration 115, loss = 0.24221678\n",
      "Iteration 116, loss = 0.24148272\n",
      "Iteration 117, loss = 0.24089856\n",
      "Iteration 118, loss = 0.24025481\n",
      "Iteration 119, loss = 0.23963837\n",
      "Iteration 120, loss = 0.23893972\n",
      "Iteration 121, loss = 0.23832564\n",
      "Iteration 122, loss = 0.23773891\n",
      "Iteration 123, loss = 0.23704650\n",
      "Iteration 124, loss = 0.23650544\n",
      "Iteration 125, loss = 0.23586872\n",
      "Iteration 126, loss = 0.23531896\n",
      "Iteration 127, loss = 0.23467817\n",
      "Iteration 128, loss = 0.23420288\n",
      "Iteration 129, loss = 0.23358917\n",
      "Iteration 130, loss = 0.23295823\n",
      "Iteration 131, loss = 0.23237055\n",
      "Iteration 132, loss = 0.23182715\n",
      "Iteration 133, loss = 0.23130447\n",
      "Iteration 134, loss = 0.23082669\n",
      "Iteration 135, loss = 0.23016923\n",
      "Iteration 136, loss = 0.22966962\n",
      "Iteration 137, loss = 0.22908764\n",
      "Iteration 138, loss = 0.22865575\n",
      "Iteration 139, loss = 0.22809117\n",
      "Iteration 140, loss = 0.22758512\n",
      "Iteration 141, loss = 0.22702453\n",
      "Iteration 142, loss = 0.22657385\n",
      "Iteration 143, loss = 0.22600410\n",
      "Iteration 144, loss = 0.22553066\n",
      "Iteration 145, loss = 0.22498399\n",
      "Iteration 146, loss = 0.22448507\n",
      "Iteration 147, loss = 0.22406477\n",
      "Iteration 148, loss = 0.22353664\n",
      "Iteration 149, loss = 0.22311337\n",
      "Iteration 150, loss = 0.22260115\n",
      "Iteration 151, loss = 0.22214692\n",
      "Iteration 152, loss = 0.22166754\n",
      "Iteration 153, loss = 0.22112107\n",
      "Iteration 154, loss = 0.22075522\n",
      "Iteration 155, loss = 0.22029871\n",
      "Iteration 156, loss = 0.21974505\n",
      "Iteration 157, loss = 0.21937049\n",
      "Iteration 158, loss = 0.21894323\n",
      "Iteration 159, loss = 0.21847414\n",
      "Iteration 160, loss = 0.21807492\n",
      "Iteration 161, loss = 0.21763959\n",
      "Iteration 162, loss = 0.21722448\n",
      "Iteration 163, loss = 0.21678986\n",
      "Iteration 164, loss = 0.21637585\n",
      "Iteration 165, loss = 0.21598252\n",
      "Iteration 166, loss = 0.21551265\n",
      "Iteration 167, loss = 0.21521970\n",
      "Iteration 168, loss = 0.21470878\n",
      "Iteration 169, loss = 0.21430197\n",
      "Iteration 170, loss = 0.21395476\n",
      "Iteration 171, loss = 0.21356551\n",
      "Iteration 172, loss = 0.21313120\n",
      "Iteration 173, loss = 0.21271676\n",
      "Iteration 174, loss = 0.21238096\n",
      "Iteration 175, loss = 0.21199229\n",
      "Iteration 176, loss = 0.21161073\n",
      "Iteration 177, loss = 0.21118792\n",
      "Iteration 178, loss = 0.21085535\n",
      "Iteration 179, loss = 0.21052371\n",
      "Iteration 180, loss = 0.21011080\n",
      "Iteration 181, loss = 0.20975666\n",
      "Iteration 182, loss = 0.20942844\n",
      "Iteration 183, loss = 0.20902651\n",
      "Iteration 184, loss = 0.20875665\n",
      "Iteration 185, loss = 0.20830163\n",
      "Iteration 186, loss = 0.20799805\n",
      "Iteration 187, loss = 0.20759365\n",
      "Iteration 188, loss = 0.20731740\n",
      "Iteration 189, loss = 0.20696361\n",
      "Iteration 190, loss = 0.20663680\n",
      "Iteration 191, loss = 0.20628204\n",
      "Iteration 192, loss = 0.20596623\n",
      "Iteration 193, loss = 0.20563118\n",
      "Iteration 194, loss = 0.20532452\n",
      "Iteration 195, loss = 0.20499725\n",
      "Iteration 196, loss = 0.20466215\n",
      "Iteration 197, loss = 0.20432581\n",
      "Iteration 198, loss = 0.20398788\n",
      "Iteration 199, loss = 0.20370897\n",
      "Iteration 200, loss = 0.20342504\n",
      "[CV] END activation=logistic, alpha=0.01, batch_size=50, hidden_layer_sizes=(100,), solver=sgd; total time= 4.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.15006040\n",
      "Iteration 2, loss = 1.72578482\n",
      "Iteration 3, loss = 1.29086520\n",
      "Iteration 4, loss = 1.00364346\n",
      "Iteration 5, loss = 0.82970413\n",
      "Iteration 6, loss = 0.71836182\n",
      "Iteration 7, loss = 0.64245417\n",
      "Iteration 8, loss = 0.58778396\n",
      "Iteration 9, loss = 0.54668027\n",
      "Iteration 10, loss = 0.51476286\n",
      "Iteration 11, loss = 0.48943404\n",
      "Iteration 12, loss = 0.46882315\n",
      "Iteration 13, loss = 0.45168051\n",
      "Iteration 14, loss = 0.43728204\n",
      "Iteration 15, loss = 0.42514283\n",
      "Iteration 16, loss = 0.41442153\n",
      "Iteration 17, loss = 0.40525730\n",
      "Iteration 18, loss = 0.39719299\n",
      "Iteration 19, loss = 0.39000717\n",
      "Iteration 20, loss = 0.38353703\n",
      "Iteration 21, loss = 0.37772886\n",
      "Iteration 22, loss = 0.37235561\n",
      "Iteration 23, loss = 0.36755233\n",
      "Iteration 24, loss = 0.36304403\n",
      "Iteration 25, loss = 0.35894243\n",
      "Iteration 26, loss = 0.35517303\n",
      "Iteration 27, loss = 0.35164460\n",
      "Iteration 28, loss = 0.34823459\n",
      "Iteration 29, loss = 0.34507508\n",
      "Iteration 30, loss = 0.34210206\n",
      "Iteration 31, loss = 0.33939711\n",
      "Iteration 32, loss = 0.33674313\n",
      "Iteration 33, loss = 0.33426797\n",
      "Iteration 34, loss = 0.33191626\n",
      "Iteration 35, loss = 0.32953722\n",
      "Iteration 36, loss = 0.32741251\n",
      "Iteration 37, loss = 0.32525236\n",
      "Iteration 38, loss = 0.32334455\n",
      "Iteration 39, loss = 0.32140785\n",
      "Iteration 40, loss = 0.31946708\n",
      "Iteration 41, loss = 0.31764687\n",
      "Iteration 42, loss = 0.31596279\n",
      "Iteration 43, loss = 0.31424050\n",
      "Iteration 44, loss = 0.31255859\n",
      "Iteration 45, loss = 0.31097201\n",
      "Iteration 46, loss = 0.30942668\n",
      "Iteration 47, loss = 0.30788992\n",
      "Iteration 48, loss = 0.30633617\n",
      "Iteration 49, loss = 0.30490020\n",
      "Iteration 50, loss = 0.30356443\n",
      "Iteration 51, loss = 0.30201931\n",
      "Iteration 52, loss = 0.30063430\n",
      "Iteration 53, loss = 0.29944180\n",
      "Iteration 54, loss = 0.29807538\n",
      "Iteration 55, loss = 0.29677301\n",
      "Iteration 56, loss = 0.29549209\n",
      "Iteration 57, loss = 0.29410070\n",
      "Iteration 58, loss = 0.29304080\n",
      "Iteration 59, loss = 0.29173386\n",
      "Iteration 60, loss = 0.29057409\n",
      "Iteration 61, loss = 0.28923219\n",
      "Iteration 62, loss = 0.28813575\n",
      "Iteration 63, loss = 0.28702972\n",
      "Iteration 64, loss = 0.28595327\n",
      "Iteration 65, loss = 0.28469840\n",
      "Iteration 66, loss = 0.28362783\n",
      "Iteration 67, loss = 0.28255160\n",
      "Iteration 68, loss = 0.28141733\n",
      "Iteration 69, loss = 0.28032866\n",
      "Iteration 70, loss = 0.27926015\n",
      "Iteration 71, loss = 0.27820653\n",
      "Iteration 72, loss = 0.27707853\n",
      "Iteration 73, loss = 0.27606801\n",
      "Iteration 74, loss = 0.27504178\n",
      "Iteration 75, loss = 0.27405553\n",
      "Iteration 76, loss = 0.27303550\n",
      "Iteration 77, loss = 0.27205299\n",
      "Iteration 78, loss = 0.27106702\n",
      "Iteration 79, loss = 0.27007219\n",
      "Iteration 80, loss = 0.26912357\n",
      "Iteration 81, loss = 0.26818724\n",
      "Iteration 82, loss = 0.26722781\n",
      "Iteration 83, loss = 0.26624903\n",
      "Iteration 84, loss = 0.26535393\n",
      "Iteration 85, loss = 0.26445693\n",
      "Iteration 86, loss = 0.26355164\n",
      "Iteration 87, loss = 0.26258910\n",
      "Iteration 88, loss = 0.26166285\n",
      "Iteration 89, loss = 0.26084963\n",
      "Iteration 90, loss = 0.26000516\n",
      "Iteration 91, loss = 0.25911759\n",
      "Iteration 92, loss = 0.25830640\n",
      "Iteration 93, loss = 0.25739767\n",
      "Iteration 94, loss = 0.25661830\n",
      "Iteration 95, loss = 0.25574331\n",
      "Iteration 96, loss = 0.25500819\n",
      "Iteration 97, loss = 0.25416362\n",
      "Iteration 98, loss = 0.25333750\n",
      "Iteration 99, loss = 0.25253389\n",
      "Iteration 100, loss = 0.25177239\n",
      "Iteration 101, loss = 0.25089572\n",
      "Iteration 102, loss = 0.25020348\n",
      "Iteration 103, loss = 0.24945255\n",
      "Iteration 104, loss = 0.24876184\n",
      "Iteration 105, loss = 0.24794191\n",
      "Iteration 106, loss = 0.24722406\n",
      "Iteration 107, loss = 0.24652601\n",
      "Iteration 108, loss = 0.24579075\n",
      "Iteration 109, loss = 0.24511645\n",
      "Iteration 110, loss = 0.24438200\n",
      "Iteration 111, loss = 0.24370962\n",
      "Iteration 112, loss = 0.24302011\n",
      "Iteration 113, loss = 0.24230380\n",
      "Iteration 114, loss = 0.24164988\n",
      "Iteration 115, loss = 0.24098142\n",
      "Iteration 116, loss = 0.24029993\n",
      "Iteration 117, loss = 0.23974367\n",
      "Iteration 118, loss = 0.23893523\n",
      "Iteration 119, loss = 0.23837113\n",
      "Iteration 120, loss = 0.23778047\n",
      "Iteration 121, loss = 0.23717877\n",
      "Iteration 122, loss = 0.23643811\n",
      "Iteration 123, loss = 0.23591096\n",
      "Iteration 124, loss = 0.23535571\n",
      "Iteration 125, loss = 0.23472119\n",
      "Iteration 126, loss = 0.23409451\n",
      "Iteration 127, loss = 0.23350520\n",
      "Iteration 128, loss = 0.23288955\n",
      "Iteration 129, loss = 0.23236697\n",
      "Iteration 130, loss = 0.23181352\n",
      "Iteration 131, loss = 0.23119359\n",
      "Iteration 132, loss = 0.23069943\n",
      "Iteration 133, loss = 0.23006782\n",
      "Iteration 134, loss = 0.22956615\n",
      "Iteration 135, loss = 0.22898764\n",
      "Iteration 136, loss = 0.22840622\n",
      "Iteration 137, loss = 0.22796034\n",
      "Iteration 138, loss = 0.22745083\n",
      "Iteration 139, loss = 0.22691152\n",
      "Iteration 140, loss = 0.22639538\n",
      "Iteration 141, loss = 0.22576826\n",
      "Iteration 142, loss = 0.22535916\n",
      "Iteration 143, loss = 0.22476849\n",
      "Iteration 144, loss = 0.22433850\n",
      "Iteration 145, loss = 0.22386009\n",
      "Iteration 146, loss = 0.22335447\n",
      "Iteration 147, loss = 0.22289349\n",
      "Iteration 148, loss = 0.22239750\n",
      "Iteration 149, loss = 0.22194013\n",
      "Iteration 150, loss = 0.22141888\n",
      "Iteration 151, loss = 0.22095802\n",
      "Iteration 152, loss = 0.22048216\n",
      "Iteration 153, loss = 0.22003329\n",
      "Iteration 154, loss = 0.21951237\n",
      "Iteration 155, loss = 0.21916118\n",
      "Iteration 156, loss = 0.21869908\n",
      "Iteration 157, loss = 0.21822539\n",
      "Iteration 158, loss = 0.21779861\n",
      "Iteration 159, loss = 0.21736767\n",
      "Iteration 160, loss = 0.21695486\n",
      "Iteration 161, loss = 0.21651853\n",
      "Iteration 162, loss = 0.21608341\n",
      "Iteration 163, loss = 0.21567970\n",
      "Iteration 164, loss = 0.21523030\n",
      "Iteration 165, loss = 0.21480179\n",
      "Iteration 166, loss = 0.21436759\n",
      "Iteration 167, loss = 0.21400399\n",
      "Iteration 168, loss = 0.21361203\n",
      "Iteration 169, loss = 0.21320619\n",
      "Iteration 170, loss = 0.21280121\n",
      "Iteration 171, loss = 0.21238212\n",
      "Iteration 172, loss = 0.21203142\n",
      "Iteration 173, loss = 0.21161716\n",
      "Iteration 174, loss = 0.21126685\n",
      "Iteration 175, loss = 0.21082484\n",
      "Iteration 176, loss = 0.21050816\n",
      "Iteration 177, loss = 0.21015432\n",
      "Iteration 178, loss = 0.20972755\n",
      "Iteration 179, loss = 0.20940035\n",
      "Iteration 180, loss = 0.20904742\n",
      "Iteration 181, loss = 0.20868497\n",
      "Iteration 182, loss = 0.20825096\n",
      "Iteration 183, loss = 0.20790464\n",
      "Iteration 184, loss = 0.20753213\n",
      "Iteration 185, loss = 0.20723398\n",
      "Iteration 186, loss = 0.20690024\n",
      "Iteration 187, loss = 0.20651028\n",
      "Iteration 188, loss = 0.20620491\n",
      "Iteration 189, loss = 0.20586243\n",
      "Iteration 190, loss = 0.20554909\n",
      "Iteration 191, loss = 0.20515349\n",
      "Iteration 192, loss = 0.20487877\n",
      "Iteration 193, loss = 0.20447499\n",
      "Iteration 194, loss = 0.20421627\n",
      "Iteration 195, loss = 0.20384344\n",
      "Iteration 196, loss = 0.20353448\n",
      "Iteration 197, loss = 0.20326815\n",
      "Iteration 198, loss = 0.20289027\n",
      "Iteration 199, loss = 0.20258664\n",
      "Iteration 200, loss = 0.20229156\n",
      "[CV] END activation=logistic, alpha=0.01, batch_size=50, hidden_layer_sizes=(100,), solver=sgd; total time= 4.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.17092459\n",
      "Iteration 2, loss = 1.76068785\n",
      "Iteration 3, loss = 1.32229562\n",
      "Iteration 4, loss = 1.02178456\n",
      "Iteration 5, loss = 0.84034432\n",
      "Iteration 6, loss = 0.72575417\n",
      "Iteration 7, loss = 0.64871537\n",
      "Iteration 8, loss = 0.59337032\n",
      "Iteration 9, loss = 0.55198707\n",
      "Iteration 10, loss = 0.51993984\n",
      "Iteration 11, loss = 0.49424863\n",
      "Iteration 12, loss = 0.47357207\n",
      "Iteration 13, loss = 0.45619896\n",
      "Iteration 14, loss = 0.44156320\n",
      "Iteration 15, loss = 0.42924889\n",
      "Iteration 16, loss = 0.41847970\n",
      "Iteration 17, loss = 0.40915126\n",
      "Iteration 18, loss = 0.40092377\n",
      "Iteration 19, loss = 0.39367344\n",
      "Iteration 20, loss = 0.38699986\n",
      "Iteration 21, loss = 0.38091190\n",
      "Iteration 22, loss = 0.37567130\n",
      "Iteration 23, loss = 0.37066400\n",
      "Iteration 24, loss = 0.36603445\n",
      "Iteration 25, loss = 0.36187026\n",
      "Iteration 26, loss = 0.35793728\n",
      "Iteration 27, loss = 0.35433472\n",
      "Iteration 28, loss = 0.35089324\n",
      "Iteration 29, loss = 0.34770333\n",
      "Iteration 30, loss = 0.34465260\n",
      "Iteration 31, loss = 0.34183210\n",
      "Iteration 32, loss = 0.33906655\n",
      "Iteration 33, loss = 0.33640501\n",
      "Iteration 34, loss = 0.33414990\n",
      "Iteration 35, loss = 0.33163087\n",
      "Iteration 36, loss = 0.32938815\n",
      "Iteration 37, loss = 0.32717256\n",
      "Iteration 38, loss = 0.32508065\n",
      "Iteration 39, loss = 0.32309958\n",
      "Iteration 40, loss = 0.32112893\n",
      "Iteration 41, loss = 0.31919682\n",
      "Iteration 42, loss = 0.31745532\n",
      "Iteration 43, loss = 0.31559012\n",
      "Iteration 44, loss = 0.31386887\n",
      "Iteration 45, loss = 0.31223264\n",
      "Iteration 46, loss = 0.31047634\n",
      "Iteration 47, loss = 0.30901231\n",
      "Iteration 48, loss = 0.30740159\n",
      "Iteration 49, loss = 0.30591288\n",
      "Iteration 50, loss = 0.30440828\n",
      "Iteration 51, loss = 0.30292210\n",
      "Iteration 52, loss = 0.30146807\n",
      "Iteration 53, loss = 0.30011434\n",
      "Iteration 54, loss = 0.29878899\n",
      "Iteration 55, loss = 0.29746132\n",
      "Iteration 56, loss = 0.29602381\n",
      "Iteration 57, loss = 0.29482670\n",
      "Iteration 58, loss = 0.29344354\n",
      "Iteration 59, loss = 0.29226546\n",
      "Iteration 60, loss = 0.29100316\n",
      "Iteration 61, loss = 0.28973867\n",
      "Iteration 62, loss = 0.28857197\n",
      "Iteration 63, loss = 0.28733532\n",
      "Iteration 64, loss = 0.28625782\n",
      "Iteration 65, loss = 0.28511701\n",
      "Iteration 66, loss = 0.28388766\n",
      "Iteration 67, loss = 0.28278781\n",
      "Iteration 68, loss = 0.28175234\n",
      "Iteration 69, loss = 0.28061234\n",
      "Iteration 70, loss = 0.27954432\n",
      "Iteration 71, loss = 0.27851998\n",
      "Iteration 72, loss = 0.27740652\n",
      "Iteration 73, loss = 0.27634245\n",
      "Iteration 74, loss = 0.27535668\n",
      "Iteration 75, loss = 0.27438987\n",
      "Iteration 76, loss = 0.27336023\n",
      "Iteration 77, loss = 0.27233806\n",
      "Iteration 78, loss = 0.27141003\n",
      "Iteration 79, loss = 0.27039351\n",
      "Iteration 80, loss = 0.26949304\n",
      "Iteration 81, loss = 0.26853141\n",
      "Iteration 82, loss = 0.26753711\n",
      "Iteration 83, loss = 0.26661095\n",
      "Iteration 84, loss = 0.26572625\n",
      "Iteration 85, loss = 0.26488205\n",
      "Iteration 86, loss = 0.26392599\n",
      "Iteration 87, loss = 0.26302033\n",
      "Iteration 88, loss = 0.26218054\n",
      "Iteration 89, loss = 0.26129584\n",
      "Iteration 90, loss = 0.26046003\n",
      "Iteration 91, loss = 0.25959150\n",
      "Iteration 92, loss = 0.25881553\n",
      "Iteration 93, loss = 0.25797757\n",
      "Iteration 94, loss = 0.25716857\n",
      "Iteration 95, loss = 0.25633392\n",
      "Iteration 96, loss = 0.25551825\n",
      "Iteration 97, loss = 0.25464618\n",
      "Iteration 98, loss = 0.25379596\n",
      "Iteration 99, loss = 0.25318670\n",
      "Iteration 100, loss = 0.25240160\n",
      "Iteration 101, loss = 0.25157017\n",
      "Iteration 102, loss = 0.25089746\n",
      "Iteration 103, loss = 0.25015671\n",
      "Iteration 104, loss = 0.24938440\n",
      "Iteration 105, loss = 0.24863855\n",
      "Iteration 106, loss = 0.24788940\n",
      "Iteration 107, loss = 0.24711462\n",
      "Iteration 108, loss = 0.24649532\n",
      "Iteration 109, loss = 0.24583377\n",
      "Iteration 110, loss = 0.24502319\n",
      "Iteration 111, loss = 0.24435728\n",
      "Iteration 112, loss = 0.24372227\n",
      "Iteration 113, loss = 0.24303368\n",
      "Iteration 114, loss = 0.24228589\n",
      "Iteration 115, loss = 0.24164453\n",
      "Iteration 116, loss = 0.24107356\n",
      "Iteration 117, loss = 0.24039801\n",
      "Iteration 118, loss = 0.23967385\n",
      "Iteration 119, loss = 0.23907752\n",
      "Iteration 120, loss = 0.23843442\n",
      "Iteration 121, loss = 0.23776832\n",
      "Iteration 122, loss = 0.23721080\n",
      "Iteration 123, loss = 0.23662267\n",
      "Iteration 124, loss = 0.23591365\n",
      "Iteration 125, loss = 0.23535254\n",
      "Iteration 126, loss = 0.23484502\n",
      "Iteration 127, loss = 0.23410817\n",
      "Iteration 128, loss = 0.23366800\n",
      "Iteration 129, loss = 0.23300278\n",
      "Iteration 130, loss = 0.23242422\n",
      "Iteration 131, loss = 0.23183644\n",
      "Iteration 132, loss = 0.23124929\n",
      "Iteration 133, loss = 0.23071947\n",
      "Iteration 134, loss = 0.23016189\n",
      "Iteration 135, loss = 0.22964328\n",
      "Iteration 136, loss = 0.22908437\n",
      "Iteration 137, loss = 0.22850550\n",
      "Iteration 138, loss = 0.22800450\n",
      "Iteration 139, loss = 0.22747318\n",
      "Iteration 140, loss = 0.22694419\n",
      "Iteration 141, loss = 0.22643656\n",
      "Iteration 142, loss = 0.22591688\n",
      "Iteration 143, loss = 0.22544726\n",
      "Iteration 144, loss = 0.22484955\n",
      "Iteration 145, loss = 0.22429392\n",
      "Iteration 146, loss = 0.22385591\n",
      "Iteration 147, loss = 0.22335298\n",
      "Iteration 148, loss = 0.22289666\n",
      "Iteration 149, loss = 0.22243592\n",
      "Iteration 150, loss = 0.22193072\n",
      "Iteration 151, loss = 0.22146601\n",
      "Iteration 152, loss = 0.22096949\n",
      "Iteration 153, loss = 0.22049596\n",
      "Iteration 154, loss = 0.22010964\n",
      "Iteration 155, loss = 0.21954752\n",
      "Iteration 156, loss = 0.21915288\n",
      "Iteration 157, loss = 0.21866411\n",
      "Iteration 158, loss = 0.21822823\n",
      "Iteration 159, loss = 0.21776100\n",
      "Iteration 160, loss = 0.21739538\n",
      "Iteration 161, loss = 0.21693583\n",
      "Iteration 162, loss = 0.21648039\n",
      "Iteration 163, loss = 0.21603421\n",
      "Iteration 164, loss = 0.21564987\n",
      "Iteration 165, loss = 0.21519321\n",
      "Iteration 166, loss = 0.21476171\n",
      "Iteration 167, loss = 0.21431081\n",
      "Iteration 168, loss = 0.21396768\n",
      "Iteration 169, loss = 0.21345606\n",
      "Iteration 170, loss = 0.21316215\n",
      "Iteration 171, loss = 0.21269678\n",
      "Iteration 172, loss = 0.21235197\n",
      "Iteration 173, loss = 0.21191782\n",
      "Iteration 174, loss = 0.21149665\n",
      "Iteration 175, loss = 0.21118720\n",
      "Iteration 176, loss = 0.21077715\n",
      "Iteration 177, loss = 0.21034220\n",
      "Iteration 178, loss = 0.21005541\n",
      "Iteration 179, loss = 0.20960350\n",
      "Iteration 180, loss = 0.20928197\n",
      "Iteration 181, loss = 0.20888786\n",
      "Iteration 182, loss = 0.20852286\n",
      "Iteration 183, loss = 0.20813560\n",
      "Iteration 184, loss = 0.20779800\n",
      "Iteration 185, loss = 0.20741290\n",
      "Iteration 186, loss = 0.20705242\n",
      "Iteration 187, loss = 0.20671748\n",
      "Iteration 188, loss = 0.20643243\n",
      "Iteration 189, loss = 0.20596323\n",
      "Iteration 190, loss = 0.20569113\n",
      "Iteration 191, loss = 0.20540109\n",
      "Iteration 192, loss = 0.20496638\n",
      "Iteration 193, loss = 0.20469553\n",
      "Iteration 194, loss = 0.20428278\n",
      "Iteration 195, loss = 0.20399812\n",
      "Iteration 196, loss = 0.20367938\n",
      "Iteration 197, loss = 0.20335208\n",
      "Iteration 198, loss = 0.20300976\n",
      "Iteration 199, loss = 0.20275882\n",
      "Iteration 200, loss = 0.20239182\n",
      "[CV] END activation=logistic, alpha=0.01, batch_size=50, hidden_layer_sizes=(100,), solver=sgd; total time= 4.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.15597706\n",
      "Iteration 2, loss = 1.75155940\n",
      "Iteration 3, loss = 1.30699601\n",
      "Iteration 4, loss = 1.00465245\n",
      "Iteration 5, loss = 0.82486881\n",
      "Iteration 6, loss = 0.71271357\n",
      "Iteration 7, loss = 0.63743028\n",
      "Iteration 8, loss = 0.58370979\n",
      "Iteration 9, loss = 0.54371681\n",
      "Iteration 10, loss = 0.51257023\n",
      "Iteration 11, loss = 0.48810999\n",
      "Iteration 12, loss = 0.46811921\n",
      "Iteration 13, loss = 0.45156052\n",
      "Iteration 14, loss = 0.43767977\n",
      "Iteration 15, loss = 0.42589713\n",
      "Iteration 16, loss = 0.41568801\n",
      "Iteration 17, loss = 0.40668844\n",
      "Iteration 18, loss = 0.39876977\n",
      "Iteration 19, loss = 0.39182677\n",
      "Iteration 20, loss = 0.38539219\n",
      "Iteration 21, loss = 0.37976683\n",
      "Iteration 22, loss = 0.37453882\n",
      "Iteration 23, loss = 0.36980409\n",
      "Iteration 24, loss = 0.36545792\n",
      "Iteration 25, loss = 0.36135355\n",
      "Iteration 26, loss = 0.35753363\n",
      "Iteration 27, loss = 0.35398675\n",
      "Iteration 28, loss = 0.35074332\n",
      "Iteration 29, loss = 0.34756318\n",
      "Iteration 30, loss = 0.34472243\n",
      "Iteration 31, loss = 0.34198620\n",
      "Iteration 32, loss = 0.33915682\n",
      "Iteration 33, loss = 0.33673465\n",
      "Iteration 34, loss = 0.33441899\n",
      "Iteration 35, loss = 0.33194996\n",
      "Iteration 36, loss = 0.32981845\n",
      "Iteration 37, loss = 0.32770052\n",
      "Iteration 38, loss = 0.32569250\n",
      "Iteration 39, loss = 0.32374902\n",
      "Iteration 40, loss = 0.32182701\n",
      "Iteration 41, loss = 0.31999768\n",
      "Iteration 42, loss = 0.31822411\n",
      "Iteration 43, loss = 0.31652029\n",
      "Iteration 44, loss = 0.31491230\n",
      "Iteration 45, loss = 0.31322717\n",
      "Iteration 46, loss = 0.31167226\n",
      "Iteration 47, loss = 0.31006404\n",
      "Iteration 48, loss = 0.30851856\n",
      "Iteration 49, loss = 0.30716431\n",
      "Iteration 50, loss = 0.30563717\n",
      "Iteration 51, loss = 0.30420992\n",
      "Iteration 52, loss = 0.30277427\n",
      "Iteration 53, loss = 0.30148686\n",
      "Iteration 54, loss = 0.30009952\n",
      "Iteration 55, loss = 0.29874778\n",
      "Iteration 56, loss = 0.29755375\n",
      "Iteration 57, loss = 0.29622977\n",
      "Iteration 58, loss = 0.29496477\n",
      "Iteration 59, loss = 0.29382781\n",
      "Iteration 60, loss = 0.29255891\n",
      "Iteration 61, loss = 0.29138564\n",
      "Iteration 62, loss = 0.29028924\n",
      "Iteration 63, loss = 0.28898292\n",
      "Iteration 64, loss = 0.28788749\n",
      "Iteration 65, loss = 0.28675616\n",
      "Iteration 66, loss = 0.28565098\n",
      "Iteration 67, loss = 0.28458138\n",
      "Iteration 68, loss = 0.28353893\n",
      "Iteration 69, loss = 0.28239671\n",
      "Iteration 70, loss = 0.28140436\n",
      "Iteration 71, loss = 0.28029295\n",
      "Iteration 72, loss = 0.27934683\n",
      "Iteration 73, loss = 0.27826804\n",
      "Iteration 74, loss = 0.27736997\n",
      "Iteration 75, loss = 0.27632398\n",
      "Iteration 76, loss = 0.27533980\n",
      "Iteration 77, loss = 0.27439267\n",
      "Iteration 78, loss = 0.27341956\n",
      "Iteration 79, loss = 0.27248962\n",
      "Iteration 80, loss = 0.27153276\n",
      "Iteration 81, loss = 0.27052604\n",
      "Iteration 82, loss = 0.26973464\n",
      "Iteration 83, loss = 0.26875556\n",
      "Iteration 84, loss = 0.26788226\n",
      "Iteration 85, loss = 0.26693327\n",
      "Iteration 86, loss = 0.26607295\n",
      "Iteration 87, loss = 0.26517426\n",
      "Iteration 88, loss = 0.26431926\n",
      "Iteration 89, loss = 0.26351781\n",
      "Iteration 90, loss = 0.26265600\n",
      "Iteration 91, loss = 0.26180227\n",
      "Iteration 92, loss = 0.26095219\n",
      "Iteration 93, loss = 0.26016820\n",
      "Iteration 94, loss = 0.25935430\n",
      "Iteration 95, loss = 0.25855971\n",
      "Iteration 96, loss = 0.25771188\n",
      "Iteration 97, loss = 0.25688763\n",
      "Iteration 98, loss = 0.25623186\n",
      "Iteration 99, loss = 0.25538333\n",
      "Iteration 100, loss = 0.25465765\n",
      "Iteration 101, loss = 0.25390655\n",
      "Iteration 102, loss = 0.25307441\n",
      "Iteration 103, loss = 0.25239114\n",
      "Iteration 104, loss = 0.25163596\n",
      "Iteration 105, loss = 0.25089041\n",
      "Iteration 106, loss = 0.25018978\n",
      "Iteration 107, loss = 0.24951071\n",
      "Iteration 108, loss = 0.24874352\n",
      "Iteration 109, loss = 0.24802147\n",
      "Iteration 110, loss = 0.24738701\n",
      "Iteration 111, loss = 0.24667198\n",
      "Iteration 112, loss = 0.24599274\n",
      "Iteration 113, loss = 0.24536357\n",
      "Iteration 114, loss = 0.24462438\n",
      "Iteration 115, loss = 0.24395397\n",
      "Iteration 116, loss = 0.24330766\n",
      "Iteration 117, loss = 0.24267228\n",
      "Iteration 118, loss = 0.24197207\n",
      "Iteration 119, loss = 0.24135431\n",
      "Iteration 120, loss = 0.24075353\n",
      "Iteration 121, loss = 0.24003998\n",
      "Iteration 122, loss = 0.23949616\n",
      "Iteration 123, loss = 0.23878404\n",
      "Iteration 124, loss = 0.23819962\n",
      "Iteration 125, loss = 0.23761392\n",
      "Iteration 126, loss = 0.23698854\n",
      "Iteration 127, loss = 0.23636820\n",
      "Iteration 128, loss = 0.23580477\n",
      "Iteration 129, loss = 0.23517379\n",
      "Iteration 130, loss = 0.23466800\n",
      "Iteration 131, loss = 0.23404083\n",
      "Iteration 132, loss = 0.23354651\n",
      "Iteration 133, loss = 0.23294829\n",
      "Iteration 134, loss = 0.23236411\n",
      "Iteration 135, loss = 0.23177941\n",
      "Iteration 136, loss = 0.23129721\n",
      "Iteration 137, loss = 0.23075103\n",
      "Iteration 138, loss = 0.23020294\n",
      "Iteration 139, loss = 0.22963283\n",
      "Iteration 140, loss = 0.22914185\n",
      "Iteration 141, loss = 0.22859468\n",
      "Iteration 142, loss = 0.22804918\n",
      "Iteration 143, loss = 0.22758482\n",
      "Iteration 144, loss = 0.22705353\n",
      "Iteration 145, loss = 0.22654298\n",
      "Iteration 146, loss = 0.22604074\n",
      "Iteration 147, loss = 0.22549170\n",
      "Iteration 148, loss = 0.22503755\n",
      "Iteration 149, loss = 0.22454987\n",
      "Iteration 150, loss = 0.22407272\n",
      "Iteration 151, loss = 0.22356604\n",
      "Iteration 152, loss = 0.22306279\n",
      "Iteration 153, loss = 0.22266267\n",
      "Iteration 154, loss = 0.22215295\n",
      "Iteration 155, loss = 0.22166830\n",
      "Iteration 156, loss = 0.22123752\n",
      "Iteration 157, loss = 0.22076367\n",
      "Iteration 158, loss = 0.22035870\n",
      "Iteration 159, loss = 0.21980455\n",
      "Iteration 160, loss = 0.21945398\n",
      "Iteration 161, loss = 0.21898954\n",
      "Iteration 162, loss = 0.21853448\n",
      "Iteration 163, loss = 0.21805236\n",
      "Iteration 164, loss = 0.21771407\n",
      "Iteration 165, loss = 0.21723824\n",
      "Iteration 166, loss = 0.21685102\n",
      "Iteration 167, loss = 0.21641146\n",
      "Iteration 168, loss = 0.21601457\n",
      "Iteration 169, loss = 0.21553061\n",
      "Iteration 170, loss = 0.21513668\n",
      "Iteration 171, loss = 0.21478210\n",
      "Iteration 172, loss = 0.21435238\n",
      "Iteration 173, loss = 0.21399083\n",
      "Iteration 174, loss = 0.21349251\n",
      "Iteration 175, loss = 0.21314923\n",
      "Iteration 176, loss = 0.21277977\n",
      "Iteration 177, loss = 0.21240615\n",
      "Iteration 178, loss = 0.21197554\n",
      "Iteration 179, loss = 0.21166098\n",
      "Iteration 180, loss = 0.21125143\n",
      "Iteration 181, loss = 0.21093327\n",
      "Iteration 182, loss = 0.21050615\n",
      "Iteration 183, loss = 0.21012671\n",
      "Iteration 184, loss = 0.20978401\n",
      "Iteration 185, loss = 0.20931654\n",
      "Iteration 186, loss = 0.20910060\n",
      "Iteration 187, loss = 0.20869955\n",
      "Iteration 188, loss = 0.20835975\n",
      "Iteration 189, loss = 0.20797256\n",
      "Iteration 190, loss = 0.20772623\n",
      "Iteration 191, loss = 0.20731705\n",
      "Iteration 192, loss = 0.20693232\n",
      "Iteration 193, loss = 0.20668617\n",
      "Iteration 194, loss = 0.20632781\n",
      "Iteration 195, loss = 0.20602050\n",
      "Iteration 196, loss = 0.20563032\n",
      "Iteration 197, loss = 0.20533276\n",
      "Iteration 198, loss = 0.20503351\n",
      "Iteration 199, loss = 0.20467201\n",
      "Iteration 200, loss = 0.20435013\n",
      "[CV] END activation=logistic, alpha=0.01, batch_size=50, hidden_layer_sizes=(100,), solver=sgd; total time= 4.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.71762052\n",
      "Iteration 2, loss = 0.35812006\n",
      "Iteration 3, loss = 0.31069096\n",
      "Iteration 4, loss = 0.28043383\n",
      "Iteration 5, loss = 0.26094951\n",
      "Iteration 6, loss = 0.24433206\n",
      "Iteration 7, loss = 0.23375030\n",
      "Iteration 8, loss = 0.22822993\n",
      "Iteration 9, loss = 0.21836439\n",
      "Iteration 10, loss = 0.21502617\n",
      "Iteration 11, loss = 0.20748788\n",
      "Iteration 12, loss = 0.20446033\n",
      "Iteration 13, loss = 0.19930367\n",
      "Iteration 14, loss = 0.19772735\n",
      "Iteration 15, loss = 0.19197145\n",
      "Iteration 16, loss = 0.19152045\n",
      "Iteration 17, loss = 0.18911845\n",
      "Iteration 18, loss = 0.18620827\n",
      "Iteration 19, loss = 0.18374146\n",
      "Iteration 20, loss = 0.18054147\n",
      "Iteration 21, loss = 0.18039112\n",
      "Iteration 22, loss = 0.17926353\n",
      "Iteration 23, loss = 0.17625430\n",
      "Iteration 24, loss = 0.17772198\n",
      "Iteration 25, loss = 0.17453148\n",
      "Iteration 26, loss = 0.17392800\n",
      "Iteration 27, loss = 0.17237165\n",
      "Iteration 28, loss = 0.17153613\n",
      "Iteration 29, loss = 0.17296282\n",
      "Iteration 30, loss = 0.17005291\n",
      "Iteration 31, loss = 0.16935015\n",
      "Iteration 32, loss = 0.17026471\n",
      "Iteration 33, loss = 0.16821340\n",
      "Iteration 34, loss = 0.16625926\n",
      "Iteration 35, loss = 0.16607727\n",
      "Iteration 36, loss = 0.16570293\n",
      "Iteration 37, loss = 0.16684329\n",
      "Iteration 38, loss = 0.16352601\n",
      "Iteration 39, loss = 0.16436764\n",
      "Iteration 40, loss = 0.16198872\n",
      "Iteration 41, loss = 0.16295530\n",
      "Iteration 42, loss = 0.16203400\n",
      "Iteration 43, loss = 0.16134008\n",
      "Iteration 44, loss = 0.16138881\n",
      "Iteration 45, loss = 0.16156189\n",
      "Iteration 46, loss = 0.16187004\n",
      "Iteration 47, loss = 0.15934934\n",
      "Iteration 48, loss = 0.15866947\n",
      "Iteration 49, loss = 0.15952488\n",
      "Iteration 50, loss = 0.15790250\n",
      "Iteration 51, loss = 0.15770134\n",
      "Iteration 52, loss = 0.15798823\n",
      "Iteration 53, loss = 0.15853354\n",
      "Iteration 54, loss = 0.15802626\n",
      "Iteration 55, loss = 0.15872979\n",
      "Iteration 56, loss = 0.15842572\n",
      "Iteration 57, loss = 0.15789946\n",
      "Iteration 58, loss = 0.15730565\n",
      "Iteration 59, loss = 0.15723325\n",
      "Iteration 60, loss = 0.15707579\n",
      "Iteration 61, loss = 0.15743511\n",
      "Iteration 62, loss = 0.15679401\n",
      "Iteration 63, loss = 0.15630343\n",
      "Iteration 64, loss = 0.15615190\n",
      "Iteration 65, loss = 0.15636682\n",
      "Iteration 66, loss = 0.15571637\n",
      "Iteration 67, loss = 0.15534163\n",
      "Iteration 68, loss = 0.15459092\n",
      "Iteration 69, loss = 0.15571668\n",
      "Iteration 70, loss = 0.15575781\n",
      "Iteration 71, loss = 0.15516478\n",
      "Iteration 72, loss = 0.15515788\n",
      "Iteration 73, loss = 0.15543891\n",
      "Iteration 74, loss = 0.15482958\n",
      "Iteration 75, loss = 0.15511004\n",
      "Iteration 76, loss = 0.15394132\n",
      "Iteration 77, loss = 0.15450675\n",
      "Iteration 78, loss = 0.15293370\n",
      "Iteration 79, loss = 0.15411891\n",
      "Iteration 80, loss = 0.15423875\n",
      "Iteration 81, loss = 0.15402971\n",
      "Iteration 82, loss = 0.15368978\n",
      "Iteration 83, loss = 0.15392385\n",
      "Iteration 84, loss = 0.15430895\n",
      "Iteration 85, loss = 0.15214275\n",
      "Iteration 86, loss = 0.15354430\n",
      "Iteration 87, loss = 0.15309617\n",
      "Iteration 88, loss = 0.15249229\n",
      "Iteration 89, loss = 0.15349768\n",
      "Iteration 90, loss = 0.15310924\n",
      "Iteration 91, loss = 0.15353341\n",
      "Iteration 92, loss = 0.15281999\n",
      "Iteration 93, loss = 0.15244680\n",
      "Iteration 94, loss = 0.15306865\n",
      "Iteration 95, loss = 0.15194032\n",
      "Iteration 96, loss = 0.15214119\n",
      "Iteration 97, loss = 0.15300833\n",
      "Iteration 98, loss = 0.15142190\n",
      "Iteration 99, loss = 0.15260613\n",
      "Iteration 100, loss = 0.15194806\n",
      "Iteration 101, loss = 0.15088108\n",
      "Iteration 102, loss = 0.15187187\n",
      "Iteration 103, loss = 0.15077844\n",
      "Iteration 104, loss = 0.15161449\n",
      "Iteration 105, loss = 0.15259418\n",
      "Iteration 106, loss = 0.15088314\n",
      "Iteration 107, loss = 0.15121398\n",
      "Iteration 108, loss = 0.15098059\n",
      "Iteration 109, loss = 0.15084676\n",
      "Iteration 110, loss = 0.15134678\n",
      "Iteration 111, loss = 0.15069390\n",
      "Iteration 112, loss = 0.15200100\n",
      "Iteration 113, loss = 0.15017285\n",
      "Iteration 114, loss = 0.15208828\n",
      "Iteration 115, loss = 0.15115523\n",
      "Iteration 116, loss = 0.15185333\n",
      "Iteration 117, loss = 0.15083369\n",
      "Iteration 118, loss = 0.15088215\n",
      "Iteration 119, loss = 0.15101588\n",
      "Iteration 120, loss = 0.15127211\n",
      "Iteration 121, loss = 0.15101916\n",
      "Iteration 122, loss = 0.15000850\n",
      "Iteration 123, loss = 0.15016541\n",
      "Iteration 124, loss = 0.15200238\n",
      "Iteration 125, loss = 0.15014620\n",
      "Iteration 126, loss = 0.15061993\n",
      "Iteration 127, loss = 0.15060700\n",
      "Iteration 128, loss = 0.15019567\n",
      "Iteration 129, loss = 0.15114559\n",
      "Iteration 130, loss = 0.15101389\n",
      "Iteration 131, loss = 0.15018780\n",
      "Iteration 132, loss = 0.15023378\n",
      "Iteration 133, loss = 0.14996256\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.01, batch_size=50, hidden_layer_sizes=(400, 100), solver=adam; total time=13.9min\n",
      "Iteration 1, loss = 0.72530265\n",
      "Iteration 2, loss = 0.35893955\n",
      "Iteration 3, loss = 0.30808579\n",
      "Iteration 4, loss = 0.27976121\n",
      "Iteration 5, loss = 0.25675918\n",
      "Iteration 6, loss = 0.24224959\n",
      "Iteration 7, loss = 0.23174737\n",
      "Iteration 8, loss = 0.22352603\n",
      "Iteration 9, loss = 0.21733142\n",
      "Iteration 10, loss = 0.21131529\n",
      "Iteration 11, loss = 0.20662543\n",
      "Iteration 12, loss = 0.20060054\n",
      "Iteration 13, loss = 0.20252332\n",
      "Iteration 14, loss = 0.19786861\n",
      "Iteration 15, loss = 0.19390132\n",
      "Iteration 16, loss = 0.18908025\n",
      "Iteration 17, loss = 0.18914110\n",
      "Iteration 18, loss = 0.18668000\n",
      "Iteration 19, loss = 0.18422561\n",
      "Iteration 20, loss = 0.18205004\n",
      "Iteration 21, loss = 0.18182856\n",
      "Iteration 22, loss = 0.18012957\n",
      "Iteration 23, loss = 0.17856951\n",
      "Iteration 24, loss = 0.17677287\n",
      "Iteration 25, loss = 0.17624567\n",
      "Iteration 26, loss = 0.17495659\n",
      "Iteration 27, loss = 0.17338306\n",
      "Iteration 28, loss = 0.17208792\n",
      "Iteration 29, loss = 0.17084381\n",
      "Iteration 30, loss = 0.16969026\n",
      "Iteration 31, loss = 0.16923842\n",
      "Iteration 32, loss = 0.16918747\n",
      "Iteration 33, loss = 0.16788680\n",
      "Iteration 34, loss = 0.16756633\n",
      "Iteration 35, loss = 0.16566719\n",
      "Iteration 36, loss = 0.16584587\n",
      "Iteration 37, loss = 0.16422160\n",
      "Iteration 38, loss = 0.16539509\n",
      "Iteration 39, loss = 0.16261708\n",
      "Iteration 40, loss = 0.16471285\n",
      "Iteration 41, loss = 0.16410925\n",
      "Iteration 42, loss = 0.16291194\n",
      "Iteration 43, loss = 0.16178204\n",
      "Iteration 44, loss = 0.16066421\n",
      "Iteration 45, loss = 0.16166261\n",
      "Iteration 46, loss = 0.16104726\n",
      "Iteration 47, loss = 0.16124486\n",
      "Iteration 48, loss = 0.15939910\n",
      "Iteration 49, loss = 0.15948740\n",
      "Iteration 50, loss = 0.15894243\n",
      "Iteration 51, loss = 0.15877081\n",
      "Iteration 52, loss = 0.15960495\n",
      "Iteration 53, loss = 0.15841629\n",
      "Iteration 54, loss = 0.15819396\n",
      "Iteration 55, loss = 0.15904816\n",
      "Iteration 56, loss = 0.15756589\n",
      "Iteration 57, loss = 0.15707984\n",
      "Iteration 58, loss = 0.15787590\n",
      "Iteration 59, loss = 0.15655038\n",
      "Iteration 60, loss = 0.15634074\n",
      "Iteration 61, loss = 0.15661077\n",
      "Iteration 62, loss = 0.15664560\n",
      "Iteration 63, loss = 0.15694982\n",
      "Iteration 64, loss = 0.15733282\n",
      "Iteration 65, loss = 0.15568333\n",
      "Iteration 66, loss = 0.15712141\n",
      "Iteration 67, loss = 0.15565078\n",
      "Iteration 68, loss = 0.15502224\n",
      "Iteration 69, loss = 0.15698324\n",
      "Iteration 70, loss = 0.15560382\n",
      "Iteration 71, loss = 0.15522675\n",
      "Iteration 72, loss = 0.15586255\n",
      "Iteration 73, loss = 0.15484187\n",
      "Iteration 74, loss = 0.15552255\n",
      "Iteration 75, loss = 0.15542136\n",
      "Iteration 76, loss = 0.15515763\n",
      "Iteration 77, loss = 0.15465680\n",
      "Iteration 78, loss = 0.15434270\n",
      "Iteration 79, loss = 0.15533558\n",
      "Iteration 80, loss = 0.15491032\n",
      "Iteration 81, loss = 0.15441833\n",
      "Iteration 82, loss = 0.15360921\n",
      "Iteration 83, loss = 0.15438737\n",
      "Iteration 84, loss = 0.15413082\n",
      "Iteration 85, loss = 0.15432114\n",
      "Iteration 86, loss = 0.15433847\n",
      "Iteration 87, loss = 0.15256559\n",
      "Iteration 88, loss = 0.15248221\n",
      "Iteration 89, loss = 0.15397963\n",
      "Iteration 90, loss = 0.15287749\n",
      "Iteration 91, loss = 0.15304983\n",
      "Iteration 92, loss = 0.15316991\n",
      "Iteration 93, loss = 0.15317908\n",
      "Iteration 94, loss = 0.15361124\n",
      "Iteration 95, loss = 0.15340680\n",
      "Iteration 96, loss = 0.15300373\n",
      "Iteration 97, loss = 0.15343760\n",
      "Iteration 98, loss = 0.15375579\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.01, batch_size=50, hidden_layer_sizes=(400, 100), solver=adam; total time= 9.2min\n",
      "Iteration 1, loss = 0.71927412\n",
      "Iteration 2, loss = 0.35842072\n",
      "Iteration 3, loss = 0.31150849\n",
      "Iteration 4, loss = 0.28124432\n",
      "Iteration 5, loss = 0.25984159\n",
      "Iteration 6, loss = 0.24454308\n",
      "Iteration 7, loss = 0.23215491\n",
      "Iteration 8, loss = 0.22258260\n",
      "Iteration 9, loss = 0.21799212\n",
      "Iteration 10, loss = 0.21034682\n",
      "Iteration 11, loss = 0.20593542\n",
      "Iteration 12, loss = 0.20343353\n",
      "Iteration 13, loss = 0.19878972\n",
      "Iteration 14, loss = 0.19600864\n",
      "Iteration 15, loss = 0.19196743\n",
      "Iteration 16, loss = 0.19003708\n",
      "Iteration 17, loss = 0.18610379\n",
      "Iteration 18, loss = 0.18585764\n",
      "Iteration 19, loss = 0.18339887\n",
      "Iteration 20, loss = 0.18173407\n",
      "Iteration 21, loss = 0.17869067\n",
      "Iteration 22, loss = 0.17979931\n",
      "Iteration 23, loss = 0.17779880\n",
      "Iteration 24, loss = 0.17692931\n",
      "Iteration 25, loss = 0.17651245\n",
      "Iteration 26, loss = 0.17375213\n",
      "Iteration 27, loss = 0.17263697\n",
      "Iteration 28, loss = 0.17207186\n",
      "Iteration 29, loss = 0.17045047\n",
      "Iteration 30, loss = 0.17011326\n",
      "Iteration 31, loss = 0.16876494\n",
      "Iteration 32, loss = 0.16813342\n",
      "Iteration 33, loss = 0.16700599\n",
      "Iteration 34, loss = 0.16703721\n",
      "Iteration 35, loss = 0.16700177\n",
      "Iteration 36, loss = 0.16572214\n",
      "Iteration 37, loss = 0.16448896\n",
      "Iteration 38, loss = 0.16572394\n",
      "Iteration 39, loss = 0.16290386\n",
      "Iteration 40, loss = 0.16268763\n",
      "Iteration 41, loss = 0.16291235\n",
      "Iteration 42, loss = 0.16177245\n",
      "Iteration 43, loss = 0.16054482\n",
      "Iteration 44, loss = 0.16135065\n",
      "Iteration 45, loss = 0.15980240\n",
      "Iteration 46, loss = 0.15978663\n",
      "Iteration 47, loss = 0.16037771\n",
      "Iteration 48, loss = 0.15842562\n",
      "Iteration 49, loss = 0.15933502\n",
      "Iteration 50, loss = 0.16016892\n",
      "Iteration 51, loss = 0.15798642\n",
      "Iteration 52, loss = 0.15798785\n",
      "Iteration 53, loss = 0.15939565\n",
      "Iteration 54, loss = 0.15792712\n",
      "Iteration 55, loss = 0.15696198\n",
      "Iteration 56, loss = 0.15726011\n",
      "Iteration 57, loss = 0.15864408\n",
      "Iteration 58, loss = 0.15736314\n",
      "Iteration 59, loss = 0.15654963\n",
      "Iteration 60, loss = 0.15690355\n",
      "Iteration 61, loss = 0.15681212\n",
      "Iteration 62, loss = 0.15586726\n",
      "Iteration 63, loss = 0.15551823\n",
      "Iteration 64, loss = 0.15542474\n",
      "Iteration 65, loss = 0.15581358\n",
      "Iteration 66, loss = 0.15577816\n",
      "Iteration 67, loss = 0.15596044\n",
      "Iteration 68, loss = 0.15503004\n",
      "Iteration 69, loss = 0.15547728\n",
      "Iteration 70, loss = 0.15489826\n",
      "Iteration 71, loss = 0.15554217\n",
      "Iteration 72, loss = 0.15575414\n",
      "Iteration 73, loss = 0.15363269\n",
      "Iteration 74, loss = 0.15417244\n",
      "Iteration 75, loss = 0.15501204\n",
      "Iteration 76, loss = 0.15528620\n",
      "Iteration 77, loss = 0.15344894\n",
      "Iteration 78, loss = 0.15466738\n",
      "Iteration 79, loss = 0.15446929\n",
      "Iteration 80, loss = 0.15441481\n",
      "Iteration 81, loss = 0.15326425\n",
      "Iteration 82, loss = 0.15377120\n",
      "Iteration 83, loss = 0.15293060\n",
      "Iteration 84, loss = 0.15387804\n",
      "Iteration 85, loss = 0.15378674\n",
      "Iteration 86, loss = 0.15407734\n",
      "Iteration 87, loss = 0.15256171\n",
      "Iteration 88, loss = 0.15405914\n",
      "Iteration 89, loss = 0.15317242\n",
      "Iteration 90, loss = 0.15315202\n",
      "Iteration 91, loss = 0.15388327\n",
      "Iteration 92, loss = 0.15204104\n",
      "Iteration 93, loss = 0.15287970\n",
      "Iteration 94, loss = 0.15304019\n",
      "Iteration 95, loss = 0.15313637\n",
      "Iteration 96, loss = 0.15291917\n",
      "Iteration 97, loss = 0.15356569\n",
      "Iteration 98, loss = 0.15308115\n",
      "Iteration 99, loss = 0.15313870\n",
      "Iteration 100, loss = 0.15264827\n",
      "Iteration 101, loss = 0.15165731\n",
      "Iteration 102, loss = 0.15301379\n",
      "Iteration 103, loss = 0.15349065\n",
      "Iteration 104, loss = 0.15239642\n",
      "Iteration 105, loss = 0.15280568\n",
      "Iteration 106, loss = 0.15318964\n",
      "Iteration 107, loss = 0.15289329\n",
      "Iteration 108, loss = 0.15233766\n",
      "Iteration 109, loss = 0.15189328\n",
      "Iteration 110, loss = 0.15166660\n",
      "Iteration 111, loss = 0.15300256\n",
      "Iteration 112, loss = 0.15255594\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.01, batch_size=50, hidden_layer_sizes=(400, 100), solver=adam; total time=10.6min\n",
      "Iteration 1, loss = 0.71324536\n",
      "Iteration 2, loss = 0.35980656\n",
      "Iteration 3, loss = 0.30898515\n",
      "Iteration 4, loss = 0.27932806\n",
      "Iteration 5, loss = 0.26017324\n",
      "Iteration 6, loss = 0.24570019\n",
      "Iteration 7, loss = 0.23341228\n",
      "Iteration 8, loss = 0.22471049\n",
      "Iteration 9, loss = 0.21782113\n",
      "Iteration 10, loss = 0.21204895\n",
      "Iteration 11, loss = 0.20874550\n",
      "Iteration 12, loss = 0.20544568\n",
      "Iteration 13, loss = 0.19886372\n",
      "Iteration 14, loss = 0.19671420\n",
      "Iteration 15, loss = 0.19333705\n",
      "Iteration 16, loss = 0.19085274\n",
      "Iteration 17, loss = 0.18915647\n",
      "Iteration 18, loss = 0.18691631\n",
      "Iteration 19, loss = 0.18399934\n",
      "Iteration 20, loss = 0.18383009\n",
      "Iteration 21, loss = 0.18154511\n",
      "Iteration 22, loss = 0.17835761\n",
      "Iteration 23, loss = 0.17925917\n",
      "Iteration 24, loss = 0.17810046\n",
      "Iteration 25, loss = 0.17434855\n",
      "Iteration 26, loss = 0.17438857\n",
      "Iteration 27, loss = 0.17164216\n",
      "Iteration 28, loss = 0.17255185\n",
      "Iteration 29, loss = 0.17151255\n",
      "Iteration 30, loss = 0.17005683\n",
      "Iteration 31, loss = 0.16876978\n",
      "Iteration 32, loss = 0.16799355\n",
      "Iteration 33, loss = 0.16768312\n",
      "Iteration 34, loss = 0.16653432\n",
      "Iteration 35, loss = 0.16697403\n",
      "Iteration 36, loss = 0.16696045\n",
      "Iteration 37, loss = 0.16403302\n",
      "Iteration 38, loss = 0.16461013\n",
      "Iteration 39, loss = 0.16489249\n",
      "Iteration 40, loss = 0.16356253\n",
      "Iteration 41, loss = 0.16221368\n",
      "Iteration 42, loss = 0.16246282\n",
      "Iteration 43, loss = 0.16188393\n",
      "Iteration 44, loss = 0.16084444\n",
      "Iteration 45, loss = 0.16209921\n",
      "Iteration 46, loss = 0.16117581\n",
      "Iteration 47, loss = 0.16085125\n",
      "Iteration 48, loss = 0.15906801\n",
      "Iteration 49, loss = 0.15969600\n",
      "Iteration 50, loss = 0.16020051\n",
      "Iteration 51, loss = 0.16049274\n",
      "Iteration 52, loss = 0.15849893\n",
      "Iteration 53, loss = 0.15852716\n",
      "Iteration 54, loss = 0.15798154\n",
      "Iteration 55, loss = 0.15771297\n",
      "Iteration 56, loss = 0.15820520\n",
      "Iteration 57, loss = 0.15808584\n",
      "Iteration 58, loss = 0.15742176\n",
      "Iteration 59, loss = 0.15707118\n",
      "Iteration 60, loss = 0.15632912\n",
      "Iteration 61, loss = 0.15671588\n",
      "Iteration 62, loss = 0.15716800\n",
      "Iteration 63, loss = 0.15658033\n",
      "Iteration 64, loss = 0.15602526\n",
      "Iteration 65, loss = 0.15599736\n",
      "Iteration 66, loss = 0.15633385\n",
      "Iteration 67, loss = 0.15603146\n",
      "Iteration 68, loss = 0.15623893\n",
      "Iteration 69, loss = 0.15544852\n",
      "Iteration 70, loss = 0.15502814\n",
      "Iteration 71, loss = 0.15549429\n",
      "Iteration 72, loss = 0.15474887\n",
      "Iteration 73, loss = 0.15549756\n",
      "Iteration 74, loss = 0.15490201\n",
      "Iteration 75, loss = 0.15443123\n",
      "Iteration 76, loss = 0.15454375\n",
      "Iteration 77, loss = 0.15466309\n",
      "Iteration 78, loss = 0.15408111\n",
      "Iteration 79, loss = 0.15407666\n",
      "Iteration 80, loss = 0.15423356\n",
      "Iteration 81, loss = 0.15391229\n",
      "Iteration 82, loss = 0.15402797\n",
      "Iteration 83, loss = 0.15272240\n",
      "Iteration 84, loss = 0.15460597\n",
      "Iteration 85, loss = 0.15443567\n",
      "Iteration 86, loss = 0.15354939\n",
      "Iteration 87, loss = 0.15383483\n",
      "Iteration 88, loss = 0.15407342\n",
      "Iteration 89, loss = 0.15355600\n",
      "Iteration 90, loss = 0.15294151\n",
      "Iteration 91, loss = 0.15277093\n",
      "Iteration 92, loss = 0.15249797\n",
      "Iteration 93, loss = 0.15313091\n",
      "Iteration 94, loss = 0.15275253\n",
      "Iteration 95, loss = 0.15255364\n",
      "Iteration 96, loss = 0.15191596\n",
      "Iteration 97, loss = 0.15271153\n",
      "Iteration 98, loss = 0.15185743\n",
      "Iteration 99, loss = 0.15285410\n",
      "Iteration 100, loss = 0.15216091\n",
      "Iteration 101, loss = 0.15252590\n",
      "Iteration 102, loss = 0.15203332\n",
      "Iteration 103, loss = 0.15148897\n",
      "Iteration 104, loss = 0.15208452\n",
      "Iteration 105, loss = 0.15195581\n",
      "Iteration 106, loss = 0.15205154\n",
      "Iteration 107, loss = 0.15160812\n",
      "Iteration 108, loss = 0.15100186\n",
      "Iteration 109, loss = 0.15233754\n",
      "Iteration 110, loss = 0.15164511\n",
      "Iteration 111, loss = 0.15178688\n",
      "Iteration 112, loss = 0.15097721\n",
      "Iteration 113, loss = 0.15191162\n",
      "Iteration 114, loss = 0.15093374\n",
      "Iteration 115, loss = 0.15132360\n",
      "Iteration 116, loss = 0.15069481\n",
      "Iteration 117, loss = 0.15114463\n",
      "Iteration 118, loss = 0.15104125\n",
      "Iteration 119, loss = 0.15100791\n",
      "Iteration 120, loss = 0.15083972\n",
      "Iteration 121, loss = 0.15115450\n",
      "Iteration 122, loss = 0.15047792\n",
      "Iteration 123, loss = 0.15010141\n",
      "Iteration 124, loss = 0.15120322\n",
      "Iteration 125, loss = 0.15044151\n",
      "Iteration 126, loss = 0.15018770\n",
      "Iteration 127, loss = 0.15081777\n",
      "Iteration 128, loss = 0.14996329\n",
      "Iteration 129, loss = 0.15071059\n",
      "Iteration 130, loss = 0.15102787\n",
      "Iteration 131, loss = 0.15062382\n",
      "Iteration 132, loss = 0.14953572\n",
      "Iteration 133, loss = 0.15141352\n",
      "Iteration 134, loss = 0.15036777\n",
      "Iteration 135, loss = 0.15107142\n",
      "Iteration 136, loss = 0.14975808\n",
      "Iteration 137, loss = 0.15017826\n",
      "Iteration 138, loss = 0.15072560\n",
      "Iteration 139, loss = 0.14961662\n",
      "Iteration 140, loss = 0.15003998\n",
      "Iteration 141, loss = 0.14958576\n",
      "Iteration 142, loss = 0.14975500\n",
      "Iteration 143, loss = 0.15043064\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.01, batch_size=50, hidden_layer_sizes=(400, 100), solver=adam; total time=13.7min\n",
      "Iteration 1, loss = 0.72811766\n",
      "Iteration 2, loss = 0.35905075\n",
      "Iteration 3, loss = 0.31222045\n",
      "Iteration 4, loss = 0.28058179\n",
      "Iteration 5, loss = 0.25803055\n",
      "Iteration 6, loss = 0.24629726\n",
      "Iteration 7, loss = 0.23460030\n",
      "Iteration 8, loss = 0.22630327\n",
      "Iteration 9, loss = 0.21828387\n",
      "Iteration 10, loss = 0.21414637\n",
      "Iteration 11, loss = 0.20751940\n",
      "Iteration 12, loss = 0.20499379\n",
      "Iteration 13, loss = 0.20037943\n",
      "Iteration 14, loss = 0.19868579\n",
      "Iteration 15, loss = 0.19528812\n",
      "Iteration 16, loss = 0.19203495\n",
      "Iteration 17, loss = 0.19060290\n",
      "Iteration 18, loss = 0.18690061\n",
      "Iteration 19, loss = 0.18565552\n",
      "Iteration 20, loss = 0.18345156\n",
      "Iteration 21, loss = 0.18262138\n",
      "Iteration 22, loss = 0.18085215\n",
      "Iteration 23, loss = 0.18077523\n",
      "Iteration 24, loss = 0.17737022\n",
      "Iteration 25, loss = 0.17695934\n",
      "Iteration 26, loss = 0.17492465\n",
      "Iteration 27, loss = 0.17435323\n",
      "Iteration 28, loss = 0.17372392\n",
      "Iteration 29, loss = 0.17331220\n",
      "Iteration 30, loss = 0.17179164\n",
      "Iteration 31, loss = 0.16935790\n",
      "Iteration 32, loss = 0.16859867\n",
      "Iteration 33, loss = 0.16966363\n",
      "Iteration 34, loss = 0.16812384\n",
      "Iteration 35, loss = 0.16718216\n",
      "Iteration 36, loss = 0.16715188\n",
      "Iteration 37, loss = 0.16573296\n",
      "Iteration 38, loss = 0.16689070\n",
      "Iteration 39, loss = 0.16465293\n",
      "Iteration 40, loss = 0.16310216\n",
      "Iteration 41, loss = 0.16392065\n",
      "Iteration 42, loss = 0.16326625\n",
      "Iteration 43, loss = 0.16344663\n",
      "Iteration 44, loss = 0.16403652\n",
      "Iteration 45, loss = 0.16287171\n",
      "Iteration 46, loss = 0.16098432\n",
      "Iteration 47, loss = 0.16122630\n",
      "Iteration 48, loss = 0.16196442\n",
      "Iteration 49, loss = 0.16089625\n",
      "Iteration 50, loss = 0.15938040\n",
      "Iteration 51, loss = 0.16143952\n",
      "Iteration 52, loss = 0.16023032\n",
      "Iteration 53, loss = 0.15818994\n",
      "Iteration 54, loss = 0.15997614\n",
      "Iteration 55, loss = 0.15866308\n",
      "Iteration 56, loss = 0.16060485\n",
      "Iteration 57, loss = 0.15946993\n",
      "Iteration 58, loss = 0.15818940\n",
      "Iteration 59, loss = 0.15840240\n",
      "Iteration 60, loss = 0.15864731\n",
      "Iteration 61, loss = 0.15807912\n",
      "Iteration 62, loss = 0.15720539\n",
      "Iteration 63, loss = 0.15685028\n",
      "Iteration 64, loss = 0.15668852\n",
      "Iteration 65, loss = 0.15715707\n",
      "Iteration 66, loss = 0.15613200\n",
      "Iteration 67, loss = 0.15623575\n",
      "Iteration 68, loss = 0.15657155\n",
      "Iteration 69, loss = 0.15626614\n",
      "Iteration 70, loss = 0.15630411\n",
      "Iteration 71, loss = 0.15564367\n",
      "Iteration 72, loss = 0.15553502\n",
      "Iteration 73, loss = 0.15492865\n",
      "Iteration 74, loss = 0.15588519\n",
      "Iteration 75, loss = 0.15469647\n",
      "Iteration 76, loss = 0.15554191\n",
      "Iteration 77, loss = 0.15481617\n",
      "Iteration 78, loss = 0.15570573\n",
      "Iteration 79, loss = 0.15470094\n",
      "Iteration 80, loss = 0.15508245\n",
      "Iteration 81, loss = 0.15458132\n",
      "Iteration 82, loss = 0.15351258\n",
      "Iteration 83, loss = 0.15427279\n",
      "Iteration 84, loss = 0.15507659\n",
      "Iteration 85, loss = 0.15340507\n",
      "Iteration 86, loss = 0.15377417\n",
      "Iteration 87, loss = 0.15410299\n",
      "Iteration 88, loss = 0.15412638\n",
      "Iteration 89, loss = 0.15463158\n",
      "Iteration 90, loss = 0.15334192\n",
      "Iteration 91, loss = 0.15243392\n",
      "Iteration 92, loss = 0.15248438\n",
      "Iteration 93, loss = 0.15369415\n",
      "Iteration 94, loss = 0.15303869\n",
      "Iteration 95, loss = 0.15452987\n",
      "Iteration 96, loss = 0.15379183\n",
      "Iteration 97, loss = 0.15382062\n",
      "Iteration 98, loss = 0.15207701\n",
      "Iteration 99, loss = 0.15292927\n",
      "Iteration 100, loss = 0.15263012\n",
      "Iteration 101, loss = 0.15383146\n",
      "Iteration 102, loss = 0.15342656\n",
      "Iteration 103, loss = 0.15208207\n",
      "Iteration 104, loss = 0.15256780\n",
      "Iteration 105, loss = 0.15174580\n",
      "Iteration 106, loss = 0.15393267\n",
      "Iteration 107, loss = 0.15328388\n",
      "Iteration 108, loss = 0.15192107\n",
      "Iteration 109, loss = 0.15250714\n",
      "Iteration 110, loss = 0.15336722\n",
      "Iteration 111, loss = 0.15224970\n",
      "Iteration 112, loss = 0.15187888\n",
      "Iteration 113, loss = 0.15286203\n",
      "Iteration 114, loss = 0.15206873\n",
      "Iteration 115, loss = 0.15240731\n",
      "Iteration 116, loss = 0.15319009\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.01, batch_size=50, hidden_layer_sizes=(400, 100), solver=adam; total time=10.9min\n",
      "Iteration 1, loss = 2.31995271\n",
      "Iteration 2, loss = 2.30562210\n",
      "Iteration 3, loss = 2.28787347\n",
      "Iteration 4, loss = 2.26081546\n",
      "Iteration 5, loss = 2.21374932\n",
      "Iteration 6, loss = 2.12329461\n",
      "Iteration 7, loss = 1.95348273\n",
      "Iteration 8, loss = 1.70692631\n",
      "Iteration 9, loss = 1.46655067\n",
      "Iteration 10, loss = 1.27141903\n",
      "Iteration 11, loss = 1.11803541\n",
      "Iteration 12, loss = 1.00111659\n",
      "Iteration 13, loss = 0.91254112\n",
      "Iteration 14, loss = 0.84255610\n",
      "Iteration 15, loss = 0.78491156\n",
      "Iteration 16, loss = 0.73621765\n",
      "Iteration 17, loss = 0.69444973\n",
      "Iteration 18, loss = 0.65874775\n",
      "Iteration 19, loss = 0.62787183\n",
      "Iteration 20, loss = 0.60170232\n",
      "Iteration 21, loss = 0.57902344\n",
      "Iteration 22, loss = 0.55949128\n",
      "Iteration 23, loss = 0.54259315\n",
      "Iteration 24, loss = 0.52799547\n",
      "Iteration 25, loss = 0.51496200\n",
      "Iteration 26, loss = 0.50354612\n",
      "Iteration 27, loss = 0.49333562\n",
      "Iteration 28, loss = 0.48426156\n",
      "Iteration 29, loss = 0.47583410\n",
      "Iteration 30, loss = 0.46845229\n",
      "Iteration 31, loss = 0.46149860\n",
      "Iteration 32, loss = 0.45530646\n",
      "Iteration 33, loss = 0.44920657\n",
      "Iteration 34, loss = 0.44384985\n",
      "Iteration 35, loss = 0.43888071\n",
      "Iteration 36, loss = 0.43446582\n",
      "Iteration 37, loss = 0.43006446\n",
      "Iteration 38, loss = 0.42576493\n",
      "Iteration 39, loss = 0.42221337\n",
      "Iteration 40, loss = 0.41844495\n",
      "Iteration 41, loss = 0.41516527\n",
      "Iteration 42, loss = 0.41168953\n",
      "Iteration 43, loss = 0.40863771\n",
      "Iteration 44, loss = 0.40588610\n",
      "Iteration 45, loss = 0.40293603\n",
      "Iteration 46, loss = 0.40043884\n",
      "Iteration 47, loss = 0.39785796\n",
      "Iteration 48, loss = 0.39536856\n",
      "Iteration 49, loss = 0.39289949\n",
      "Iteration 50, loss = 0.39080877\n",
      "Iteration 51, loss = 0.38863081\n",
      "Iteration 52, loss = 0.38636460\n",
      "Iteration 53, loss = 0.38450563\n",
      "Iteration 54, loss = 0.38237931\n",
      "Iteration 55, loss = 0.38042738\n",
      "Iteration 56, loss = 0.37877988\n",
      "Iteration 57, loss = 0.37692462\n",
      "Iteration 58, loss = 0.37536248\n",
      "Iteration 59, loss = 0.37349952\n",
      "Iteration 60, loss = 0.37183957\n",
      "Iteration 61, loss = 0.37042712\n",
      "Iteration 62, loss = 0.36859274\n",
      "Iteration 63, loss = 0.36711134\n",
      "Iteration 64, loss = 0.36562096\n",
      "Iteration 65, loss = 0.36412570\n",
      "Iteration 66, loss = 0.36270880\n",
      "Iteration 67, loss = 0.36131816\n",
      "Iteration 68, loss = 0.35988929\n",
      "Iteration 69, loss = 0.35857192\n",
      "Iteration 70, loss = 0.35720608\n",
      "Iteration 71, loss = 0.35605483\n",
      "Iteration 72, loss = 0.35471775\n",
      "Iteration 73, loss = 0.35336324\n",
      "Iteration 74, loss = 0.35224052\n",
      "Iteration 75, loss = 0.35097152\n",
      "Iteration 76, loss = 0.34963989\n",
      "Iteration 77, loss = 0.34866038\n",
      "Iteration 78, loss = 0.34741167\n",
      "Iteration 79, loss = 0.34647662\n",
      "Iteration 80, loss = 0.34505577\n",
      "Iteration 81, loss = 0.34396406\n",
      "Iteration 82, loss = 0.34303991\n",
      "Iteration 83, loss = 0.34206519\n",
      "Iteration 84, loss = 0.34085955\n",
      "Iteration 85, loss = 0.33983627\n",
      "Iteration 86, loss = 0.33889482\n",
      "Iteration 87, loss = 0.33783868\n",
      "Iteration 88, loss = 0.33678713\n",
      "Iteration 89, loss = 0.33566845\n",
      "Iteration 90, loss = 0.33483661\n",
      "Iteration 91, loss = 0.33399176\n",
      "Iteration 92, loss = 0.33282168\n",
      "Iteration 93, loss = 0.33199920\n",
      "Iteration 94, loss = 0.33093391\n",
      "Iteration 95, loss = 0.33020564\n",
      "Iteration 96, loss = 0.32931254\n",
      "Iteration 97, loss = 0.32831564\n",
      "Iteration 98, loss = 0.32752420\n",
      "Iteration 99, loss = 0.32649927\n",
      "Iteration 100, loss = 0.32556413\n",
      "Iteration 101, loss = 0.32470363\n",
      "Iteration 102, loss = 0.32385640\n",
      "Iteration 103, loss = 0.32310809\n",
      "Iteration 104, loss = 0.32235334\n",
      "Iteration 105, loss = 0.32144011\n",
      "Iteration 106, loss = 0.32058094\n",
      "Iteration 107, loss = 0.31961011\n",
      "Iteration 108, loss = 0.31877854\n",
      "Iteration 109, loss = 0.31814539\n",
      "Iteration 110, loss = 0.31732245\n",
      "Iteration 111, loss = 0.31637746\n",
      "Iteration 112, loss = 0.31591317\n",
      "Iteration 113, loss = 0.31511888\n",
      "Iteration 114, loss = 0.31406927\n",
      "Iteration 115, loss = 0.31329480\n",
      "Iteration 116, loss = 0.31257313\n",
      "Iteration 117, loss = 0.31175889\n",
      "Iteration 118, loss = 0.31108289\n",
      "Iteration 119, loss = 0.31010381\n",
      "Iteration 120, loss = 0.30956524\n",
      "Iteration 121, loss = 0.30875851\n",
      "Iteration 122, loss = 0.30794778\n",
      "Iteration 123, loss = 0.30728746\n",
      "Iteration 124, loss = 0.30656969\n",
      "Iteration 125, loss = 0.30581441\n",
      "Iteration 126, loss = 0.30499557\n",
      "Iteration 127, loss = 0.30428832\n",
      "Iteration 128, loss = 0.30348033\n",
      "Iteration 129, loss = 0.30287438\n",
      "Iteration 130, loss = 0.30207404\n",
      "Iteration 131, loss = 0.30148978\n",
      "Iteration 132, loss = 0.30065119\n",
      "Iteration 133, loss = 0.30007703\n",
      "Iteration 134, loss = 0.29929716\n",
      "Iteration 135, loss = 0.29851601\n",
      "Iteration 136, loss = 0.29799435\n",
      "Iteration 137, loss = 0.29715054\n",
      "Iteration 138, loss = 0.29667508\n",
      "Iteration 139, loss = 0.29580722\n",
      "Iteration 140, loss = 0.29515896\n",
      "Iteration 141, loss = 0.29441776\n",
      "Iteration 142, loss = 0.29380543\n",
      "Iteration 143, loss = 0.29316330\n",
      "Iteration 144, loss = 0.29263111\n",
      "Iteration 145, loss = 0.29177897\n",
      "Iteration 146, loss = 0.29127994\n",
      "Iteration 147, loss = 0.29048450\n",
      "Iteration 148, loss = 0.28993502\n",
      "Iteration 149, loss = 0.28915230\n",
      "Iteration 150, loss = 0.28855310\n",
      "Iteration 151, loss = 0.28776871\n",
      "Iteration 152, loss = 0.28756221\n",
      "Iteration 153, loss = 0.28664979\n",
      "Iteration 154, loss = 0.28600596\n",
      "Iteration 155, loss = 0.28531837\n",
      "Iteration 156, loss = 0.28471640\n",
      "Iteration 157, loss = 0.28422169\n",
      "Iteration 158, loss = 0.28333761\n",
      "Iteration 159, loss = 0.28282810\n",
      "Iteration 160, loss = 0.28213502\n",
      "Iteration 161, loss = 0.28161692\n",
      "Iteration 162, loss = 0.28105740\n",
      "Iteration 163, loss = 0.28035572\n",
      "Iteration 164, loss = 0.27988028\n",
      "Iteration 165, loss = 0.27912477\n",
      "Iteration 166, loss = 0.27852129\n",
      "Iteration 167, loss = 0.27820518\n",
      "Iteration 168, loss = 0.27756319\n",
      "Iteration 169, loss = 0.27686266\n",
      "Iteration 170, loss = 0.27621120\n",
      "Iteration 171, loss = 0.27564430\n",
      "Iteration 172, loss = 0.27494919\n",
      "Iteration 173, loss = 0.27451849\n",
      "Iteration 174, loss = 0.27388344\n",
      "Iteration 175, loss = 0.27326237\n",
      "Iteration 176, loss = 0.27273280\n",
      "Iteration 177, loss = 0.27216527\n",
      "Iteration 178, loss = 0.27156533\n",
      "Iteration 179, loss = 0.27107048\n",
      "Iteration 180, loss = 0.27045156\n",
      "Iteration 181, loss = 0.26978041\n",
      "Iteration 182, loss = 0.26946603\n",
      "Iteration 183, loss = 0.26879008\n",
      "Iteration 184, loss = 0.26822632\n",
      "Iteration 185, loss = 0.26787869\n",
      "Iteration 186, loss = 0.26721799\n",
      "Iteration 187, loss = 0.26678798\n",
      "Iteration 188, loss = 0.26614852\n",
      "Iteration 189, loss = 0.26546417\n",
      "Iteration 190, loss = 0.26496623\n",
      "Iteration 191, loss = 0.26450793\n",
      "Iteration 192, loss = 0.26391459\n",
      "Iteration 193, loss = 0.26352926\n",
      "Iteration 194, loss = 0.26287422\n",
      "Iteration 195, loss = 0.26250926\n",
      "Iteration 196, loss = 0.26196878\n",
      "Iteration 197, loss = 0.26142782\n",
      "Iteration 198, loss = 0.26086675\n",
      "Iteration 199, loss = 0.26041900\n",
      "Iteration 200, loss = 0.25985457\n",
      "[CV] END activation=logistic, alpha=0.01, batch_size=50, hidden_layer_sizes=(400, 100), solver=sgd; total time=11.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.32200995\n",
      "Iteration 2, loss = 2.30742456\n",
      "Iteration 3, loss = 2.28952141\n",
      "Iteration 4, loss = 2.26238770\n",
      "Iteration 5, loss = 2.21388627\n",
      "Iteration 6, loss = 2.12195440\n",
      "Iteration 7, loss = 1.95419539\n",
      "Iteration 8, loss = 1.71767733\n",
      "Iteration 9, loss = 1.48134552\n",
      "Iteration 10, loss = 1.29007934\n",
      "Iteration 11, loss = 1.14348460\n",
      "Iteration 12, loss = 1.03186360\n",
      "Iteration 13, loss = 0.94601608\n",
      "Iteration 14, loss = 0.87870308\n",
      "Iteration 15, loss = 0.82263786\n",
      "Iteration 16, loss = 0.77442268\n",
      "Iteration 17, loss = 0.73235425\n",
      "Iteration 18, loss = 0.69551442\n",
      "Iteration 19, loss = 0.66308836\n",
      "Iteration 20, loss = 0.63445789\n",
      "Iteration 21, loss = 0.60974381\n",
      "Iteration 22, loss = 0.58760789\n",
      "Iteration 23, loss = 0.56807061\n",
      "Iteration 24, loss = 0.55051888\n",
      "Iteration 25, loss = 0.53505957\n",
      "Iteration 26, loss = 0.52116765\n",
      "Iteration 27, loss = 0.50848255\n",
      "Iteration 28, loss = 0.49731855\n",
      "Iteration 29, loss = 0.48688425\n",
      "Iteration 30, loss = 0.47772450\n",
      "Iteration 31, loss = 0.46924331\n",
      "Iteration 32, loss = 0.46178094\n",
      "Iteration 33, loss = 0.45461435\n",
      "Iteration 34, loss = 0.44845249\n",
      "Iteration 35, loss = 0.44249865\n",
      "Iteration 36, loss = 0.43703978\n",
      "Iteration 37, loss = 0.43211844\n",
      "Iteration 38, loss = 0.42762836\n",
      "Iteration 39, loss = 0.42322368\n",
      "Iteration 40, loss = 0.41911996\n",
      "Iteration 41, loss = 0.41528481\n",
      "Iteration 42, loss = 0.41157105\n",
      "Iteration 43, loss = 0.40827219\n",
      "Iteration 44, loss = 0.40510006\n",
      "Iteration 45, loss = 0.40214065\n",
      "Iteration 46, loss = 0.39934594\n",
      "Iteration 47, loss = 0.39667550\n",
      "Iteration 48, loss = 0.39395651\n",
      "Iteration 49, loss = 0.39128851\n",
      "Iteration 50, loss = 0.38891605\n",
      "Iteration 51, loss = 0.38669427\n",
      "Iteration 52, loss = 0.38441827\n",
      "Iteration 53, loss = 0.38240650\n",
      "Iteration 54, loss = 0.38018637\n",
      "Iteration 55, loss = 0.37849858\n",
      "Iteration 56, loss = 0.37647405\n",
      "Iteration 57, loss = 0.37450351\n",
      "Iteration 58, loss = 0.37282299\n",
      "Iteration 59, loss = 0.37094963\n",
      "Iteration 60, loss = 0.36927607\n",
      "Iteration 61, loss = 0.36783891\n",
      "Iteration 62, loss = 0.36618973\n",
      "Iteration 63, loss = 0.36456448\n",
      "Iteration 64, loss = 0.36306911\n",
      "Iteration 65, loss = 0.36152707\n",
      "Iteration 66, loss = 0.36010284\n",
      "Iteration 67, loss = 0.35877477\n",
      "Iteration 68, loss = 0.35737276\n",
      "Iteration 69, loss = 0.35595075\n",
      "Iteration 70, loss = 0.35478308\n",
      "Iteration 71, loss = 0.35351685\n",
      "Iteration 72, loss = 0.35219028\n",
      "Iteration 73, loss = 0.35121028\n",
      "Iteration 74, loss = 0.34989608\n",
      "Iteration 75, loss = 0.34871531\n",
      "Iteration 76, loss = 0.34758821\n",
      "Iteration 77, loss = 0.34648945\n",
      "Iteration 78, loss = 0.34534297\n",
      "Iteration 79, loss = 0.34427368\n",
      "Iteration 80, loss = 0.34324261\n",
      "Iteration 81, loss = 0.34226063\n",
      "Iteration 82, loss = 0.34130203\n",
      "Iteration 83, loss = 0.34017020\n",
      "Iteration 84, loss = 0.33937208\n",
      "Iteration 85, loss = 0.33812509\n",
      "Iteration 86, loss = 0.33731339\n",
      "Iteration 87, loss = 0.33626301\n",
      "Iteration 88, loss = 0.33549438\n",
      "Iteration 89, loss = 0.33457386\n",
      "Iteration 90, loss = 0.33358945\n",
      "Iteration 91, loss = 0.33274669\n",
      "Iteration 92, loss = 0.33192285\n",
      "Iteration 93, loss = 0.33105173\n",
      "Iteration 94, loss = 0.33020289\n",
      "Iteration 95, loss = 0.32933111\n",
      "Iteration 96, loss = 0.32841304\n",
      "Iteration 97, loss = 0.32761873\n",
      "Iteration 98, loss = 0.32677450\n",
      "Iteration 99, loss = 0.32613658\n",
      "Iteration 100, loss = 0.32525197\n",
      "Iteration 101, loss = 0.32423520\n",
      "Iteration 102, loss = 0.32347730\n",
      "Iteration 103, loss = 0.32313156\n",
      "Iteration 104, loss = 0.32209017\n",
      "Iteration 105, loss = 0.32155727\n",
      "Iteration 106, loss = 0.32055475\n",
      "Iteration 107, loss = 0.31991595\n",
      "Iteration 108, loss = 0.31918146\n",
      "Iteration 109, loss = 0.31830960\n",
      "Iteration 110, loss = 0.31775245\n",
      "Iteration 111, loss = 0.31673471\n",
      "Iteration 112, loss = 0.31603935\n",
      "Iteration 113, loss = 0.31530920\n",
      "Iteration 114, loss = 0.31457926\n",
      "Iteration 115, loss = 0.31402129\n",
      "Iteration 116, loss = 0.31316348\n",
      "Iteration 117, loss = 0.31274862\n",
      "Iteration 118, loss = 0.31190473\n",
      "Iteration 119, loss = 0.31124037\n",
      "Iteration 120, loss = 0.31039266\n",
      "Iteration 121, loss = 0.30977059\n",
      "Iteration 122, loss = 0.30930720\n",
      "Iteration 123, loss = 0.30854401\n",
      "Iteration 124, loss = 0.30773386\n",
      "Iteration 125, loss = 0.30715759\n",
      "Iteration 126, loss = 0.30650830\n",
      "Iteration 127, loss = 0.30587843\n",
      "Iteration 128, loss = 0.30528666\n",
      "Iteration 129, loss = 0.30451266\n",
      "Iteration 130, loss = 0.30391679\n",
      "Iteration 131, loss = 0.30318358\n",
      "Iteration 132, loss = 0.30259641\n",
      "Iteration 133, loss = 0.30195499\n",
      "Iteration 134, loss = 0.30124597\n",
      "Iteration 135, loss = 0.30067966\n",
      "Iteration 136, loss = 0.29996284\n",
      "Iteration 137, loss = 0.29947806\n",
      "Iteration 138, loss = 0.29861333\n",
      "Iteration 139, loss = 0.29812578\n",
      "Iteration 140, loss = 0.29751529\n",
      "Iteration 141, loss = 0.29681411\n",
      "Iteration 142, loss = 0.29635236\n",
      "Iteration 143, loss = 0.29569131\n",
      "Iteration 144, loss = 0.29503447\n",
      "Iteration 145, loss = 0.29448506\n",
      "Iteration 146, loss = 0.29375086\n",
      "Iteration 147, loss = 0.29336591\n",
      "Iteration 148, loss = 0.29266512\n",
      "Iteration 149, loss = 0.29202832\n",
      "Iteration 150, loss = 0.29141706\n",
      "Iteration 151, loss = 0.29093177\n",
      "Iteration 152, loss = 0.29016946\n",
      "Iteration 153, loss = 0.28965736\n",
      "Iteration 154, loss = 0.28911030\n",
      "Iteration 155, loss = 0.28855349\n",
      "Iteration 156, loss = 0.28796819\n",
      "Iteration 157, loss = 0.28728732\n",
      "Iteration 158, loss = 0.28673373\n",
      "Iteration 159, loss = 0.28626485\n",
      "Iteration 160, loss = 0.28568912\n",
      "Iteration 161, loss = 0.28503243\n",
      "Iteration 162, loss = 0.28449958\n",
      "Iteration 163, loss = 0.28381949\n",
      "Iteration 164, loss = 0.28332942\n",
      "Iteration 165, loss = 0.28269364\n",
      "Iteration 166, loss = 0.28226028\n",
      "Iteration 167, loss = 0.28170292\n",
      "Iteration 168, loss = 0.28109539\n",
      "Iteration 169, loss = 0.28047791\n",
      "Iteration 170, loss = 0.27976644\n",
      "Iteration 171, loss = 0.27942092\n",
      "Iteration 172, loss = 0.27871093\n",
      "Iteration 173, loss = 0.27831720\n",
      "Iteration 174, loss = 0.27771846\n",
      "Iteration 175, loss = 0.27730175\n",
      "Iteration 176, loss = 0.27669983\n",
      "Iteration 177, loss = 0.27612455\n",
      "Iteration 178, loss = 0.27560771\n",
      "Iteration 179, loss = 0.27516086\n",
      "Iteration 180, loss = 0.27451450\n",
      "Iteration 181, loss = 0.27400294\n",
      "Iteration 182, loss = 0.27355174\n",
      "Iteration 183, loss = 0.27297634\n",
      "Iteration 184, loss = 0.27231008\n",
      "Iteration 185, loss = 0.27184277\n",
      "Iteration 186, loss = 0.27153656\n",
      "Iteration 187, loss = 0.27091398\n",
      "Iteration 188, loss = 0.27032786\n",
      "Iteration 189, loss = 0.26986350\n",
      "Iteration 190, loss = 0.26913758\n",
      "Iteration 191, loss = 0.26881268\n",
      "Iteration 192, loss = 0.26823412\n",
      "Iteration 193, loss = 0.26788652\n",
      "Iteration 194, loss = 0.26729847\n",
      "Iteration 195, loss = 0.26679995\n",
      "Iteration 196, loss = 0.26641740\n",
      "Iteration 197, loss = 0.26587180\n",
      "Iteration 198, loss = 0.26528817\n",
      "Iteration 199, loss = 0.26501330\n",
      "Iteration 200, loss = 0.26426911\n",
      "[CV] END activation=logistic, alpha=0.01, batch_size=50, hidden_layer_sizes=(400, 100), solver=sgd; total time=11.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.32149002\n",
      "Iteration 2, loss = 2.30559317\n",
      "Iteration 3, loss = 2.28716350\n",
      "Iteration 4, loss = 2.25865615\n",
      "Iteration 5, loss = 2.20905142\n",
      "Iteration 6, loss = 2.11388505\n",
      "Iteration 7, loss = 1.94494642\n",
      "Iteration 8, loss = 1.71554525\n",
      "Iteration 9, loss = 1.49179684\n",
      "Iteration 10, loss = 1.30930887\n",
      "Iteration 11, loss = 1.16271523\n",
      "Iteration 12, loss = 1.04453565\n",
      "Iteration 13, loss = 0.94852683\n",
      "Iteration 14, loss = 0.86889373\n",
      "Iteration 15, loss = 0.80249115\n",
      "Iteration 16, loss = 0.74684091\n",
      "Iteration 17, loss = 0.70081500\n",
      "Iteration 18, loss = 0.66280501\n",
      "Iteration 19, loss = 0.63074097\n",
      "Iteration 20, loss = 0.60363800\n",
      "Iteration 21, loss = 0.58060404\n",
      "Iteration 22, loss = 0.56048042\n",
      "Iteration 23, loss = 0.54275582\n",
      "Iteration 24, loss = 0.52718073\n",
      "Iteration 25, loss = 0.51336202\n",
      "Iteration 26, loss = 0.50110346\n",
      "Iteration 27, loss = 0.48996727\n",
      "Iteration 28, loss = 0.48010456\n",
      "Iteration 29, loss = 0.47105450\n",
      "Iteration 30, loss = 0.46274213\n",
      "Iteration 31, loss = 0.45547760\n",
      "Iteration 32, loss = 0.44879547\n",
      "Iteration 33, loss = 0.44245930\n",
      "Iteration 34, loss = 0.43704157\n",
      "Iteration 35, loss = 0.43181624\n",
      "Iteration 36, loss = 0.42697000\n",
      "Iteration 37, loss = 0.42253541\n",
      "Iteration 38, loss = 0.41825164\n",
      "Iteration 39, loss = 0.41450119\n",
      "Iteration 40, loss = 0.41083445\n",
      "Iteration 41, loss = 0.40767610\n",
      "Iteration 42, loss = 0.40422693\n",
      "Iteration 43, loss = 0.40125409\n",
      "Iteration 44, loss = 0.39830403\n",
      "Iteration 45, loss = 0.39569003\n",
      "Iteration 46, loss = 0.39304375\n",
      "Iteration 47, loss = 0.39018645\n",
      "Iteration 48, loss = 0.38804652\n",
      "Iteration 49, loss = 0.38567180\n",
      "Iteration 50, loss = 0.38354057\n",
      "Iteration 51, loss = 0.38139070\n",
      "Iteration 52, loss = 0.37914022\n",
      "Iteration 53, loss = 0.37733549\n",
      "Iteration 54, loss = 0.37559377\n",
      "Iteration 55, loss = 0.37354601\n",
      "Iteration 56, loss = 0.37172975\n",
      "Iteration 57, loss = 0.37011615\n",
      "Iteration 58, loss = 0.36835448\n",
      "Iteration 59, loss = 0.36659583\n",
      "Iteration 60, loss = 0.36500660\n",
      "Iteration 61, loss = 0.36348101\n",
      "Iteration 62, loss = 0.36193035\n",
      "Iteration 63, loss = 0.36052758\n",
      "Iteration 64, loss = 0.35895248\n",
      "Iteration 65, loss = 0.35774267\n",
      "Iteration 66, loss = 0.35630150\n",
      "Iteration 67, loss = 0.35478095\n",
      "Iteration 68, loss = 0.35362631\n",
      "Iteration 69, loss = 0.35225996\n",
      "Iteration 70, loss = 0.35111543\n",
      "Iteration 71, loss = 0.34963071\n",
      "Iteration 72, loss = 0.34867520\n",
      "Iteration 73, loss = 0.34716297\n",
      "Iteration 74, loss = 0.34610256\n",
      "Iteration 75, loss = 0.34480742\n",
      "Iteration 76, loss = 0.34389532\n",
      "Iteration 77, loss = 0.34262558\n",
      "Iteration 78, loss = 0.34159234\n",
      "Iteration 79, loss = 0.34037970\n",
      "Iteration 80, loss = 0.33926752\n",
      "Iteration 81, loss = 0.33838771\n",
      "Iteration 82, loss = 0.33725001\n",
      "Iteration 83, loss = 0.33632277\n",
      "Iteration 84, loss = 0.33494677\n",
      "Iteration 85, loss = 0.33429973\n",
      "Iteration 86, loss = 0.33324022\n",
      "Iteration 87, loss = 0.33234867\n",
      "Iteration 88, loss = 0.33136700\n",
      "Iteration 89, loss = 0.33054156\n",
      "Iteration 90, loss = 0.32956253\n",
      "Iteration 91, loss = 0.32858698\n",
      "Iteration 92, loss = 0.32762151\n",
      "Iteration 93, loss = 0.32696552\n",
      "Iteration 94, loss = 0.32603249\n",
      "Iteration 95, loss = 0.32494307\n",
      "Iteration 96, loss = 0.32433662\n",
      "Iteration 97, loss = 0.32339763\n",
      "Iteration 98, loss = 0.32246125\n",
      "Iteration 99, loss = 0.32170141\n",
      "Iteration 100, loss = 0.32089821\n",
      "Iteration 101, loss = 0.32005925\n",
      "Iteration 102, loss = 0.31940286\n",
      "Iteration 103, loss = 0.31847325\n",
      "Iteration 104, loss = 0.31778785\n",
      "Iteration 105, loss = 0.31672712\n",
      "Iteration 106, loss = 0.31605576\n",
      "Iteration 107, loss = 0.31553022\n",
      "Iteration 108, loss = 0.31456342\n",
      "Iteration 109, loss = 0.31375853\n",
      "Iteration 110, loss = 0.31311922\n",
      "Iteration 111, loss = 0.31224641\n",
      "Iteration 112, loss = 0.31145650\n",
      "Iteration 113, loss = 0.31091256\n",
      "Iteration 114, loss = 0.31016171\n",
      "Iteration 115, loss = 0.30927863\n",
      "Iteration 116, loss = 0.30872686\n",
      "Iteration 117, loss = 0.30797370\n",
      "Iteration 118, loss = 0.30703568\n",
      "Iteration 119, loss = 0.30640879\n",
      "Iteration 120, loss = 0.30563232\n",
      "Iteration 121, loss = 0.30515993\n",
      "Iteration 122, loss = 0.30459389\n",
      "Iteration 123, loss = 0.30371668\n",
      "Iteration 124, loss = 0.30317348\n",
      "Iteration 125, loss = 0.30242562\n",
      "Iteration 126, loss = 0.30170101\n",
      "Iteration 127, loss = 0.30117289\n",
      "Iteration 128, loss = 0.30039509\n",
      "Iteration 129, loss = 0.29964387\n",
      "Iteration 130, loss = 0.29913784\n",
      "Iteration 131, loss = 0.29825476\n",
      "Iteration 132, loss = 0.29769790\n",
      "Iteration 133, loss = 0.29716865\n",
      "Iteration 134, loss = 0.29655795\n",
      "Iteration 135, loss = 0.29566313\n",
      "Iteration 136, loss = 0.29525100\n",
      "Iteration 137, loss = 0.29435937\n",
      "Iteration 138, loss = 0.29391760\n",
      "Iteration 139, loss = 0.29338137\n",
      "Iteration 140, loss = 0.29257852\n",
      "Iteration 141, loss = 0.29202116\n",
      "Iteration 142, loss = 0.29123021\n",
      "Iteration 143, loss = 0.29076569\n",
      "Iteration 144, loss = 0.29022455\n",
      "Iteration 145, loss = 0.28948990\n",
      "Iteration 146, loss = 0.28879615\n",
      "Iteration 147, loss = 0.28831315\n",
      "Iteration 148, loss = 0.28773525\n",
      "Iteration 149, loss = 0.28699518\n",
      "Iteration 150, loss = 0.28650861\n",
      "Iteration 151, loss = 0.28583896\n",
      "Iteration 152, loss = 0.28527094\n",
      "Iteration 153, loss = 0.28463187\n",
      "Iteration 154, loss = 0.28412468\n",
      "Iteration 155, loss = 0.28343714\n",
      "Iteration 156, loss = 0.28304455\n",
      "Iteration 157, loss = 0.28233318\n",
      "Iteration 158, loss = 0.28188256\n",
      "Iteration 159, loss = 0.28108862\n",
      "Iteration 160, loss = 0.28047807\n",
      "Iteration 161, loss = 0.28009192\n",
      "Iteration 162, loss = 0.27934618\n",
      "Iteration 163, loss = 0.27871526\n",
      "Iteration 164, loss = 0.27830947\n",
      "Iteration 165, loss = 0.27788812\n",
      "Iteration 166, loss = 0.27719702\n",
      "Iteration 167, loss = 0.27653698\n",
      "Iteration 168, loss = 0.27631072\n",
      "Iteration 169, loss = 0.27560639\n",
      "Iteration 170, loss = 0.27499259\n",
      "Iteration 171, loss = 0.27448054\n",
      "Iteration 172, loss = 0.27397330\n",
      "Iteration 173, loss = 0.27320491\n",
      "Iteration 174, loss = 0.27275716\n",
      "Iteration 175, loss = 0.27222637\n",
      "Iteration 176, loss = 0.27175954\n",
      "Iteration 177, loss = 0.27123890\n",
      "Iteration 178, loss = 0.27076045\n",
      "Iteration 179, loss = 0.27018627\n",
      "Iteration 180, loss = 0.26977511\n",
      "Iteration 181, loss = 0.26914859\n",
      "Iteration 182, loss = 0.26876658\n",
      "Iteration 183, loss = 0.26805371\n",
      "Iteration 184, loss = 0.26760982\n",
      "Iteration 185, loss = 0.26707308\n",
      "Iteration 186, loss = 0.26650537\n",
      "Iteration 187, loss = 0.26616374\n",
      "Iteration 188, loss = 0.26565584\n",
      "Iteration 189, loss = 0.26504507\n",
      "Iteration 190, loss = 0.26460452\n",
      "Iteration 191, loss = 0.26419677\n",
      "Iteration 192, loss = 0.26357588\n",
      "Iteration 193, loss = 0.26315611\n",
      "Iteration 194, loss = 0.26246206\n",
      "Iteration 195, loss = 0.26219659\n",
      "Iteration 196, loss = 0.26169389\n",
      "Iteration 197, loss = 0.26131917\n",
      "Iteration 198, loss = 0.26084961\n",
      "Iteration 199, loss = 0.26030143\n",
      "Iteration 200, loss = 0.25976544\n",
      "[CV] END activation=logistic, alpha=0.01, batch_size=50, hidden_layer_sizes=(400, 100), solver=sgd; total time=11.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.32202802\n",
      "Iteration 2, loss = 2.30382740\n",
      "Iteration 3, loss = 2.28334317\n",
      "Iteration 4, loss = 2.25075677\n",
      "Iteration 5, loss = 2.19129551\n",
      "Iteration 6, loss = 2.07679115\n",
      "Iteration 7, loss = 1.88322866\n",
      "Iteration 8, loss = 1.64108236\n",
      "Iteration 9, loss = 1.41264188\n",
      "Iteration 10, loss = 1.23075684\n",
      "Iteration 11, loss = 1.09223714\n",
      "Iteration 12, loss = 0.98642009\n",
      "Iteration 13, loss = 0.90382173\n",
      "Iteration 14, loss = 0.83645250\n",
      "Iteration 15, loss = 0.77952776\n",
      "Iteration 16, loss = 0.73052402\n",
      "Iteration 17, loss = 0.68838643\n",
      "Iteration 18, loss = 0.65196472\n",
      "Iteration 19, loss = 0.62076142\n",
      "Iteration 20, loss = 0.59448250\n",
      "Iteration 21, loss = 0.57148566\n",
      "Iteration 22, loss = 0.55227986\n",
      "Iteration 23, loss = 0.53538218\n",
      "Iteration 24, loss = 0.52064744\n",
      "Iteration 25, loss = 0.50794141\n",
      "Iteration 26, loss = 0.49674877\n",
      "Iteration 27, loss = 0.48676622\n",
      "Iteration 28, loss = 0.47797697\n",
      "Iteration 29, loss = 0.47004253\n",
      "Iteration 30, loss = 0.46290708\n",
      "Iteration 31, loss = 0.45656537\n",
      "Iteration 32, loss = 0.45070078\n",
      "Iteration 33, loss = 0.44545464\n",
      "Iteration 34, loss = 0.44058987\n",
      "Iteration 35, loss = 0.43609397\n",
      "Iteration 36, loss = 0.43173892\n",
      "Iteration 37, loss = 0.42786210\n",
      "Iteration 38, loss = 0.42442182\n",
      "Iteration 39, loss = 0.42117278\n",
      "Iteration 40, loss = 0.41772321\n",
      "Iteration 41, loss = 0.41499542\n",
      "Iteration 42, loss = 0.41196459\n",
      "Iteration 43, loss = 0.40922091\n",
      "Iteration 44, loss = 0.40675750\n",
      "Iteration 45, loss = 0.40425623\n",
      "Iteration 46, loss = 0.40180280\n",
      "Iteration 47, loss = 0.39968351\n",
      "Iteration 48, loss = 0.39750824\n",
      "Iteration 49, loss = 0.39530486\n",
      "Iteration 50, loss = 0.39314961\n",
      "Iteration 51, loss = 0.39136748\n",
      "Iteration 52, loss = 0.38942819\n",
      "Iteration 53, loss = 0.38776707\n",
      "Iteration 54, loss = 0.38593559\n",
      "Iteration 55, loss = 0.38430420\n",
      "Iteration 56, loss = 0.38238031\n",
      "Iteration 57, loss = 0.38079192\n",
      "Iteration 58, loss = 0.37917115\n",
      "Iteration 59, loss = 0.37755785\n",
      "Iteration 60, loss = 0.37594820\n",
      "Iteration 61, loss = 0.37449399\n",
      "Iteration 62, loss = 0.37293384\n",
      "Iteration 63, loss = 0.37149122\n",
      "Iteration 64, loss = 0.37009205\n",
      "Iteration 65, loss = 0.36855573\n",
      "Iteration 66, loss = 0.36700543\n",
      "Iteration 67, loss = 0.36589832\n",
      "Iteration 68, loss = 0.36435980\n",
      "Iteration 69, loss = 0.36304436\n",
      "Iteration 70, loss = 0.36178093\n",
      "Iteration 71, loss = 0.36052109\n",
      "Iteration 72, loss = 0.35918751\n",
      "Iteration 73, loss = 0.35789387\n",
      "Iteration 74, loss = 0.35653081\n",
      "Iteration 75, loss = 0.35538919\n",
      "Iteration 76, loss = 0.35423888\n",
      "Iteration 77, loss = 0.35293078\n",
      "Iteration 78, loss = 0.35173207\n",
      "Iteration 79, loss = 0.35084167\n",
      "Iteration 80, loss = 0.34955675\n",
      "Iteration 81, loss = 0.34829796\n",
      "Iteration 82, loss = 0.34715483\n",
      "Iteration 83, loss = 0.34600387\n",
      "Iteration 84, loss = 0.34482556\n",
      "Iteration 85, loss = 0.34383543\n",
      "Iteration 86, loss = 0.34258894\n",
      "Iteration 87, loss = 0.34168420\n",
      "Iteration 88, loss = 0.34045967\n",
      "Iteration 89, loss = 0.33930260\n",
      "Iteration 90, loss = 0.33842093\n",
      "Iteration 91, loss = 0.33729767\n",
      "Iteration 92, loss = 0.33627608\n",
      "Iteration 93, loss = 0.33525543\n",
      "Iteration 94, loss = 0.33437482\n",
      "Iteration 95, loss = 0.33307034\n",
      "Iteration 96, loss = 0.33226868\n",
      "Iteration 97, loss = 0.33128102\n",
      "Iteration 98, loss = 0.33024787\n",
      "Iteration 99, loss = 0.32926501\n",
      "Iteration 100, loss = 0.32851420\n",
      "Iteration 101, loss = 0.32740658\n",
      "Iteration 102, loss = 0.32651098\n",
      "Iteration 103, loss = 0.32554179\n",
      "Iteration 104, loss = 0.32463598\n",
      "Iteration 105, loss = 0.32393882\n",
      "Iteration 106, loss = 0.32283597\n",
      "Iteration 107, loss = 0.32199814\n",
      "Iteration 108, loss = 0.32112262\n",
      "Iteration 109, loss = 0.32013374\n",
      "Iteration 110, loss = 0.31940334\n",
      "Iteration 111, loss = 0.31851791\n",
      "Iteration 112, loss = 0.31751973\n",
      "Iteration 113, loss = 0.31676758\n",
      "Iteration 114, loss = 0.31603993\n",
      "Iteration 115, loss = 0.31506840\n",
      "Iteration 116, loss = 0.31425279\n",
      "Iteration 117, loss = 0.31346682\n",
      "Iteration 118, loss = 0.31249954\n",
      "Iteration 119, loss = 0.31184053\n",
      "Iteration 120, loss = 0.31101952\n",
      "Iteration 121, loss = 0.31029707\n",
      "Iteration 122, loss = 0.30935638\n",
      "Iteration 123, loss = 0.30860024\n",
      "Iteration 124, loss = 0.30798838\n",
      "Iteration 125, loss = 0.30719267\n",
      "Iteration 126, loss = 0.30643678\n",
      "Iteration 127, loss = 0.30575249\n",
      "Iteration 128, loss = 0.30493014\n",
      "Iteration 129, loss = 0.30418246\n",
      "Iteration 130, loss = 0.30345330\n",
      "Iteration 131, loss = 0.30272516\n",
      "Iteration 132, loss = 0.30201687\n",
      "Iteration 133, loss = 0.30129052\n",
      "Iteration 134, loss = 0.30068169\n",
      "Iteration 135, loss = 0.29989475\n",
      "Iteration 136, loss = 0.29916700\n",
      "Iteration 137, loss = 0.29846216\n",
      "Iteration 138, loss = 0.29768641\n",
      "Iteration 139, loss = 0.29708196\n",
      "Iteration 140, loss = 0.29646207\n",
      "Iteration 141, loss = 0.29573662\n",
      "Iteration 142, loss = 0.29509669\n",
      "Iteration 143, loss = 0.29425587\n",
      "Iteration 144, loss = 0.29367427\n",
      "Iteration 145, loss = 0.29315614\n",
      "Iteration 146, loss = 0.29240153\n",
      "Iteration 147, loss = 0.29176323\n",
      "Iteration 148, loss = 0.29103711\n",
      "Iteration 149, loss = 0.29035284\n",
      "Iteration 150, loss = 0.28983929\n",
      "Iteration 151, loss = 0.28922199\n",
      "Iteration 152, loss = 0.28852520\n",
      "Iteration 153, loss = 0.28782394\n",
      "Iteration 154, loss = 0.28736346\n",
      "Iteration 155, loss = 0.28672416\n",
      "Iteration 156, loss = 0.28608611\n",
      "Iteration 157, loss = 0.28561996\n",
      "Iteration 158, loss = 0.28470237\n",
      "Iteration 159, loss = 0.28416920\n",
      "Iteration 160, loss = 0.28358248\n",
      "Iteration 161, loss = 0.28320229\n",
      "Iteration 162, loss = 0.28239236\n",
      "Iteration 163, loss = 0.28168387\n",
      "Iteration 164, loss = 0.28125741\n",
      "Iteration 165, loss = 0.28065229\n",
      "Iteration 166, loss = 0.28005330\n",
      "Iteration 167, loss = 0.27948568\n",
      "Iteration 168, loss = 0.27891130\n",
      "Iteration 169, loss = 0.27852280\n",
      "Iteration 170, loss = 0.27766582\n",
      "Iteration 171, loss = 0.27731523\n",
      "Iteration 172, loss = 0.27653113\n",
      "Iteration 173, loss = 0.27607328\n",
      "Iteration 174, loss = 0.27549378\n",
      "Iteration 175, loss = 0.27504252\n",
      "Iteration 176, loss = 0.27438208\n",
      "Iteration 177, loss = 0.27398474\n",
      "Iteration 178, loss = 0.27329340\n",
      "Iteration 179, loss = 0.27275402\n",
      "Iteration 180, loss = 0.27214663\n",
      "Iteration 181, loss = 0.27172816\n",
      "Iteration 182, loss = 0.27114430\n",
      "Iteration 183, loss = 0.27063242\n",
      "Iteration 184, loss = 0.27020838\n",
      "Iteration 185, loss = 0.26950590\n",
      "Iteration 186, loss = 0.26917380\n",
      "Iteration 187, loss = 0.26858141\n",
      "Iteration 188, loss = 0.26814440\n",
      "Iteration 189, loss = 0.26750670\n",
      "Iteration 190, loss = 0.26701746\n",
      "Iteration 191, loss = 0.26650616\n",
      "Iteration 192, loss = 0.26604926\n",
      "Iteration 193, loss = 0.26560797\n",
      "Iteration 194, loss = 0.26499013\n",
      "Iteration 195, loss = 0.26447783\n",
      "Iteration 196, loss = 0.26401803\n",
      "Iteration 197, loss = 0.26345996\n",
      "Iteration 198, loss = 0.26303027\n",
      "Iteration 199, loss = 0.26260561\n",
      "Iteration 200, loss = 0.26223232\n",
      "[CV] END activation=logistic, alpha=0.01, batch_size=50, hidden_layer_sizes=(400, 100), solver=sgd; total time=11.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.32328481\n",
      "Iteration 2, loss = 2.30612871\n",
      "Iteration 3, loss = 2.28771532\n",
      "Iteration 4, loss = 2.25973787\n",
      "Iteration 5, loss = 2.20927732\n",
      "Iteration 6, loss = 2.11440581\n",
      "Iteration 7, loss = 1.95106467\n",
      "Iteration 8, loss = 1.73372225\n",
      "Iteration 9, loss = 1.50496122\n",
      "Iteration 10, loss = 1.30498746\n",
      "Iteration 11, loss = 1.14518567\n",
      "Iteration 12, loss = 1.01951546\n",
      "Iteration 13, loss = 0.92165752\n",
      "Iteration 14, loss = 0.84428042\n",
      "Iteration 15, loss = 0.78140269\n",
      "Iteration 16, loss = 0.72910564\n",
      "Iteration 17, loss = 0.68535479\n",
      "Iteration 18, loss = 0.64903712\n",
      "Iteration 19, loss = 0.61842246\n",
      "Iteration 20, loss = 0.59287494\n",
      "Iteration 21, loss = 0.57138269\n",
      "Iteration 22, loss = 0.55268373\n",
      "Iteration 23, loss = 0.53673027\n",
      "Iteration 24, loss = 0.52294537\n",
      "Iteration 25, loss = 0.51033220\n",
      "Iteration 26, loss = 0.49968363\n",
      "Iteration 27, loss = 0.49002062\n",
      "Iteration 28, loss = 0.48115803\n",
      "Iteration 29, loss = 0.47321915\n",
      "Iteration 30, loss = 0.46615432\n",
      "Iteration 31, loss = 0.45995156\n",
      "Iteration 32, loss = 0.45404298\n",
      "Iteration 33, loss = 0.44858278\n",
      "Iteration 34, loss = 0.44365943\n",
      "Iteration 35, loss = 0.43904183\n",
      "Iteration 36, loss = 0.43455313\n",
      "Iteration 37, loss = 0.43076401\n",
      "Iteration 38, loss = 0.42693007\n",
      "Iteration 39, loss = 0.42351863\n",
      "Iteration 40, loss = 0.42035425\n",
      "Iteration 41, loss = 0.41709357\n",
      "Iteration 42, loss = 0.41418203\n",
      "Iteration 43, loss = 0.41142003\n",
      "Iteration 44, loss = 0.40891122\n",
      "Iteration 45, loss = 0.40626640\n",
      "Iteration 46, loss = 0.40391863\n",
      "Iteration 47, loss = 0.40163416\n",
      "Iteration 48, loss = 0.39953314\n",
      "Iteration 49, loss = 0.39745641\n",
      "Iteration 50, loss = 0.39535238\n",
      "Iteration 51, loss = 0.39323367\n",
      "Iteration 52, loss = 0.39140905\n",
      "Iteration 53, loss = 0.38966333\n",
      "Iteration 54, loss = 0.38791322\n",
      "Iteration 55, loss = 0.38618664\n",
      "Iteration 56, loss = 0.38462308\n",
      "Iteration 57, loss = 0.38288792\n",
      "Iteration 58, loss = 0.38128586\n",
      "Iteration 59, loss = 0.37987343\n",
      "Iteration 60, loss = 0.37832579\n",
      "Iteration 61, loss = 0.37685351\n",
      "Iteration 62, loss = 0.37554664\n",
      "Iteration 63, loss = 0.37401883\n",
      "Iteration 64, loss = 0.37259154\n",
      "Iteration 65, loss = 0.37121298\n",
      "Iteration 66, loss = 0.36996091\n",
      "Iteration 67, loss = 0.36862052\n",
      "Iteration 68, loss = 0.36735120\n",
      "Iteration 69, loss = 0.36609049\n",
      "Iteration 70, loss = 0.36490681\n",
      "Iteration 71, loss = 0.36365096\n",
      "Iteration 72, loss = 0.36236416\n",
      "Iteration 73, loss = 0.36114378\n",
      "Iteration 74, loss = 0.36003565\n",
      "Iteration 75, loss = 0.35876987\n",
      "Iteration 76, loss = 0.35765303\n",
      "Iteration 77, loss = 0.35657355\n",
      "Iteration 78, loss = 0.35528990\n",
      "Iteration 79, loss = 0.35423399\n",
      "Iteration 80, loss = 0.35313066\n",
      "Iteration 81, loss = 0.35194750\n",
      "Iteration 82, loss = 0.35112967\n",
      "Iteration 83, loss = 0.34977141\n",
      "Iteration 84, loss = 0.34880314\n",
      "Iteration 85, loss = 0.34766294\n",
      "Iteration 86, loss = 0.34696148\n",
      "Iteration 87, loss = 0.34573026\n",
      "Iteration 88, loss = 0.34487865\n",
      "Iteration 89, loss = 0.34377190\n",
      "Iteration 90, loss = 0.34281359\n",
      "Iteration 91, loss = 0.34177941\n",
      "Iteration 92, loss = 0.34077845\n",
      "Iteration 93, loss = 0.33974918\n",
      "Iteration 94, loss = 0.33871958\n",
      "Iteration 95, loss = 0.33797382\n",
      "Iteration 96, loss = 0.33685780\n",
      "Iteration 97, loss = 0.33607378\n",
      "Iteration 98, loss = 0.33516963\n",
      "Iteration 99, loss = 0.33389451\n",
      "Iteration 100, loss = 0.33339491\n",
      "Iteration 101, loss = 0.33224068\n",
      "Iteration 102, loss = 0.33149640\n",
      "Iteration 103, loss = 0.33059946\n",
      "Iteration 104, loss = 0.32979896\n",
      "Iteration 105, loss = 0.32883659\n",
      "Iteration 106, loss = 0.32779334\n",
      "Iteration 107, loss = 0.32714880\n",
      "Iteration 108, loss = 0.32623579\n",
      "Iteration 109, loss = 0.32540722\n",
      "Iteration 110, loss = 0.32449722\n",
      "Iteration 111, loss = 0.32372906\n",
      "Iteration 112, loss = 0.32286479\n",
      "Iteration 113, loss = 0.32216978\n",
      "Iteration 114, loss = 0.32131106\n",
      "Iteration 115, loss = 0.32042298\n",
      "Iteration 116, loss = 0.31953860\n",
      "Iteration 117, loss = 0.31875841\n",
      "Iteration 118, loss = 0.31785963\n",
      "Iteration 119, loss = 0.31719579\n",
      "Iteration 120, loss = 0.31650554\n",
      "Iteration 121, loss = 0.31565863\n",
      "Iteration 122, loss = 0.31492437\n",
      "Iteration 123, loss = 0.31391087\n",
      "Iteration 124, loss = 0.31332698\n",
      "Iteration 125, loss = 0.31247542\n",
      "Iteration 126, loss = 0.31191307\n",
      "Iteration 127, loss = 0.31096613\n",
      "Iteration 128, loss = 0.31036696\n",
      "Iteration 129, loss = 0.30965150\n",
      "Iteration 130, loss = 0.30874271\n",
      "Iteration 131, loss = 0.30802687\n",
      "Iteration 132, loss = 0.30736451\n",
      "Iteration 133, loss = 0.30663431\n",
      "Iteration 134, loss = 0.30579783\n",
      "Iteration 135, loss = 0.30507890\n",
      "Iteration 136, loss = 0.30439881\n",
      "Iteration 137, loss = 0.30369241\n",
      "Iteration 138, loss = 0.30309565\n",
      "Iteration 139, loss = 0.30217215\n",
      "Iteration 140, loss = 0.30156890\n",
      "Iteration 141, loss = 0.30087509\n",
      "Iteration 142, loss = 0.30021000\n",
      "Iteration 143, loss = 0.29950561\n",
      "Iteration 144, loss = 0.29872231\n",
      "Iteration 145, loss = 0.29815584\n",
      "Iteration 146, loss = 0.29747383\n",
      "Iteration 147, loss = 0.29683112\n",
      "Iteration 148, loss = 0.29611058\n",
      "Iteration 149, loss = 0.29533853\n",
      "Iteration 150, loss = 0.29464651\n",
      "Iteration 151, loss = 0.29409667\n",
      "Iteration 152, loss = 0.29326391\n",
      "Iteration 153, loss = 0.29272014\n",
      "Iteration 154, loss = 0.29211537\n",
      "Iteration 155, loss = 0.29149950\n",
      "Iteration 156, loss = 0.29089224\n",
      "Iteration 157, loss = 0.29002621\n",
      "Iteration 158, loss = 0.28936763\n",
      "Iteration 159, loss = 0.28875778\n",
      "Iteration 160, loss = 0.28808533\n",
      "Iteration 161, loss = 0.28765033\n",
      "Iteration 162, loss = 0.28689032\n",
      "Iteration 163, loss = 0.28618923\n",
      "Iteration 164, loss = 0.28567390\n",
      "Iteration 165, loss = 0.28489634\n",
      "Iteration 166, loss = 0.28440083\n",
      "Iteration 167, loss = 0.28377368\n",
      "Iteration 168, loss = 0.28303506\n",
      "Iteration 169, loss = 0.28260218\n",
      "Iteration 170, loss = 0.28187808\n",
      "Iteration 171, loss = 0.28130322\n",
      "Iteration 172, loss = 0.28070437\n",
      "Iteration 173, loss = 0.28007537\n",
      "Iteration 174, loss = 0.27953949\n",
      "Iteration 175, loss = 0.27880621\n",
      "Iteration 176, loss = 0.27823487\n",
      "Iteration 177, loss = 0.27764964\n",
      "Iteration 178, loss = 0.27713124\n",
      "Iteration 179, loss = 0.27656544\n",
      "Iteration 180, loss = 0.27606088\n",
      "Iteration 181, loss = 0.27535526\n",
      "Iteration 182, loss = 0.27476905\n",
      "Iteration 183, loss = 0.27413352\n",
      "Iteration 184, loss = 0.27349951\n",
      "Iteration 185, loss = 0.27311343\n",
      "Iteration 186, loss = 0.27253653\n",
      "Iteration 187, loss = 0.27201261\n",
      "Iteration 188, loss = 0.27129294\n",
      "Iteration 189, loss = 0.27080101\n",
      "Iteration 190, loss = 0.27017301\n",
      "Iteration 191, loss = 0.26968058\n",
      "Iteration 192, loss = 0.26913315\n",
      "Iteration 193, loss = 0.26871426\n",
      "Iteration 194, loss = 0.26811404\n",
      "Iteration 195, loss = 0.26756870\n",
      "Iteration 196, loss = 0.26691435\n",
      "Iteration 197, loss = 0.26647366\n",
      "Iteration 198, loss = 0.26599067\n",
      "Iteration 199, loss = 0.26550346\n",
      "Iteration 200, loss = 0.26473217\n",
      "[CV] END activation=logistic, alpha=0.01, batch_size=50, hidden_layer_sizes=(400, 100), solver=sgd; total time=11.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.06442720\n",
      "Iteration 2, loss = 0.43803448\n",
      "Iteration 3, loss = 0.34869481\n",
      "Iteration 4, loss = 0.30084484\n",
      "Iteration 5, loss = 0.26931959\n",
      "Iteration 6, loss = 0.24907142\n",
      "Iteration 7, loss = 0.23782647\n",
      "Iteration 8, loss = 0.22361197\n",
      "Iteration 9, loss = 0.21709580\n",
      "Iteration 10, loss = 0.20881743\n",
      "Iteration 11, loss = 0.20417706\n",
      "Iteration 12, loss = 0.19777647\n",
      "Iteration 13, loss = 0.19547739\n",
      "Iteration 14, loss = 0.19087519\n",
      "Iteration 15, loss = 0.18891050\n",
      "Iteration 16, loss = 0.18570571\n",
      "Iteration 17, loss = 0.18320956\n",
      "Iteration 18, loss = 0.18377488\n",
      "Iteration 19, loss = 0.18032309\n",
      "Iteration 20, loss = 0.17742645\n",
      "Iteration 21, loss = 0.17646984\n",
      "Iteration 22, loss = 0.17486921\n",
      "Iteration 23, loss = 0.17490058\n",
      "Iteration 24, loss = 0.17289030\n",
      "Iteration 25, loss = 0.17143495\n",
      "Iteration 26, loss = 0.17470523\n",
      "Iteration 27, loss = 0.17223139\n",
      "Iteration 28, loss = 0.16991541\n",
      "Iteration 29, loss = 0.16955933\n",
      "Iteration 30, loss = 0.16549887\n",
      "Iteration 31, loss = 0.17017765\n",
      "Iteration 32, loss = 0.16811050\n",
      "Iteration 33, loss = 0.16835098\n",
      "Iteration 34, loss = 0.16299490\n",
      "Iteration 35, loss = 0.16677030\n",
      "Iteration 36, loss = 0.16792320\n",
      "Iteration 37, loss = 0.16561027\n",
      "Iteration 38, loss = 0.16306287\n",
      "Iteration 39, loss = 0.16418231\n",
      "Iteration 40, loss = 0.16249489\n",
      "Iteration 41, loss = 0.16317431\n",
      "Iteration 42, loss = 0.16321567\n",
      "Iteration 43, loss = 0.16490527\n",
      "Iteration 44, loss = 0.16367360\n",
      "Iteration 45, loss = 0.16094114\n",
      "Iteration 46, loss = 0.16270164\n",
      "Iteration 47, loss = 0.16244894\n",
      "Iteration 48, loss = 0.16046770\n",
      "Iteration 49, loss = 0.16021840\n",
      "Iteration 50, loss = 0.16404080\n",
      "Iteration 51, loss = 0.16068653\n",
      "Iteration 52, loss = 0.16076892\n",
      "Iteration 53, loss = 0.16110058\n",
      "Iteration 54, loss = 0.16100371\n",
      "Iteration 55, loss = 0.15968257\n",
      "Iteration 56, loss = 0.16146653\n",
      "Iteration 57, loss = 0.16016079\n",
      "Iteration 58, loss = 0.15916133\n",
      "Iteration 59, loss = 0.15996656\n",
      "Iteration 60, loss = 0.15958270\n",
      "Iteration 61, loss = 0.15784337\n",
      "Iteration 62, loss = 0.15815202\n",
      "Iteration 63, loss = 0.16049680\n",
      "Iteration 64, loss = 0.16002875\n",
      "Iteration 65, loss = 0.15812828\n",
      "Iteration 66, loss = 0.15712344\n",
      "Iteration 67, loss = 0.15830202\n",
      "Iteration 68, loss = 0.15922677\n",
      "Iteration 69, loss = 0.15910224\n",
      "Iteration 70, loss = 0.15725746\n",
      "Iteration 71, loss = 0.15722427\n",
      "Iteration 72, loss = 0.15659156\n",
      "Iteration 73, loss = 0.15779735\n",
      "Iteration 74, loss = 0.15832251\n",
      "Iteration 75, loss = 0.15648799\n",
      "Iteration 76, loss = 0.15750270\n",
      "Iteration 77, loss = 0.15642039\n",
      "Iteration 78, loss = 0.15691395\n",
      "Iteration 79, loss = 0.15808681\n",
      "Iteration 80, loss = 0.15762912\n",
      "Iteration 81, loss = 0.15584128\n",
      "Iteration 82, loss = 0.15700092\n",
      "Iteration 83, loss = 0.15377442\n",
      "Iteration 84, loss = 0.16110111\n",
      "Iteration 85, loss = 0.15506960\n",
      "Iteration 86, loss = 0.15513073\n",
      "Iteration 87, loss = 0.15487448\n",
      "Iteration 88, loss = 0.15560447\n",
      "Iteration 89, loss = 0.15494690\n",
      "Iteration 90, loss = 0.15669569\n",
      "Iteration 91, loss = 0.15525740\n",
      "Iteration 92, loss = 0.15581396\n",
      "Iteration 93, loss = 0.15527751\n",
      "Iteration 94, loss = 0.15627073\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.01, batch_size=50, hidden_layer_sizes=(100, 100, 100), solver=adam; total time= 3.9min\n",
      "Iteration 1, loss = 1.10431908\n",
      "Iteration 2, loss = 0.41120381\n",
      "Iteration 3, loss = 0.33055934\n",
      "Iteration 4, loss = 0.28827673\n",
      "Iteration 5, loss = 0.26302844\n",
      "Iteration 6, loss = 0.24253568\n",
      "Iteration 7, loss = 0.22914940\n",
      "Iteration 8, loss = 0.21996212\n",
      "Iteration 9, loss = 0.21138957\n",
      "Iteration 10, loss = 0.20601785\n",
      "Iteration 11, loss = 0.19776339\n",
      "Iteration 12, loss = 0.19438911\n",
      "Iteration 13, loss = 0.19069789\n",
      "Iteration 14, loss = 0.18999375\n",
      "Iteration 15, loss = 0.18496253\n",
      "Iteration 16, loss = 0.18516408\n",
      "Iteration 17, loss = 0.18038225\n",
      "Iteration 18, loss = 0.17973653\n",
      "Iteration 19, loss = 0.17802398\n",
      "Iteration 20, loss = 0.17633078\n",
      "Iteration 21, loss = 0.17452372\n",
      "Iteration 22, loss = 0.17343398\n",
      "Iteration 23, loss = 0.17191053\n",
      "Iteration 24, loss = 0.16976921\n",
      "Iteration 25, loss = 0.17082636\n",
      "Iteration 26, loss = 0.17047889\n",
      "Iteration 27, loss = 0.16871781\n",
      "Iteration 28, loss = 0.17042803\n",
      "Iteration 29, loss = 0.16945568\n",
      "Iteration 30, loss = 0.16841104\n",
      "Iteration 31, loss = 0.16539488\n",
      "Iteration 32, loss = 0.16546366\n",
      "Iteration 33, loss = 0.16591197\n",
      "Iteration 34, loss = 0.16619524\n",
      "Iteration 35, loss = 0.16567644\n",
      "Iteration 36, loss = 0.16508571\n",
      "Iteration 37, loss = 0.16591111\n",
      "Iteration 38, loss = 0.16395402\n",
      "Iteration 39, loss = 0.16318925\n",
      "Iteration 40, loss = 0.16434797\n",
      "Iteration 41, loss = 0.16424724\n",
      "Iteration 42, loss = 0.16101329\n",
      "Iteration 43, loss = 0.16249336\n",
      "Iteration 44, loss = 0.16124989\n",
      "Iteration 45, loss = 0.16110108\n",
      "Iteration 46, loss = 0.16273558\n",
      "Iteration 47, loss = 0.16325709\n",
      "Iteration 48, loss = 0.16151950\n",
      "Iteration 49, loss = 0.15937250\n",
      "Iteration 50, loss = 0.15826311\n",
      "Iteration 51, loss = 0.16043790\n",
      "Iteration 52, loss = 0.16183701\n",
      "Iteration 53, loss = 0.16034878\n",
      "Iteration 54, loss = 0.15932911\n",
      "Iteration 55, loss = 0.15879127\n",
      "Iteration 56, loss = 0.15845249\n",
      "Iteration 57, loss = 0.16028704\n",
      "Iteration 58, loss = 0.15728707\n",
      "Iteration 59, loss = 0.16131603\n",
      "Iteration 60, loss = 0.15798831\n",
      "Iteration 61, loss = 0.15839918\n",
      "Iteration 62, loss = 0.15952661\n",
      "Iteration 63, loss = 0.15843942\n",
      "Iteration 64, loss = 0.15861342\n",
      "Iteration 65, loss = 0.15849286\n",
      "Iteration 66, loss = 0.15766151\n",
      "Iteration 67, loss = 0.15688695\n",
      "Iteration 68, loss = 0.15803911\n",
      "Iteration 69, loss = 0.15864599\n",
      "Iteration 70, loss = 0.15753121\n",
      "Iteration 71, loss = 0.15676540\n",
      "Iteration 72, loss = 0.15687936\n",
      "Iteration 73, loss = 0.15737992\n",
      "Iteration 74, loss = 0.15785491\n",
      "Iteration 75, loss = 0.15594573\n",
      "Iteration 76, loss = 0.15753991\n",
      "Iteration 77, loss = 0.15637719\n",
      "Iteration 78, loss = 0.15850722\n",
      "Iteration 79, loss = 0.15568133\n",
      "Iteration 80, loss = 0.15961837\n",
      "Iteration 81, loss = 0.15439461\n",
      "Iteration 82, loss = 0.15630069\n",
      "Iteration 83, loss = 0.15635138\n",
      "Iteration 84, loss = 0.15606033\n",
      "Iteration 85, loss = 0.15577652\n",
      "Iteration 86, loss = 0.15829351\n",
      "Iteration 87, loss = 0.15383872\n",
      "Iteration 88, loss = 0.15785563\n",
      "Iteration 89, loss = 0.15558062\n",
      "Iteration 90, loss = 0.15529444\n",
      "Iteration 91, loss = 0.15481553\n",
      "Iteration 92, loss = 0.15603931\n",
      "Iteration 93, loss = 0.15431216\n",
      "Iteration 94, loss = 0.15647388\n",
      "Iteration 95, loss = 0.15350081\n",
      "Iteration 96, loss = 0.15753268\n",
      "Iteration 97, loss = 0.15438260\n",
      "Iteration 98, loss = 0.15538292\n",
      "Iteration 99, loss = 0.15330595\n",
      "Iteration 100, loss = 0.15368909\n",
      "Iteration 101, loss = 0.15643275\n",
      "Iteration 102, loss = 0.15395638\n",
      "Iteration 103, loss = 0.15511408\n",
      "Iteration 104, loss = 0.15397152\n",
      "Iteration 105, loss = 0.15686544\n",
      "Iteration 106, loss = 0.15465333\n",
      "Iteration 107, loss = 0.15470129\n",
      "Iteration 108, loss = 0.15630923\n",
      "Iteration 109, loss = 0.15481624\n",
      "Iteration 110, loss = 0.15581438\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.01, batch_size=50, hidden_layer_sizes=(100, 100, 100), solver=adam; total time= 4.5min\n",
      "Iteration 1, loss = 1.11623680\n",
      "Iteration 2, loss = 0.43735112\n",
      "Iteration 3, loss = 0.34112883\n",
      "Iteration 4, loss = 0.29293696\n",
      "Iteration 5, loss = 0.26434695\n",
      "Iteration 6, loss = 0.24580763\n",
      "Iteration 7, loss = 0.23219260\n",
      "Iteration 8, loss = 0.22028235\n",
      "Iteration 9, loss = 0.21202385\n",
      "Iteration 10, loss = 0.20748914\n",
      "Iteration 11, loss = 0.19979885\n",
      "Iteration 12, loss = 0.19609745\n",
      "Iteration 13, loss = 0.19184318\n",
      "Iteration 14, loss = 0.18832548\n",
      "Iteration 15, loss = 0.18606248\n",
      "Iteration 16, loss = 0.18303959\n",
      "Iteration 17, loss = 0.18250324\n",
      "Iteration 18, loss = 0.18038377\n",
      "Iteration 19, loss = 0.17838249\n",
      "Iteration 20, loss = 0.17337701\n",
      "Iteration 21, loss = 0.17515168\n",
      "Iteration 22, loss = 0.17318456\n",
      "Iteration 23, loss = 0.17299428\n",
      "Iteration 24, loss = 0.17227920\n",
      "Iteration 25, loss = 0.17180831\n",
      "Iteration 26, loss = 0.16972433\n",
      "Iteration 27, loss = 0.16775675\n",
      "Iteration 28, loss = 0.17112944\n",
      "Iteration 29, loss = 0.16562418\n",
      "Iteration 30, loss = 0.16748456\n",
      "Iteration 31, loss = 0.16536087\n",
      "Iteration 32, loss = 0.16575180\n",
      "Iteration 33, loss = 0.16582380\n",
      "Iteration 34, loss = 0.16734318\n",
      "Iteration 35, loss = 0.16528864\n",
      "Iteration 36, loss = 0.16325640\n",
      "Iteration 37, loss = 0.16298647\n",
      "Iteration 38, loss = 0.16375707\n",
      "Iteration 39, loss = 0.16353400\n",
      "Iteration 40, loss = 0.16163685\n",
      "Iteration 41, loss = 0.16180222\n",
      "Iteration 42, loss = 0.16104802\n",
      "Iteration 43, loss = 0.16328339\n",
      "Iteration 44, loss = 0.15842771\n",
      "Iteration 45, loss = 0.16369126\n",
      "Iteration 46, loss = 0.16071757\n",
      "Iteration 47, loss = 0.16017001\n",
      "Iteration 48, loss = 0.15941238\n",
      "Iteration 49, loss = 0.15816977\n",
      "Iteration 50, loss = 0.15866712\n",
      "Iteration 51, loss = 0.15959778\n",
      "Iteration 52, loss = 0.15983765\n",
      "Iteration 53, loss = 0.15798009\n",
      "Iteration 54, loss = 0.15791994\n",
      "Iteration 55, loss = 0.15673280\n",
      "Iteration 56, loss = 0.15830505\n",
      "Iteration 57, loss = 0.15603803\n",
      "Iteration 58, loss = 0.15756155\n",
      "Iteration 59, loss = 0.15772334\n",
      "Iteration 60, loss = 0.15707672\n",
      "Iteration 61, loss = 0.15742967\n",
      "Iteration 62, loss = 0.15710615\n",
      "Iteration 63, loss = 0.15507048\n",
      "Iteration 64, loss = 0.15438273\n",
      "Iteration 65, loss = 0.15712193\n",
      "Iteration 66, loss = 0.15654977\n",
      "Iteration 67, loss = 0.15696159\n",
      "Iteration 68, loss = 0.15656359\n",
      "Iteration 69, loss = 0.15602229\n",
      "Iteration 70, loss = 0.15592713\n",
      "Iteration 71, loss = 0.15468901\n",
      "Iteration 72, loss = 0.15551205\n",
      "Iteration 73, loss = 0.15498703\n",
      "Iteration 74, loss = 0.15733183\n",
      "Iteration 75, loss = 0.15576587\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.01, batch_size=50, hidden_layer_sizes=(100, 100, 100), solver=adam; total time= 3.1min\n",
      "Iteration 1, loss = 1.14396653\n",
      "Iteration 2, loss = 0.47376013\n",
      "Iteration 3, loss = 0.35603892\n",
      "Iteration 4, loss = 0.30292118\n",
      "Iteration 5, loss = 0.27058342\n",
      "Iteration 6, loss = 0.24692866\n",
      "Iteration 7, loss = 0.23239511\n",
      "Iteration 8, loss = 0.21931313\n",
      "Iteration 9, loss = 0.21287234\n",
      "Iteration 10, loss = 0.20416079\n",
      "Iteration 11, loss = 0.19826753\n",
      "Iteration 12, loss = 0.19484842\n",
      "Iteration 13, loss = 0.18945945\n",
      "Iteration 14, loss = 0.18729235\n",
      "Iteration 15, loss = 0.18463211\n",
      "Iteration 16, loss = 0.18282812\n",
      "Iteration 17, loss = 0.17938001\n",
      "Iteration 18, loss = 0.17779576\n",
      "Iteration 19, loss = 0.17875092\n",
      "Iteration 20, loss = 0.17373335\n",
      "Iteration 21, loss = 0.17484952\n",
      "Iteration 22, loss = 0.17256533\n",
      "Iteration 23, loss = 0.17234314\n",
      "Iteration 24, loss = 0.17076365\n",
      "Iteration 25, loss = 0.16988629\n",
      "Iteration 26, loss = 0.16995085\n",
      "Iteration 27, loss = 0.16870159\n",
      "Iteration 28, loss = 0.16746262\n",
      "Iteration 29, loss = 0.16860587\n",
      "Iteration 30, loss = 0.16649029\n",
      "Iteration 31, loss = 0.16767866\n",
      "Iteration 32, loss = 0.16689448\n",
      "Iteration 33, loss = 0.16372574\n",
      "Iteration 34, loss = 0.16603692\n",
      "Iteration 35, loss = 0.16364029\n",
      "Iteration 36, loss = 0.16359102\n",
      "Iteration 37, loss = 0.16232076\n",
      "Iteration 38, loss = 0.16414055\n",
      "Iteration 39, loss = 0.16390638\n",
      "Iteration 40, loss = 0.16093691\n",
      "Iteration 41, loss = 0.16212451\n",
      "Iteration 42, loss = 0.16162585\n",
      "Iteration 43, loss = 0.16201095\n",
      "Iteration 44, loss = 0.16365442\n",
      "Iteration 45, loss = 0.16169674\n",
      "Iteration 46, loss = 0.16142977\n",
      "Iteration 47, loss = 0.16213481\n",
      "Iteration 48, loss = 0.16302757\n",
      "Iteration 49, loss = 0.15854251\n",
      "Iteration 50, loss = 0.16009313\n",
      "Iteration 51, loss = 0.16126271\n",
      "Iteration 52, loss = 0.15927395\n",
      "Iteration 53, loss = 0.16058152\n",
      "Iteration 54, loss = 0.15870099\n",
      "Iteration 55, loss = 0.16022644\n",
      "Iteration 56, loss = 0.15770244\n",
      "Iteration 57, loss = 0.15687526\n",
      "Iteration 58, loss = 0.16038005\n",
      "Iteration 59, loss = 0.16104826\n",
      "Iteration 60, loss = 0.15679185\n",
      "Iteration 61, loss = 0.15724192\n",
      "Iteration 62, loss = 0.15896784\n",
      "Iteration 63, loss = 0.15703750\n",
      "Iteration 64, loss = 0.15667927\n",
      "Iteration 65, loss = 0.15809107\n",
      "Iteration 66, loss = 0.15751024\n",
      "Iteration 67, loss = 0.15578836\n",
      "Iteration 68, loss = 0.15708787\n",
      "Iteration 69, loss = 0.15805460\n",
      "Iteration 70, loss = 0.15682994\n",
      "Iteration 71, loss = 0.15500548\n",
      "Iteration 72, loss = 0.15609706\n",
      "Iteration 73, loss = 0.15858484\n",
      "Iteration 74, loss = 0.15658096\n",
      "Iteration 75, loss = 0.15793721\n",
      "Iteration 76, loss = 0.15469088\n",
      "Iteration 77, loss = 0.15783257\n",
      "Iteration 78, loss = 0.15618011\n",
      "Iteration 79, loss = 0.15542186\n",
      "Iteration 80, loss = 0.15733759\n",
      "Iteration 81, loss = 0.15636251\n",
      "Iteration 82, loss = 0.15424989\n",
      "Iteration 83, loss = 0.15567704\n",
      "Iteration 84, loss = 0.15582711\n",
      "Iteration 85, loss = 0.15561892\n",
      "Iteration 86, loss = 0.15311897\n",
      "Iteration 87, loss = 0.15401302\n",
      "Iteration 88, loss = 0.15552404\n",
      "Iteration 89, loss = 0.15532484\n",
      "Iteration 90, loss = 0.15595022\n",
      "Iteration 91, loss = 0.15336392\n",
      "Iteration 92, loss = 0.15521960\n",
      "Iteration 93, loss = 0.15330751\n",
      "Iteration 94, loss = 0.15451259\n",
      "Iteration 95, loss = 0.15486618\n",
      "Iteration 96, loss = 0.15255627\n",
      "Iteration 97, loss = 0.15341719\n",
      "Iteration 98, loss = 0.15538630\n",
      "Iteration 99, loss = 0.15282881\n",
      "Iteration 100, loss = 0.15396682\n",
      "Iteration 101, loss = 0.15266631\n",
      "Iteration 102, loss = 0.15525653\n",
      "Iteration 103, loss = 0.15319878\n",
      "Iteration 104, loss = 0.15178206\n",
      "Iteration 105, loss = 0.15425093\n",
      "Iteration 106, loss = 0.15406312\n",
      "Iteration 107, loss = 0.15233177\n",
      "Iteration 108, loss = 0.15229036\n",
      "Iteration 109, loss = 0.15441809\n",
      "Iteration 110, loss = 0.15239845\n",
      "Iteration 111, loss = 0.15329371\n",
      "Iteration 112, loss = 0.15363207\n",
      "Iteration 113, loss = 0.15041380\n",
      "Iteration 114, loss = 0.15484035\n",
      "Iteration 115, loss = 0.15255812\n",
      "Iteration 116, loss = 0.15322236\n",
      "Iteration 117, loss = 0.15285825\n",
      "Iteration 118, loss = 0.15350791\n",
      "Iteration 119, loss = 0.15305570\n",
      "Iteration 120, loss = 0.15191270\n",
      "Iteration 121, loss = 0.15164152\n",
      "Iteration 122, loss = 0.15108284\n",
      "Iteration 123, loss = 0.15593089\n",
      "Iteration 124, loss = 0.15177307\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.01, batch_size=50, hidden_layer_sizes=(100, 100, 100), solver=adam; total time= 5.2min\n",
      "Iteration 1, loss = 1.13222620\n",
      "Iteration 2, loss = 0.42024731\n",
      "Iteration 3, loss = 0.32916123\n",
      "Iteration 4, loss = 0.28598204\n",
      "Iteration 5, loss = 0.26001273\n",
      "Iteration 6, loss = 0.24043609\n",
      "Iteration 7, loss = 0.22828512\n",
      "Iteration 8, loss = 0.21936873\n",
      "Iteration 9, loss = 0.21043550\n",
      "Iteration 10, loss = 0.20190397\n",
      "Iteration 11, loss = 0.19835395\n",
      "Iteration 12, loss = 0.19408685\n",
      "Iteration 13, loss = 0.19053235\n",
      "Iteration 14, loss = 0.18885640\n",
      "Iteration 15, loss = 0.18588230\n",
      "Iteration 16, loss = 0.18258776\n",
      "Iteration 17, loss = 0.18190556\n",
      "Iteration 18, loss = 0.17905814\n",
      "Iteration 19, loss = 0.17694009\n",
      "Iteration 20, loss = 0.17459131\n",
      "Iteration 21, loss = 0.17491688\n",
      "Iteration 22, loss = 0.17328943\n",
      "Iteration 23, loss = 0.17245018\n",
      "Iteration 24, loss = 0.17023856\n",
      "Iteration 25, loss = 0.16964885\n",
      "Iteration 26, loss = 0.17005620\n",
      "Iteration 27, loss = 0.16721149\n",
      "Iteration 28, loss = 0.16832799\n",
      "Iteration 29, loss = 0.16770608\n",
      "Iteration 30, loss = 0.16723590\n",
      "Iteration 31, loss = 0.16776708\n",
      "Iteration 32, loss = 0.16496379\n",
      "Iteration 33, loss = 0.16643100\n",
      "Iteration 34, loss = 0.16697514\n",
      "Iteration 35, loss = 0.16463214\n",
      "Iteration 36, loss = 0.16553874\n",
      "Iteration 37, loss = 0.16256144\n",
      "Iteration 38, loss = 0.16541753\n",
      "Iteration 39, loss = 0.16532596\n",
      "Iteration 40, loss = 0.16447819\n",
      "Iteration 41, loss = 0.16305521\n",
      "Iteration 42, loss = 0.16335140\n",
      "Iteration 43, loss = 0.16208699\n",
      "Iteration 44, loss = 0.16061874\n",
      "Iteration 45, loss = 0.16374817\n",
      "Iteration 46, loss = 0.16238099\n",
      "Iteration 47, loss = 0.16005584\n",
      "Iteration 48, loss = 0.16068185\n",
      "Iteration 49, loss = 0.16264971\n",
      "Iteration 50, loss = 0.15862965\n",
      "Iteration 51, loss = 0.16133385\n",
      "Iteration 52, loss = 0.16210909\n",
      "Iteration 53, loss = 0.15908919\n",
      "Iteration 54, loss = 0.16308605\n",
      "Iteration 55, loss = 0.16107146\n",
      "Iteration 56, loss = 0.16009198\n",
      "Iteration 57, loss = 0.16048331\n",
      "Iteration 58, loss = 0.16169013\n",
      "Iteration 59, loss = 0.15975949\n",
      "Iteration 60, loss = 0.16030304\n",
      "Iteration 61, loss = 0.15952264\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.01, batch_size=50, hidden_layer_sizes=(100, 100, 100), solver=adam; total time= 2.5min\n",
      "Iteration 1, loss = 2.31767264\n",
      "Iteration 2, loss = 2.31530792\n",
      "Iteration 3, loss = 2.31505991\n",
      "Iteration 4, loss = 2.31435566\n",
      "Iteration 5, loss = 2.31445680\n",
      "Iteration 6, loss = 2.31411436\n",
      "Iteration 7, loss = 2.31397948\n",
      "Iteration 8, loss = 2.31352104\n",
      "Iteration 9, loss = 2.31344935\n",
      "Iteration 10, loss = 2.31301904\n",
      "Iteration 11, loss = 2.31269758\n",
      "Iteration 12, loss = 2.31240569\n",
      "Iteration 13, loss = 2.31192015\n",
      "Iteration 14, loss = 2.31153578\n",
      "Iteration 15, loss = 2.31095491\n",
      "Iteration 16, loss = 2.31051628\n",
      "Iteration 17, loss = 2.31022553\n",
      "Iteration 18, loss = 2.30932309\n",
      "Iteration 19, loss = 2.30869121\n",
      "Iteration 20, loss = 2.30779106\n",
      "Iteration 21, loss = 2.30679618\n",
      "Iteration 22, loss = 2.30580602\n",
      "Iteration 23, loss = 2.30444307\n",
      "Iteration 24, loss = 2.30242201\n",
      "Iteration 25, loss = 2.30042451\n",
      "Iteration 26, loss = 2.29729168\n",
      "Iteration 27, loss = 2.29378323\n",
      "Iteration 28, loss = 2.28873644\n",
      "Iteration 29, loss = 2.28162356\n",
      "Iteration 30, loss = 2.27160915\n",
      "Iteration 31, loss = 2.25523779\n",
      "Iteration 32, loss = 2.22905592\n",
      "Iteration 33, loss = 2.18346931\n",
      "Iteration 34, loss = 2.10656901\n",
      "Iteration 35, loss = 1.99646869\n",
      "Iteration 36, loss = 1.87581963\n",
      "Iteration 37, loss = 1.76258575\n",
      "Iteration 38, loss = 1.65621451\n",
      "Iteration 39, loss = 1.55209064\n",
      "Iteration 40, loss = 1.45498383\n",
      "Iteration 41, loss = 1.37046033\n",
      "Iteration 42, loss = 1.29858373\n",
      "Iteration 43, loss = 1.23623956\n",
      "Iteration 44, loss = 1.17998249\n",
      "Iteration 45, loss = 1.12719998\n",
      "Iteration 46, loss = 1.07564381\n",
      "Iteration 47, loss = 1.02524648\n",
      "Iteration 48, loss = 0.97686323\n",
      "Iteration 49, loss = 0.93270419\n",
      "Iteration 50, loss = 0.89427700\n",
      "Iteration 51, loss = 0.86159109\n",
      "Iteration 52, loss = 0.83413301\n",
      "Iteration 53, loss = 0.81048676\n",
      "Iteration 54, loss = 0.79017982\n",
      "Iteration 55, loss = 0.77196019\n",
      "Iteration 56, loss = 0.75543296\n",
      "Iteration 57, loss = 0.74007351\n",
      "Iteration 58, loss = 0.72566445\n",
      "Iteration 59, loss = 0.71219030\n",
      "Iteration 60, loss = 0.69948519\n",
      "Iteration 61, loss = 0.68734322\n",
      "Iteration 62, loss = 0.67588161\n",
      "Iteration 63, loss = 0.66495728\n",
      "Iteration 64, loss = 0.65469253\n",
      "Iteration 65, loss = 0.64458053\n",
      "Iteration 66, loss = 0.63525418\n",
      "Iteration 67, loss = 0.62617518\n",
      "Iteration 68, loss = 0.61787454\n",
      "Iteration 69, loss = 0.60940247\n",
      "Iteration 70, loss = 0.60161530\n",
      "Iteration 71, loss = 0.59402527\n",
      "Iteration 72, loss = 0.58710860\n",
      "Iteration 73, loss = 0.58027856\n",
      "Iteration 74, loss = 0.57356837\n",
      "Iteration 75, loss = 0.56762836\n",
      "Iteration 76, loss = 0.56161194\n",
      "Iteration 77, loss = 0.55571746\n",
      "Iteration 78, loss = 0.55037130\n",
      "Iteration 79, loss = 0.54504296\n",
      "Iteration 80, loss = 0.53989220\n",
      "Iteration 81, loss = 0.53493361\n",
      "Iteration 82, loss = 0.53017614\n",
      "Iteration 83, loss = 0.52558756\n",
      "Iteration 84, loss = 0.52103771\n",
      "Iteration 85, loss = 0.51684135\n",
      "Iteration 86, loss = 0.51238250\n",
      "Iteration 87, loss = 0.50821704\n",
      "Iteration 88, loss = 0.50408706\n",
      "Iteration 89, loss = 0.50018168\n",
      "Iteration 90, loss = 0.49619907\n",
      "Iteration 91, loss = 0.49246640\n",
      "Iteration 92, loss = 0.48865272\n",
      "Iteration 93, loss = 0.48504169\n",
      "Iteration 94, loss = 0.48134876\n",
      "Iteration 95, loss = 0.47759394\n",
      "Iteration 96, loss = 0.47412983\n",
      "Iteration 97, loss = 0.47088973\n",
      "Iteration 98, loss = 0.46721888\n",
      "Iteration 99, loss = 0.46382841\n",
      "Iteration 100, loss = 0.46044816\n",
      "Iteration 101, loss = 0.45706601\n",
      "Iteration 102, loss = 0.45381875\n",
      "Iteration 103, loss = 0.45061957\n",
      "Iteration 104, loss = 0.44785018\n",
      "Iteration 105, loss = 0.44426848\n",
      "Iteration 106, loss = 0.44096261\n",
      "Iteration 107, loss = 0.43801429\n",
      "Iteration 108, loss = 0.43465623\n",
      "Iteration 109, loss = 0.43186766\n",
      "Iteration 110, loss = 0.42866739\n",
      "Iteration 111, loss = 0.42561251\n",
      "Iteration 112, loss = 0.42244137\n",
      "Iteration 113, loss = 0.41968930\n",
      "Iteration 114, loss = 0.41678406\n",
      "Iteration 115, loss = 0.41381659\n",
      "Iteration 116, loss = 0.41089612\n",
      "Iteration 117, loss = 0.40770802\n",
      "Iteration 118, loss = 0.40482275\n",
      "Iteration 119, loss = 0.40211819\n",
      "Iteration 120, loss = 0.39910597\n",
      "Iteration 121, loss = 0.39622189\n",
      "Iteration 122, loss = 0.39339487\n",
      "Iteration 123, loss = 0.39047841\n",
      "Iteration 124, loss = 0.38782692\n",
      "Iteration 125, loss = 0.38514189\n",
      "Iteration 126, loss = 0.38213273\n",
      "Iteration 127, loss = 0.37957046\n",
      "Iteration 128, loss = 0.37656052\n",
      "Iteration 129, loss = 0.37402205\n",
      "Iteration 130, loss = 0.37127294\n",
      "Iteration 131, loss = 0.36882690\n",
      "Iteration 132, loss = 0.36608322\n",
      "Iteration 133, loss = 0.36348482\n",
      "Iteration 134, loss = 0.36074593\n",
      "Iteration 135, loss = 0.35814647\n",
      "Iteration 136, loss = 0.35542352\n",
      "Iteration 137, loss = 0.35301299\n",
      "Iteration 138, loss = 0.35036974\n",
      "Iteration 139, loss = 0.34807361\n",
      "Iteration 140, loss = 0.34545459\n",
      "Iteration 141, loss = 0.34310857\n",
      "Iteration 142, loss = 0.34065721\n",
      "Iteration 143, loss = 0.33810000\n",
      "Iteration 144, loss = 0.33579130\n",
      "Iteration 145, loss = 0.33350596\n",
      "Iteration 146, loss = 0.33094657\n",
      "Iteration 147, loss = 0.32870876\n",
      "Iteration 148, loss = 0.32659171\n",
      "Iteration 149, loss = 0.32436265\n",
      "Iteration 150, loss = 0.32222480\n",
      "Iteration 151, loss = 0.32003295\n",
      "Iteration 152, loss = 0.31783349\n",
      "Iteration 153, loss = 0.31581366\n",
      "Iteration 154, loss = 0.31375931\n",
      "Iteration 155, loss = 0.31171288\n",
      "Iteration 156, loss = 0.30966316\n",
      "Iteration 157, loss = 0.30771598\n",
      "Iteration 158, loss = 0.30584910\n",
      "Iteration 159, loss = 0.30376828\n",
      "Iteration 160, loss = 0.30184509\n",
      "Iteration 161, loss = 0.30004340\n",
      "Iteration 162, loss = 0.29820957\n",
      "Iteration 163, loss = 0.29670530\n",
      "Iteration 164, loss = 0.29474775\n",
      "Iteration 165, loss = 0.29311755\n",
      "Iteration 166, loss = 0.29143943\n",
      "Iteration 167, loss = 0.28967201\n",
      "Iteration 168, loss = 0.28813632\n",
      "Iteration 169, loss = 0.28657702\n",
      "Iteration 170, loss = 0.28502430\n",
      "Iteration 171, loss = 0.28343795\n",
      "Iteration 172, loss = 0.28200900\n",
      "Iteration 173, loss = 0.28042922\n",
      "Iteration 174, loss = 0.27897789\n",
      "Iteration 175, loss = 0.27750741\n",
      "Iteration 176, loss = 0.27616661\n",
      "Iteration 177, loss = 0.27498061\n",
      "Iteration 178, loss = 0.27344282\n",
      "Iteration 179, loss = 0.27232658\n",
      "Iteration 180, loss = 0.27080652\n",
      "Iteration 181, loss = 0.26950024\n",
      "Iteration 182, loss = 0.26833310\n",
      "Iteration 183, loss = 0.26709300\n",
      "Iteration 184, loss = 0.26593599\n",
      "Iteration 185, loss = 0.26483259\n",
      "Iteration 186, loss = 0.26352967\n",
      "Iteration 187, loss = 0.26270838\n",
      "Iteration 188, loss = 0.26145916\n",
      "Iteration 189, loss = 0.26032770\n",
      "Iteration 190, loss = 0.25920725\n",
      "Iteration 191, loss = 0.25810510\n",
      "Iteration 192, loss = 0.25707950\n",
      "Iteration 193, loss = 0.25607223\n",
      "Iteration 194, loss = 0.25501290\n",
      "Iteration 195, loss = 0.25402790\n",
      "Iteration 196, loss = 0.25296030\n",
      "Iteration 197, loss = 0.25209131\n",
      "Iteration 198, loss = 0.25120166\n",
      "Iteration 199, loss = 0.25022509\n",
      "Iteration 200, loss = 0.24938014\n",
      "[CV] END activation=logistic, alpha=0.01, batch_size=50, hidden_layer_sizes=(100, 100, 100), solver=sgd; total time= 5.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.31819866\n",
      "Iteration 2, loss = 2.31555550\n",
      "Iteration 3, loss = 2.31534260\n",
      "Iteration 4, loss = 2.31539945\n",
      "Iteration 5, loss = 2.31497140\n",
      "Iteration 6, loss = 2.31465505\n",
      "Iteration 7, loss = 2.31438919\n",
      "Iteration 8, loss = 2.31434635\n",
      "Iteration 9, loss = 2.31385223\n",
      "Iteration 10, loss = 2.31396532\n",
      "Iteration 11, loss = 2.31357204\n",
      "Iteration 12, loss = 2.31349563\n",
      "Iteration 13, loss = 2.31322372\n",
      "Iteration 14, loss = 2.31292565\n",
      "Iteration 15, loss = 2.31252369\n",
      "Iteration 16, loss = 2.31220630\n",
      "Iteration 17, loss = 2.31203960\n",
      "Iteration 18, loss = 2.31178315\n",
      "Iteration 19, loss = 2.31106451\n",
      "Iteration 20, loss = 2.31064718\n",
      "Iteration 21, loss = 2.31012050\n",
      "Iteration 22, loss = 2.30951117\n",
      "Iteration 23, loss = 2.30844245\n",
      "Iteration 24, loss = 2.30771905\n",
      "Iteration 25, loss = 2.30683889\n",
      "Iteration 26, loss = 2.30534187\n",
      "Iteration 27, loss = 2.30401803\n",
      "Iteration 28, loss = 2.30207112\n",
      "Iteration 29, loss = 2.29944089\n",
      "Iteration 30, loss = 2.29619514\n",
      "Iteration 31, loss = 2.29213842\n",
      "Iteration 32, loss = 2.28598575\n",
      "Iteration 33, loss = 2.27703865\n",
      "Iteration 34, loss = 2.26324724\n",
      "Iteration 35, loss = 2.24054022\n",
      "Iteration 36, loss = 2.20110931\n",
      "Iteration 37, loss = 2.13162463\n",
      "Iteration 38, loss = 2.02743632\n",
      "Iteration 39, loss = 1.91314543\n",
      "Iteration 40, loss = 1.81730404\n",
      "Iteration 41, loss = 1.73654086\n",
      "Iteration 42, loss = 1.65693077\n",
      "Iteration 43, loss = 1.57316415\n",
      "Iteration 44, loss = 1.49066452\n",
      "Iteration 45, loss = 1.41756008\n",
      "Iteration 46, loss = 1.35489543\n",
      "Iteration 47, loss = 1.29994216\n",
      "Iteration 48, loss = 1.24852546\n",
      "Iteration 49, loss = 1.19854626\n",
      "Iteration 50, loss = 1.14867949\n",
      "Iteration 51, loss = 1.09942491\n",
      "Iteration 52, loss = 1.05111181\n",
      "Iteration 53, loss = 1.00564860\n",
      "Iteration 54, loss = 0.96459133\n",
      "Iteration 55, loss = 0.92825800\n",
      "Iteration 56, loss = 0.89681294\n",
      "Iteration 57, loss = 0.87051729\n",
      "Iteration 58, loss = 0.84784721\n",
      "Iteration 59, loss = 0.82827139\n",
      "Iteration 60, loss = 0.81094218\n",
      "Iteration 61, loss = 0.79516787\n",
      "Iteration 62, loss = 0.78072349\n",
      "Iteration 63, loss = 0.76707195\n",
      "Iteration 64, loss = 0.75361914\n",
      "Iteration 65, loss = 0.74101246\n",
      "Iteration 66, loss = 0.72871284\n",
      "Iteration 67, loss = 0.71693595\n",
      "Iteration 68, loss = 0.70521022\n",
      "Iteration 69, loss = 0.69396652\n",
      "Iteration 70, loss = 0.68305213\n",
      "Iteration 71, loss = 0.67238992\n",
      "Iteration 72, loss = 0.66188424\n",
      "Iteration 73, loss = 0.65217828\n",
      "Iteration 74, loss = 0.64257486\n",
      "Iteration 75, loss = 0.63308766\n",
      "Iteration 76, loss = 0.62434045\n",
      "Iteration 77, loss = 0.61585271\n",
      "Iteration 78, loss = 0.60745869\n",
      "Iteration 79, loss = 0.59945979\n",
      "Iteration 80, loss = 0.59195505\n",
      "Iteration 81, loss = 0.58485867\n",
      "Iteration 82, loss = 0.57772955\n",
      "Iteration 83, loss = 0.57074380\n",
      "Iteration 84, loss = 0.56419208\n",
      "Iteration 85, loss = 0.55802491\n",
      "Iteration 86, loss = 0.55210238\n",
      "Iteration 87, loss = 0.54604529\n",
      "Iteration 88, loss = 0.54063034\n",
      "Iteration 89, loss = 0.53495742\n",
      "Iteration 90, loss = 0.52970806\n",
      "Iteration 91, loss = 0.52463850\n",
      "Iteration 92, loss = 0.51968842\n",
      "Iteration 93, loss = 0.51471091\n",
      "Iteration 94, loss = 0.51015379\n",
      "Iteration 95, loss = 0.50537606\n",
      "Iteration 96, loss = 0.50123630\n",
      "Iteration 97, loss = 0.49660719\n",
      "Iteration 98, loss = 0.49251019\n",
      "Iteration 99, loss = 0.48837176\n",
      "Iteration 100, loss = 0.48407787\n",
      "Iteration 101, loss = 0.48012915\n",
      "Iteration 102, loss = 0.47610841\n",
      "Iteration 103, loss = 0.47238717\n",
      "Iteration 104, loss = 0.46877333\n",
      "Iteration 105, loss = 0.46519728\n",
      "Iteration 106, loss = 0.46128274\n",
      "Iteration 107, loss = 0.45792559\n",
      "Iteration 108, loss = 0.45438037\n",
      "Iteration 109, loss = 0.45088682\n",
      "Iteration 110, loss = 0.44765193\n",
      "Iteration 111, loss = 0.44418685\n",
      "Iteration 112, loss = 0.44115732\n",
      "Iteration 113, loss = 0.43781754\n",
      "Iteration 114, loss = 0.43464652\n",
      "Iteration 115, loss = 0.43150638\n",
      "Iteration 116, loss = 0.42827523\n",
      "Iteration 117, loss = 0.42543585\n",
      "Iteration 118, loss = 0.42228196\n",
      "Iteration 119, loss = 0.41927050\n",
      "Iteration 120, loss = 0.41625725\n",
      "Iteration 121, loss = 0.41344350\n",
      "Iteration 122, loss = 0.41070378\n",
      "Iteration 123, loss = 0.40783487\n",
      "Iteration 124, loss = 0.40505068\n",
      "Iteration 125, loss = 0.40216359\n",
      "Iteration 126, loss = 0.39934501\n",
      "Iteration 127, loss = 0.39657663\n",
      "Iteration 128, loss = 0.39398141\n",
      "Iteration 129, loss = 0.39130342\n",
      "Iteration 130, loss = 0.38852897\n",
      "Iteration 131, loss = 0.38591560\n",
      "Iteration 132, loss = 0.38325011\n",
      "Iteration 133, loss = 0.38068087\n",
      "Iteration 134, loss = 0.37798236\n",
      "Iteration 135, loss = 0.37527283\n",
      "Iteration 136, loss = 0.37286610\n",
      "Iteration 137, loss = 0.37038168\n",
      "Iteration 138, loss = 0.36785224\n",
      "Iteration 139, loss = 0.36515028\n",
      "Iteration 140, loss = 0.36284370\n",
      "Iteration 141, loss = 0.36026388\n",
      "Iteration 142, loss = 0.35781704\n",
      "Iteration 143, loss = 0.35560039\n",
      "Iteration 144, loss = 0.35328904\n",
      "Iteration 145, loss = 0.35079771\n",
      "Iteration 146, loss = 0.34872784\n",
      "Iteration 147, loss = 0.34623116\n",
      "Iteration 148, loss = 0.34392700\n",
      "Iteration 149, loss = 0.34172387\n",
      "Iteration 150, loss = 0.33961039\n",
      "Iteration 151, loss = 0.33728184\n",
      "Iteration 152, loss = 0.33526818\n",
      "Iteration 153, loss = 0.33315360\n",
      "Iteration 154, loss = 0.33091732\n",
      "Iteration 155, loss = 0.32905069\n",
      "Iteration 156, loss = 0.32690475\n",
      "Iteration 157, loss = 0.32491455\n",
      "Iteration 158, loss = 0.32288271\n",
      "Iteration 159, loss = 0.32100061\n",
      "Iteration 160, loss = 0.31898053\n",
      "Iteration 161, loss = 0.31700785\n",
      "Iteration 162, loss = 0.31519093\n",
      "Iteration 163, loss = 0.31334999\n",
      "Iteration 164, loss = 0.31153135\n",
      "Iteration 165, loss = 0.30966416\n",
      "Iteration 166, loss = 0.30802680\n",
      "Iteration 167, loss = 0.30606925\n",
      "Iteration 168, loss = 0.30447565\n",
      "Iteration 169, loss = 0.30264606\n",
      "Iteration 170, loss = 0.30096780\n",
      "Iteration 171, loss = 0.29938115\n",
      "Iteration 172, loss = 0.29767818\n",
      "Iteration 173, loss = 0.29602367\n",
      "Iteration 174, loss = 0.29435423\n",
      "Iteration 175, loss = 0.29279034\n",
      "Iteration 176, loss = 0.29135519\n",
      "Iteration 177, loss = 0.28977759\n",
      "Iteration 178, loss = 0.28823840\n",
      "Iteration 179, loss = 0.28688975\n",
      "Iteration 180, loss = 0.28525923\n",
      "Iteration 181, loss = 0.28398323\n",
      "Iteration 182, loss = 0.28248399\n",
      "Iteration 183, loss = 0.28113985\n",
      "Iteration 184, loss = 0.27973861\n",
      "Iteration 185, loss = 0.27830004\n",
      "Iteration 186, loss = 0.27700136\n",
      "Iteration 187, loss = 0.27569579\n",
      "Iteration 188, loss = 0.27433817\n",
      "Iteration 189, loss = 0.27322345\n",
      "Iteration 190, loss = 0.27187326\n",
      "Iteration 191, loss = 0.27071535\n",
      "Iteration 192, loss = 0.26940737\n",
      "Iteration 193, loss = 0.26820121\n",
      "Iteration 194, loss = 0.26690554\n",
      "Iteration 195, loss = 0.26588573\n",
      "Iteration 196, loss = 0.26462619\n",
      "Iteration 197, loss = 0.26364862\n",
      "Iteration 198, loss = 0.26253590\n",
      "Iteration 199, loss = 0.26133414\n",
      "Iteration 200, loss = 0.26022938\n",
      "[CV] END activation=logistic, alpha=0.01, batch_size=50, hidden_layer_sizes=(100, 100, 100), solver=sgd; total time= 5.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.31663336\n",
      "Iteration 2, loss = 2.31527059\n",
      "Iteration 3, loss = 2.31493505\n",
      "Iteration 4, loss = 2.31500761\n",
      "Iteration 5, loss = 2.31473650\n",
      "Iteration 6, loss = 2.31452907\n",
      "Iteration 7, loss = 2.31428437\n",
      "Iteration 8, loss = 2.31403639\n",
      "Iteration 9, loss = 2.31377391\n",
      "Iteration 10, loss = 2.31358644\n",
      "Iteration 11, loss = 2.31320351\n",
      "Iteration 12, loss = 2.31283543\n",
      "Iteration 13, loss = 2.31271858\n",
      "Iteration 14, loss = 2.31240562\n",
      "Iteration 15, loss = 2.31217731\n",
      "Iteration 16, loss = 2.31121714\n",
      "Iteration 17, loss = 2.31082218\n",
      "Iteration 18, loss = 2.31047144\n",
      "Iteration 19, loss = 2.30972621\n",
      "Iteration 20, loss = 2.30890994\n",
      "Iteration 21, loss = 2.30779705\n",
      "Iteration 22, loss = 2.30692086\n",
      "Iteration 23, loss = 2.30560437\n",
      "Iteration 24, loss = 2.30375029\n",
      "Iteration 25, loss = 2.30180323\n",
      "Iteration 26, loss = 2.29846626\n",
      "Iteration 27, loss = 2.29420924\n",
      "Iteration 28, loss = 2.28805525\n",
      "Iteration 29, loss = 2.27877424\n",
      "Iteration 30, loss = 2.26339161\n",
      "Iteration 31, loss = 2.23724804\n",
      "Iteration 32, loss = 2.19157038\n",
      "Iteration 33, loss = 2.11937525\n",
      "Iteration 34, loss = 2.02996138\n",
      "Iteration 35, loss = 1.94876562\n",
      "Iteration 36, loss = 1.88468213\n",
      "Iteration 37, loss = 1.83227278\n",
      "Iteration 38, loss = 1.78603244\n",
      "Iteration 39, loss = 1.74397282\n",
      "Iteration 40, loss = 1.70444572\n",
      "Iteration 41, loss = 1.66576820\n",
      "Iteration 42, loss = 1.62716463\n",
      "Iteration 43, loss = 1.58421984\n",
      "Iteration 44, loss = 1.53241690\n",
      "Iteration 45, loss = 1.46623438\n",
      "Iteration 46, loss = 1.38542690\n",
      "Iteration 47, loss = 1.30259931\n",
      "Iteration 48, loss = 1.23178389\n",
      "Iteration 49, loss = 1.17642679\n",
      "Iteration 50, loss = 1.13286411\n",
      "Iteration 51, loss = 1.09734219\n",
      "Iteration 52, loss = 1.06575065\n",
      "Iteration 53, loss = 1.03662349\n",
      "Iteration 54, loss = 1.00872723\n",
      "Iteration 55, loss = 0.98041086\n",
      "Iteration 56, loss = 0.95138689\n",
      "Iteration 57, loss = 0.92099631\n",
      "Iteration 58, loss = 0.88909903\n",
      "Iteration 59, loss = 0.85644374\n",
      "Iteration 60, loss = 0.82397175\n",
      "Iteration 61, loss = 0.79379583\n",
      "Iteration 62, loss = 0.76631272\n",
      "Iteration 63, loss = 0.74189065\n",
      "Iteration 64, loss = 0.72102806\n",
      "Iteration 65, loss = 0.70236572\n",
      "Iteration 66, loss = 0.68604782\n",
      "Iteration 67, loss = 0.67166527\n",
      "Iteration 68, loss = 0.65855496\n",
      "Iteration 69, loss = 0.64645215\n",
      "Iteration 70, loss = 0.63573565\n",
      "Iteration 71, loss = 0.62540432\n",
      "Iteration 72, loss = 0.61596598\n",
      "Iteration 73, loss = 0.60710252\n",
      "Iteration 74, loss = 0.59896605\n",
      "Iteration 75, loss = 0.59105718\n",
      "Iteration 76, loss = 0.58352185\n",
      "Iteration 77, loss = 0.57644779\n",
      "Iteration 78, loss = 0.57006719\n",
      "Iteration 79, loss = 0.56349300\n",
      "Iteration 80, loss = 0.55758052\n",
      "Iteration 81, loss = 0.55171037\n",
      "Iteration 82, loss = 0.54609226\n",
      "Iteration 83, loss = 0.54071910\n",
      "Iteration 84, loss = 0.53566659\n",
      "Iteration 85, loss = 0.53070058\n",
      "Iteration 86, loss = 0.52604797\n",
      "Iteration 87, loss = 0.52118972\n",
      "Iteration 88, loss = 0.51635077\n",
      "Iteration 89, loss = 0.51207935\n",
      "Iteration 90, loss = 0.50785872\n",
      "Iteration 91, loss = 0.50351633\n",
      "Iteration 92, loss = 0.49928942\n",
      "Iteration 93, loss = 0.49531819\n",
      "Iteration 94, loss = 0.49142240\n",
      "Iteration 95, loss = 0.48758335\n",
      "Iteration 96, loss = 0.48367350\n",
      "Iteration 97, loss = 0.48016233\n",
      "Iteration 98, loss = 0.47630956\n",
      "Iteration 99, loss = 0.47261419\n",
      "Iteration 100, loss = 0.46903392\n",
      "Iteration 101, loss = 0.46540192\n",
      "Iteration 102, loss = 0.46189604\n",
      "Iteration 103, loss = 0.45830225\n",
      "Iteration 104, loss = 0.45461366\n",
      "Iteration 105, loss = 0.45174191\n",
      "Iteration 106, loss = 0.44810017\n",
      "Iteration 107, loss = 0.44468642\n",
      "Iteration 108, loss = 0.44152049\n",
      "Iteration 109, loss = 0.43803385\n",
      "Iteration 110, loss = 0.43476116\n",
      "Iteration 111, loss = 0.43146139\n",
      "Iteration 112, loss = 0.42812474\n",
      "Iteration 113, loss = 0.42496628\n",
      "Iteration 114, loss = 0.42186671\n",
      "Iteration 115, loss = 0.41842950\n",
      "Iteration 116, loss = 0.41547679\n",
      "Iteration 117, loss = 0.41233780\n",
      "Iteration 118, loss = 0.40921537\n",
      "Iteration 119, loss = 0.40632025\n",
      "Iteration 120, loss = 0.40308023\n",
      "Iteration 121, loss = 0.40009840\n",
      "Iteration 122, loss = 0.39713463\n",
      "Iteration 123, loss = 0.39406122\n",
      "Iteration 124, loss = 0.39138081\n",
      "Iteration 125, loss = 0.38873032\n",
      "Iteration 126, loss = 0.38555856\n",
      "Iteration 127, loss = 0.38287716\n",
      "Iteration 128, loss = 0.38016403\n",
      "Iteration 129, loss = 0.37776536\n",
      "Iteration 130, loss = 0.37487499\n",
      "Iteration 131, loss = 0.37236579\n",
      "Iteration 132, loss = 0.36977499\n",
      "Iteration 133, loss = 0.36716861\n",
      "Iteration 134, loss = 0.36471749\n",
      "Iteration 135, loss = 0.36239087\n",
      "Iteration 136, loss = 0.36017258\n",
      "Iteration 137, loss = 0.35770028\n",
      "Iteration 138, loss = 0.35540415\n",
      "Iteration 139, loss = 0.35313135\n",
      "Iteration 140, loss = 0.35086985\n",
      "Iteration 141, loss = 0.34877255\n",
      "Iteration 142, loss = 0.34656128\n",
      "Iteration 143, loss = 0.34455365\n",
      "Iteration 144, loss = 0.34255804\n",
      "Iteration 145, loss = 0.34016954\n",
      "Iteration 146, loss = 0.33848213\n",
      "Iteration 147, loss = 0.33638087\n",
      "Iteration 148, loss = 0.33444476\n",
      "Iteration 149, loss = 0.33241059\n",
      "Iteration 150, loss = 0.33041042\n",
      "Iteration 151, loss = 0.32867689\n",
      "Iteration 152, loss = 0.32676705\n",
      "Iteration 153, loss = 0.32503711\n",
      "Iteration 154, loss = 0.32321272\n",
      "Iteration 155, loss = 0.32157281\n",
      "Iteration 156, loss = 0.31980669\n",
      "Iteration 157, loss = 0.31804099\n",
      "Iteration 158, loss = 0.31641862\n",
      "Iteration 159, loss = 0.31470703\n",
      "Iteration 160, loss = 0.31299810\n",
      "Iteration 161, loss = 0.31139858\n",
      "Iteration 162, loss = 0.30973050\n",
      "Iteration 163, loss = 0.30809906\n",
      "Iteration 164, loss = 0.30657264\n",
      "Iteration 165, loss = 0.30505284\n",
      "Iteration 166, loss = 0.30348728\n",
      "Iteration 167, loss = 0.30182587\n",
      "Iteration 168, loss = 0.30046638\n",
      "Iteration 169, loss = 0.29901719\n",
      "Iteration 170, loss = 0.29759827\n",
      "Iteration 171, loss = 0.29614935\n",
      "Iteration 172, loss = 0.29464908\n",
      "Iteration 173, loss = 0.29335048\n",
      "Iteration 174, loss = 0.29188856\n",
      "Iteration 175, loss = 0.29057642\n",
      "Iteration 176, loss = 0.28935222\n",
      "Iteration 177, loss = 0.28797205\n",
      "Iteration 178, loss = 0.28693972\n",
      "Iteration 179, loss = 0.28526065\n",
      "Iteration 180, loss = 0.28408534\n",
      "Iteration 181, loss = 0.28274443\n",
      "Iteration 182, loss = 0.28157574\n",
      "Iteration 183, loss = 0.28020565\n",
      "Iteration 184, loss = 0.27914318\n",
      "Iteration 185, loss = 0.27778728\n",
      "Iteration 186, loss = 0.27672760\n",
      "Iteration 187, loss = 0.27556337\n",
      "Iteration 188, loss = 0.27430795\n",
      "Iteration 189, loss = 0.27302988\n",
      "Iteration 190, loss = 0.27197384\n",
      "Iteration 191, loss = 0.27075951\n",
      "Iteration 192, loss = 0.26985661\n",
      "Iteration 193, loss = 0.26862811\n",
      "Iteration 194, loss = 0.26743492\n",
      "Iteration 195, loss = 0.26655753\n",
      "Iteration 196, loss = 0.26534061\n",
      "Iteration 197, loss = 0.26434610\n",
      "Iteration 198, loss = 0.26330344\n",
      "Iteration 199, loss = 0.26231806\n",
      "Iteration 200, loss = 0.26124588\n",
      "[CV] END activation=logistic, alpha=0.01, batch_size=50, hidden_layer_sizes=(100, 100, 100), solver=sgd; total time= 5.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.31743411\n",
      "Iteration 2, loss = 2.31523287\n",
      "Iteration 3, loss = 2.31517330\n",
      "Iteration 4, loss = 2.31495038\n",
      "Iteration 5, loss = 2.31478939\n",
      "Iteration 6, loss = 2.31436019\n",
      "Iteration 7, loss = 2.31430811\n",
      "Iteration 8, loss = 2.31414087\n",
      "Iteration 9, loss = 2.31381607\n",
      "Iteration 10, loss = 2.31336315\n",
      "Iteration 11, loss = 2.31321953\n",
      "Iteration 12, loss = 2.31318973\n",
      "Iteration 13, loss = 2.31231158\n",
      "Iteration 14, loss = 2.31253588\n",
      "Iteration 15, loss = 2.31214866\n",
      "Iteration 16, loss = 2.31160816\n",
      "Iteration 17, loss = 2.31101778\n",
      "Iteration 18, loss = 2.31036539\n",
      "Iteration 19, loss = 2.30968407\n",
      "Iteration 20, loss = 2.30945555\n",
      "Iteration 21, loss = 2.30835266\n",
      "Iteration 22, loss = 2.30743521\n",
      "Iteration 23, loss = 2.30651435\n",
      "Iteration 24, loss = 2.30474337\n",
      "Iteration 25, loss = 2.30279963\n",
      "Iteration 26, loss = 2.30041640\n",
      "Iteration 27, loss = 2.29696477\n",
      "Iteration 28, loss = 2.29244716\n",
      "Iteration 29, loss = 2.28529999\n",
      "Iteration 30, loss = 2.27448484\n",
      "Iteration 31, loss = 2.25698955\n",
      "Iteration 32, loss = 2.22676581\n",
      "Iteration 33, loss = 2.17460254\n",
      "Iteration 34, loss = 2.09256394\n",
      "Iteration 35, loss = 1.99400505\n",
      "Iteration 36, loss = 1.90521577\n",
      "Iteration 37, loss = 1.83387540\n",
      "Iteration 38, loss = 1.77567951\n",
      "Iteration 39, loss = 1.72437913\n",
      "Iteration 40, loss = 1.67831220\n",
      "Iteration 41, loss = 1.63484133\n",
      "Iteration 42, loss = 1.59097602\n",
      "Iteration 43, loss = 1.54357742\n",
      "Iteration 44, loss = 1.48750972\n",
      "Iteration 45, loss = 1.42195067\n",
      "Iteration 46, loss = 1.35064699\n",
      "Iteration 47, loss = 1.28516830\n",
      "Iteration 48, loss = 1.23174586\n",
      "Iteration 49, loss = 1.18856759\n",
      "Iteration 50, loss = 1.15230372\n",
      "Iteration 51, loss = 1.12058624\n",
      "Iteration 52, loss = 1.09224348\n",
      "Iteration 53, loss = 1.06615253\n",
      "Iteration 54, loss = 1.04269020\n",
      "Iteration 55, loss = 1.01999898\n",
      "Iteration 56, loss = 0.99892526\n",
      "Iteration 57, loss = 0.97844060\n",
      "Iteration 58, loss = 0.95840434\n",
      "Iteration 59, loss = 0.93907175\n",
      "Iteration 60, loss = 0.91949363\n",
      "Iteration 61, loss = 0.90006770\n",
      "Iteration 62, loss = 0.88064244\n",
      "Iteration 63, loss = 0.86052810\n",
      "Iteration 64, loss = 0.83984839\n",
      "Iteration 65, loss = 0.81822036\n",
      "Iteration 66, loss = 0.79521118\n",
      "Iteration 67, loss = 0.77153220\n",
      "Iteration 68, loss = 0.74691804\n",
      "Iteration 69, loss = 0.72285388\n",
      "Iteration 70, loss = 0.69965303\n",
      "Iteration 71, loss = 0.67854048\n",
      "Iteration 72, loss = 0.66049264\n",
      "Iteration 73, loss = 0.64446972\n",
      "Iteration 74, loss = 0.63063841\n",
      "Iteration 75, loss = 0.61836762\n",
      "Iteration 76, loss = 0.60753675\n",
      "Iteration 77, loss = 0.59768053\n",
      "Iteration 78, loss = 0.58928614\n",
      "Iteration 79, loss = 0.58083610\n",
      "Iteration 80, loss = 0.57352986\n",
      "Iteration 81, loss = 0.56631391\n",
      "Iteration 82, loss = 0.55976899\n",
      "Iteration 83, loss = 0.55347229\n",
      "Iteration 84, loss = 0.54753538\n",
      "Iteration 85, loss = 0.54160783\n",
      "Iteration 86, loss = 0.53621687\n",
      "Iteration 87, loss = 0.53084418\n",
      "Iteration 88, loss = 0.52565915\n",
      "Iteration 89, loss = 0.52085074\n",
      "Iteration 90, loss = 0.51598256\n",
      "Iteration 91, loss = 0.51118087\n",
      "Iteration 92, loss = 0.50655973\n",
      "Iteration 93, loss = 0.50215653\n",
      "Iteration 94, loss = 0.49780149\n",
      "Iteration 95, loss = 0.49364523\n",
      "Iteration 96, loss = 0.48916092\n",
      "Iteration 97, loss = 0.48526579\n",
      "Iteration 98, loss = 0.48090327\n",
      "Iteration 99, loss = 0.47716072\n",
      "Iteration 100, loss = 0.47309483\n",
      "Iteration 101, loss = 0.46915109\n",
      "Iteration 102, loss = 0.46535937\n",
      "Iteration 103, loss = 0.46157164\n",
      "Iteration 104, loss = 0.45811852\n",
      "Iteration 105, loss = 0.45415708\n",
      "Iteration 106, loss = 0.45040413\n",
      "Iteration 107, loss = 0.44680337\n",
      "Iteration 108, loss = 0.44324337\n",
      "Iteration 109, loss = 0.43987527\n",
      "Iteration 110, loss = 0.43638143\n",
      "Iteration 111, loss = 0.43301845\n",
      "Iteration 112, loss = 0.42951696\n",
      "Iteration 113, loss = 0.42632327\n",
      "Iteration 114, loss = 0.42287531\n",
      "Iteration 115, loss = 0.41974673\n",
      "Iteration 116, loss = 0.41631932\n",
      "Iteration 117, loss = 0.41343466\n",
      "Iteration 118, loss = 0.41049310\n",
      "Iteration 119, loss = 0.40726621\n",
      "Iteration 120, loss = 0.40437099\n",
      "Iteration 121, loss = 0.40108338\n",
      "Iteration 122, loss = 0.39840968\n",
      "Iteration 123, loss = 0.39550690\n",
      "Iteration 124, loss = 0.39256361\n",
      "Iteration 125, loss = 0.39013202\n",
      "Iteration 126, loss = 0.38729048\n",
      "Iteration 127, loss = 0.38463731\n",
      "Iteration 128, loss = 0.38213331\n",
      "Iteration 129, loss = 0.37944536\n",
      "Iteration 130, loss = 0.37694607\n",
      "Iteration 131, loss = 0.37472332\n",
      "Iteration 132, loss = 0.37233044\n",
      "Iteration 133, loss = 0.37002856\n",
      "Iteration 134, loss = 0.36746775\n",
      "Iteration 135, loss = 0.36533451\n",
      "Iteration 136, loss = 0.36279597\n",
      "Iteration 137, loss = 0.36078760\n",
      "Iteration 138, loss = 0.35849350\n",
      "Iteration 139, loss = 0.35634612\n",
      "Iteration 140, loss = 0.35431850\n",
      "Iteration 141, loss = 0.35210385\n",
      "Iteration 142, loss = 0.34978511\n",
      "Iteration 143, loss = 0.34817443\n",
      "Iteration 144, loss = 0.34601182\n",
      "Iteration 145, loss = 0.34420768\n",
      "Iteration 146, loss = 0.34225614\n",
      "Iteration 147, loss = 0.34037900\n",
      "Iteration 148, loss = 0.33859474\n",
      "Iteration 149, loss = 0.33672240\n",
      "Iteration 150, loss = 0.33487004\n",
      "Iteration 151, loss = 0.33315613\n",
      "Iteration 152, loss = 0.33131592\n",
      "Iteration 153, loss = 0.32950301\n",
      "Iteration 154, loss = 0.32780373\n",
      "Iteration 155, loss = 0.32633457\n",
      "Iteration 156, loss = 0.32470924\n",
      "Iteration 157, loss = 0.32286068\n",
      "Iteration 158, loss = 0.32146105\n",
      "Iteration 159, loss = 0.31983675\n",
      "Iteration 160, loss = 0.31819012\n",
      "Iteration 161, loss = 0.31654801\n",
      "Iteration 162, loss = 0.31503976\n",
      "Iteration 163, loss = 0.31363586\n",
      "Iteration 164, loss = 0.31219061\n",
      "Iteration 165, loss = 0.31079209\n",
      "Iteration 166, loss = 0.30922954\n",
      "Iteration 167, loss = 0.30798278\n",
      "Iteration 168, loss = 0.30631769\n",
      "Iteration 169, loss = 0.30503507\n",
      "Iteration 170, loss = 0.30368662\n",
      "Iteration 171, loss = 0.30242906\n",
      "Iteration 172, loss = 0.30076072\n",
      "Iteration 173, loss = 0.29968725\n",
      "Iteration 174, loss = 0.29834791\n",
      "Iteration 175, loss = 0.29702447\n",
      "Iteration 176, loss = 0.29583663\n",
      "Iteration 177, loss = 0.29445500\n",
      "Iteration 178, loss = 0.29347805\n",
      "Iteration 179, loss = 0.29219864\n",
      "Iteration 180, loss = 0.29077224\n",
      "Iteration 181, loss = 0.28965164\n",
      "Iteration 182, loss = 0.28847210\n",
      "Iteration 183, loss = 0.28714550\n",
      "Iteration 184, loss = 0.28625186\n",
      "Iteration 185, loss = 0.28489729\n",
      "Iteration 186, loss = 0.28370330\n",
      "Iteration 187, loss = 0.28285652\n",
      "Iteration 188, loss = 0.28164330\n",
      "Iteration 189, loss = 0.28057313\n",
      "Iteration 190, loss = 0.27957897\n",
      "Iteration 191, loss = 0.27844072\n",
      "Iteration 192, loss = 0.27728401\n",
      "Iteration 193, loss = 0.27646005\n",
      "Iteration 194, loss = 0.27533827\n",
      "Iteration 195, loss = 0.27408738\n",
      "Iteration 196, loss = 0.27313767\n",
      "Iteration 197, loss = 0.27214091\n",
      "Iteration 198, loss = 0.27113799\n",
      "Iteration 199, loss = 0.27003902\n",
      "Iteration 200, loss = 0.26876072\n",
      "[CV] END activation=logistic, alpha=0.01, batch_size=50, hidden_layer_sizes=(100, 100, 100), solver=sgd; total time= 5.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.31811983\n",
      "Iteration 2, loss = 2.31504646\n",
      "Iteration 3, loss = 2.31490156\n",
      "Iteration 4, loss = 2.31487389\n",
      "Iteration 5, loss = 2.31466048\n",
      "Iteration 6, loss = 2.31429341\n",
      "Iteration 7, loss = 2.31387994\n",
      "Iteration 8, loss = 2.31385860\n",
      "Iteration 9, loss = 2.31360667\n",
      "Iteration 10, loss = 2.31322734\n",
      "Iteration 11, loss = 2.31300881\n",
      "Iteration 12, loss = 2.31262666\n",
      "Iteration 13, loss = 2.31244380\n",
      "Iteration 14, loss = 2.31176949\n",
      "Iteration 15, loss = 2.31163435\n",
      "Iteration 16, loss = 2.31076675\n",
      "Iteration 17, loss = 2.31030112\n",
      "Iteration 18, loss = 2.30979346\n",
      "Iteration 19, loss = 2.30898724\n",
      "Iteration 20, loss = 2.30782014\n",
      "Iteration 21, loss = 2.30692954\n",
      "Iteration 22, loss = 2.30535513\n",
      "Iteration 23, loss = 2.30372512\n",
      "Iteration 24, loss = 2.30150674\n",
      "Iteration 25, loss = 2.29831106\n",
      "Iteration 26, loss = 2.29396257\n",
      "Iteration 27, loss = 2.28802145\n",
      "Iteration 28, loss = 2.27875899\n",
      "Iteration 29, loss = 2.26368856\n",
      "Iteration 30, loss = 2.23711343\n",
      "Iteration 31, loss = 2.18777639\n",
      "Iteration 32, loss = 2.10093950\n",
      "Iteration 33, loss = 1.98667961\n",
      "Iteration 34, loss = 1.88974245\n",
      "Iteration 35, loss = 1.82862937\n",
      "Iteration 36, loss = 1.78856258\n",
      "Iteration 37, loss = 1.75659048\n",
      "Iteration 38, loss = 1.72393946\n",
      "Iteration 39, loss = 1.68469037\n",
      "Iteration 40, loss = 1.63251499\n",
      "Iteration 41, loss = 1.56827181\n",
      "Iteration 42, loss = 1.49895372\n",
      "Iteration 43, loss = 1.43389358\n",
      "Iteration 44, loss = 1.37792284\n",
      "Iteration 45, loss = 1.33011976\n",
      "Iteration 46, loss = 1.28825140\n",
      "Iteration 47, loss = 1.25013081\n",
      "Iteration 48, loss = 1.21350180\n",
      "Iteration 49, loss = 1.17844416\n",
      "Iteration 50, loss = 1.14506160\n",
      "Iteration 51, loss = 1.11346824\n",
      "Iteration 52, loss = 1.08384934\n",
      "Iteration 53, loss = 1.05607632\n",
      "Iteration 54, loss = 1.02958387\n",
      "Iteration 55, loss = 1.00418416\n",
      "Iteration 56, loss = 0.97911271\n",
      "Iteration 57, loss = 0.95353093\n",
      "Iteration 58, loss = 0.92718694\n",
      "Iteration 59, loss = 0.89959583\n",
      "Iteration 60, loss = 0.87025380\n",
      "Iteration 61, loss = 0.84019236\n",
      "Iteration 62, loss = 0.81031308\n",
      "Iteration 63, loss = 0.78174849\n",
      "Iteration 64, loss = 0.75534439\n",
      "Iteration 65, loss = 0.73235531\n",
      "Iteration 66, loss = 0.71228134\n",
      "Iteration 67, loss = 0.69452085\n",
      "Iteration 68, loss = 0.67953697\n",
      "Iteration 69, loss = 0.66572898\n",
      "Iteration 70, loss = 0.65333175\n",
      "Iteration 71, loss = 0.64239393\n",
      "Iteration 72, loss = 0.63202858\n",
      "Iteration 73, loss = 0.62278000\n",
      "Iteration 74, loss = 0.61384960\n",
      "Iteration 75, loss = 0.60570035\n",
      "Iteration 76, loss = 0.59766284\n",
      "Iteration 77, loss = 0.59025440\n",
      "Iteration 78, loss = 0.58330033\n",
      "Iteration 79, loss = 0.57668694\n",
      "Iteration 80, loss = 0.57002396\n",
      "Iteration 81, loss = 0.56387518\n",
      "Iteration 82, loss = 0.55798707\n",
      "Iteration 83, loss = 0.55202247\n",
      "Iteration 84, loss = 0.54625376\n",
      "Iteration 85, loss = 0.54094146\n",
      "Iteration 86, loss = 0.53557297\n",
      "Iteration 87, loss = 0.53004805\n",
      "Iteration 88, loss = 0.52530426\n",
      "Iteration 89, loss = 0.52027577\n",
      "Iteration 90, loss = 0.51512526\n",
      "Iteration 91, loss = 0.51040028\n",
      "Iteration 92, loss = 0.50562708\n",
      "Iteration 93, loss = 0.50105353\n",
      "Iteration 94, loss = 0.49632392\n",
      "Iteration 95, loss = 0.49163205\n",
      "Iteration 96, loss = 0.48727609\n",
      "Iteration 97, loss = 0.48267582\n",
      "Iteration 98, loss = 0.47802208\n",
      "Iteration 99, loss = 0.47363438\n",
      "Iteration 100, loss = 0.46920286\n",
      "Iteration 101, loss = 0.46513157\n",
      "Iteration 102, loss = 0.46063501\n",
      "Iteration 103, loss = 0.45657714\n",
      "Iteration 104, loss = 0.45241443\n",
      "Iteration 105, loss = 0.44787774\n",
      "Iteration 106, loss = 0.44394150\n",
      "Iteration 107, loss = 0.43999735\n",
      "Iteration 108, loss = 0.43605788\n",
      "Iteration 109, loss = 0.43211555\n",
      "Iteration 110, loss = 0.42814579\n",
      "Iteration 111, loss = 0.42443054\n",
      "Iteration 112, loss = 0.42078881\n",
      "Iteration 113, loss = 0.41712475\n",
      "Iteration 114, loss = 0.41341565\n",
      "Iteration 115, loss = 0.41009988\n",
      "Iteration 116, loss = 0.40650458\n",
      "Iteration 117, loss = 0.40288340\n",
      "Iteration 118, loss = 0.40002084\n",
      "Iteration 119, loss = 0.39687131\n",
      "Iteration 120, loss = 0.39372166\n",
      "Iteration 121, loss = 0.39067373\n",
      "Iteration 122, loss = 0.38777161\n",
      "Iteration 123, loss = 0.38487793\n",
      "Iteration 124, loss = 0.38221735\n",
      "Iteration 125, loss = 0.37938791\n",
      "Iteration 126, loss = 0.37678168\n",
      "Iteration 127, loss = 0.37406148\n",
      "Iteration 128, loss = 0.37155871\n",
      "Iteration 129, loss = 0.36897908\n",
      "Iteration 130, loss = 0.36653076\n",
      "Iteration 131, loss = 0.36411927\n",
      "Iteration 132, loss = 0.36159621\n",
      "Iteration 133, loss = 0.35944983\n",
      "Iteration 134, loss = 0.35718867\n",
      "Iteration 135, loss = 0.35522687\n",
      "Iteration 136, loss = 0.35300203\n",
      "Iteration 137, loss = 0.35088383\n",
      "Iteration 138, loss = 0.34869522\n",
      "Iteration 139, loss = 0.34672840\n",
      "Iteration 140, loss = 0.34489821\n",
      "Iteration 141, loss = 0.34265443\n",
      "Iteration 142, loss = 0.34048276\n",
      "Iteration 143, loss = 0.33895584\n",
      "Iteration 144, loss = 0.33691279\n",
      "Iteration 145, loss = 0.33513724\n",
      "Iteration 146, loss = 0.33326965\n",
      "Iteration 147, loss = 0.33132090\n",
      "Iteration 148, loss = 0.32956544\n",
      "Iteration 149, loss = 0.32795442\n",
      "Iteration 150, loss = 0.32618083\n",
      "Iteration 151, loss = 0.32446618\n",
      "Iteration 152, loss = 0.32281436\n",
      "Iteration 153, loss = 0.32112723\n",
      "Iteration 154, loss = 0.31972579\n",
      "Iteration 155, loss = 0.31798190\n",
      "Iteration 156, loss = 0.31645385\n",
      "Iteration 157, loss = 0.31464899\n",
      "Iteration 158, loss = 0.31320596\n",
      "Iteration 159, loss = 0.31159228\n",
      "Iteration 160, loss = 0.30986483\n",
      "Iteration 161, loss = 0.30855386\n",
      "Iteration 162, loss = 0.30704760\n",
      "Iteration 163, loss = 0.30555850\n",
      "Iteration 164, loss = 0.30430816\n",
      "Iteration 165, loss = 0.30296213\n",
      "Iteration 166, loss = 0.30134248\n",
      "Iteration 167, loss = 0.30020118\n",
      "Iteration 168, loss = 0.29876723\n",
      "Iteration 169, loss = 0.29733004\n",
      "Iteration 170, loss = 0.29601627\n",
      "Iteration 171, loss = 0.29459226\n",
      "Iteration 172, loss = 0.29334092\n",
      "Iteration 173, loss = 0.29224076\n",
      "Iteration 174, loss = 0.29088839\n",
      "Iteration 175, loss = 0.28963142\n",
      "Iteration 176, loss = 0.28821364\n",
      "Iteration 177, loss = 0.28713317\n",
      "Iteration 178, loss = 0.28595899\n",
      "Iteration 179, loss = 0.28479109\n",
      "Iteration 180, loss = 0.28357701\n",
      "Iteration 181, loss = 0.28225089\n",
      "Iteration 182, loss = 0.28124427\n",
      "Iteration 183, loss = 0.27991598\n",
      "Iteration 184, loss = 0.27888525\n",
      "Iteration 185, loss = 0.27786668\n",
      "Iteration 186, loss = 0.27668760\n",
      "Iteration 187, loss = 0.27551796\n",
      "Iteration 188, loss = 0.27448249\n",
      "Iteration 189, loss = 0.27356577\n",
      "Iteration 190, loss = 0.27222556\n",
      "Iteration 191, loss = 0.27117380\n",
      "Iteration 192, loss = 0.27024292\n",
      "Iteration 193, loss = 0.26918436\n",
      "Iteration 194, loss = 0.26808347\n",
      "Iteration 195, loss = 0.26712906\n",
      "Iteration 196, loss = 0.26608122\n",
      "Iteration 197, loss = 0.26506022\n",
      "Iteration 198, loss = 0.26411242\n",
      "Iteration 199, loss = 0.26330479\n",
      "Iteration 200, loss = 0.26232614\n",
      "[CV] END activation=logistic, alpha=0.01, batch_size=50, hidden_layer_sizes=(100, 100, 100), solver=sgd; total time= 5.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.87259696\n",
      "Iteration 2, loss = 0.36773298\n",
      "Iteration 3, loss = 0.30141320\n",
      "Iteration 4, loss = 0.26844862\n",
      "Iteration 5, loss = 0.24492041\n",
      "Iteration 6, loss = 0.22715817\n",
      "Iteration 7, loss = 0.21191744\n",
      "Iteration 8, loss = 0.20031760\n",
      "Iteration 9, loss = 0.18985707\n",
      "Iteration 10, loss = 0.18092674\n",
      "Iteration 11, loss = 0.17269210\n",
      "Iteration 12, loss = 0.16582421\n",
      "Iteration 13, loss = 0.15953865\n",
      "Iteration 14, loss = 0.15432389\n",
      "Iteration 15, loss = 0.14970281\n",
      "Iteration 16, loss = 0.14513310\n",
      "Iteration 17, loss = 0.14117731\n",
      "Iteration 18, loss = 0.13809246\n",
      "Iteration 19, loss = 0.13501882\n",
      "Iteration 20, loss = 0.13150497\n",
      "Iteration 21, loss = 0.12932683\n",
      "Iteration 22, loss = 0.12697883\n",
      "Iteration 23, loss = 0.12503067\n",
      "Iteration 24, loss = 0.12329784\n",
      "Iteration 25, loss = 0.12125632\n",
      "Iteration 26, loss = 0.11966738\n",
      "Iteration 27, loss = 0.11863778\n",
      "Iteration 28, loss = 0.11655320\n",
      "Iteration 29, loss = 0.11536863\n",
      "Iteration 30, loss = 0.11436605\n",
      "Iteration 31, loss = 0.11312381\n",
      "Iteration 32, loss = 0.11254462\n",
      "Iteration 33, loss = 0.11140240\n",
      "Iteration 34, loss = 0.11046948\n",
      "Iteration 35, loss = 0.10963885\n",
      "Iteration 36, loss = 0.10873164\n",
      "Iteration 37, loss = 0.10815749\n",
      "Iteration 38, loss = 0.10737105\n",
      "Iteration 39, loss = 0.10718995\n",
      "Iteration 40, loss = 0.10630943\n",
      "Iteration 41, loss = 0.10613797\n",
      "Iteration 42, loss = 0.10555056\n",
      "Iteration 43, loss = 0.10463265\n",
      "Iteration 44, loss = 0.10398280\n",
      "Iteration 45, loss = 0.10416716\n",
      "Iteration 46, loss = 0.10296372\n",
      "Iteration 47, loss = 0.10352285\n",
      "Iteration 48, loss = 0.10337635\n",
      "Iteration 49, loss = 0.10244421\n",
      "Iteration 50, loss = 0.10203082\n",
      "Iteration 51, loss = 0.10177876\n",
      "Iteration 52, loss = 0.10180101\n",
      "Iteration 53, loss = 0.10161052\n",
      "Iteration 54, loss = 0.10111375\n",
      "Iteration 55, loss = 0.10116692\n",
      "Iteration 56, loss = 0.10062890\n",
      "Iteration 57, loss = 0.10071149\n",
      "Iteration 58, loss = 0.10067758\n",
      "Iteration 59, loss = 0.09966350\n",
      "Iteration 60, loss = 0.09983610\n",
      "Iteration 61, loss = 0.10022875\n",
      "Iteration 62, loss = 0.09937190\n",
      "Iteration 63, loss = 0.09933666\n",
      "Iteration 64, loss = 0.09924447\n",
      "Iteration 65, loss = 0.09936141\n",
      "Iteration 66, loss = 0.09927072\n",
      "Iteration 67, loss = 0.09896578\n",
      "Iteration 68, loss = 0.09867825\n",
      "Iteration 69, loss = 0.09823387\n",
      "Iteration 70, loss = 0.09874532\n",
      "Iteration 71, loss = 0.09808418\n",
      "Iteration 72, loss = 0.09828471\n",
      "Iteration 73, loss = 0.09817173\n",
      "Iteration 74, loss = 0.09834148\n",
      "Iteration 75, loss = 0.09768360\n",
      "Iteration 76, loss = 0.09781692\n",
      "Iteration 77, loss = 0.09760826\n",
      "Iteration 78, loss = 0.09759662\n",
      "Iteration 79, loss = 0.09755999\n",
      "Iteration 80, loss = 0.09748743\n",
      "Iteration 81, loss = 0.09746577\n",
      "Iteration 82, loss = 0.09710614\n",
      "Iteration 83, loss = 0.09718891\n",
      "Iteration 84, loss = 0.09724075\n",
      "Iteration 85, loss = 0.09751630\n",
      "Iteration 86, loss = 0.09687281\n",
      "Iteration 87, loss = 0.09697353\n",
      "Iteration 88, loss = 0.09712927\n",
      "Iteration 89, loss = 0.09714320\n",
      "Iteration 90, loss = 0.09678776\n",
      "Iteration 91, loss = 0.09666534\n",
      "Iteration 92, loss = 0.09672490\n",
      "Iteration 93, loss = 0.09650318\n",
      "Iteration 94, loss = 0.09612779\n",
      "Iteration 95, loss = 0.09639446\n",
      "Iteration 96, loss = 0.09589852\n",
      "Iteration 97, loss = 0.09697393\n",
      "Iteration 98, loss = 0.09603040\n",
      "Iteration 99, loss = 0.09613630\n",
      "Iteration 100, loss = 0.09609391\n",
      "Iteration 101, loss = 0.09613217\n",
      "Iteration 102, loss = 0.09595248\n",
      "Iteration 103, loss = 0.09607587\n",
      "Iteration 104, loss = 0.09584336\n",
      "Iteration 105, loss = 0.09569772\n",
      "Iteration 106, loss = 0.09587757\n",
      "Iteration 107, loss = 0.09605418\n",
      "Iteration 108, loss = 0.09571087\n",
      "Iteration 109, loss = 0.09584883\n",
      "Iteration 110, loss = 0.09540712\n",
      "Iteration 111, loss = 0.09545867\n",
      "Iteration 112, loss = 0.09558784\n",
      "Iteration 113, loss = 0.09566313\n",
      "Iteration 114, loss = 0.09511549\n",
      "Iteration 115, loss = 0.09506702\n",
      "Iteration 116, loss = 0.09534694\n",
      "Iteration 117, loss = 0.09557981\n",
      "Iteration 118, loss = 0.09498583\n",
      "Iteration 119, loss = 0.09554037\n",
      "Iteration 120, loss = 0.09543663\n",
      "Iteration 121, loss = 0.09510714\n",
      "Iteration 122, loss = 0.09472074\n",
      "Iteration 123, loss = 0.09494138\n",
      "Iteration 124, loss = 0.09502656\n",
      "Iteration 125, loss = 0.09574395\n",
      "Iteration 126, loss = 0.09500192\n",
      "Iteration 127, loss = 0.09465823\n",
      "Iteration 128, loss = 0.09488814\n",
      "Iteration 129, loss = 0.09493829\n",
      "Iteration 130, loss = 0.09506914\n",
      "Iteration 131, loss = 0.09512508\n",
      "Iteration 132, loss = 0.09546785\n",
      "Iteration 133, loss = 0.09511843\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.01, batch_size=100, hidden_layer_sizes=(100,), solver=adam; total time= 1.9min\n",
      "Iteration 1, loss = 0.86672497\n",
      "Iteration 2, loss = 0.36629775\n",
      "Iteration 3, loss = 0.29982485\n",
      "Iteration 4, loss = 0.26606621\n",
      "Iteration 5, loss = 0.24291781\n",
      "Iteration 6, loss = 0.22477917\n",
      "Iteration 7, loss = 0.21179898\n",
      "Iteration 8, loss = 0.20046444\n",
      "Iteration 9, loss = 0.18994232\n",
      "Iteration 10, loss = 0.18121395\n",
      "Iteration 11, loss = 0.17382744\n",
      "Iteration 12, loss = 0.16676035\n",
      "Iteration 13, loss = 0.16081014\n",
      "Iteration 14, loss = 0.15561787\n",
      "Iteration 15, loss = 0.15117752\n",
      "Iteration 16, loss = 0.14647694\n",
      "Iteration 17, loss = 0.14281764\n",
      "Iteration 18, loss = 0.13992374\n",
      "Iteration 19, loss = 0.13615571\n",
      "Iteration 20, loss = 0.13361267\n",
      "Iteration 21, loss = 0.13084659\n",
      "Iteration 22, loss = 0.12876380\n",
      "Iteration 23, loss = 0.12608868\n",
      "Iteration 24, loss = 0.12420706\n",
      "Iteration 25, loss = 0.12220849\n",
      "Iteration 26, loss = 0.12072488\n",
      "Iteration 27, loss = 0.11933842\n",
      "Iteration 28, loss = 0.11765162\n",
      "Iteration 29, loss = 0.11647870\n",
      "Iteration 30, loss = 0.11525771\n",
      "Iteration 31, loss = 0.11470028\n",
      "Iteration 32, loss = 0.11367129\n",
      "Iteration 33, loss = 0.11269054\n",
      "Iteration 34, loss = 0.11152938\n",
      "Iteration 35, loss = 0.11089702\n",
      "Iteration 36, loss = 0.10990439\n",
      "Iteration 37, loss = 0.10935092\n",
      "Iteration 38, loss = 0.10878050\n",
      "Iteration 39, loss = 0.10822687\n",
      "Iteration 40, loss = 0.10708948\n",
      "Iteration 41, loss = 0.10696516\n",
      "Iteration 42, loss = 0.10654568\n",
      "Iteration 43, loss = 0.10606268\n",
      "Iteration 44, loss = 0.10603155\n",
      "Iteration 45, loss = 0.10533704\n",
      "Iteration 46, loss = 0.10441084\n",
      "Iteration 47, loss = 0.10410957\n",
      "Iteration 48, loss = 0.10383116\n",
      "Iteration 49, loss = 0.10371665\n",
      "Iteration 50, loss = 0.10334775\n",
      "Iteration 51, loss = 0.10273861\n",
      "Iteration 52, loss = 0.10256356\n",
      "Iteration 53, loss = 0.10274041\n",
      "Iteration 54, loss = 0.10209330\n",
      "Iteration 55, loss = 0.10201323\n",
      "Iteration 56, loss = 0.10150150\n",
      "Iteration 57, loss = 0.10171471\n",
      "Iteration 58, loss = 0.10120123\n",
      "Iteration 59, loss = 0.10101807\n",
      "Iteration 60, loss = 0.10128018\n",
      "Iteration 61, loss = 0.10055678\n",
      "Iteration 62, loss = 0.10045258\n",
      "Iteration 63, loss = 0.10036182\n",
      "Iteration 64, loss = 0.09989359\n",
      "Iteration 65, loss = 0.10044856\n",
      "Iteration 66, loss = 0.10003685\n",
      "Iteration 67, loss = 0.09976512\n",
      "Iteration 68, loss = 0.09937665\n",
      "Iteration 69, loss = 0.09947959\n",
      "Iteration 70, loss = 0.09859384\n",
      "Iteration 71, loss = 0.09933103\n",
      "Iteration 72, loss = 0.09906076\n",
      "Iteration 73, loss = 0.09940331\n",
      "Iteration 74, loss = 0.09892002\n",
      "Iteration 75, loss = 0.09908410\n",
      "Iteration 76, loss = 0.09873290\n",
      "Iteration 77, loss = 0.09903671\n",
      "Iteration 78, loss = 0.09800730\n",
      "Iteration 79, loss = 0.09810993\n",
      "Iteration 80, loss = 0.09856547\n",
      "Iteration 81, loss = 0.09825616\n",
      "Iteration 82, loss = 0.09848153\n",
      "Iteration 83, loss = 0.09813158\n",
      "Iteration 84, loss = 0.09793791\n",
      "Iteration 85, loss = 0.09747710\n",
      "Iteration 86, loss = 0.09769814\n",
      "Iteration 87, loss = 0.09782816\n",
      "Iteration 88, loss = 0.09737418\n",
      "Iteration 89, loss = 0.09779590\n",
      "Iteration 90, loss = 0.09719990\n",
      "Iteration 91, loss = 0.09734260\n",
      "Iteration 92, loss = 0.09685014\n",
      "Iteration 93, loss = 0.09708065\n",
      "Iteration 94, loss = 0.09663830\n",
      "Iteration 95, loss = 0.09687172\n",
      "Iteration 96, loss = 0.09669559\n",
      "Iteration 97, loss = 0.09678866\n",
      "Iteration 98, loss = 0.09688407\n",
      "Iteration 99, loss = 0.09650310\n",
      "Iteration 100, loss = 0.09603161\n",
      "Iteration 101, loss = 0.09721830\n",
      "Iteration 102, loss = 0.09646823\n",
      "Iteration 103, loss = 0.09644471\n",
      "Iteration 104, loss = 0.09645388\n",
      "Iteration 105, loss = 0.09609326\n",
      "Iteration 106, loss = 0.09635830\n",
      "Iteration 107, loss = 0.09612447\n",
      "Iteration 108, loss = 0.09595590\n",
      "Iteration 109, loss = 0.09603236\n",
      "Iteration 110, loss = 0.09592723\n",
      "Iteration 111, loss = 0.09627837\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.01, batch_size=100, hidden_layer_sizes=(100,), solver=adam; total time= 1.5min\n",
      "Iteration 1, loss = 0.87257177\n",
      "Iteration 2, loss = 0.36878961\n",
      "Iteration 3, loss = 0.30034154\n",
      "Iteration 4, loss = 0.26553663\n",
      "Iteration 5, loss = 0.24136480\n",
      "Iteration 6, loss = 0.22324627\n",
      "Iteration 7, loss = 0.20870328\n",
      "Iteration 8, loss = 0.19673708\n",
      "Iteration 9, loss = 0.18602330\n",
      "Iteration 10, loss = 0.17768374\n",
      "Iteration 11, loss = 0.17009431\n",
      "Iteration 12, loss = 0.16313941\n",
      "Iteration 13, loss = 0.15739818\n",
      "Iteration 14, loss = 0.15224346\n",
      "Iteration 15, loss = 0.14779371\n",
      "Iteration 16, loss = 0.14339964\n",
      "Iteration 17, loss = 0.13983433\n",
      "Iteration 18, loss = 0.13673424\n",
      "Iteration 19, loss = 0.13351985\n",
      "Iteration 20, loss = 0.13061957\n",
      "Iteration 21, loss = 0.12839011\n",
      "Iteration 22, loss = 0.12605271\n",
      "Iteration 23, loss = 0.12404817\n",
      "Iteration 24, loss = 0.12164961\n",
      "Iteration 25, loss = 0.12051455\n",
      "Iteration 26, loss = 0.11987443\n",
      "Iteration 27, loss = 0.11742817\n",
      "Iteration 28, loss = 0.11623417\n",
      "Iteration 29, loss = 0.11493511\n",
      "Iteration 30, loss = 0.11368870\n",
      "Iteration 31, loss = 0.11256310\n",
      "Iteration 32, loss = 0.11206651\n",
      "Iteration 33, loss = 0.11050292\n",
      "Iteration 34, loss = 0.11034426\n",
      "Iteration 35, loss = 0.10948980\n",
      "Iteration 36, loss = 0.10875254\n",
      "Iteration 37, loss = 0.10761166\n",
      "Iteration 38, loss = 0.10742702\n",
      "Iteration 39, loss = 0.10680502\n",
      "Iteration 40, loss = 0.10595461\n",
      "Iteration 41, loss = 0.10549134\n",
      "Iteration 42, loss = 0.10546187\n",
      "Iteration 43, loss = 0.10430705\n",
      "Iteration 44, loss = 0.10407328\n",
      "Iteration 45, loss = 0.10364822\n",
      "Iteration 46, loss = 0.10332127\n",
      "Iteration 47, loss = 0.10309319\n",
      "Iteration 48, loss = 0.10274053\n",
      "Iteration 49, loss = 0.10202163\n",
      "Iteration 50, loss = 0.10184043\n",
      "Iteration 51, loss = 0.10174313\n",
      "Iteration 52, loss = 0.10128983\n",
      "Iteration 53, loss = 0.10155513\n",
      "Iteration 54, loss = 0.10107219\n",
      "Iteration 55, loss = 0.10079850\n",
      "Iteration 56, loss = 0.10101949\n",
      "Iteration 57, loss = 0.10093118\n",
      "Iteration 58, loss = 0.10031747\n",
      "Iteration 59, loss = 0.10020124\n",
      "Iteration 60, loss = 0.09968583\n",
      "Iteration 61, loss = 0.09907790\n",
      "Iteration 62, loss = 0.09922513\n",
      "Iteration 63, loss = 0.09936469\n",
      "Iteration 64, loss = 0.09917833\n",
      "Iteration 65, loss = 0.09877877\n",
      "Iteration 66, loss = 0.09907253\n",
      "Iteration 67, loss = 0.09819326\n",
      "Iteration 68, loss = 0.09878279\n",
      "Iteration 69, loss = 0.09813007\n",
      "Iteration 70, loss = 0.09839256\n",
      "Iteration 71, loss = 0.09861101\n",
      "Iteration 72, loss = 0.09781078\n",
      "Iteration 73, loss = 0.09811597\n",
      "Iteration 74, loss = 0.09785380\n",
      "Iteration 75, loss = 0.09782808\n",
      "Iteration 76, loss = 0.09774343\n",
      "Iteration 77, loss = 0.09713503\n",
      "Iteration 78, loss = 0.09706307\n",
      "Iteration 79, loss = 0.09706411\n",
      "Iteration 80, loss = 0.09747437\n",
      "Iteration 81, loss = 0.09711888\n",
      "Iteration 82, loss = 0.09709671\n",
      "Iteration 83, loss = 0.09714838\n",
      "Iteration 84, loss = 0.09682461\n",
      "Iteration 85, loss = 0.09667152\n",
      "Iteration 86, loss = 0.09698093\n",
      "Iteration 87, loss = 0.09641724\n",
      "Iteration 88, loss = 0.09653402\n",
      "Iteration 89, loss = 0.09619599\n",
      "Iteration 90, loss = 0.09601008\n",
      "Iteration 91, loss = 0.09642969\n",
      "Iteration 92, loss = 0.09631031\n",
      "Iteration 93, loss = 0.09574678\n",
      "Iteration 94, loss = 0.09586142\n",
      "Iteration 95, loss = 0.09637265\n",
      "Iteration 96, loss = 0.09559395\n",
      "Iteration 97, loss = 0.09537531\n",
      "Iteration 98, loss = 0.09573076\n",
      "Iteration 99, loss = 0.09596050\n",
      "Iteration 100, loss = 0.09577037\n",
      "Iteration 101, loss = 0.09562347\n",
      "Iteration 102, loss = 0.09566240\n",
      "Iteration 103, loss = 0.09544027\n",
      "Iteration 104, loss = 0.09559891\n",
      "Iteration 105, loss = 0.09493726\n",
      "Iteration 106, loss = 0.09543811\n",
      "Iteration 107, loss = 0.09534634\n",
      "Iteration 108, loss = 0.09558071\n",
      "Iteration 109, loss = 0.09527589\n",
      "Iteration 110, loss = 0.09471712\n",
      "Iteration 111, loss = 0.09498886\n",
      "Iteration 112, loss = 0.09468102\n",
      "Iteration 113, loss = 0.09496552\n",
      "Iteration 114, loss = 0.09493083\n",
      "Iteration 115, loss = 0.09492682\n",
      "Iteration 116, loss = 0.09460849\n",
      "Iteration 117, loss = 0.09466647\n",
      "Iteration 118, loss = 0.09488370\n",
      "Iteration 119, loss = 0.09426380\n",
      "Iteration 120, loss = 0.09441571\n",
      "Iteration 121, loss = 0.09461129\n",
      "Iteration 122, loss = 0.09415697\n",
      "Iteration 123, loss = 0.09425340\n",
      "Iteration 124, loss = 0.09424057\n",
      "Iteration 125, loss = 0.09422676\n",
      "Iteration 126, loss = 0.09436338\n",
      "Iteration 127, loss = 0.09411541\n",
      "Iteration 128, loss = 0.09430529\n",
      "Iteration 129, loss = 0.09427410\n",
      "Iteration 130, loss = 0.09429366\n",
      "Iteration 131, loss = 0.09385140\n",
      "Iteration 132, loss = 0.09414190\n",
      "Iteration 133, loss = 0.09350641\n",
      "Iteration 134, loss = 0.09384825\n",
      "Iteration 135, loss = 0.09387780\n",
      "Iteration 136, loss = 0.09348321\n",
      "Iteration 137, loss = 0.09407054\n",
      "Iteration 138, loss = 0.09402179\n",
      "Iteration 139, loss = 0.09367658\n",
      "Iteration 140, loss = 0.09415820\n",
      "Iteration 141, loss = 0.09382438\n",
      "Iteration 142, loss = 0.09348164\n",
      "Iteration 143, loss = 0.09357426\n",
      "Iteration 144, loss = 0.09368202\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.01, batch_size=100, hidden_layer_sizes=(100,), solver=adam; total time= 2.0min\n",
      "Iteration 1, loss = 0.86548027\n",
      "Iteration 2, loss = 0.36689369\n",
      "Iteration 3, loss = 0.30228319\n",
      "Iteration 4, loss = 0.26817834\n",
      "Iteration 5, loss = 0.24482033\n",
      "Iteration 6, loss = 0.22691562\n",
      "Iteration 7, loss = 0.21268503\n",
      "Iteration 8, loss = 0.20085867\n",
      "Iteration 9, loss = 0.19054148\n",
      "Iteration 10, loss = 0.18196967\n",
      "Iteration 11, loss = 0.17365848\n",
      "Iteration 12, loss = 0.16776069\n",
      "Iteration 13, loss = 0.16139909\n",
      "Iteration 14, loss = 0.15552795\n",
      "Iteration 15, loss = 0.15123999\n",
      "Iteration 16, loss = 0.14681714\n",
      "Iteration 17, loss = 0.14353125\n",
      "Iteration 18, loss = 0.13908685\n",
      "Iteration 19, loss = 0.13626218\n",
      "Iteration 20, loss = 0.13310473\n",
      "Iteration 21, loss = 0.13064639\n",
      "Iteration 22, loss = 0.12807907\n",
      "Iteration 23, loss = 0.12603963\n",
      "Iteration 24, loss = 0.12413755\n",
      "Iteration 25, loss = 0.12171334\n",
      "Iteration 26, loss = 0.12054047\n",
      "Iteration 27, loss = 0.11884378\n",
      "Iteration 28, loss = 0.11730851\n",
      "Iteration 29, loss = 0.11648888\n",
      "Iteration 30, loss = 0.11501510\n",
      "Iteration 31, loss = 0.11407422\n",
      "Iteration 32, loss = 0.11290753\n",
      "Iteration 33, loss = 0.11196737\n",
      "Iteration 34, loss = 0.11061350\n",
      "Iteration 35, loss = 0.11015592\n",
      "Iteration 36, loss = 0.10942817\n",
      "Iteration 37, loss = 0.10859445\n",
      "Iteration 38, loss = 0.10796599\n",
      "Iteration 39, loss = 0.10688948\n",
      "Iteration 40, loss = 0.10668691\n",
      "Iteration 41, loss = 0.10648830\n",
      "Iteration 42, loss = 0.10544573\n",
      "Iteration 43, loss = 0.10540514\n",
      "Iteration 44, loss = 0.10512952\n",
      "Iteration 45, loss = 0.10418027\n",
      "Iteration 46, loss = 0.10404208\n",
      "Iteration 47, loss = 0.10389903\n",
      "Iteration 48, loss = 0.10290729\n",
      "Iteration 49, loss = 0.10276693\n",
      "Iteration 50, loss = 0.10235558\n",
      "Iteration 51, loss = 0.10221617\n",
      "Iteration 52, loss = 0.10196975\n",
      "Iteration 53, loss = 0.10155686\n",
      "Iteration 54, loss = 0.10126907\n",
      "Iteration 55, loss = 0.10122069\n",
      "Iteration 56, loss = 0.10107240\n",
      "Iteration 57, loss = 0.10065894\n",
      "Iteration 58, loss = 0.10019512\n",
      "Iteration 59, loss = 0.10031512\n",
      "Iteration 60, loss = 0.10069286\n",
      "Iteration 61, loss = 0.09960624\n",
      "Iteration 62, loss = 0.09970575\n",
      "Iteration 63, loss = 0.10003364\n",
      "Iteration 64, loss = 0.09930347\n",
      "Iteration 65, loss = 0.09949072\n",
      "Iteration 66, loss = 0.09970961\n",
      "Iteration 67, loss = 0.09901701\n",
      "Iteration 68, loss = 0.09898846\n",
      "Iteration 69, loss = 0.09869495\n",
      "Iteration 70, loss = 0.09846613\n",
      "Iteration 71, loss = 0.09805290\n",
      "Iteration 72, loss = 0.09876179\n",
      "Iteration 73, loss = 0.09782285\n",
      "Iteration 74, loss = 0.09848680\n",
      "Iteration 75, loss = 0.09827197\n",
      "Iteration 76, loss = 0.09789260\n",
      "Iteration 77, loss = 0.09807541\n",
      "Iteration 78, loss = 0.09760651\n",
      "Iteration 79, loss = 0.09764202\n",
      "Iteration 80, loss = 0.09736149\n",
      "Iteration 81, loss = 0.09726310\n",
      "Iteration 82, loss = 0.09744093\n",
      "Iteration 83, loss = 0.09722402\n",
      "Iteration 84, loss = 0.09709548\n",
      "Iteration 85, loss = 0.09686734\n",
      "Iteration 86, loss = 0.09715529\n",
      "Iteration 87, loss = 0.09652786\n",
      "Iteration 88, loss = 0.09704454\n",
      "Iteration 89, loss = 0.09739046\n",
      "Iteration 90, loss = 0.09643879\n",
      "Iteration 91, loss = 0.09682825\n",
      "Iteration 92, loss = 0.09662750\n",
      "Iteration 93, loss = 0.09623795\n",
      "Iteration 94, loss = 0.09634056\n",
      "Iteration 95, loss = 0.09646893\n",
      "Iteration 96, loss = 0.09592560\n",
      "Iteration 97, loss = 0.09655470\n",
      "Iteration 98, loss = 0.09602169\n",
      "Iteration 99, loss = 0.09575058\n",
      "Iteration 100, loss = 0.09592792\n",
      "Iteration 101, loss = 0.09579597\n",
      "Iteration 102, loss = 0.09569577\n",
      "Iteration 103, loss = 0.09564531\n",
      "Iteration 104, loss = 0.09558705\n",
      "Iteration 105, loss = 0.09604531\n",
      "Iteration 106, loss = 0.09565646\n",
      "Iteration 107, loss = 0.09535154\n",
      "Iteration 108, loss = 0.09560705\n",
      "Iteration 109, loss = 0.09584250\n",
      "Iteration 110, loss = 0.09518399\n",
      "Iteration 111, loss = 0.09553372\n",
      "Iteration 112, loss = 0.09528060\n",
      "Iteration 113, loss = 0.09513409\n",
      "Iteration 114, loss = 0.09511179\n",
      "Iteration 115, loss = 0.09533897\n",
      "Iteration 116, loss = 0.09549234\n",
      "Iteration 117, loss = 0.09530516\n",
      "Iteration 118, loss = 0.09503948\n",
      "Iteration 119, loss = 0.09516199\n",
      "Iteration 120, loss = 0.09547070\n",
      "Iteration 121, loss = 0.09482206\n",
      "Iteration 122, loss = 0.09487848\n",
      "Iteration 123, loss = 0.09474314\n",
      "Iteration 124, loss = 0.09455853\n",
      "Iteration 125, loss = 0.09503478\n",
      "Iteration 126, loss = 0.09463202\n",
      "Iteration 127, loss = 0.09486819\n",
      "Iteration 128, loss = 0.09473900\n",
      "Iteration 129, loss = 0.09449468\n",
      "Iteration 130, loss = 0.09477341\n",
      "Iteration 131, loss = 0.09463894\n",
      "Iteration 132, loss = 0.09452004\n",
      "Iteration 133, loss = 0.09419597\n",
      "Iteration 134, loss = 0.09438377\n",
      "Iteration 135, loss = 0.09440959\n",
      "Iteration 136, loss = 0.09466205\n",
      "Iteration 137, loss = 0.09447428\n",
      "Iteration 138, loss = 0.09433298\n",
      "Iteration 139, loss = 0.09441316\n",
      "Iteration 140, loss = 0.09441694\n",
      "Iteration 141, loss = 0.09426106\n",
      "Iteration 142, loss = 0.09413228\n",
      "Iteration 143, loss = 0.09455716\n",
      "Iteration 144, loss = 0.09400715\n",
      "Iteration 145, loss = 0.09382031\n",
      "Iteration 146, loss = 0.09381716\n",
      "Iteration 147, loss = 0.09398090\n",
      "Iteration 148, loss = 0.09380833\n",
      "Iteration 149, loss = 0.09377374\n",
      "Iteration 150, loss = 0.09371782\n",
      "Iteration 151, loss = 0.09402221\n",
      "Iteration 152, loss = 0.09361643\n",
      "Iteration 153, loss = 0.09415154\n",
      "Iteration 154, loss = 0.09401399\n",
      "Iteration 155, loss = 0.09371037\n",
      "Iteration 156, loss = 0.09373202\n",
      "Iteration 157, loss = 0.09367249\n",
      "Iteration 158, loss = 0.09366207\n",
      "Iteration 159, loss = 0.09381402\n",
      "Iteration 160, loss = 0.09424416\n",
      "Iteration 161, loss = 0.09354438\n",
      "Iteration 162, loss = 0.09407460\n",
      "Iteration 163, loss = 0.09354588\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.01, batch_size=100, hidden_layer_sizes=(100,), solver=adam; total time= 2.3min\n",
      "Iteration 1, loss = 0.88237516\n",
      "Iteration 2, loss = 0.37176712\n",
      "Iteration 3, loss = 0.30414026\n",
      "Iteration 4, loss = 0.27023960\n",
      "Iteration 5, loss = 0.24658563\n",
      "Iteration 6, loss = 0.22873852\n",
      "Iteration 7, loss = 0.21377524\n",
      "Iteration 8, loss = 0.20128023\n",
      "Iteration 9, loss = 0.19161784\n",
      "Iteration 10, loss = 0.18244186\n",
      "Iteration 11, loss = 0.17465304\n",
      "Iteration 12, loss = 0.16868733\n",
      "Iteration 13, loss = 0.16195694\n",
      "Iteration 14, loss = 0.15633354\n",
      "Iteration 15, loss = 0.15163387\n",
      "Iteration 16, loss = 0.14783767\n",
      "Iteration 17, loss = 0.14358835\n",
      "Iteration 18, loss = 0.14003052\n",
      "Iteration 19, loss = 0.13690905\n",
      "Iteration 20, loss = 0.13446061\n",
      "Iteration 21, loss = 0.13142263\n",
      "Iteration 22, loss = 0.12838462\n",
      "Iteration 23, loss = 0.12681254\n",
      "Iteration 24, loss = 0.12466138\n",
      "Iteration 25, loss = 0.12256466\n",
      "Iteration 26, loss = 0.12090875\n",
      "Iteration 27, loss = 0.11954698\n",
      "Iteration 28, loss = 0.11840600\n",
      "Iteration 29, loss = 0.11740668\n",
      "Iteration 30, loss = 0.11600411\n",
      "Iteration 31, loss = 0.11468149\n",
      "Iteration 32, loss = 0.11325221\n",
      "Iteration 33, loss = 0.11242347\n",
      "Iteration 34, loss = 0.11131319\n",
      "Iteration 35, loss = 0.11063798\n",
      "Iteration 36, loss = 0.10986771\n",
      "Iteration 37, loss = 0.10981144\n",
      "Iteration 38, loss = 0.10828511\n",
      "Iteration 39, loss = 0.10794339\n",
      "Iteration 40, loss = 0.10778164\n",
      "Iteration 41, loss = 0.10722636\n",
      "Iteration 42, loss = 0.10604302\n",
      "Iteration 43, loss = 0.10555034\n",
      "Iteration 44, loss = 0.10482512\n",
      "Iteration 45, loss = 0.10499201\n",
      "Iteration 46, loss = 0.10459841\n",
      "Iteration 47, loss = 0.10410757\n",
      "Iteration 48, loss = 0.10376633\n",
      "Iteration 49, loss = 0.10343974\n",
      "Iteration 50, loss = 0.10280637\n",
      "Iteration 51, loss = 0.10236506\n",
      "Iteration 52, loss = 0.10267776\n",
      "Iteration 53, loss = 0.10212573\n",
      "Iteration 54, loss = 0.10220317\n",
      "Iteration 55, loss = 0.10186700\n",
      "Iteration 56, loss = 0.10100013\n",
      "Iteration 57, loss = 0.10147730\n",
      "Iteration 58, loss = 0.10089565\n",
      "Iteration 59, loss = 0.10087910\n",
      "Iteration 60, loss = 0.10059001\n",
      "Iteration 61, loss = 0.10060922\n",
      "Iteration 62, loss = 0.10027567\n",
      "Iteration 63, loss = 0.10018110\n",
      "Iteration 64, loss = 0.10011398\n",
      "Iteration 65, loss = 0.10013625\n",
      "Iteration 66, loss = 0.09969453\n",
      "Iteration 67, loss = 0.09932143\n",
      "Iteration 68, loss = 0.09943932\n",
      "Iteration 69, loss = 0.09937197\n",
      "Iteration 70, loss = 0.09912084\n",
      "Iteration 71, loss = 0.09934751\n",
      "Iteration 72, loss = 0.09872992\n",
      "Iteration 73, loss = 0.09876212\n",
      "Iteration 74, loss = 0.09877652\n",
      "Iteration 75, loss = 0.09871466\n",
      "Iteration 76, loss = 0.09879510\n",
      "Iteration 77, loss = 0.09891068\n",
      "Iteration 78, loss = 0.09843720\n",
      "Iteration 79, loss = 0.09841573\n",
      "Iteration 80, loss = 0.09815432\n",
      "Iteration 81, loss = 0.09779488\n",
      "Iteration 82, loss = 0.09793884\n",
      "Iteration 83, loss = 0.09756563\n",
      "Iteration 84, loss = 0.09847105\n",
      "Iteration 85, loss = 0.09753095\n",
      "Iteration 86, loss = 0.09795170\n",
      "Iteration 87, loss = 0.09768318\n",
      "Iteration 88, loss = 0.09736333\n",
      "Iteration 89, loss = 0.09749825\n",
      "Iteration 90, loss = 0.09764645\n",
      "Iteration 91, loss = 0.09734037\n",
      "Iteration 92, loss = 0.09742903\n",
      "Iteration 93, loss = 0.09703996\n",
      "Iteration 94, loss = 0.09727208\n",
      "Iteration 95, loss = 0.09775392\n",
      "Iteration 96, loss = 0.09678190\n",
      "Iteration 97, loss = 0.09683481\n",
      "Iteration 98, loss = 0.09762550\n",
      "Iteration 99, loss = 0.09717349\n",
      "Iteration 100, loss = 0.09686238\n",
      "Iteration 101, loss = 0.09640423\n",
      "Iteration 102, loss = 0.09680095\n",
      "Iteration 103, loss = 0.09661548\n",
      "Iteration 104, loss = 0.09668739\n",
      "Iteration 105, loss = 0.09666167\n",
      "Iteration 106, loss = 0.09612091\n",
      "Iteration 107, loss = 0.09644382\n",
      "Iteration 108, loss = 0.09648778\n",
      "Iteration 109, loss = 0.09659715\n",
      "Iteration 110, loss = 0.09603133\n",
      "Iteration 111, loss = 0.09602796\n",
      "Iteration 112, loss = 0.09639867\n",
      "Iteration 113, loss = 0.09636491\n",
      "Iteration 114, loss = 0.09614349\n",
      "Iteration 115, loss = 0.09604323\n",
      "Iteration 116, loss = 0.09607435\n",
      "Iteration 117, loss = 0.09601243\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.01, batch_size=100, hidden_layer_sizes=(100,), solver=adam; total time= 1.6min\n",
      "Iteration 1, loss = 2.24148582\n",
      "Iteration 2, loss = 2.06838152\n",
      "Iteration 3, loss = 1.85934466\n",
      "Iteration 4, loss = 1.62443098\n",
      "Iteration 5, loss = 1.40189601\n",
      "Iteration 6, loss = 1.21556408\n",
      "Iteration 7, loss = 1.06884627\n",
      "Iteration 8, loss = 0.95519776\n",
      "Iteration 9, loss = 0.86646522\n",
      "Iteration 10, loss = 0.79612933\n",
      "Iteration 11, loss = 0.73920814\n",
      "Iteration 12, loss = 0.69253195\n",
      "Iteration 13, loss = 0.65372912\n",
      "Iteration 14, loss = 0.62089423\n",
      "Iteration 15, loss = 0.59296336\n",
      "Iteration 16, loss = 0.56882713\n",
      "Iteration 17, loss = 0.54777863\n",
      "Iteration 18, loss = 0.52940435\n",
      "Iteration 19, loss = 0.51295626\n",
      "Iteration 20, loss = 0.49860191\n",
      "Iteration 21, loss = 0.48557808\n",
      "Iteration 22, loss = 0.47398221\n",
      "Iteration 23, loss = 0.46341690\n",
      "Iteration 24, loss = 0.45389276\n",
      "Iteration 25, loss = 0.44523083\n",
      "Iteration 26, loss = 0.43727338\n",
      "Iteration 27, loss = 0.42996281\n",
      "Iteration 28, loss = 0.42320006\n",
      "Iteration 29, loss = 0.41701931\n",
      "Iteration 30, loss = 0.41120738\n",
      "Iteration 31, loss = 0.40574197\n",
      "Iteration 32, loss = 0.40087083\n",
      "Iteration 33, loss = 0.39605647\n",
      "Iteration 34, loss = 0.39169019\n",
      "Iteration 35, loss = 0.38751939\n",
      "Iteration 36, loss = 0.38366076\n",
      "Iteration 37, loss = 0.37985860\n",
      "Iteration 38, loss = 0.37640823\n",
      "Iteration 39, loss = 0.37307613\n",
      "Iteration 40, loss = 0.36992224\n",
      "Iteration 41, loss = 0.36689649\n",
      "Iteration 42, loss = 0.36400583\n",
      "Iteration 43, loss = 0.36127414\n",
      "Iteration 44, loss = 0.35864660\n",
      "Iteration 45, loss = 0.35608544\n",
      "Iteration 46, loss = 0.35360357\n",
      "Iteration 47, loss = 0.35131383\n",
      "Iteration 48, loss = 0.34915184\n",
      "Iteration 49, loss = 0.34692746\n",
      "Iteration 50, loss = 0.34481920\n",
      "Iteration 51, loss = 0.34288559\n",
      "Iteration 52, loss = 0.34095902\n",
      "Iteration 53, loss = 0.33901798\n",
      "Iteration 54, loss = 0.33718347\n",
      "Iteration 55, loss = 0.33542921\n",
      "Iteration 56, loss = 0.33376477\n",
      "Iteration 57, loss = 0.33209248\n",
      "Iteration 58, loss = 0.33047436\n",
      "Iteration 59, loss = 0.32893398\n",
      "Iteration 60, loss = 0.32739779\n",
      "Iteration 61, loss = 0.32587430\n",
      "Iteration 62, loss = 0.32448674\n",
      "Iteration 63, loss = 0.32303936\n",
      "Iteration 64, loss = 0.32165977\n",
      "Iteration 65, loss = 0.32032267\n",
      "Iteration 66, loss = 0.31901796\n",
      "Iteration 67, loss = 0.31769282\n",
      "Iteration 68, loss = 0.31642428\n",
      "Iteration 69, loss = 0.31523953\n",
      "Iteration 70, loss = 0.31397165\n",
      "Iteration 71, loss = 0.31282543\n",
      "Iteration 72, loss = 0.31165270\n",
      "Iteration 73, loss = 0.31051437\n",
      "Iteration 74, loss = 0.30946318\n",
      "Iteration 75, loss = 0.30830506\n",
      "Iteration 76, loss = 0.30728098\n",
      "Iteration 77, loss = 0.30613951\n",
      "Iteration 78, loss = 0.30512265\n",
      "Iteration 79, loss = 0.30411092\n",
      "Iteration 80, loss = 0.30312759\n",
      "Iteration 81, loss = 0.30215398\n",
      "Iteration 82, loss = 0.30117596\n",
      "Iteration 83, loss = 0.30019361\n",
      "Iteration 84, loss = 0.29924324\n",
      "Iteration 85, loss = 0.29831309\n",
      "Iteration 86, loss = 0.29740257\n",
      "Iteration 87, loss = 0.29648554\n",
      "Iteration 88, loss = 0.29558064\n",
      "Iteration 89, loss = 0.29476052\n",
      "Iteration 90, loss = 0.29384378\n",
      "Iteration 91, loss = 0.29304289\n",
      "Iteration 92, loss = 0.29215379\n",
      "Iteration 93, loss = 0.29127060\n",
      "Iteration 94, loss = 0.29050550\n",
      "Iteration 95, loss = 0.28971141\n",
      "Iteration 96, loss = 0.28886024\n",
      "Iteration 97, loss = 0.28808103\n",
      "Iteration 98, loss = 0.28725921\n",
      "Iteration 99, loss = 0.28650618\n",
      "Iteration 100, loss = 0.28574379\n",
      "Iteration 101, loss = 0.28494433\n",
      "Iteration 102, loss = 0.28423228\n",
      "Iteration 103, loss = 0.28343930\n",
      "Iteration 104, loss = 0.28272439\n",
      "Iteration 105, loss = 0.28198566\n",
      "Iteration 106, loss = 0.28130225\n",
      "Iteration 107, loss = 0.28053544\n",
      "Iteration 108, loss = 0.27979074\n",
      "Iteration 109, loss = 0.27907225\n",
      "Iteration 110, loss = 0.27842305\n",
      "Iteration 111, loss = 0.27767857\n",
      "Iteration 112, loss = 0.27702817\n",
      "Iteration 113, loss = 0.27631682\n",
      "Iteration 114, loss = 0.27564838\n",
      "Iteration 115, loss = 0.27495863\n",
      "Iteration 116, loss = 0.27428106\n",
      "Iteration 117, loss = 0.27362259\n",
      "Iteration 118, loss = 0.27298366\n",
      "Iteration 119, loss = 0.27228358\n",
      "Iteration 120, loss = 0.27164819\n",
      "Iteration 121, loss = 0.27098549\n",
      "Iteration 122, loss = 0.27042274\n",
      "Iteration 123, loss = 0.26974585\n",
      "Iteration 124, loss = 0.26906831\n",
      "Iteration 125, loss = 0.26847796\n",
      "Iteration 126, loss = 0.26781764\n",
      "Iteration 127, loss = 0.26721723\n",
      "Iteration 128, loss = 0.26664112\n",
      "Iteration 129, loss = 0.26597636\n",
      "Iteration 130, loss = 0.26538144\n",
      "Iteration 131, loss = 0.26482849\n",
      "Iteration 132, loss = 0.26418984\n",
      "Iteration 133, loss = 0.26353082\n",
      "Iteration 134, loss = 0.26302776\n",
      "Iteration 135, loss = 0.26235582\n",
      "Iteration 136, loss = 0.26180426\n",
      "Iteration 137, loss = 0.26124167\n",
      "Iteration 138, loss = 0.26070159\n",
      "Iteration 139, loss = 0.26009011\n",
      "Iteration 140, loss = 0.25954025\n",
      "Iteration 141, loss = 0.25893936\n",
      "Iteration 142, loss = 0.25838061\n",
      "Iteration 143, loss = 0.25784469\n",
      "Iteration 144, loss = 0.25725381\n",
      "Iteration 145, loss = 0.25668790\n",
      "Iteration 146, loss = 0.25612920\n",
      "Iteration 147, loss = 0.25558634\n",
      "Iteration 148, loss = 0.25507609\n",
      "Iteration 149, loss = 0.25453421\n",
      "Iteration 150, loss = 0.25398447\n",
      "Iteration 151, loss = 0.25344349\n",
      "Iteration 152, loss = 0.25290807\n",
      "Iteration 153, loss = 0.25233555\n",
      "Iteration 154, loss = 0.25183053\n",
      "Iteration 155, loss = 0.25131275\n",
      "Iteration 156, loss = 0.25073971\n",
      "Iteration 157, loss = 0.25029946\n",
      "Iteration 158, loss = 0.24971649\n",
      "Iteration 159, loss = 0.24921391\n",
      "Iteration 160, loss = 0.24868159\n",
      "Iteration 161, loss = 0.24818428\n",
      "Iteration 162, loss = 0.24767740\n",
      "Iteration 163, loss = 0.24715277\n",
      "Iteration 164, loss = 0.24665013\n",
      "Iteration 165, loss = 0.24618201\n",
      "Iteration 166, loss = 0.24567048\n",
      "Iteration 167, loss = 0.24510958\n",
      "Iteration 168, loss = 0.24470360\n",
      "Iteration 169, loss = 0.24415882\n",
      "Iteration 170, loss = 0.24365805\n",
      "Iteration 171, loss = 0.24318446\n",
      "Iteration 172, loss = 0.24269966\n",
      "Iteration 173, loss = 0.24224584\n",
      "Iteration 174, loss = 0.24174440\n",
      "Iteration 175, loss = 0.24123560\n",
      "Iteration 176, loss = 0.24084101\n",
      "Iteration 177, loss = 0.24028917\n",
      "Iteration 178, loss = 0.23983119\n",
      "Iteration 179, loss = 0.23939003\n",
      "Iteration 180, loss = 0.23891302\n",
      "Iteration 181, loss = 0.23840521\n",
      "Iteration 182, loss = 0.23795554\n",
      "Iteration 183, loss = 0.23751022\n",
      "Iteration 184, loss = 0.23705176\n",
      "Iteration 185, loss = 0.23658670\n",
      "Iteration 186, loss = 0.23610444\n",
      "Iteration 187, loss = 0.23571824\n",
      "Iteration 188, loss = 0.23523031\n",
      "Iteration 189, loss = 0.23481342\n",
      "Iteration 190, loss = 0.23433622\n",
      "Iteration 191, loss = 0.23388881\n",
      "Iteration 192, loss = 0.23343201\n",
      "Iteration 193, loss = 0.23298934\n",
      "Iteration 194, loss = 0.23258522\n",
      "Iteration 195, loss = 0.23208752\n",
      "Iteration 196, loss = 0.23167331\n",
      "Iteration 197, loss = 0.23126060\n",
      "Iteration 198, loss = 0.23086412\n",
      "Iteration 199, loss = 0.23042900\n",
      "Iteration 200, loss = 0.22999963\n",
      "[CV] END activation=logistic, alpha=0.01, batch_size=100, hidden_layer_sizes=(100,), solver=sgd; total time= 1.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.23772694\n",
      "Iteration 2, loss = 2.05913104\n",
      "Iteration 3, loss = 1.84482521\n",
      "Iteration 4, loss = 1.60437516\n",
      "Iteration 5, loss = 1.37783084\n",
      "Iteration 6, loss = 1.19061138\n",
      "Iteration 7, loss = 1.04533885\n",
      "Iteration 8, loss = 0.93411288\n",
      "Iteration 9, loss = 0.84830861\n",
      "Iteration 10, loss = 0.78077624\n",
      "Iteration 11, loss = 0.72674275\n",
      "Iteration 12, loss = 0.68259191\n",
      "Iteration 13, loss = 0.64588633\n",
      "Iteration 14, loss = 0.61477879\n",
      "Iteration 15, loss = 0.58829121\n",
      "Iteration 16, loss = 0.56532847\n",
      "Iteration 17, loss = 0.54514561\n",
      "Iteration 18, loss = 0.52745541\n",
      "Iteration 19, loss = 0.51173362\n",
      "Iteration 20, loss = 0.49769991\n",
      "Iteration 21, loss = 0.48518713\n",
      "Iteration 22, loss = 0.47380917\n",
      "Iteration 23, loss = 0.46346395\n",
      "Iteration 24, loss = 0.45412430\n",
      "Iteration 25, loss = 0.44552219\n",
      "Iteration 26, loss = 0.43763564\n",
      "Iteration 27, loss = 0.43043179\n",
      "Iteration 28, loss = 0.42372472\n",
      "Iteration 29, loss = 0.41761616\n",
      "Iteration 30, loss = 0.41177172\n",
      "Iteration 31, loss = 0.40652573\n",
      "Iteration 32, loss = 0.40140387\n",
      "Iteration 33, loss = 0.39678709\n",
      "Iteration 34, loss = 0.39232652\n",
      "Iteration 35, loss = 0.38818430\n",
      "Iteration 36, loss = 0.38429580\n",
      "Iteration 37, loss = 0.38048351\n",
      "Iteration 38, loss = 0.37702714\n",
      "Iteration 39, loss = 0.37371599\n",
      "Iteration 40, loss = 0.37059194\n",
      "Iteration 41, loss = 0.36748795\n",
      "Iteration 42, loss = 0.36461155\n",
      "Iteration 43, loss = 0.36184898\n",
      "Iteration 44, loss = 0.35923845\n",
      "Iteration 45, loss = 0.35672592\n",
      "Iteration 46, loss = 0.35431450\n",
      "Iteration 47, loss = 0.35196482\n",
      "Iteration 48, loss = 0.34967892\n",
      "Iteration 49, loss = 0.34757119\n",
      "Iteration 50, loss = 0.34548167\n",
      "Iteration 51, loss = 0.34342448\n",
      "Iteration 52, loss = 0.34145769\n",
      "Iteration 53, loss = 0.33965164\n",
      "Iteration 54, loss = 0.33776328\n",
      "Iteration 55, loss = 0.33607211\n",
      "Iteration 56, loss = 0.33436540\n",
      "Iteration 57, loss = 0.33267092\n",
      "Iteration 58, loss = 0.33109117\n",
      "Iteration 59, loss = 0.32945908\n",
      "Iteration 60, loss = 0.32797630\n",
      "Iteration 61, loss = 0.32647458\n",
      "Iteration 62, loss = 0.32499750\n",
      "Iteration 63, loss = 0.32358919\n",
      "Iteration 64, loss = 0.32218822\n",
      "Iteration 65, loss = 0.32084406\n",
      "Iteration 66, loss = 0.31955354\n",
      "Iteration 67, loss = 0.31825381\n",
      "Iteration 68, loss = 0.31697853\n",
      "Iteration 69, loss = 0.31576957\n",
      "Iteration 70, loss = 0.31453279\n",
      "Iteration 71, loss = 0.31335167\n",
      "Iteration 72, loss = 0.31215020\n",
      "Iteration 73, loss = 0.31105157\n",
      "Iteration 74, loss = 0.30989528\n",
      "Iteration 75, loss = 0.30878651\n",
      "Iteration 76, loss = 0.30775799\n",
      "Iteration 77, loss = 0.30664048\n",
      "Iteration 78, loss = 0.30559428\n",
      "Iteration 79, loss = 0.30454829\n",
      "Iteration 80, loss = 0.30355813\n",
      "Iteration 81, loss = 0.30255339\n",
      "Iteration 82, loss = 0.30155463\n",
      "Iteration 83, loss = 0.30060629\n",
      "Iteration 84, loss = 0.29964551\n",
      "Iteration 85, loss = 0.29873710\n",
      "Iteration 86, loss = 0.29778330\n",
      "Iteration 87, loss = 0.29684211\n",
      "Iteration 88, loss = 0.29597153\n",
      "Iteration 89, loss = 0.29501430\n",
      "Iteration 90, loss = 0.29416913\n",
      "Iteration 91, loss = 0.29332820\n",
      "Iteration 92, loss = 0.29246341\n",
      "Iteration 93, loss = 0.29159152\n",
      "Iteration 94, loss = 0.29070147\n",
      "Iteration 95, loss = 0.28994822\n",
      "Iteration 96, loss = 0.28910307\n",
      "Iteration 97, loss = 0.28823411\n",
      "Iteration 98, loss = 0.28752011\n",
      "Iteration 99, loss = 0.28668123\n",
      "Iteration 100, loss = 0.28587792\n",
      "Iteration 101, loss = 0.28510401\n",
      "Iteration 102, loss = 0.28433532\n",
      "Iteration 103, loss = 0.28351465\n",
      "Iteration 104, loss = 0.28278652\n",
      "Iteration 105, loss = 0.28200593\n",
      "Iteration 106, loss = 0.28128389\n",
      "Iteration 107, loss = 0.28056779\n",
      "Iteration 108, loss = 0.27980565\n",
      "Iteration 109, loss = 0.27904776\n",
      "Iteration 110, loss = 0.27832447\n",
      "Iteration 111, loss = 0.27765151\n",
      "Iteration 112, loss = 0.27692334\n",
      "Iteration 113, loss = 0.27625031\n",
      "Iteration 114, loss = 0.27552189\n",
      "Iteration 115, loss = 0.27480349\n",
      "Iteration 116, loss = 0.27412392\n",
      "Iteration 117, loss = 0.27343560\n",
      "Iteration 118, loss = 0.27274440\n",
      "Iteration 119, loss = 0.27209443\n",
      "Iteration 120, loss = 0.27141887\n",
      "Iteration 121, loss = 0.27075709\n",
      "Iteration 122, loss = 0.27008550\n",
      "Iteration 123, loss = 0.26943115\n",
      "Iteration 124, loss = 0.26880596\n",
      "Iteration 125, loss = 0.26813226\n",
      "Iteration 126, loss = 0.26747694\n",
      "Iteration 127, loss = 0.26688234\n",
      "Iteration 128, loss = 0.26619566\n",
      "Iteration 129, loss = 0.26556537\n",
      "Iteration 130, loss = 0.26489766\n",
      "Iteration 131, loss = 0.26432968\n",
      "Iteration 132, loss = 0.26369851\n",
      "Iteration 133, loss = 0.26310943\n",
      "Iteration 134, loss = 0.26247931\n",
      "Iteration 135, loss = 0.26189037\n",
      "Iteration 136, loss = 0.26129075\n",
      "Iteration 137, loss = 0.26063173\n",
      "Iteration 138, loss = 0.26002088\n",
      "Iteration 139, loss = 0.25947406\n",
      "Iteration 140, loss = 0.25889189\n",
      "Iteration 141, loss = 0.25828994\n",
      "Iteration 142, loss = 0.25765442\n",
      "Iteration 143, loss = 0.25714895\n",
      "Iteration 144, loss = 0.25650605\n",
      "Iteration 145, loss = 0.25599403\n",
      "Iteration 146, loss = 0.25540810\n",
      "Iteration 147, loss = 0.25481096\n",
      "Iteration 148, loss = 0.25430695\n",
      "Iteration 149, loss = 0.25367168\n",
      "Iteration 150, loss = 0.25317578\n",
      "Iteration 151, loss = 0.25256597\n",
      "Iteration 152, loss = 0.25205483\n",
      "Iteration 153, loss = 0.25149414\n",
      "Iteration 154, loss = 0.25097513\n",
      "Iteration 155, loss = 0.25037725\n",
      "Iteration 156, loss = 0.24990392\n",
      "Iteration 157, loss = 0.24930680\n",
      "Iteration 158, loss = 0.24875607\n",
      "Iteration 159, loss = 0.24825792\n",
      "Iteration 160, loss = 0.24768771\n",
      "Iteration 161, loss = 0.24719621\n",
      "Iteration 162, loss = 0.24666469\n",
      "Iteration 163, loss = 0.24614129\n",
      "Iteration 164, loss = 0.24562597\n",
      "Iteration 165, loss = 0.24511554\n",
      "Iteration 166, loss = 0.24462177\n",
      "Iteration 167, loss = 0.24411000\n",
      "Iteration 168, loss = 0.24358165\n",
      "Iteration 169, loss = 0.24308373\n",
      "Iteration 170, loss = 0.24258832\n",
      "Iteration 171, loss = 0.24207689\n",
      "Iteration 172, loss = 0.24158380\n",
      "Iteration 173, loss = 0.24107426\n",
      "Iteration 174, loss = 0.24063065\n",
      "Iteration 175, loss = 0.24008598\n",
      "Iteration 176, loss = 0.23965889\n",
      "Iteration 177, loss = 0.23917476\n",
      "Iteration 178, loss = 0.23870515\n",
      "Iteration 179, loss = 0.23820705\n",
      "Iteration 180, loss = 0.23770558\n",
      "Iteration 181, loss = 0.23726793\n",
      "Iteration 182, loss = 0.23676718\n",
      "Iteration 183, loss = 0.23629295\n",
      "Iteration 184, loss = 0.23582529\n",
      "Iteration 185, loss = 0.23538066\n",
      "Iteration 186, loss = 0.23494024\n",
      "Iteration 187, loss = 0.23446033\n",
      "Iteration 188, loss = 0.23400109\n",
      "Iteration 189, loss = 0.23355334\n",
      "Iteration 190, loss = 0.23305514\n",
      "Iteration 191, loss = 0.23264775\n",
      "Iteration 192, loss = 0.23220588\n",
      "Iteration 193, loss = 0.23172091\n",
      "Iteration 194, loss = 0.23135700\n",
      "Iteration 195, loss = 0.23089286\n",
      "Iteration 196, loss = 0.23044865\n",
      "Iteration 197, loss = 0.22999454\n",
      "Iteration 198, loss = 0.22958572\n",
      "Iteration 199, loss = 0.22916077\n",
      "Iteration 200, loss = 0.22869868\n",
      "[CV] END activation=logistic, alpha=0.01, batch_size=100, hidden_layer_sizes=(100,), solver=sgd; total time= 1.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.24091783\n",
      "Iteration 2, loss = 2.06909873\n",
      "Iteration 3, loss = 1.85803215\n",
      "Iteration 4, loss = 1.61539405\n",
      "Iteration 5, loss = 1.38178475\n",
      "Iteration 6, loss = 1.18775720\n",
      "Iteration 7, loss = 1.03847878\n",
      "Iteration 8, loss = 0.92579898\n",
      "Iteration 9, loss = 0.83996525\n",
      "Iteration 10, loss = 0.77326075\n",
      "Iteration 11, loss = 0.72007060\n",
      "Iteration 12, loss = 0.67680667\n",
      "Iteration 13, loss = 0.64073540\n",
      "Iteration 14, loss = 0.61051245\n",
      "Iteration 15, loss = 0.58461875\n",
      "Iteration 16, loss = 0.56218415\n",
      "Iteration 17, loss = 0.54253281\n",
      "Iteration 18, loss = 0.52511285\n",
      "Iteration 19, loss = 0.50970425\n",
      "Iteration 20, loss = 0.49590025\n",
      "Iteration 21, loss = 0.48352910\n",
      "Iteration 22, loss = 0.47228227\n",
      "Iteration 23, loss = 0.46211676\n",
      "Iteration 24, loss = 0.45276385\n",
      "Iteration 25, loss = 0.44424455\n",
      "Iteration 26, loss = 0.43646325\n",
      "Iteration 27, loss = 0.42916870\n",
      "Iteration 28, loss = 0.42254260\n",
      "Iteration 29, loss = 0.41625233\n",
      "Iteration 30, loss = 0.41056344\n",
      "Iteration 31, loss = 0.40509553\n",
      "Iteration 32, loss = 0.40004093\n",
      "Iteration 33, loss = 0.39530868\n",
      "Iteration 34, loss = 0.39089855\n",
      "Iteration 35, loss = 0.38671828\n",
      "Iteration 36, loss = 0.38269183\n",
      "Iteration 37, loss = 0.37893039\n",
      "Iteration 38, loss = 0.37540317\n",
      "Iteration 39, loss = 0.37196702\n",
      "Iteration 40, loss = 0.36879300\n",
      "Iteration 41, loss = 0.36576310\n",
      "Iteration 42, loss = 0.36281297\n",
      "Iteration 43, loss = 0.36002167\n",
      "Iteration 44, loss = 0.35734392\n",
      "Iteration 45, loss = 0.35470736\n",
      "Iteration 46, loss = 0.35230325\n",
      "Iteration 47, loss = 0.34992633\n",
      "Iteration 48, loss = 0.34759264\n",
      "Iteration 49, loss = 0.34544032\n",
      "Iteration 50, loss = 0.34330206\n",
      "Iteration 51, loss = 0.34128228\n",
      "Iteration 52, loss = 0.33924800\n",
      "Iteration 53, loss = 0.33735834\n",
      "Iteration 54, loss = 0.33547297\n",
      "Iteration 55, loss = 0.33371336\n",
      "Iteration 56, loss = 0.33195920\n",
      "Iteration 57, loss = 0.33024387\n",
      "Iteration 58, loss = 0.32858987\n",
      "Iteration 59, loss = 0.32695237\n",
      "Iteration 60, loss = 0.32538759\n",
      "Iteration 61, loss = 0.32387596\n",
      "Iteration 62, loss = 0.32240894\n",
      "Iteration 63, loss = 0.32097835\n",
      "Iteration 64, loss = 0.31951972\n",
      "Iteration 65, loss = 0.31815315\n",
      "Iteration 66, loss = 0.31679984\n",
      "Iteration 67, loss = 0.31545616\n",
      "Iteration 68, loss = 0.31416913\n",
      "Iteration 69, loss = 0.31288222\n",
      "Iteration 70, loss = 0.31163976\n",
      "Iteration 71, loss = 0.31041941\n",
      "Iteration 72, loss = 0.30918387\n",
      "Iteration 73, loss = 0.30807734\n",
      "Iteration 74, loss = 0.30689251\n",
      "Iteration 75, loss = 0.30575153\n",
      "Iteration 76, loss = 0.30465837\n",
      "Iteration 77, loss = 0.30356142\n",
      "Iteration 78, loss = 0.30250420\n",
      "Iteration 79, loss = 0.30144808\n",
      "Iteration 80, loss = 0.30040106\n",
      "Iteration 81, loss = 0.29934045\n",
      "Iteration 82, loss = 0.29830196\n",
      "Iteration 83, loss = 0.29728956\n",
      "Iteration 84, loss = 0.29632344\n",
      "Iteration 85, loss = 0.29536646\n",
      "Iteration 86, loss = 0.29443260\n",
      "Iteration 87, loss = 0.29346435\n",
      "Iteration 88, loss = 0.29252775\n",
      "Iteration 89, loss = 0.29162974\n",
      "Iteration 90, loss = 0.29069218\n",
      "Iteration 91, loss = 0.28977954\n",
      "Iteration 92, loss = 0.28886774\n",
      "Iteration 93, loss = 0.28801001\n",
      "Iteration 94, loss = 0.28718434\n",
      "Iteration 95, loss = 0.28627551\n",
      "Iteration 96, loss = 0.28549299\n",
      "Iteration 97, loss = 0.28454919\n",
      "Iteration 98, loss = 0.28374803\n",
      "Iteration 99, loss = 0.28289658\n",
      "Iteration 100, loss = 0.28210343\n",
      "Iteration 101, loss = 0.28127291\n",
      "Iteration 102, loss = 0.28047978\n",
      "Iteration 103, loss = 0.27966482\n",
      "Iteration 104, loss = 0.27889723\n",
      "Iteration 105, loss = 0.27811764\n",
      "Iteration 106, loss = 0.27735259\n",
      "Iteration 107, loss = 0.27654478\n",
      "Iteration 108, loss = 0.27581867\n",
      "Iteration 109, loss = 0.27502115\n",
      "Iteration 110, loss = 0.27430011\n",
      "Iteration 111, loss = 0.27352671\n",
      "Iteration 112, loss = 0.27271425\n",
      "Iteration 113, loss = 0.27210118\n",
      "Iteration 114, loss = 0.27132897\n",
      "Iteration 115, loss = 0.27061187\n",
      "Iteration 116, loss = 0.26990386\n",
      "Iteration 117, loss = 0.26920973\n",
      "Iteration 118, loss = 0.26844851\n",
      "Iteration 119, loss = 0.26779872\n",
      "Iteration 120, loss = 0.26715038\n",
      "Iteration 121, loss = 0.26641269\n",
      "Iteration 122, loss = 0.26567223\n",
      "Iteration 123, loss = 0.26507195\n",
      "Iteration 124, loss = 0.26439070\n",
      "Iteration 125, loss = 0.26369730\n",
      "Iteration 126, loss = 0.26302496\n",
      "Iteration 127, loss = 0.26237541\n",
      "Iteration 128, loss = 0.26171430\n",
      "Iteration 129, loss = 0.26104644\n",
      "Iteration 130, loss = 0.26036445\n",
      "Iteration 131, loss = 0.25974250\n",
      "Iteration 132, loss = 0.25914689\n",
      "Iteration 133, loss = 0.25843180\n",
      "Iteration 134, loss = 0.25781928\n",
      "Iteration 135, loss = 0.25717437\n",
      "Iteration 136, loss = 0.25655121\n",
      "Iteration 137, loss = 0.25595569\n",
      "Iteration 138, loss = 0.25536635\n",
      "Iteration 139, loss = 0.25474027\n",
      "Iteration 140, loss = 0.25406130\n",
      "Iteration 141, loss = 0.25349883\n",
      "Iteration 142, loss = 0.25288887\n",
      "Iteration 143, loss = 0.25225821\n",
      "Iteration 144, loss = 0.25165792\n",
      "Iteration 145, loss = 0.25108397\n",
      "Iteration 146, loss = 0.25050188\n",
      "Iteration 147, loss = 0.24995249\n",
      "Iteration 148, loss = 0.24934977\n",
      "Iteration 149, loss = 0.24873875\n",
      "Iteration 150, loss = 0.24817973\n",
      "Iteration 151, loss = 0.24763174\n",
      "Iteration 152, loss = 0.24703314\n",
      "Iteration 153, loss = 0.24649139\n",
      "Iteration 154, loss = 0.24590333\n",
      "Iteration 155, loss = 0.24534363\n",
      "Iteration 156, loss = 0.24481730\n",
      "Iteration 157, loss = 0.24425031\n",
      "Iteration 158, loss = 0.24371923\n",
      "Iteration 159, loss = 0.24316572\n",
      "Iteration 160, loss = 0.24263079\n",
      "Iteration 161, loss = 0.24206714\n",
      "Iteration 162, loss = 0.24157385\n",
      "Iteration 163, loss = 0.24097974\n",
      "Iteration 164, loss = 0.24050491\n",
      "Iteration 165, loss = 0.23998411\n",
      "Iteration 166, loss = 0.23947252\n",
      "Iteration 167, loss = 0.23893448\n",
      "Iteration 168, loss = 0.23840892\n",
      "Iteration 169, loss = 0.23792296\n",
      "Iteration 170, loss = 0.23741590\n",
      "Iteration 171, loss = 0.23690858\n",
      "Iteration 172, loss = 0.23637333\n",
      "Iteration 173, loss = 0.23588225\n",
      "Iteration 174, loss = 0.23543155\n",
      "Iteration 175, loss = 0.23488986\n",
      "Iteration 176, loss = 0.23439857\n",
      "Iteration 177, loss = 0.23394489\n",
      "Iteration 178, loss = 0.23341685\n",
      "Iteration 179, loss = 0.23292164\n",
      "Iteration 180, loss = 0.23249692\n",
      "Iteration 181, loss = 0.23199976\n",
      "Iteration 182, loss = 0.23152995\n",
      "Iteration 183, loss = 0.23105212\n",
      "Iteration 184, loss = 0.23055946\n",
      "Iteration 185, loss = 0.23011254\n",
      "Iteration 186, loss = 0.22962402\n",
      "Iteration 187, loss = 0.22921803\n",
      "Iteration 188, loss = 0.22872169\n",
      "Iteration 189, loss = 0.22827339\n",
      "Iteration 190, loss = 0.22784171\n",
      "Iteration 191, loss = 0.22737547\n",
      "Iteration 192, loss = 0.22695569\n",
      "Iteration 193, loss = 0.22647200\n",
      "Iteration 194, loss = 0.22603985\n",
      "Iteration 195, loss = 0.22560641\n",
      "Iteration 196, loss = 0.22517790\n",
      "Iteration 197, loss = 0.22475004\n",
      "Iteration 198, loss = 0.22431899\n",
      "Iteration 199, loss = 0.22385847\n",
      "Iteration 200, loss = 0.22345174\n",
      "[CV] END activation=logistic, alpha=0.01, batch_size=100, hidden_layer_sizes=(100,), solver=sgd; total time= 1.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.24006580\n",
      "Iteration 2, loss = 2.07026425\n",
      "Iteration 3, loss = 1.86118161\n",
      "Iteration 4, loss = 1.61857276\n",
      "Iteration 5, loss = 1.38597104\n",
      "Iteration 6, loss = 1.19463806\n",
      "Iteration 7, loss = 1.04762188\n",
      "Iteration 8, loss = 0.93623828\n",
      "Iteration 9, loss = 0.85061591\n",
      "Iteration 10, loss = 0.78340692\n",
      "Iteration 11, loss = 0.72956596\n",
      "Iteration 12, loss = 0.68553151\n",
      "Iteration 13, loss = 0.64872341\n",
      "Iteration 14, loss = 0.61762230\n",
      "Iteration 15, loss = 0.59096113\n",
      "Iteration 16, loss = 0.56786397\n",
      "Iteration 17, loss = 0.54767490\n",
      "Iteration 18, loss = 0.52981532\n",
      "Iteration 19, loss = 0.51396325\n",
      "Iteration 20, loss = 0.49971899\n",
      "Iteration 21, loss = 0.48706286\n",
      "Iteration 22, loss = 0.47559587\n",
      "Iteration 23, loss = 0.46510401\n",
      "Iteration 24, loss = 0.45561935\n",
      "Iteration 25, loss = 0.44697153\n",
      "Iteration 26, loss = 0.43889942\n",
      "Iteration 27, loss = 0.43169764\n",
      "Iteration 28, loss = 0.42489640\n",
      "Iteration 29, loss = 0.41862731\n",
      "Iteration 30, loss = 0.41278530\n",
      "Iteration 31, loss = 0.40734686\n",
      "Iteration 32, loss = 0.40227724\n",
      "Iteration 33, loss = 0.39746087\n",
      "Iteration 34, loss = 0.39306798\n",
      "Iteration 35, loss = 0.38881930\n",
      "Iteration 36, loss = 0.38489134\n",
      "Iteration 37, loss = 0.38116541\n",
      "Iteration 38, loss = 0.37761670\n",
      "Iteration 39, loss = 0.37423852\n",
      "Iteration 40, loss = 0.37104204\n",
      "Iteration 41, loss = 0.36798076\n",
      "Iteration 42, loss = 0.36508894\n",
      "Iteration 43, loss = 0.36227911\n",
      "Iteration 44, loss = 0.35959860\n",
      "Iteration 45, loss = 0.35705194\n",
      "Iteration 46, loss = 0.35460090\n",
      "Iteration 47, loss = 0.35227534\n",
      "Iteration 48, loss = 0.35001906\n",
      "Iteration 49, loss = 0.34789982\n",
      "Iteration 50, loss = 0.34573975\n",
      "Iteration 51, loss = 0.34373471\n",
      "Iteration 52, loss = 0.34181351\n",
      "Iteration 53, loss = 0.33991844\n",
      "Iteration 54, loss = 0.33812461\n",
      "Iteration 55, loss = 0.33620302\n",
      "Iteration 56, loss = 0.33467679\n",
      "Iteration 57, loss = 0.33296788\n",
      "Iteration 58, loss = 0.33135080\n",
      "Iteration 59, loss = 0.32978831\n",
      "Iteration 60, loss = 0.32822390\n",
      "Iteration 61, loss = 0.32677578\n",
      "Iteration 62, loss = 0.32528848\n",
      "Iteration 63, loss = 0.32389414\n",
      "Iteration 64, loss = 0.32253125\n",
      "Iteration 65, loss = 0.32120553\n",
      "Iteration 66, loss = 0.31985271\n",
      "Iteration 67, loss = 0.31858891\n",
      "Iteration 68, loss = 0.31730858\n",
      "Iteration 69, loss = 0.31605058\n",
      "Iteration 70, loss = 0.31489578\n",
      "Iteration 71, loss = 0.31371549\n",
      "Iteration 72, loss = 0.31257097\n",
      "Iteration 73, loss = 0.31144528\n",
      "Iteration 74, loss = 0.31029259\n",
      "Iteration 75, loss = 0.30923139\n",
      "Iteration 76, loss = 0.30817295\n",
      "Iteration 77, loss = 0.30712511\n",
      "Iteration 78, loss = 0.30610927\n",
      "Iteration 79, loss = 0.30506989\n",
      "Iteration 80, loss = 0.30404167\n",
      "Iteration 81, loss = 0.30310856\n",
      "Iteration 82, loss = 0.30212814\n",
      "Iteration 83, loss = 0.30117623\n",
      "Iteration 84, loss = 0.30025751\n",
      "Iteration 85, loss = 0.29929838\n",
      "Iteration 86, loss = 0.29840747\n",
      "Iteration 87, loss = 0.29751336\n",
      "Iteration 88, loss = 0.29655308\n",
      "Iteration 89, loss = 0.29574796\n",
      "Iteration 90, loss = 0.29485323\n",
      "Iteration 91, loss = 0.29400588\n",
      "Iteration 92, loss = 0.29317466\n",
      "Iteration 93, loss = 0.29235681\n",
      "Iteration 94, loss = 0.29146445\n",
      "Iteration 95, loss = 0.29071989\n",
      "Iteration 96, loss = 0.28988114\n",
      "Iteration 97, loss = 0.28914443\n",
      "Iteration 98, loss = 0.28833065\n",
      "Iteration 99, loss = 0.28753182\n",
      "Iteration 100, loss = 0.28680241\n",
      "Iteration 101, loss = 0.28596705\n",
      "Iteration 102, loss = 0.28528090\n",
      "Iteration 103, loss = 0.28447886\n",
      "Iteration 104, loss = 0.28382401\n",
      "Iteration 105, loss = 0.28301079\n",
      "Iteration 106, loss = 0.28232644\n",
      "Iteration 107, loss = 0.28157234\n",
      "Iteration 108, loss = 0.28083813\n",
      "Iteration 109, loss = 0.28015433\n",
      "Iteration 110, loss = 0.27945226\n",
      "Iteration 111, loss = 0.27873383\n",
      "Iteration 112, loss = 0.27806730\n",
      "Iteration 113, loss = 0.27738867\n",
      "Iteration 114, loss = 0.27668926\n",
      "Iteration 115, loss = 0.27602363\n",
      "Iteration 116, loss = 0.27534609\n",
      "Iteration 117, loss = 0.27466110\n",
      "Iteration 118, loss = 0.27400805\n",
      "Iteration 119, loss = 0.27335768\n",
      "Iteration 120, loss = 0.27273645\n",
      "Iteration 121, loss = 0.27203082\n",
      "Iteration 122, loss = 0.27138470\n",
      "Iteration 123, loss = 0.27072787\n",
      "Iteration 124, loss = 0.27009639\n",
      "Iteration 125, loss = 0.26948203\n",
      "Iteration 126, loss = 0.26887373\n",
      "Iteration 127, loss = 0.26823309\n",
      "Iteration 128, loss = 0.26761444\n",
      "Iteration 129, loss = 0.26701033\n",
      "Iteration 130, loss = 0.26630244\n",
      "Iteration 131, loss = 0.26581019\n",
      "Iteration 132, loss = 0.26517200\n",
      "Iteration 133, loss = 0.26455077\n",
      "Iteration 134, loss = 0.26393685\n",
      "Iteration 135, loss = 0.26336470\n",
      "Iteration 136, loss = 0.26278294\n",
      "Iteration 137, loss = 0.26218069\n",
      "Iteration 138, loss = 0.26161691\n",
      "Iteration 139, loss = 0.26095530\n",
      "Iteration 140, loss = 0.26040163\n",
      "Iteration 141, loss = 0.25985851\n",
      "Iteration 142, loss = 0.25924655\n",
      "Iteration 143, loss = 0.25866515\n",
      "Iteration 144, loss = 0.25810247\n",
      "Iteration 145, loss = 0.25755566\n",
      "Iteration 146, loss = 0.25701013\n",
      "Iteration 147, loss = 0.25639632\n",
      "Iteration 148, loss = 0.25586722\n",
      "Iteration 149, loss = 0.25531379\n",
      "Iteration 150, loss = 0.25474346\n",
      "Iteration 151, loss = 0.25417354\n",
      "Iteration 152, loss = 0.25365869\n",
      "Iteration 153, loss = 0.25308223\n",
      "Iteration 154, loss = 0.25256746\n",
      "Iteration 155, loss = 0.25202915\n",
      "Iteration 156, loss = 0.25147866\n",
      "Iteration 157, loss = 0.25094633\n",
      "Iteration 158, loss = 0.25043964\n",
      "Iteration 159, loss = 0.24985894\n",
      "Iteration 160, loss = 0.24927908\n",
      "Iteration 161, loss = 0.24873383\n",
      "Iteration 162, loss = 0.24831080\n",
      "Iteration 163, loss = 0.24778534\n",
      "Iteration 164, loss = 0.24723392\n",
      "Iteration 165, loss = 0.24673226\n",
      "Iteration 166, loss = 0.24623874\n",
      "Iteration 167, loss = 0.24570603\n",
      "Iteration 168, loss = 0.24525007\n",
      "Iteration 169, loss = 0.24469570\n",
      "Iteration 170, loss = 0.24415325\n",
      "Iteration 171, loss = 0.24372352\n",
      "Iteration 172, loss = 0.24320174\n",
      "Iteration 173, loss = 0.24267967\n",
      "Iteration 174, loss = 0.24216310\n",
      "Iteration 175, loss = 0.24172575\n",
      "Iteration 176, loss = 0.24119508\n",
      "Iteration 177, loss = 0.24074422\n",
      "Iteration 178, loss = 0.24021221\n",
      "Iteration 179, loss = 0.23977458\n",
      "Iteration 180, loss = 0.23927153\n",
      "Iteration 181, loss = 0.23880770\n",
      "Iteration 182, loss = 0.23828880\n",
      "Iteration 183, loss = 0.23784716\n",
      "Iteration 184, loss = 0.23735309\n",
      "Iteration 185, loss = 0.23684015\n",
      "Iteration 186, loss = 0.23647292\n",
      "Iteration 187, loss = 0.23591298\n",
      "Iteration 188, loss = 0.23553910\n",
      "Iteration 189, loss = 0.23505099\n",
      "Iteration 190, loss = 0.23460783\n",
      "Iteration 191, loss = 0.23413209\n",
      "Iteration 192, loss = 0.23368878\n",
      "Iteration 193, loss = 0.23320184\n",
      "Iteration 194, loss = 0.23275653\n",
      "Iteration 195, loss = 0.23230880\n",
      "Iteration 196, loss = 0.23185080\n",
      "Iteration 197, loss = 0.23145396\n",
      "Iteration 198, loss = 0.23100950\n",
      "Iteration 199, loss = 0.23048420\n",
      "Iteration 200, loss = 0.23011725\n",
      "[CV] END activation=logistic, alpha=0.01, batch_size=100, hidden_layer_sizes=(100,), solver=sgd; total time= 1.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.23814001\n",
      "Iteration 2, loss = 2.06293467\n",
      "Iteration 3, loss = 1.85363667\n",
      "Iteration 4, loss = 1.61544261\n",
      "Iteration 5, loss = 1.38610214\n",
      "Iteration 6, loss = 1.19415276\n",
      "Iteration 7, loss = 1.04489877\n",
      "Iteration 8, loss = 0.93127127\n",
      "Iteration 9, loss = 0.84398934\n",
      "Iteration 10, loss = 0.77566336\n",
      "Iteration 11, loss = 0.72135794\n",
      "Iteration 12, loss = 0.67713642\n",
      "Iteration 13, loss = 0.64053930\n",
      "Iteration 14, loss = 0.60977454\n",
      "Iteration 15, loss = 0.58370144\n",
      "Iteration 16, loss = 0.56137361\n",
      "Iteration 17, loss = 0.54176995\n",
      "Iteration 18, loss = 0.52457649\n",
      "Iteration 19, loss = 0.50948246\n",
      "Iteration 20, loss = 0.49604269\n",
      "Iteration 21, loss = 0.48391059\n",
      "Iteration 22, loss = 0.47300062\n",
      "Iteration 23, loss = 0.46316505\n",
      "Iteration 24, loss = 0.45418685\n",
      "Iteration 25, loss = 0.44605602\n",
      "Iteration 26, loss = 0.43849405\n",
      "Iteration 27, loss = 0.43153262\n",
      "Iteration 28, loss = 0.42514561\n",
      "Iteration 29, loss = 0.41914389\n",
      "Iteration 30, loss = 0.41368022\n",
      "Iteration 31, loss = 0.40841070\n",
      "Iteration 32, loss = 0.40357449\n",
      "Iteration 33, loss = 0.39906161\n",
      "Iteration 34, loss = 0.39478342\n",
      "Iteration 35, loss = 0.39083063\n",
      "Iteration 36, loss = 0.38694080\n",
      "Iteration 37, loss = 0.38334848\n",
      "Iteration 38, loss = 0.37989255\n",
      "Iteration 39, loss = 0.37665195\n",
      "Iteration 40, loss = 0.37359736\n",
      "Iteration 41, loss = 0.37067827\n",
      "Iteration 42, loss = 0.36784148\n",
      "Iteration 43, loss = 0.36512373\n",
      "Iteration 44, loss = 0.36249538\n",
      "Iteration 45, loss = 0.36008031\n",
      "Iteration 46, loss = 0.35761807\n",
      "Iteration 47, loss = 0.35541514\n",
      "Iteration 48, loss = 0.35320969\n",
      "Iteration 49, loss = 0.35103854\n",
      "Iteration 50, loss = 0.34898858\n",
      "Iteration 51, loss = 0.34708579\n",
      "Iteration 52, loss = 0.34507964\n",
      "Iteration 53, loss = 0.34325309\n",
      "Iteration 54, loss = 0.34143592\n",
      "Iteration 55, loss = 0.33971139\n",
      "Iteration 56, loss = 0.33795402\n",
      "Iteration 57, loss = 0.33639329\n",
      "Iteration 58, loss = 0.33477570\n",
      "Iteration 59, loss = 0.33323465\n",
      "Iteration 60, loss = 0.33171639\n",
      "Iteration 61, loss = 0.33021305\n",
      "Iteration 62, loss = 0.32883072\n",
      "Iteration 63, loss = 0.32745491\n",
      "Iteration 64, loss = 0.32603606\n",
      "Iteration 65, loss = 0.32474018\n",
      "Iteration 66, loss = 0.32345197\n",
      "Iteration 67, loss = 0.32213787\n",
      "Iteration 68, loss = 0.32088197\n",
      "Iteration 69, loss = 0.31971050\n",
      "Iteration 70, loss = 0.31845796\n",
      "Iteration 71, loss = 0.31742044\n",
      "Iteration 72, loss = 0.31626713\n",
      "Iteration 73, loss = 0.31508822\n",
      "Iteration 74, loss = 0.31401155\n",
      "Iteration 75, loss = 0.31291177\n",
      "Iteration 76, loss = 0.31184197\n",
      "Iteration 77, loss = 0.31082544\n",
      "Iteration 78, loss = 0.30978060\n",
      "Iteration 79, loss = 0.30879041\n",
      "Iteration 80, loss = 0.30775856\n",
      "Iteration 81, loss = 0.30682109\n",
      "Iteration 82, loss = 0.30585583\n",
      "Iteration 83, loss = 0.30493604\n",
      "Iteration 84, loss = 0.30397375\n",
      "Iteration 85, loss = 0.30302562\n",
      "Iteration 86, loss = 0.30216419\n",
      "Iteration 87, loss = 0.30129667\n",
      "Iteration 88, loss = 0.30039712\n",
      "Iteration 89, loss = 0.29948548\n",
      "Iteration 90, loss = 0.29866668\n",
      "Iteration 91, loss = 0.29784720\n",
      "Iteration 92, loss = 0.29701928\n",
      "Iteration 93, loss = 0.29619109\n",
      "Iteration 94, loss = 0.29535700\n",
      "Iteration 95, loss = 0.29451836\n",
      "Iteration 96, loss = 0.29376186\n",
      "Iteration 97, loss = 0.29297559\n",
      "Iteration 98, loss = 0.29217377\n",
      "Iteration 99, loss = 0.29139752\n",
      "Iteration 100, loss = 0.29064972\n",
      "Iteration 101, loss = 0.28987537\n",
      "Iteration 102, loss = 0.28913341\n",
      "Iteration 103, loss = 0.28836586\n",
      "Iteration 104, loss = 0.28768239\n",
      "Iteration 105, loss = 0.28690867\n",
      "Iteration 106, loss = 0.28624270\n",
      "Iteration 107, loss = 0.28547755\n",
      "Iteration 108, loss = 0.28481189\n",
      "Iteration 109, loss = 0.28409977\n",
      "Iteration 110, loss = 0.28336913\n",
      "Iteration 111, loss = 0.28268754\n",
      "Iteration 112, loss = 0.28196686\n",
      "Iteration 113, loss = 0.28134803\n",
      "Iteration 114, loss = 0.28063136\n",
      "Iteration 115, loss = 0.27998152\n",
      "Iteration 116, loss = 0.27932257\n",
      "Iteration 117, loss = 0.27861014\n",
      "Iteration 118, loss = 0.27796543\n",
      "Iteration 119, loss = 0.27734491\n",
      "Iteration 120, loss = 0.27670920\n",
      "Iteration 121, loss = 0.27602969\n",
      "Iteration 122, loss = 0.27537894\n",
      "Iteration 123, loss = 0.27477632\n",
      "Iteration 124, loss = 0.27413575\n",
      "Iteration 125, loss = 0.27347460\n",
      "Iteration 126, loss = 0.27282363\n",
      "Iteration 127, loss = 0.27227840\n",
      "Iteration 128, loss = 0.27159879\n",
      "Iteration 129, loss = 0.27100886\n",
      "Iteration 130, loss = 0.27039297\n",
      "Iteration 131, loss = 0.26982221\n",
      "Iteration 132, loss = 0.26919915\n",
      "Iteration 133, loss = 0.26855500\n",
      "Iteration 134, loss = 0.26797987\n",
      "Iteration 135, loss = 0.26737617\n",
      "Iteration 136, loss = 0.26676459\n",
      "Iteration 137, loss = 0.26623899\n",
      "Iteration 138, loss = 0.26560308\n",
      "Iteration 139, loss = 0.26504478\n",
      "Iteration 140, loss = 0.26445415\n",
      "Iteration 141, loss = 0.26387459\n",
      "Iteration 142, loss = 0.26329034\n",
      "Iteration 143, loss = 0.26273382\n",
      "Iteration 144, loss = 0.26217389\n",
      "Iteration 145, loss = 0.26158584\n",
      "Iteration 146, loss = 0.26104614\n",
      "Iteration 147, loss = 0.26047578\n",
      "Iteration 148, loss = 0.25988900\n",
      "Iteration 149, loss = 0.25935856\n",
      "Iteration 150, loss = 0.25879244\n",
      "Iteration 151, loss = 0.25822600\n",
      "Iteration 152, loss = 0.25766634\n",
      "Iteration 153, loss = 0.25719285\n",
      "Iteration 154, loss = 0.25659861\n",
      "Iteration 155, loss = 0.25603223\n",
      "Iteration 156, loss = 0.25545219\n",
      "Iteration 157, loss = 0.25502560\n",
      "Iteration 158, loss = 0.25440489\n",
      "Iteration 159, loss = 0.25388470\n",
      "Iteration 160, loss = 0.25338850\n",
      "Iteration 161, loss = 0.25282490\n",
      "Iteration 162, loss = 0.25234360\n",
      "Iteration 163, loss = 0.25175938\n",
      "Iteration 164, loss = 0.25120568\n",
      "Iteration 165, loss = 0.25073722\n",
      "Iteration 166, loss = 0.25025121\n",
      "Iteration 167, loss = 0.24973884\n",
      "Iteration 168, loss = 0.24917078\n",
      "Iteration 169, loss = 0.24867251\n",
      "Iteration 170, loss = 0.24817083\n",
      "Iteration 171, loss = 0.24762508\n",
      "Iteration 172, loss = 0.24715083\n",
      "Iteration 173, loss = 0.24664074\n",
      "Iteration 174, loss = 0.24613487\n",
      "Iteration 175, loss = 0.24568118\n",
      "Iteration 176, loss = 0.24514514\n",
      "Iteration 177, loss = 0.24468208\n",
      "Iteration 178, loss = 0.24417461\n",
      "Iteration 179, loss = 0.24367120\n",
      "Iteration 180, loss = 0.24318693\n",
      "Iteration 181, loss = 0.24268027\n",
      "Iteration 182, loss = 0.24221182\n",
      "Iteration 183, loss = 0.24172142\n",
      "Iteration 184, loss = 0.24124503\n",
      "Iteration 185, loss = 0.24080807\n",
      "Iteration 186, loss = 0.24030269\n",
      "Iteration 187, loss = 0.23984471\n",
      "Iteration 188, loss = 0.23936917\n",
      "Iteration 189, loss = 0.23889635\n",
      "Iteration 190, loss = 0.23841032\n",
      "Iteration 191, loss = 0.23794462\n",
      "Iteration 192, loss = 0.23755359\n",
      "Iteration 193, loss = 0.23705397\n",
      "Iteration 194, loss = 0.23659175\n",
      "Iteration 195, loss = 0.23614452\n",
      "Iteration 196, loss = 0.23567309\n",
      "Iteration 197, loss = 0.23523860\n",
      "Iteration 198, loss = 0.23477717\n",
      "Iteration 199, loss = 0.23433653\n",
      "Iteration 200, loss = 0.23388533\n",
      "[CV] END activation=logistic, alpha=0.01, batch_size=100, hidden_layer_sizes=(100,), solver=sgd; total time= 1.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.86037021\n",
      "Iteration 2, loss = 0.34616053\n",
      "Iteration 3, loss = 0.28530127\n",
      "Iteration 4, loss = 0.25139992\n",
      "Iteration 5, loss = 0.22641392\n",
      "Iteration 6, loss = 0.20558405\n",
      "Iteration 7, loss = 0.19109251\n",
      "Iteration 8, loss = 0.17931788\n",
      "Iteration 9, loss = 0.16936086\n",
      "Iteration 10, loss = 0.16141831\n",
      "Iteration 11, loss = 0.15626805\n",
      "Iteration 12, loss = 0.14945523\n",
      "Iteration 13, loss = 0.14486654\n",
      "Iteration 14, loss = 0.14043718\n",
      "Iteration 15, loss = 0.13798414\n",
      "Iteration 16, loss = 0.13403383\n",
      "Iteration 17, loss = 0.13261621\n",
      "Iteration 18, loss = 0.12879910\n",
      "Iteration 19, loss = 0.12684065\n",
      "Iteration 20, loss = 0.12602469\n",
      "Iteration 21, loss = 0.12363964\n",
      "Iteration 22, loss = 0.12248077\n",
      "Iteration 23, loss = 0.12151847\n",
      "Iteration 24, loss = 0.11914161\n",
      "Iteration 25, loss = 0.11689210\n",
      "Iteration 26, loss = 0.11558193\n",
      "Iteration 27, loss = 0.11555191\n",
      "Iteration 28, loss = 0.11458211\n",
      "Iteration 29, loss = 0.11406317\n",
      "Iteration 30, loss = 0.11129629\n",
      "Iteration 31, loss = 0.11267019\n",
      "Iteration 32, loss = 0.11285871\n",
      "Iteration 33, loss = 0.10979032\n",
      "Iteration 34, loss = 0.10892592\n",
      "Iteration 35, loss = 0.11067049\n",
      "Iteration 36, loss = 0.10791279\n",
      "Iteration 37, loss = 0.10887483\n",
      "Iteration 38, loss = 0.10703199\n",
      "Iteration 39, loss = 0.10528794\n",
      "Iteration 40, loss = 0.10384133\n",
      "Iteration 41, loss = 0.10553446\n",
      "Iteration 42, loss = 0.10475723\n",
      "Iteration 43, loss = 0.10400298\n",
      "Iteration 44, loss = 0.10406177\n",
      "Iteration 45, loss = 0.10300925\n",
      "Iteration 46, loss = 0.10328894\n",
      "Iteration 47, loss = 0.10093505\n",
      "Iteration 48, loss = 0.10295878\n",
      "Iteration 49, loss = 0.10268631\n",
      "Iteration 50, loss = 0.10126549\n",
      "Iteration 51, loss = 0.10140849\n",
      "Iteration 52, loss = 0.10057544\n",
      "Iteration 53, loss = 0.10156207\n",
      "Iteration 54, loss = 0.09874209\n",
      "Iteration 55, loss = 0.10109697\n",
      "Iteration 56, loss = 0.09944484\n",
      "Iteration 57, loss = 0.10046614\n",
      "Iteration 58, loss = 0.09725338\n",
      "Iteration 59, loss = 0.09878565\n",
      "Iteration 60, loss = 0.09977181\n",
      "Iteration 61, loss = 0.09828011\n",
      "Iteration 62, loss = 0.09732475\n",
      "Iteration 63, loss = 0.09749476\n",
      "Iteration 64, loss = 0.09591981\n",
      "Iteration 65, loss = 0.09772162\n",
      "Iteration 66, loss = 0.09631809\n",
      "Iteration 67, loss = 0.09750105\n",
      "Iteration 68, loss = 0.09617544\n",
      "Iteration 69, loss = 0.09590552\n",
      "Iteration 70, loss = 0.09524444\n",
      "Iteration 71, loss = 0.09422833\n",
      "Iteration 72, loss = 0.09504563\n",
      "Iteration 73, loss = 0.09425596\n",
      "Iteration 74, loss = 0.09676833\n",
      "Iteration 75, loss = 0.09469766\n",
      "Iteration 76, loss = 0.09472184\n",
      "Iteration 77, loss = 0.09348474\n",
      "Iteration 78, loss = 0.09320628\n",
      "Iteration 79, loss = 0.09323070\n",
      "Iteration 80, loss = 0.09388405\n",
      "Iteration 81, loss = 0.09325325\n",
      "Iteration 82, loss = 0.09380366\n",
      "Iteration 83, loss = 0.09283581\n",
      "Iteration 84, loss = 0.09393640\n",
      "Iteration 85, loss = 0.09247226\n",
      "Iteration 86, loss = 0.09295399\n",
      "Iteration 87, loss = 0.09316183\n",
      "Iteration 88, loss = 0.09229926\n",
      "Iteration 89, loss = 0.09336218\n",
      "Iteration 90, loss = 0.09340956\n",
      "Iteration 91, loss = 0.09100992\n",
      "Iteration 92, loss = 0.09073485\n",
      "Iteration 93, loss = 0.09166259\n",
      "Iteration 94, loss = 0.09201397\n",
      "Iteration 95, loss = 0.09205490\n",
      "Iteration 96, loss = 0.09137602\n",
      "Iteration 97, loss = 0.09217648\n",
      "Iteration 98, loss = 0.09130461\n",
      "Iteration 99, loss = 0.09154374\n",
      "Iteration 100, loss = 0.09106675\n",
      "Iteration 101, loss = 0.08997518\n",
      "Iteration 102, loss = 0.09157995\n",
      "Iteration 103, loss = 0.09135657\n",
      "Iteration 104, loss = 0.09134577\n",
      "Iteration 105, loss = 0.08974980\n",
      "Iteration 106, loss = 0.09074316\n",
      "Iteration 107, loss = 0.09244069\n",
      "Iteration 108, loss = 0.09091120\n",
      "Iteration 109, loss = 0.08976798\n",
      "Iteration 110, loss = 0.09112801\n",
      "Iteration 111, loss = 0.09023891\n",
      "Iteration 112, loss = 0.08950203\n",
      "Iteration 113, loss = 0.09064551\n",
      "Iteration 114, loss = 0.09046595\n",
      "Iteration 115, loss = 0.09049782\n",
      "Iteration 116, loss = 0.09017119\n",
      "Iteration 117, loss = 0.08809400\n",
      "Iteration 118, loss = 0.08860897\n",
      "Iteration 119, loss = 0.09010638\n",
      "Iteration 120, loss = 0.08957112\n",
      "Iteration 121, loss = 0.08903578\n",
      "Iteration 122, loss = 0.08979058\n",
      "Iteration 123, loss = 0.08909368\n",
      "Iteration 124, loss = 0.08912260\n",
      "Iteration 125, loss = 0.08841829\n",
      "Iteration 126, loss = 0.08947644\n",
      "Iteration 127, loss = 0.09001138\n",
      "Iteration 128, loss = 0.08902918\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.01, batch_size=100, hidden_layer_sizes=(400, 100), solver=adam; total time= 7.0min\n",
      "Iteration 1, loss = 0.87773108\n",
      "Iteration 2, loss = 0.34711159\n",
      "Iteration 3, loss = 0.28762252\n",
      "Iteration 4, loss = 0.25324921\n",
      "Iteration 5, loss = 0.23049746\n",
      "Iteration 6, loss = 0.21116102\n",
      "Iteration 7, loss = 0.19529785\n",
      "Iteration 8, loss = 0.18367268\n",
      "Iteration 9, loss = 0.17531317\n",
      "Iteration 10, loss = 0.16506169\n",
      "Iteration 11, loss = 0.15885675\n",
      "Iteration 12, loss = 0.15318305\n",
      "Iteration 13, loss = 0.14771213\n",
      "Iteration 14, loss = 0.14427336\n",
      "Iteration 15, loss = 0.14069372\n",
      "Iteration 16, loss = 0.13753085\n",
      "Iteration 17, loss = 0.13556187\n",
      "Iteration 18, loss = 0.13250289\n",
      "Iteration 19, loss = 0.13110276\n",
      "Iteration 20, loss = 0.12835958\n",
      "Iteration 21, loss = 0.12604185\n",
      "Iteration 22, loss = 0.12435655\n",
      "Iteration 23, loss = 0.12200691\n",
      "Iteration 24, loss = 0.12285117\n",
      "Iteration 25, loss = 0.12003767\n",
      "Iteration 26, loss = 0.11789516\n",
      "Iteration 27, loss = 0.11578085\n",
      "Iteration 28, loss = 0.11667298\n",
      "Iteration 29, loss = 0.11632760\n",
      "Iteration 30, loss = 0.11395490\n",
      "Iteration 31, loss = 0.11543848\n",
      "Iteration 32, loss = 0.11355276\n",
      "Iteration 33, loss = 0.11161327\n",
      "Iteration 34, loss = 0.11087850\n",
      "Iteration 35, loss = 0.10880153\n",
      "Iteration 36, loss = 0.10969730\n",
      "Iteration 37, loss = 0.10948183\n",
      "Iteration 38, loss = 0.10934049\n",
      "Iteration 39, loss = 0.10665707\n",
      "Iteration 40, loss = 0.10593241\n",
      "Iteration 41, loss = 0.10677675\n",
      "Iteration 42, loss = 0.10623869\n",
      "Iteration 43, loss = 0.10649784\n",
      "Iteration 44, loss = 0.10357281\n",
      "Iteration 45, loss = 0.10395162\n",
      "Iteration 46, loss = 0.10518185\n",
      "Iteration 47, loss = 0.10245143\n",
      "Iteration 48, loss = 0.10222729\n",
      "Iteration 49, loss = 0.10200518\n",
      "Iteration 50, loss = 0.10281247\n",
      "Iteration 51, loss = 0.10260294\n",
      "Iteration 52, loss = 0.10135200\n",
      "Iteration 53, loss = 0.10127395\n",
      "Iteration 54, loss = 0.10244736\n",
      "Iteration 55, loss = 0.09909925\n",
      "Iteration 56, loss = 0.10216643\n",
      "Iteration 57, loss = 0.09867469\n",
      "Iteration 58, loss = 0.09844074\n",
      "Iteration 59, loss = 0.10054958\n",
      "Iteration 60, loss = 0.09786550\n",
      "Iteration 61, loss = 0.10074814\n",
      "Iteration 62, loss = 0.10026950\n",
      "Iteration 63, loss = 0.09725922\n",
      "Iteration 64, loss = 0.09714616\n",
      "Iteration 65, loss = 0.09664896\n",
      "Iteration 66, loss = 0.09760841\n",
      "Iteration 67, loss = 0.09855503\n",
      "Iteration 68, loss = 0.09959218\n",
      "Iteration 69, loss = 0.09578960\n",
      "Iteration 70, loss = 0.09641062\n",
      "Iteration 71, loss = 0.09780852\n",
      "Iteration 72, loss = 0.09639927\n",
      "Iteration 73, loss = 0.09557036\n",
      "Iteration 74, loss = 0.09645773\n",
      "Iteration 75, loss = 0.09603016\n",
      "Iteration 76, loss = 0.09510874\n",
      "Iteration 77, loss = 0.09482024\n",
      "Iteration 78, loss = 0.09456042\n",
      "Iteration 79, loss = 0.09410990\n",
      "Iteration 80, loss = 0.09545406\n",
      "Iteration 81, loss = 0.09382762\n",
      "Iteration 82, loss = 0.09489425\n",
      "Iteration 83, loss = 0.09342682\n",
      "Iteration 84, loss = 0.09495092\n",
      "Iteration 85, loss = 0.09476828\n",
      "Iteration 86, loss = 0.09390938\n",
      "Iteration 87, loss = 0.09467573\n",
      "Iteration 88, loss = 0.09270617\n",
      "Iteration 89, loss = 0.09234763\n",
      "Iteration 90, loss = 0.09272703\n",
      "Iteration 91, loss = 0.09378673\n",
      "Iteration 92, loss = 0.09387695\n",
      "Iteration 93, loss = 0.09358429\n",
      "Iteration 94, loss = 0.09307339\n",
      "Iteration 95, loss = 0.09361544\n",
      "Iteration 96, loss = 0.09261931\n",
      "Iteration 97, loss = 0.09336598\n",
      "Iteration 98, loss = 0.09486243\n",
      "Iteration 99, loss = 0.09166617\n",
      "Iteration 100, loss = 0.09268598\n",
      "Iteration 101, loss = 0.09159993\n",
      "Iteration 102, loss = 0.09168363\n",
      "Iteration 103, loss = 0.09327018\n",
      "Iteration 104, loss = 0.09282302\n",
      "Iteration 105, loss = 0.09128760\n",
      "Iteration 106, loss = 0.09139846\n",
      "Iteration 107, loss = 0.09094830\n",
      "Iteration 108, loss = 0.09221132\n",
      "Iteration 109, loss = 0.09350186\n",
      "Iteration 110, loss = 0.09187101\n",
      "Iteration 111, loss = 0.09059357\n",
      "Iteration 112, loss = 0.09327107\n",
      "Iteration 113, loss = 0.09201860\n",
      "Iteration 114, loss = 0.09102893\n",
      "Iteration 115, loss = 0.09059011\n",
      "Iteration 116, loss = 0.09172724\n",
      "Iteration 117, loss = 0.09075813\n",
      "Iteration 118, loss = 0.09014177\n",
      "Iteration 119, loss = 0.09026323\n",
      "Iteration 120, loss = 0.09084683\n",
      "Iteration 121, loss = 0.09044919\n",
      "Iteration 122, loss = 0.09065427\n",
      "Iteration 123, loss = 0.09026485\n",
      "Iteration 124, loss = 0.08971899\n",
      "Iteration 125, loss = 0.09058830\n",
      "Iteration 126, loss = 0.09002035\n",
      "Iteration 127, loss = 0.09033316\n",
      "Iteration 128, loss = 0.09063976\n",
      "Iteration 129, loss = 0.08956868\n",
      "Iteration 130, loss = 0.09016294\n",
      "Iteration 131, loss = 0.09079810\n",
      "Iteration 132, loss = 0.08980019\n",
      "Iteration 133, loss = 0.08932847\n",
      "Iteration 134, loss = 0.08917339\n",
      "Iteration 135, loss = 0.09163987\n",
      "Iteration 136, loss = 0.08882482\n",
      "Iteration 137, loss = 0.08896977\n",
      "Iteration 138, loss = 0.08879259\n",
      "Iteration 139, loss = 0.08892832\n",
      "Iteration 140, loss = 0.09011369\n",
      "Iteration 141, loss = 0.08846044\n",
      "Iteration 142, loss = 0.08990075\n",
      "Iteration 143, loss = 0.09026048\n",
      "Iteration 144, loss = 0.08988046\n",
      "Iteration 145, loss = 0.08930646\n",
      "Iteration 146, loss = 0.08926797\n",
      "Iteration 147, loss = 0.09015024\n",
      "Iteration 148, loss = 0.08807376\n",
      "Iteration 149, loss = 0.08905013\n",
      "Iteration 150, loss = 0.08867550\n",
      "Iteration 151, loss = 0.08987906\n",
      "Iteration 152, loss = 0.08858320\n",
      "Iteration 153, loss = 0.08927785\n",
      "Iteration 154, loss = 0.08915526\n",
      "Iteration 155, loss = 0.08872333\n",
      "Iteration 156, loss = 0.08801130\n",
      "Iteration 157, loss = 0.09018500\n",
      "Iteration 158, loss = 0.08880910\n",
      "Iteration 159, loss = 0.08895812\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.01, batch_size=100, hidden_layer_sizes=(400, 100), solver=adam; total time= 8.6min\n",
      "Iteration 1, loss = 0.89127088\n",
      "Iteration 2, loss = 0.34963995\n",
      "Iteration 3, loss = 0.28824163\n",
      "Iteration 4, loss = 0.25207852\n",
      "Iteration 5, loss = 0.22730943\n",
      "Iteration 6, loss = 0.20777364\n",
      "Iteration 7, loss = 0.19271221\n",
      "Iteration 8, loss = 0.18168167\n",
      "Iteration 9, loss = 0.16986213\n",
      "Iteration 10, loss = 0.16394696\n",
      "Iteration 11, loss = 0.15544230\n",
      "Iteration 12, loss = 0.15004612\n",
      "Iteration 13, loss = 0.14551537\n",
      "Iteration 14, loss = 0.14072520\n",
      "Iteration 15, loss = 0.13926088\n",
      "Iteration 16, loss = 0.13434306\n",
      "Iteration 17, loss = 0.13212899\n",
      "Iteration 18, loss = 0.12870979\n",
      "Iteration 19, loss = 0.12766383\n",
      "Iteration 20, loss = 0.12496256\n",
      "Iteration 21, loss = 0.12378604\n",
      "Iteration 22, loss = 0.12221009\n",
      "Iteration 23, loss = 0.12042604\n",
      "Iteration 24, loss = 0.11777956\n",
      "Iteration 25, loss = 0.11620479\n",
      "Iteration 26, loss = 0.11556997\n",
      "Iteration 27, loss = 0.11426047\n",
      "Iteration 28, loss = 0.11465003\n",
      "Iteration 29, loss = 0.11242341\n",
      "Iteration 30, loss = 0.11216860\n",
      "Iteration 31, loss = 0.11116439\n",
      "Iteration 32, loss = 0.10853319\n",
      "Iteration 33, loss = 0.10850794\n",
      "Iteration 34, loss = 0.10842210\n",
      "Iteration 35, loss = 0.10778163\n",
      "Iteration 36, loss = 0.10874013\n",
      "Iteration 37, loss = 0.10694664\n",
      "Iteration 38, loss = 0.10525174\n",
      "Iteration 39, loss = 0.10640383\n",
      "Iteration 40, loss = 0.10404491\n",
      "Iteration 41, loss = 0.10267476\n",
      "Iteration 42, loss = 0.10496149\n",
      "Iteration 43, loss = 0.10344921\n",
      "Iteration 44, loss = 0.10299942\n",
      "Iteration 45, loss = 0.10255368\n",
      "Iteration 46, loss = 0.10218460\n",
      "Iteration 47, loss = 0.10171698\n",
      "Iteration 48, loss = 0.10113972\n",
      "Iteration 49, loss = 0.10047390\n",
      "Iteration 50, loss = 0.10029242\n",
      "Iteration 51, loss = 0.10171472\n",
      "Iteration 52, loss = 0.09876471\n",
      "Iteration 53, loss = 0.10008721\n",
      "Iteration 54, loss = 0.10037022\n",
      "Iteration 55, loss = 0.09804836\n",
      "Iteration 56, loss = 0.09812880\n",
      "Iteration 57, loss = 0.09900044\n",
      "Iteration 58, loss = 0.09949518\n",
      "Iteration 59, loss = 0.09720071\n",
      "Iteration 60, loss = 0.09564362\n",
      "Iteration 61, loss = 0.09817443\n",
      "Iteration 62, loss = 0.09675981\n",
      "Iteration 63, loss = 0.09630505\n",
      "Iteration 64, loss = 0.09558103\n",
      "Iteration 65, loss = 0.09561318\n",
      "Iteration 66, loss = 0.09617377\n",
      "Iteration 67, loss = 0.09547815\n",
      "Iteration 68, loss = 0.09750060\n",
      "Iteration 69, loss = 0.09347860\n",
      "Iteration 70, loss = 0.09498479\n",
      "Iteration 71, loss = 0.09583718\n",
      "Iteration 72, loss = 0.09388443\n",
      "Iteration 73, loss = 0.09358361\n",
      "Iteration 74, loss = 0.09436534\n",
      "Iteration 75, loss = 0.09355102\n",
      "Iteration 76, loss = 0.09490297\n",
      "Iteration 77, loss = 0.09341051\n",
      "Iteration 78, loss = 0.09319833\n",
      "Iteration 79, loss = 0.09358434\n",
      "Iteration 80, loss = 0.09358579\n",
      "Iteration 81, loss = 0.09229080\n",
      "Iteration 82, loss = 0.09312051\n",
      "Iteration 83, loss = 0.09270225\n",
      "Iteration 84, loss = 0.09519889\n",
      "Iteration 85, loss = 0.09221025\n",
      "Iteration 86, loss = 0.09293086\n",
      "Iteration 87, loss = 0.09313697\n",
      "Iteration 88, loss = 0.09339389\n",
      "Iteration 89, loss = 0.09080248\n",
      "Iteration 90, loss = 0.09036302\n",
      "Iteration 91, loss = 0.09453359\n",
      "Iteration 92, loss = 0.09265502\n",
      "Iteration 93, loss = 0.09146737\n",
      "Iteration 94, loss = 0.09177785\n",
      "Iteration 95, loss = 0.09071119\n",
      "Iteration 96, loss = 0.09147976\n",
      "Iteration 97, loss = 0.09179038\n",
      "Iteration 98, loss = 0.09148954\n",
      "Iteration 99, loss = 0.09036997\n",
      "Iteration 100, loss = 0.09105707\n",
      "Iteration 101, loss = 0.09177957\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.01, batch_size=100, hidden_layer_sizes=(400, 100), solver=adam; total time= 5.5min\n",
      "Iteration 1, loss = 0.85836854\n",
      "Iteration 2, loss = 0.34300785\n",
      "Iteration 3, loss = 0.28355981\n",
      "Iteration 4, loss = 0.24849072\n",
      "Iteration 5, loss = 0.22255647\n",
      "Iteration 6, loss = 0.20518082\n",
      "Iteration 7, loss = 0.19106638\n",
      "Iteration 8, loss = 0.17978678\n",
      "Iteration 9, loss = 0.16939524\n",
      "Iteration 10, loss = 0.16203760\n",
      "Iteration 11, loss = 0.15738084\n",
      "Iteration 12, loss = 0.15040002\n",
      "Iteration 13, loss = 0.14601948\n",
      "Iteration 14, loss = 0.14245734\n",
      "Iteration 15, loss = 0.13730098\n",
      "Iteration 16, loss = 0.13466387\n",
      "Iteration 17, loss = 0.13364550\n",
      "Iteration 18, loss = 0.12865194\n",
      "Iteration 19, loss = 0.12776081\n",
      "Iteration 20, loss = 0.12761688\n",
      "Iteration 21, loss = 0.12386554\n",
      "Iteration 22, loss = 0.12197948\n",
      "Iteration 23, loss = 0.12173298\n",
      "Iteration 24, loss = 0.11960489\n",
      "Iteration 25, loss = 0.11795063\n",
      "Iteration 26, loss = 0.11515203\n",
      "Iteration 27, loss = 0.11716469\n",
      "Iteration 28, loss = 0.11391672\n",
      "Iteration 29, loss = 0.11349299\n",
      "Iteration 30, loss = 0.11215235\n",
      "Iteration 31, loss = 0.11338299\n",
      "Iteration 32, loss = 0.11215057\n",
      "Iteration 33, loss = 0.10903473\n",
      "Iteration 34, loss = 0.10886268\n",
      "Iteration 35, loss = 0.11018285\n",
      "Iteration 36, loss = 0.10768256\n",
      "Iteration 37, loss = 0.10736586\n",
      "Iteration 38, loss = 0.10610068\n",
      "Iteration 39, loss = 0.10806221\n",
      "Iteration 40, loss = 0.10686369\n",
      "Iteration 41, loss = 0.10540837\n",
      "Iteration 42, loss = 0.10376474\n",
      "Iteration 43, loss = 0.10465027\n",
      "Iteration 44, loss = 0.10383442\n",
      "Iteration 45, loss = 0.10235428\n",
      "Iteration 46, loss = 0.10229679\n",
      "Iteration 47, loss = 0.10146658\n",
      "Iteration 48, loss = 0.10241148\n",
      "Iteration 49, loss = 0.10308488\n",
      "Iteration 50, loss = 0.10215103\n",
      "Iteration 51, loss = 0.10059087\n",
      "Iteration 52, loss = 0.10076381\n",
      "Iteration 53, loss = 0.10030383\n",
      "Iteration 54, loss = 0.09910286\n",
      "Iteration 55, loss = 0.09989057\n",
      "Iteration 56, loss = 0.09801286\n",
      "Iteration 57, loss = 0.10146127\n",
      "Iteration 58, loss = 0.09915521\n",
      "Iteration 59, loss = 0.09841385\n",
      "Iteration 60, loss = 0.09718273\n",
      "Iteration 61, loss = 0.09686233\n",
      "Iteration 62, loss = 0.09625589\n",
      "Iteration 63, loss = 0.09723147\n",
      "Iteration 64, loss = 0.09692285\n",
      "Iteration 65, loss = 0.09680670\n",
      "Iteration 66, loss = 0.09592084\n",
      "Iteration 67, loss = 0.09689826\n",
      "Iteration 68, loss = 0.09668453\n",
      "Iteration 69, loss = 0.09554242\n",
      "Iteration 70, loss = 0.09669597\n",
      "Iteration 71, loss = 0.09607824\n",
      "Iteration 72, loss = 0.09563948\n",
      "Iteration 73, loss = 0.09445103\n",
      "Iteration 74, loss = 0.09466569\n",
      "Iteration 75, loss = 0.09591760\n",
      "Iteration 76, loss = 0.09446522\n",
      "Iteration 77, loss = 0.09380042\n",
      "Iteration 78, loss = 0.09442999\n",
      "Iteration 79, loss = 0.09401794\n",
      "Iteration 80, loss = 0.09432841\n",
      "Iteration 81, loss = 0.09401964\n",
      "Iteration 82, loss = 0.09447930\n",
      "Iteration 83, loss = 0.09277293\n",
      "Iteration 84, loss = 0.09410792\n",
      "Iteration 85, loss = 0.09374156\n",
      "Iteration 86, loss = 0.09261743\n",
      "Iteration 87, loss = 0.09271232\n",
      "Iteration 88, loss = 0.09376184\n",
      "Iteration 89, loss = 0.09225249\n",
      "Iteration 90, loss = 0.09079979\n",
      "Iteration 91, loss = 0.09152832\n",
      "Iteration 92, loss = 0.09276389\n",
      "Iteration 93, loss = 0.09274914\n",
      "Iteration 94, loss = 0.09160385\n",
      "Iteration 95, loss = 0.09126544\n",
      "Iteration 96, loss = 0.09094439\n",
      "Iteration 97, loss = 0.09203974\n",
      "Iteration 98, loss = 0.09238027\n",
      "Iteration 99, loss = 0.09137482\n",
      "Iteration 100, loss = 0.09038257\n",
      "Iteration 101, loss = 0.09248590\n",
      "Iteration 102, loss = 0.09180471\n",
      "Iteration 103, loss = 0.09097287\n",
      "Iteration 104, loss = 0.08999903\n",
      "Iteration 105, loss = 0.09188532\n",
      "Iteration 106, loss = 0.09085320\n",
      "Iteration 107, loss = 0.09148428\n",
      "Iteration 108, loss = 0.09101337\n",
      "Iteration 109, loss = 0.08979345\n",
      "Iteration 110, loss = 0.08950711\n",
      "Iteration 111, loss = 0.09112641\n",
      "Iteration 112, loss = 0.09094948\n",
      "Iteration 113, loss = 0.08941931\n",
      "Iteration 114, loss = 0.08870833\n",
      "Iteration 115, loss = 0.08940164\n",
      "Iteration 116, loss = 0.09127068\n",
      "Iteration 117, loss = 0.08924715\n",
      "Iteration 118, loss = 0.08997847\n",
      "Iteration 119, loss = 0.09076104\n",
      "Iteration 120, loss = 0.08999982\n",
      "Iteration 121, loss = 0.08999505\n",
      "Iteration 122, loss = 0.08979157\n",
      "Iteration 123, loss = 0.08976418\n",
      "Iteration 124, loss = 0.08938632\n",
      "Iteration 125, loss = 0.08905265\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.01, batch_size=100, hidden_layer_sizes=(400, 100), solver=adam; total time= 6.8min\n",
      "Iteration 1, loss = 0.86964652\n",
      "Iteration 2, loss = 0.35300929\n",
      "Iteration 3, loss = 0.29396193\n",
      "Iteration 4, loss = 0.25870773\n",
      "Iteration 5, loss = 0.23356225\n",
      "Iteration 6, loss = 0.21327134\n",
      "Iteration 7, loss = 0.19937848\n",
      "Iteration 8, loss = 0.18361266\n",
      "Iteration 9, loss = 0.17361569\n",
      "Iteration 10, loss = 0.16713363\n",
      "Iteration 11, loss = 0.15829045\n",
      "Iteration 12, loss = 0.15274434\n",
      "Iteration 13, loss = 0.14902384\n",
      "Iteration 14, loss = 0.14521636\n",
      "Iteration 15, loss = 0.14108080\n",
      "Iteration 16, loss = 0.13760542\n",
      "Iteration 17, loss = 0.13345232\n",
      "Iteration 18, loss = 0.13247851\n",
      "Iteration 19, loss = 0.13041837\n",
      "Iteration 20, loss = 0.12853255\n",
      "Iteration 21, loss = 0.12453754\n",
      "Iteration 22, loss = 0.12547923\n",
      "Iteration 23, loss = 0.12258881\n",
      "Iteration 24, loss = 0.12122102\n",
      "Iteration 25, loss = 0.11775031\n",
      "Iteration 26, loss = 0.11851033\n",
      "Iteration 27, loss = 0.11718383\n",
      "Iteration 28, loss = 0.11640514\n",
      "Iteration 29, loss = 0.11309377\n",
      "Iteration 30, loss = 0.11398968\n",
      "Iteration 31, loss = 0.11296373\n",
      "Iteration 32, loss = 0.11174276\n",
      "Iteration 33, loss = 0.11453390\n",
      "Iteration 34, loss = 0.11151984\n",
      "Iteration 35, loss = 0.10978111\n",
      "Iteration 36, loss = 0.10849628\n",
      "Iteration 37, loss = 0.10928361\n",
      "Iteration 38, loss = 0.10952693\n",
      "Iteration 39, loss = 0.10838228\n",
      "Iteration 40, loss = 0.10660868\n",
      "Iteration 41, loss = 0.10611069\n",
      "Iteration 42, loss = 0.10598506\n",
      "Iteration 43, loss = 0.10504630\n",
      "Iteration 44, loss = 0.10692836\n",
      "Iteration 45, loss = 0.10404148\n",
      "Iteration 46, loss = 0.10434345\n",
      "Iteration 47, loss = 0.10283367\n",
      "Iteration 48, loss = 0.10261239\n",
      "Iteration 49, loss = 0.10323338\n",
      "Iteration 50, loss = 0.10323632\n",
      "Iteration 51, loss = 0.10151675\n",
      "Iteration 52, loss = 0.10228652\n",
      "Iteration 53, loss = 0.10322123\n",
      "Iteration 54, loss = 0.10136725\n",
      "Iteration 55, loss = 0.10097175\n",
      "Iteration 56, loss = 0.10175265\n",
      "Iteration 57, loss = 0.09915924\n",
      "Iteration 58, loss = 0.09877130\n",
      "Iteration 59, loss = 0.10083313\n",
      "Iteration 60, loss = 0.09973298\n",
      "Iteration 61, loss = 0.10201259\n",
      "Iteration 62, loss = 0.09923824\n",
      "Iteration 63, loss = 0.09683776\n",
      "Iteration 64, loss = 0.09768646\n",
      "Iteration 65, loss = 0.09893429\n",
      "Iteration 66, loss = 0.09829981\n",
      "Iteration 67, loss = 0.09829307\n",
      "Iteration 68, loss = 0.09759874\n",
      "Iteration 69, loss = 0.09634142\n",
      "Iteration 70, loss = 0.09684482\n",
      "Iteration 71, loss = 0.09644623\n",
      "Iteration 72, loss = 0.09758717\n",
      "Iteration 73, loss = 0.09550760\n",
      "Iteration 74, loss = 0.09598303\n",
      "Iteration 75, loss = 0.09592143\n",
      "Iteration 76, loss = 0.09669024\n",
      "Iteration 77, loss = 0.09596962\n",
      "Iteration 78, loss = 0.09515357\n",
      "Iteration 79, loss = 0.09438509\n",
      "Iteration 80, loss = 0.09689963\n",
      "Iteration 81, loss = 0.09400505\n",
      "Iteration 82, loss = 0.09445467\n",
      "Iteration 83, loss = 0.09534556\n",
      "Iteration 84, loss = 0.09556882\n",
      "Iteration 85, loss = 0.09335797\n",
      "Iteration 86, loss = 0.09421148\n",
      "Iteration 87, loss = 0.09474529\n",
      "Iteration 88, loss = 0.09436908\n",
      "Iteration 89, loss = 0.09248874\n",
      "Iteration 90, loss = 0.09260188\n",
      "Iteration 91, loss = 0.09256676\n",
      "Iteration 92, loss = 0.09381349\n",
      "Iteration 93, loss = 0.09315884\n",
      "Iteration 94, loss = 0.09394002\n",
      "Iteration 95, loss = 0.09204781\n",
      "Iteration 96, loss = 0.09260305\n",
      "Iteration 97, loss = 0.09294552\n",
      "Iteration 98, loss = 0.09255921\n",
      "Iteration 99, loss = 0.09168917\n",
      "Iteration 100, loss = 0.09207035\n",
      "Iteration 101, loss = 0.09356253\n",
      "Iteration 102, loss = 0.09336034\n",
      "Iteration 103, loss = 0.09200219\n",
      "Iteration 104, loss = 0.09207380\n",
      "Iteration 105, loss = 0.09195930\n",
      "Iteration 106, loss = 0.09110771\n",
      "Iteration 107, loss = 0.09159794\n",
      "Iteration 108, loss = 0.09142611\n",
      "Iteration 109, loss = 0.09121516\n",
      "Iteration 110, loss = 0.09171661\n",
      "Iteration 111, loss = 0.09193524\n",
      "Iteration 112, loss = 0.09057947\n",
      "Iteration 113, loss = 0.09006976\n",
      "Iteration 114, loss = 0.09123621\n",
      "Iteration 115, loss = 0.09217327\n",
      "Iteration 116, loss = 0.09002661\n",
      "Iteration 117, loss = 0.09029950\n",
      "Iteration 118, loss = 0.09053416\n",
      "Iteration 119, loss = 0.09121709\n",
      "Iteration 120, loss = 0.08920659\n",
      "Iteration 121, loss = 0.09143926\n",
      "Iteration 122, loss = 0.09198950\n",
      "Iteration 123, loss = 0.08997139\n",
      "Iteration 124, loss = 0.08991668\n",
      "Iteration 125, loss = 0.09033578\n",
      "Iteration 126, loss = 0.09004828\n",
      "Iteration 127, loss = 0.09002854\n",
      "Iteration 128, loss = 0.09035857\n",
      "Iteration 129, loss = 0.08938992\n",
      "Iteration 130, loss = 0.08981783\n",
      "Iteration 131, loss = 0.08993196\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.01, batch_size=100, hidden_layer_sizes=(400, 100), solver=adam; total time= 7.1min\n",
      "Iteration 1, loss = 2.31167242\n",
      "Iteration 2, loss = 2.30364704\n",
      "Iteration 3, loss = 2.29686107\n",
      "Iteration 4, loss = 2.28888932\n",
      "Iteration 5, loss = 2.27992716\n",
      "Iteration 6, loss = 2.26886556\n",
      "Iteration 7, loss = 2.25473214\n",
      "Iteration 8, loss = 2.23612740\n",
      "Iteration 9, loss = 2.21143737\n",
      "Iteration 10, loss = 2.17713185\n",
      "Iteration 11, loss = 2.12930868\n",
      "Iteration 12, loss = 2.06328893\n",
      "Iteration 13, loss = 1.97611527\n",
      "Iteration 14, loss = 1.86950069\n",
      "Iteration 15, loss = 1.75077360\n",
      "Iteration 16, loss = 1.62843889\n",
      "Iteration 17, loss = 1.51053517\n",
      "Iteration 18, loss = 1.40223220\n",
      "Iteration 19, loss = 1.30417518\n",
      "Iteration 20, loss = 1.21601572\n",
      "Iteration 21, loss = 1.13681377\n",
      "Iteration 22, loss = 1.06666236\n",
      "Iteration 23, loss = 1.00434294\n",
      "Iteration 24, loss = 0.94979219\n",
      "Iteration 25, loss = 0.90160532\n",
      "Iteration 26, loss = 0.85895896\n",
      "Iteration 27, loss = 0.82104961\n",
      "Iteration 28, loss = 0.78711422\n",
      "Iteration 29, loss = 0.75639413\n",
      "Iteration 30, loss = 0.72872239\n",
      "Iteration 31, loss = 0.70323446\n",
      "Iteration 32, loss = 0.68041592\n",
      "Iteration 33, loss = 0.65952216\n",
      "Iteration 34, loss = 0.64030757\n",
      "Iteration 35, loss = 0.62291798\n",
      "Iteration 36, loss = 0.60687589\n",
      "Iteration 37, loss = 0.59212912\n",
      "Iteration 38, loss = 0.57853561\n",
      "Iteration 39, loss = 0.56609869\n",
      "Iteration 40, loss = 0.55450407\n",
      "Iteration 41, loss = 0.54382246\n",
      "Iteration 42, loss = 0.53383353\n",
      "Iteration 43, loss = 0.52458987\n",
      "Iteration 44, loss = 0.51592144\n",
      "Iteration 45, loss = 0.50787984\n",
      "Iteration 46, loss = 0.50032293\n",
      "Iteration 47, loss = 0.49322731\n",
      "Iteration 48, loss = 0.48660074\n",
      "Iteration 49, loss = 0.48031822\n",
      "Iteration 50, loss = 0.47439102\n",
      "Iteration 51, loss = 0.46896928\n",
      "Iteration 52, loss = 0.46369502\n",
      "Iteration 53, loss = 0.45891142\n",
      "Iteration 54, loss = 0.45422299\n",
      "Iteration 55, loss = 0.44988678\n",
      "Iteration 56, loss = 0.44579952\n",
      "Iteration 57, loss = 0.44176008\n",
      "Iteration 58, loss = 0.43808801\n",
      "Iteration 59, loss = 0.43441503\n",
      "Iteration 60, loss = 0.43110269\n",
      "Iteration 61, loss = 0.42785723\n",
      "Iteration 62, loss = 0.42464539\n",
      "Iteration 63, loss = 0.42185403\n",
      "Iteration 64, loss = 0.41905106\n",
      "Iteration 65, loss = 0.41635192\n",
      "Iteration 66, loss = 0.41390186\n",
      "Iteration 67, loss = 0.41137442\n",
      "Iteration 68, loss = 0.40899798\n",
      "Iteration 69, loss = 0.40663731\n",
      "Iteration 70, loss = 0.40453266\n",
      "Iteration 71, loss = 0.40225150\n",
      "Iteration 72, loss = 0.40024774\n",
      "Iteration 73, loss = 0.39829374\n",
      "Iteration 74, loss = 0.39640512\n",
      "Iteration 75, loss = 0.39468602\n",
      "Iteration 76, loss = 0.39277844\n",
      "Iteration 77, loss = 0.39107941\n",
      "Iteration 78, loss = 0.38930482\n",
      "Iteration 79, loss = 0.38773864\n",
      "Iteration 80, loss = 0.38601122\n",
      "Iteration 81, loss = 0.38446910\n",
      "Iteration 82, loss = 0.38294397\n",
      "Iteration 83, loss = 0.38149859\n",
      "Iteration 84, loss = 0.38013632\n",
      "Iteration 85, loss = 0.37860246\n",
      "Iteration 86, loss = 0.37728715\n",
      "Iteration 87, loss = 0.37592113\n",
      "Iteration 88, loss = 0.37457954\n",
      "Iteration 89, loss = 0.37333661\n",
      "Iteration 90, loss = 0.37206875\n",
      "Iteration 91, loss = 0.37076971\n",
      "Iteration 92, loss = 0.36968336\n",
      "Iteration 93, loss = 0.36840101\n",
      "Iteration 94, loss = 0.36716348\n",
      "Iteration 95, loss = 0.36601776\n",
      "Iteration 96, loss = 0.36500912\n",
      "Iteration 97, loss = 0.36385524\n",
      "Iteration 98, loss = 0.36273791\n",
      "Iteration 99, loss = 0.36168746\n",
      "Iteration 100, loss = 0.36054161\n",
      "Iteration 101, loss = 0.35957603\n",
      "Iteration 102, loss = 0.35854572\n",
      "Iteration 103, loss = 0.35743272\n",
      "Iteration 104, loss = 0.35642198\n",
      "Iteration 105, loss = 0.35542864\n",
      "Iteration 106, loss = 0.35444854\n",
      "Iteration 107, loss = 0.35361042\n",
      "Iteration 108, loss = 0.35269740\n",
      "Iteration 109, loss = 0.35170345\n",
      "Iteration 110, loss = 0.35062042\n",
      "Iteration 111, loss = 0.34990591\n",
      "Iteration 112, loss = 0.34882897\n",
      "Iteration 113, loss = 0.34803514\n",
      "Iteration 114, loss = 0.34717539\n",
      "Iteration 115, loss = 0.34631718\n",
      "Iteration 116, loss = 0.34528741\n",
      "Iteration 117, loss = 0.34445516\n",
      "Iteration 118, loss = 0.34365571\n",
      "Iteration 119, loss = 0.34272838\n",
      "Iteration 120, loss = 0.34204194\n",
      "Iteration 121, loss = 0.34126202\n",
      "Iteration 122, loss = 0.34030360\n",
      "Iteration 123, loss = 0.33952399\n",
      "Iteration 124, loss = 0.33856500\n",
      "Iteration 125, loss = 0.33801151\n",
      "Iteration 126, loss = 0.33713809\n",
      "Iteration 127, loss = 0.33629713\n",
      "Iteration 128, loss = 0.33549469\n",
      "Iteration 129, loss = 0.33476387\n",
      "Iteration 130, loss = 0.33394285\n",
      "Iteration 131, loss = 0.33314971\n",
      "Iteration 132, loss = 0.33246851\n",
      "Iteration 133, loss = 0.33170298\n",
      "Iteration 134, loss = 0.33083022\n",
      "Iteration 135, loss = 0.33019082\n",
      "Iteration 136, loss = 0.32952827\n",
      "Iteration 137, loss = 0.32864588\n",
      "Iteration 138, loss = 0.32801409\n",
      "Iteration 139, loss = 0.32724965\n",
      "Iteration 140, loss = 0.32643752\n",
      "Iteration 141, loss = 0.32584933\n",
      "Iteration 142, loss = 0.32510884\n",
      "Iteration 143, loss = 0.32441212\n",
      "Iteration 144, loss = 0.32373066\n",
      "Iteration 145, loss = 0.32303749\n",
      "Iteration 146, loss = 0.32237369\n",
      "Iteration 147, loss = 0.32160942\n",
      "Iteration 148, loss = 0.32091275\n",
      "Iteration 149, loss = 0.32029839\n",
      "Iteration 150, loss = 0.31965143\n",
      "Iteration 151, loss = 0.31887714\n",
      "Iteration 152, loss = 0.31827921\n",
      "Iteration 153, loss = 0.31765084\n",
      "Iteration 154, loss = 0.31694860\n",
      "Iteration 155, loss = 0.31620735\n",
      "Iteration 156, loss = 0.31572598\n",
      "Iteration 157, loss = 0.31503915\n",
      "Iteration 158, loss = 0.31428469\n",
      "Iteration 159, loss = 0.31373008\n",
      "Iteration 160, loss = 0.31311117\n",
      "Iteration 161, loss = 0.31242445\n",
      "Iteration 162, loss = 0.31192447\n",
      "Iteration 163, loss = 0.31111503\n",
      "Iteration 164, loss = 0.31061772\n",
      "Iteration 165, loss = 0.31003575\n",
      "Iteration 166, loss = 0.30934259\n",
      "Iteration 167, loss = 0.30857202\n",
      "Iteration 168, loss = 0.30792949\n",
      "Iteration 169, loss = 0.30750036\n",
      "Iteration 170, loss = 0.30691926\n",
      "Iteration 171, loss = 0.30641268\n",
      "Iteration 172, loss = 0.30573922\n",
      "Iteration 173, loss = 0.30514968\n",
      "Iteration 174, loss = 0.30452055\n",
      "Iteration 175, loss = 0.30405518\n",
      "Iteration 176, loss = 0.30347987\n",
      "Iteration 177, loss = 0.30291661\n",
      "Iteration 178, loss = 0.30217432\n",
      "Iteration 179, loss = 0.30177970\n",
      "Iteration 180, loss = 0.30105474\n",
      "Iteration 181, loss = 0.30060760\n",
      "Iteration 182, loss = 0.29998858\n",
      "Iteration 183, loss = 0.29945505\n",
      "Iteration 184, loss = 0.29896477\n",
      "Iteration 185, loss = 0.29842463\n",
      "Iteration 186, loss = 0.29775806\n",
      "Iteration 187, loss = 0.29737305\n",
      "Iteration 188, loss = 0.29669896\n",
      "Iteration 189, loss = 0.29612027\n",
      "Iteration 190, loss = 0.29569922\n",
      "Iteration 191, loss = 0.29518546\n",
      "Iteration 192, loss = 0.29455910\n",
      "Iteration 193, loss = 0.29405639\n",
      "Iteration 194, loss = 0.29350828\n",
      "Iteration 195, loss = 0.29297521\n",
      "Iteration 196, loss = 0.29249451\n",
      "Iteration 197, loss = 0.29192571\n",
      "Iteration 198, loss = 0.29145674\n",
      "Iteration 199, loss = 0.29093305\n",
      "Iteration 200, loss = 0.29050311\n",
      "[CV] END activation=logistic, alpha=0.01, batch_size=100, hidden_layer_sizes=(400, 100), solver=sgd; total time= 7.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.31490594\n",
      "Iteration 2, loss = 2.30393715\n",
      "Iteration 3, loss = 2.29684925\n",
      "Iteration 4, loss = 2.28888997\n",
      "Iteration 5, loss = 2.27969110\n",
      "Iteration 6, loss = 2.26809121\n",
      "Iteration 7, loss = 2.25336804\n",
      "Iteration 8, loss = 2.23415054\n",
      "Iteration 9, loss = 2.20807656\n",
      "Iteration 10, loss = 2.17203135\n",
      "Iteration 11, loss = 2.12123830\n",
      "Iteration 12, loss = 2.05114743\n",
      "Iteration 13, loss = 1.95870118\n",
      "Iteration 14, loss = 1.84704889\n",
      "Iteration 15, loss = 1.72589280\n",
      "Iteration 16, loss = 1.60716506\n",
      "Iteration 17, loss = 1.49811345\n",
      "Iteration 18, loss = 1.40035540\n",
      "Iteration 19, loss = 1.31146412\n",
      "Iteration 20, loss = 1.22942809\n",
      "Iteration 21, loss = 1.15322562\n",
      "Iteration 22, loss = 1.08286236\n",
      "Iteration 23, loss = 1.01904238\n",
      "Iteration 24, loss = 0.96185141\n",
      "Iteration 25, loss = 0.91115373\n",
      "Iteration 26, loss = 0.86635571\n",
      "Iteration 27, loss = 0.82653731\n",
      "Iteration 28, loss = 0.79125103\n",
      "Iteration 29, loss = 0.75960734\n",
      "Iteration 30, loss = 0.73146515\n",
      "Iteration 31, loss = 0.70601783\n",
      "Iteration 32, loss = 0.68301198\n",
      "Iteration 33, loss = 0.66222941\n",
      "Iteration 34, loss = 0.64356401\n",
      "Iteration 35, loss = 0.62662992\n",
      "Iteration 36, loss = 0.61115995\n",
      "Iteration 37, loss = 0.59711172\n",
      "Iteration 38, loss = 0.58406171\n",
      "Iteration 39, loss = 0.57219645\n",
      "Iteration 40, loss = 0.56114961\n",
      "Iteration 41, loss = 0.55115341\n",
      "Iteration 42, loss = 0.54148096\n",
      "Iteration 43, loss = 0.53264510\n",
      "Iteration 44, loss = 0.52446835\n",
      "Iteration 45, loss = 0.51675353\n",
      "Iteration 46, loss = 0.50936770\n",
      "Iteration 47, loss = 0.50249128\n",
      "Iteration 48, loss = 0.49607391\n",
      "Iteration 49, loss = 0.48984795\n",
      "Iteration 50, loss = 0.48388754\n",
      "Iteration 51, loss = 0.47850243\n",
      "Iteration 52, loss = 0.47328023\n",
      "Iteration 53, loss = 0.46816696\n",
      "Iteration 54, loss = 0.46349075\n",
      "Iteration 55, loss = 0.45884900\n",
      "Iteration 56, loss = 0.45444589\n",
      "Iteration 57, loss = 0.45035267\n",
      "Iteration 58, loss = 0.44629033\n",
      "Iteration 59, loss = 0.44253164\n",
      "Iteration 60, loss = 0.43883513\n",
      "Iteration 61, loss = 0.43550471\n",
      "Iteration 62, loss = 0.43200732\n",
      "Iteration 63, loss = 0.42884905\n",
      "Iteration 64, loss = 0.42586092\n",
      "Iteration 65, loss = 0.42274426\n",
      "Iteration 66, loss = 0.41995734\n",
      "Iteration 67, loss = 0.41725296\n",
      "Iteration 68, loss = 0.41445982\n",
      "Iteration 69, loss = 0.41204984\n",
      "Iteration 70, loss = 0.40957388\n",
      "Iteration 71, loss = 0.40723436\n",
      "Iteration 72, loss = 0.40487007\n",
      "Iteration 73, loss = 0.40276813\n",
      "Iteration 74, loss = 0.40052382\n",
      "Iteration 75, loss = 0.39851162\n",
      "Iteration 76, loss = 0.39654617\n",
      "Iteration 77, loss = 0.39458539\n",
      "Iteration 78, loss = 0.39273170\n",
      "Iteration 79, loss = 0.39086482\n",
      "Iteration 80, loss = 0.38925835\n",
      "Iteration 81, loss = 0.38734482\n",
      "Iteration 82, loss = 0.38572267\n",
      "Iteration 83, loss = 0.38406740\n",
      "Iteration 84, loss = 0.38244989\n",
      "Iteration 85, loss = 0.38095363\n",
      "Iteration 86, loss = 0.37944844\n",
      "Iteration 87, loss = 0.37800564\n",
      "Iteration 88, loss = 0.37663088\n",
      "Iteration 89, loss = 0.37515539\n",
      "Iteration 90, loss = 0.37377541\n",
      "Iteration 91, loss = 0.37250524\n",
      "Iteration 92, loss = 0.37117997\n",
      "Iteration 93, loss = 0.36989366\n",
      "Iteration 94, loss = 0.36864587\n",
      "Iteration 95, loss = 0.36747948\n",
      "Iteration 96, loss = 0.36622065\n",
      "Iteration 97, loss = 0.36501235\n",
      "Iteration 98, loss = 0.36394434\n",
      "Iteration 99, loss = 0.36276228\n",
      "Iteration 100, loss = 0.36164929\n",
      "Iteration 101, loss = 0.36047281\n",
      "Iteration 102, loss = 0.35940191\n",
      "Iteration 103, loss = 0.35833689\n",
      "Iteration 104, loss = 0.35727126\n",
      "Iteration 105, loss = 0.35631732\n",
      "Iteration 106, loss = 0.35528218\n",
      "Iteration 107, loss = 0.35427361\n",
      "Iteration 108, loss = 0.35317113\n",
      "Iteration 109, loss = 0.35228310\n",
      "Iteration 110, loss = 0.35138846\n",
      "Iteration 111, loss = 0.35031326\n",
      "Iteration 112, loss = 0.34934140\n",
      "Iteration 113, loss = 0.34844842\n",
      "Iteration 114, loss = 0.34753778\n",
      "Iteration 115, loss = 0.34659626\n",
      "Iteration 116, loss = 0.34575746\n",
      "Iteration 117, loss = 0.34482419\n",
      "Iteration 118, loss = 0.34404446\n",
      "Iteration 119, loss = 0.34316467\n",
      "Iteration 120, loss = 0.34223810\n",
      "Iteration 121, loss = 0.34139111\n",
      "Iteration 122, loss = 0.34067883\n",
      "Iteration 123, loss = 0.33982151\n",
      "Iteration 124, loss = 0.33896938\n",
      "Iteration 125, loss = 0.33818186\n",
      "Iteration 126, loss = 0.33731183\n",
      "Iteration 127, loss = 0.33657917\n",
      "Iteration 128, loss = 0.33589425\n",
      "Iteration 129, loss = 0.33497745\n",
      "Iteration 130, loss = 0.33431422\n",
      "Iteration 131, loss = 0.33349408\n",
      "Iteration 132, loss = 0.33268079\n",
      "Iteration 133, loss = 0.33200716\n",
      "Iteration 134, loss = 0.33130140\n",
      "Iteration 135, loss = 0.33051987\n",
      "Iteration 136, loss = 0.32982025\n",
      "Iteration 137, loss = 0.32904130\n",
      "Iteration 138, loss = 0.32847404\n",
      "Iteration 139, loss = 0.32764896\n",
      "Iteration 140, loss = 0.32695723\n",
      "Iteration 141, loss = 0.32631299\n",
      "Iteration 142, loss = 0.32553929\n",
      "Iteration 143, loss = 0.32486028\n",
      "Iteration 144, loss = 0.32419453\n",
      "Iteration 145, loss = 0.32345802\n",
      "Iteration 146, loss = 0.32288980\n",
      "Iteration 147, loss = 0.32220681\n",
      "Iteration 148, loss = 0.32147461\n",
      "Iteration 149, loss = 0.32089469\n",
      "Iteration 150, loss = 0.32025914\n",
      "Iteration 151, loss = 0.31950167\n",
      "Iteration 152, loss = 0.31896692\n",
      "Iteration 153, loss = 0.31817375\n",
      "Iteration 154, loss = 0.31762451\n",
      "Iteration 155, loss = 0.31688832\n",
      "Iteration 156, loss = 0.31634547\n",
      "Iteration 157, loss = 0.31564987\n",
      "Iteration 158, loss = 0.31509240\n",
      "Iteration 159, loss = 0.31441116\n",
      "Iteration 160, loss = 0.31395848\n",
      "Iteration 161, loss = 0.31308984\n",
      "Iteration 162, loss = 0.31276832\n",
      "Iteration 163, loss = 0.31209941\n",
      "Iteration 164, loss = 0.31135587\n",
      "Iteration 165, loss = 0.31086700\n",
      "Iteration 166, loss = 0.31030072\n",
      "Iteration 167, loss = 0.30964727\n",
      "Iteration 168, loss = 0.30906093\n",
      "Iteration 169, loss = 0.30860794\n",
      "Iteration 170, loss = 0.30801049\n",
      "Iteration 171, loss = 0.30731114\n",
      "Iteration 172, loss = 0.30686048\n",
      "Iteration 173, loss = 0.30615396\n",
      "Iteration 174, loss = 0.30566177\n",
      "Iteration 175, loss = 0.30508147\n",
      "Iteration 176, loss = 0.30464518\n",
      "Iteration 177, loss = 0.30406580\n",
      "Iteration 178, loss = 0.30350864\n",
      "Iteration 179, loss = 0.30292732\n",
      "Iteration 180, loss = 0.30244986\n",
      "Iteration 181, loss = 0.30173792\n",
      "Iteration 182, loss = 0.30134070\n",
      "Iteration 183, loss = 0.30078200\n",
      "Iteration 184, loss = 0.30041564\n",
      "Iteration 185, loss = 0.29984534\n",
      "Iteration 186, loss = 0.29921835\n",
      "Iteration 187, loss = 0.29857884\n",
      "Iteration 188, loss = 0.29824629\n",
      "Iteration 189, loss = 0.29774023\n",
      "Iteration 190, loss = 0.29721716\n",
      "Iteration 191, loss = 0.29674561\n",
      "Iteration 192, loss = 0.29622585\n",
      "Iteration 193, loss = 0.29574083\n",
      "Iteration 194, loss = 0.29510683\n",
      "Iteration 195, loss = 0.29458745\n",
      "Iteration 196, loss = 0.29418551\n",
      "Iteration 197, loss = 0.29363963\n",
      "Iteration 198, loss = 0.29316637\n",
      "Iteration 199, loss = 0.29277502\n",
      "Iteration 200, loss = 0.29222395\n",
      "[CV] END activation=logistic, alpha=0.01, batch_size=100, hidden_layer_sizes=(400, 100), solver=sgd; total time= 7.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.31555386\n",
      "Iteration 2, loss = 2.30330621\n",
      "Iteration 3, loss = 2.29544589\n",
      "Iteration 4, loss = 2.28701446\n",
      "Iteration 5, loss = 2.27679610\n",
      "Iteration 6, loss = 2.26422560\n",
      "Iteration 7, loss = 2.24820798\n",
      "Iteration 8, loss = 2.22708302\n",
      "Iteration 9, loss = 2.19836649\n",
      "Iteration 10, loss = 2.15855027\n",
      "Iteration 11, loss = 2.10250247\n",
      "Iteration 12, loss = 2.02477732\n",
      "Iteration 13, loss = 1.92158623\n",
      "Iteration 14, loss = 1.79560316\n",
      "Iteration 15, loss = 1.65686294\n",
      "Iteration 16, loss = 1.51970329\n",
      "Iteration 17, loss = 1.39549179\n",
      "Iteration 18, loss = 1.28924818\n",
      "Iteration 19, loss = 1.20107308\n",
      "Iteration 20, loss = 1.12803812\n",
      "Iteration 21, loss = 1.06693018\n",
      "Iteration 22, loss = 1.01520739\n",
      "Iteration 23, loss = 0.97022624\n",
      "Iteration 24, loss = 0.93053216\n",
      "Iteration 25, loss = 0.89487066\n",
      "Iteration 26, loss = 0.86273869\n",
      "Iteration 27, loss = 0.83285037\n",
      "Iteration 28, loss = 0.80518216\n",
      "Iteration 29, loss = 0.77940396\n",
      "Iteration 30, loss = 0.75537525\n",
      "Iteration 31, loss = 0.73264909\n",
      "Iteration 32, loss = 0.71145629\n",
      "Iteration 33, loss = 0.69178160\n",
      "Iteration 34, loss = 0.67314303\n",
      "Iteration 35, loss = 0.65573964\n",
      "Iteration 36, loss = 0.63924863\n",
      "Iteration 37, loss = 0.62400210\n",
      "Iteration 38, loss = 0.60962755\n",
      "Iteration 39, loss = 0.59612006\n",
      "Iteration 40, loss = 0.58346570\n",
      "Iteration 41, loss = 0.57160281\n",
      "Iteration 42, loss = 0.56040920\n",
      "Iteration 43, loss = 0.54977728\n",
      "Iteration 44, loss = 0.53997852\n",
      "Iteration 45, loss = 0.53055239\n",
      "Iteration 46, loss = 0.52187734\n",
      "Iteration 47, loss = 0.51355071\n",
      "Iteration 48, loss = 0.50575055\n",
      "Iteration 49, loss = 0.49843678\n",
      "Iteration 50, loss = 0.49163464\n",
      "Iteration 51, loss = 0.48514369\n",
      "Iteration 52, loss = 0.47880510\n",
      "Iteration 53, loss = 0.47313752\n",
      "Iteration 54, loss = 0.46750691\n",
      "Iteration 55, loss = 0.46236335\n",
      "Iteration 56, loss = 0.45730276\n",
      "Iteration 57, loss = 0.45268819\n",
      "Iteration 58, loss = 0.44810255\n",
      "Iteration 59, loss = 0.44390979\n",
      "Iteration 60, loss = 0.43979685\n",
      "Iteration 61, loss = 0.43601986\n",
      "Iteration 62, loss = 0.43232702\n",
      "Iteration 63, loss = 0.42872865\n",
      "Iteration 64, loss = 0.42550135\n",
      "Iteration 65, loss = 0.42234443\n",
      "Iteration 66, loss = 0.41912174\n",
      "Iteration 67, loss = 0.41609745\n",
      "Iteration 68, loss = 0.41325800\n",
      "Iteration 69, loss = 0.41042791\n",
      "Iteration 70, loss = 0.40796925\n",
      "Iteration 71, loss = 0.40548754\n",
      "Iteration 72, loss = 0.40305270\n",
      "Iteration 73, loss = 0.40052883\n",
      "Iteration 74, loss = 0.39821146\n",
      "Iteration 75, loss = 0.39611759\n",
      "Iteration 76, loss = 0.39397529\n",
      "Iteration 77, loss = 0.39191667\n",
      "Iteration 78, loss = 0.38989380\n",
      "Iteration 79, loss = 0.38795967\n",
      "Iteration 80, loss = 0.38607829\n",
      "Iteration 81, loss = 0.38433407\n",
      "Iteration 82, loss = 0.38245948\n",
      "Iteration 83, loss = 0.38077729\n",
      "Iteration 84, loss = 0.37913651\n",
      "Iteration 85, loss = 0.37741840\n",
      "Iteration 86, loss = 0.37579655\n",
      "Iteration 87, loss = 0.37415104\n",
      "Iteration 88, loss = 0.37277085\n",
      "Iteration 89, loss = 0.37111979\n",
      "Iteration 90, loss = 0.36971192\n",
      "Iteration 91, loss = 0.36837388\n",
      "Iteration 92, loss = 0.36693826\n",
      "Iteration 93, loss = 0.36569450\n",
      "Iteration 94, loss = 0.36425845\n",
      "Iteration 95, loss = 0.36303577\n",
      "Iteration 96, loss = 0.36174502\n",
      "Iteration 97, loss = 0.36039014\n",
      "Iteration 98, loss = 0.35920887\n",
      "Iteration 99, loss = 0.35802814\n",
      "Iteration 100, loss = 0.35678939\n",
      "Iteration 101, loss = 0.35566236\n",
      "Iteration 102, loss = 0.35448535\n",
      "Iteration 103, loss = 0.35339043\n",
      "Iteration 104, loss = 0.35225144\n",
      "Iteration 105, loss = 0.35122598\n",
      "Iteration 106, loss = 0.35008353\n",
      "Iteration 107, loss = 0.34915067\n",
      "Iteration 108, loss = 0.34814183\n",
      "Iteration 109, loss = 0.34694841\n",
      "Iteration 110, loss = 0.34598751\n",
      "Iteration 111, loss = 0.34494821\n",
      "Iteration 112, loss = 0.34402516\n",
      "Iteration 113, loss = 0.34315115\n",
      "Iteration 114, loss = 0.34212309\n",
      "Iteration 115, loss = 0.34120951\n",
      "Iteration 116, loss = 0.34032084\n",
      "Iteration 117, loss = 0.33939660\n",
      "Iteration 118, loss = 0.33846859\n",
      "Iteration 119, loss = 0.33752743\n",
      "Iteration 120, loss = 0.33662908\n",
      "Iteration 121, loss = 0.33593033\n",
      "Iteration 122, loss = 0.33495050\n",
      "Iteration 123, loss = 0.33405249\n",
      "Iteration 124, loss = 0.33332391\n",
      "Iteration 125, loss = 0.33263596\n",
      "Iteration 126, loss = 0.33170390\n",
      "Iteration 127, loss = 0.33088336\n",
      "Iteration 128, loss = 0.33008624\n",
      "Iteration 129, loss = 0.32931878\n",
      "Iteration 130, loss = 0.32851425\n",
      "Iteration 131, loss = 0.32761578\n",
      "Iteration 132, loss = 0.32704766\n",
      "Iteration 133, loss = 0.32631395\n",
      "Iteration 134, loss = 0.32553460\n",
      "Iteration 135, loss = 0.32475312\n",
      "Iteration 136, loss = 0.32407959\n",
      "Iteration 137, loss = 0.32328285\n",
      "Iteration 138, loss = 0.32259707\n",
      "Iteration 139, loss = 0.32196791\n",
      "Iteration 140, loss = 0.32119289\n",
      "Iteration 141, loss = 0.32055730\n",
      "Iteration 142, loss = 0.31978713\n",
      "Iteration 143, loss = 0.31918202\n",
      "Iteration 144, loss = 0.31845359\n",
      "Iteration 145, loss = 0.31777466\n",
      "Iteration 146, loss = 0.31712383\n",
      "Iteration 147, loss = 0.31653577\n",
      "Iteration 148, loss = 0.31585406\n",
      "Iteration 149, loss = 0.31521707\n",
      "Iteration 150, loss = 0.31465601\n",
      "Iteration 151, loss = 0.31397319\n",
      "Iteration 152, loss = 0.31330684\n",
      "Iteration 153, loss = 0.31267099\n",
      "Iteration 154, loss = 0.31223976\n",
      "Iteration 155, loss = 0.31144822\n",
      "Iteration 156, loss = 0.31095477\n",
      "Iteration 157, loss = 0.31029339\n",
      "Iteration 158, loss = 0.30974602\n",
      "Iteration 159, loss = 0.30910031\n",
      "Iteration 160, loss = 0.30865033\n",
      "Iteration 161, loss = 0.30785998\n",
      "Iteration 162, loss = 0.30725656\n",
      "Iteration 163, loss = 0.30683048\n",
      "Iteration 164, loss = 0.30616815\n",
      "Iteration 165, loss = 0.30565062\n",
      "Iteration 166, loss = 0.30511235\n",
      "Iteration 167, loss = 0.30463365\n",
      "Iteration 168, loss = 0.30391746\n",
      "Iteration 169, loss = 0.30351179\n",
      "Iteration 170, loss = 0.30286355\n",
      "Iteration 171, loss = 0.30243885\n",
      "Iteration 172, loss = 0.30180278\n",
      "Iteration 173, loss = 0.30134455\n",
      "Iteration 174, loss = 0.30086676\n",
      "Iteration 175, loss = 0.30026674\n",
      "Iteration 176, loss = 0.29967790\n",
      "Iteration 177, loss = 0.29926631\n",
      "Iteration 178, loss = 0.29875714\n",
      "Iteration 179, loss = 0.29814918\n",
      "Iteration 180, loss = 0.29767613\n",
      "Iteration 181, loss = 0.29714988\n",
      "Iteration 182, loss = 0.29671240\n",
      "Iteration 183, loss = 0.29607680\n",
      "Iteration 184, loss = 0.29566099\n",
      "Iteration 185, loss = 0.29520850\n",
      "Iteration 186, loss = 0.29462425\n",
      "Iteration 187, loss = 0.29419556\n",
      "Iteration 188, loss = 0.29371074\n",
      "Iteration 189, loss = 0.29323379\n",
      "Iteration 190, loss = 0.29270181\n",
      "Iteration 191, loss = 0.29218685\n",
      "Iteration 192, loss = 0.29179314\n",
      "Iteration 193, loss = 0.29123751\n",
      "Iteration 194, loss = 0.29082396\n",
      "Iteration 195, loss = 0.29027114\n",
      "Iteration 196, loss = 0.28987873\n",
      "Iteration 197, loss = 0.28937644\n",
      "Iteration 198, loss = 0.28893202\n",
      "Iteration 199, loss = 0.28845260\n",
      "Iteration 200, loss = 0.28796836\n",
      "[CV] END activation=logistic, alpha=0.01, batch_size=100, hidden_layer_sizes=(400, 100), solver=sgd; total time= 7.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.31751313\n",
      "Iteration 2, loss = 2.30401569\n",
      "Iteration 3, loss = 2.29650989\n",
      "Iteration 4, loss = 2.28819410\n",
      "Iteration 5, loss = 2.27823305\n",
      "Iteration 6, loss = 2.26608934\n",
      "Iteration 7, loss = 2.25059246\n",
      "Iteration 8, loss = 2.23007878\n",
      "Iteration 9, loss = 2.20232414\n",
      "Iteration 10, loss = 2.16334836\n",
      "Iteration 11, loss = 2.10965679\n",
      "Iteration 12, loss = 2.03686887\n",
      "Iteration 13, loss = 1.94432046\n",
      "Iteration 14, loss = 1.83516207\n",
      "Iteration 15, loss = 1.71483557\n",
      "Iteration 16, loss = 1.59142224\n",
      "Iteration 17, loss = 1.47138182\n",
      "Iteration 18, loss = 1.36074163\n",
      "Iteration 19, loss = 1.26224190\n",
      "Iteration 20, loss = 1.17658005\n",
      "Iteration 21, loss = 1.10357293\n",
      "Iteration 22, loss = 1.04127289\n",
      "Iteration 23, loss = 0.98800598\n",
      "Iteration 24, loss = 0.94168391\n",
      "Iteration 25, loss = 0.90108799\n",
      "Iteration 26, loss = 0.86490934\n",
      "Iteration 27, loss = 0.83193355\n",
      "Iteration 28, loss = 0.80173805\n",
      "Iteration 29, loss = 0.77378305\n",
      "Iteration 30, loss = 0.74791632\n",
      "Iteration 31, loss = 0.72371018\n",
      "Iteration 32, loss = 0.70126393\n",
      "Iteration 33, loss = 0.68034277\n",
      "Iteration 34, loss = 0.66099219\n",
      "Iteration 35, loss = 0.64289556\n",
      "Iteration 36, loss = 0.62609001\n",
      "Iteration 37, loss = 0.61045533\n",
      "Iteration 38, loss = 0.59611993\n",
      "Iteration 39, loss = 0.58266619\n",
      "Iteration 40, loss = 0.57046906\n",
      "Iteration 41, loss = 0.55897065\n",
      "Iteration 42, loss = 0.54839729\n",
      "Iteration 43, loss = 0.53848194\n",
      "Iteration 44, loss = 0.52926385\n",
      "Iteration 45, loss = 0.52071508\n",
      "Iteration 46, loss = 0.51273701\n",
      "Iteration 47, loss = 0.50518390\n",
      "Iteration 48, loss = 0.49800348\n",
      "Iteration 49, loss = 0.49141134\n",
      "Iteration 50, loss = 0.48529145\n",
      "Iteration 51, loss = 0.47940672\n",
      "Iteration 52, loss = 0.47370173\n",
      "Iteration 53, loss = 0.46850872\n",
      "Iteration 54, loss = 0.46359455\n",
      "Iteration 55, loss = 0.45882026\n",
      "Iteration 56, loss = 0.45435788\n",
      "Iteration 57, loss = 0.45009580\n",
      "Iteration 58, loss = 0.44609532\n",
      "Iteration 59, loss = 0.44224130\n",
      "Iteration 60, loss = 0.43856226\n",
      "Iteration 61, loss = 0.43511491\n",
      "Iteration 62, loss = 0.43172432\n",
      "Iteration 63, loss = 0.42859277\n",
      "Iteration 64, loss = 0.42548601\n",
      "Iteration 65, loss = 0.42245205\n",
      "Iteration 66, loss = 0.41962561\n",
      "Iteration 67, loss = 0.41683441\n",
      "Iteration 68, loss = 0.41423017\n",
      "Iteration 69, loss = 0.41174384\n",
      "Iteration 70, loss = 0.40931314\n",
      "Iteration 71, loss = 0.40700019\n",
      "Iteration 72, loss = 0.40459092\n",
      "Iteration 73, loss = 0.40253173\n",
      "Iteration 74, loss = 0.40032485\n",
      "Iteration 75, loss = 0.39837992\n",
      "Iteration 76, loss = 0.39642020\n",
      "Iteration 77, loss = 0.39448569\n",
      "Iteration 78, loss = 0.39253868\n",
      "Iteration 79, loss = 0.39071859\n",
      "Iteration 80, loss = 0.38897941\n",
      "Iteration 81, loss = 0.38715749\n",
      "Iteration 82, loss = 0.38541325\n",
      "Iteration 83, loss = 0.38391311\n",
      "Iteration 84, loss = 0.38218365\n",
      "Iteration 85, loss = 0.38071921\n",
      "Iteration 86, loss = 0.37923125\n",
      "Iteration 87, loss = 0.37769578\n",
      "Iteration 88, loss = 0.37618807\n",
      "Iteration 89, loss = 0.37487500\n",
      "Iteration 90, loss = 0.37342618\n",
      "Iteration 91, loss = 0.37212861\n",
      "Iteration 92, loss = 0.37085759\n",
      "Iteration 93, loss = 0.36953906\n",
      "Iteration 94, loss = 0.36822211\n",
      "Iteration 95, loss = 0.36704623\n",
      "Iteration 96, loss = 0.36571108\n",
      "Iteration 97, loss = 0.36442085\n",
      "Iteration 98, loss = 0.36342405\n",
      "Iteration 99, loss = 0.36219958\n",
      "Iteration 100, loss = 0.36091735\n",
      "Iteration 101, loss = 0.35980993\n",
      "Iteration 102, loss = 0.35883533\n",
      "Iteration 103, loss = 0.35772044\n",
      "Iteration 104, loss = 0.35661571\n",
      "Iteration 105, loss = 0.35553313\n",
      "Iteration 106, loss = 0.35451022\n",
      "Iteration 107, loss = 0.35347350\n",
      "Iteration 108, loss = 0.35248806\n",
      "Iteration 109, loss = 0.35153152\n",
      "Iteration 110, loss = 0.35053988\n",
      "Iteration 111, loss = 0.34965286\n",
      "Iteration 112, loss = 0.34861049\n",
      "Iteration 113, loss = 0.34767928\n",
      "Iteration 114, loss = 0.34671032\n",
      "Iteration 115, loss = 0.34580628\n",
      "Iteration 116, loss = 0.34485689\n",
      "Iteration 117, loss = 0.34401113\n",
      "Iteration 118, loss = 0.34306342\n",
      "Iteration 119, loss = 0.34227269\n",
      "Iteration 120, loss = 0.34125994\n",
      "Iteration 121, loss = 0.34041213\n",
      "Iteration 122, loss = 0.33964700\n",
      "Iteration 123, loss = 0.33876417\n",
      "Iteration 124, loss = 0.33797556\n",
      "Iteration 125, loss = 0.33713371\n",
      "Iteration 126, loss = 0.33626851\n",
      "Iteration 127, loss = 0.33553577\n",
      "Iteration 128, loss = 0.33477277\n",
      "Iteration 129, loss = 0.33389581\n",
      "Iteration 130, loss = 0.33304900\n",
      "Iteration 131, loss = 0.33228489\n",
      "Iteration 132, loss = 0.33147581\n",
      "Iteration 133, loss = 0.33082091\n",
      "Iteration 134, loss = 0.32989547\n",
      "Iteration 135, loss = 0.32917611\n",
      "Iteration 136, loss = 0.32845143\n",
      "Iteration 137, loss = 0.32779339\n",
      "Iteration 138, loss = 0.32705053\n",
      "Iteration 139, loss = 0.32635278\n",
      "Iteration 140, loss = 0.32553150\n",
      "Iteration 141, loss = 0.32486080\n",
      "Iteration 142, loss = 0.32416710\n",
      "Iteration 143, loss = 0.32340159\n",
      "Iteration 144, loss = 0.32273593\n",
      "Iteration 145, loss = 0.32198697\n",
      "Iteration 146, loss = 0.32127860\n",
      "Iteration 147, loss = 0.32069450\n",
      "Iteration 148, loss = 0.32004002\n",
      "Iteration 149, loss = 0.31926332\n",
      "Iteration 150, loss = 0.31855613\n",
      "Iteration 151, loss = 0.31786645\n",
      "Iteration 152, loss = 0.31725579\n",
      "Iteration 153, loss = 0.31655812\n",
      "Iteration 154, loss = 0.31599037\n",
      "Iteration 155, loss = 0.31524965\n",
      "Iteration 156, loss = 0.31446044\n",
      "Iteration 157, loss = 0.31396522\n",
      "Iteration 158, loss = 0.31340728\n",
      "Iteration 159, loss = 0.31270005\n",
      "Iteration 160, loss = 0.31190027\n",
      "Iteration 161, loss = 0.31141858\n",
      "Iteration 162, loss = 0.31082522\n",
      "Iteration 163, loss = 0.31015800\n",
      "Iteration 164, loss = 0.30947605\n",
      "Iteration 165, loss = 0.30888715\n",
      "Iteration 166, loss = 0.30825605\n",
      "Iteration 167, loss = 0.30780723\n",
      "Iteration 168, loss = 0.30708862\n",
      "Iteration 169, loss = 0.30650553\n",
      "Iteration 170, loss = 0.30592328\n",
      "Iteration 171, loss = 0.30526947\n",
      "Iteration 172, loss = 0.30472144\n",
      "Iteration 173, loss = 0.30406822\n",
      "Iteration 174, loss = 0.30367927\n",
      "Iteration 175, loss = 0.30288834\n",
      "Iteration 176, loss = 0.30238649\n",
      "Iteration 177, loss = 0.30185746\n",
      "Iteration 178, loss = 0.30117108\n",
      "Iteration 179, loss = 0.30068878\n",
      "Iteration 180, loss = 0.30010034\n",
      "Iteration 181, loss = 0.29957211\n",
      "Iteration 182, loss = 0.29894575\n",
      "Iteration 183, loss = 0.29836820\n",
      "Iteration 184, loss = 0.29784926\n",
      "Iteration 185, loss = 0.29732648\n",
      "Iteration 186, loss = 0.29669349\n",
      "Iteration 187, loss = 0.29609235\n",
      "Iteration 188, loss = 0.29572157\n",
      "Iteration 189, loss = 0.29513246\n",
      "Iteration 190, loss = 0.29461500\n",
      "Iteration 191, loss = 0.29406738\n",
      "Iteration 192, loss = 0.29356825\n",
      "Iteration 193, loss = 0.29303089\n",
      "Iteration 194, loss = 0.29243640\n",
      "Iteration 195, loss = 0.29194983\n",
      "Iteration 196, loss = 0.29139388\n",
      "Iteration 197, loss = 0.29081825\n",
      "Iteration 198, loss = 0.29044615\n",
      "Iteration 199, loss = 0.28983215\n",
      "Iteration 200, loss = 0.28933799\n",
      "[CV] END activation=logistic, alpha=0.01, batch_size=100, hidden_layer_sizes=(400, 100), solver=sgd; total time= 7.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.31316421\n",
      "Iteration 2, loss = 2.30323796\n",
      "Iteration 3, loss = 2.29571480\n",
      "Iteration 4, loss = 2.28738056\n",
      "Iteration 5, loss = 2.27725770\n",
      "Iteration 6, loss = 2.26496502\n",
      "Iteration 7, loss = 2.24953878\n",
      "Iteration 8, loss = 2.22895533\n",
      "Iteration 9, loss = 2.20127832\n",
      "Iteration 10, loss = 2.16274681\n",
      "Iteration 11, loss = 2.10893022\n",
      "Iteration 12, loss = 2.03513122\n",
      "Iteration 13, loss = 1.93873138\n",
      "Iteration 14, loss = 1.82151618\n",
      "Iteration 15, loss = 1.69099292\n",
      "Iteration 16, loss = 1.55718121\n",
      "Iteration 17, loss = 1.42923024\n",
      "Iteration 18, loss = 1.31544691\n",
      "Iteration 19, loss = 1.21816890\n",
      "Iteration 20, loss = 1.13706164\n",
      "Iteration 21, loss = 1.06977645\n",
      "Iteration 22, loss = 1.01357339\n",
      "Iteration 23, loss = 0.96540613\n",
      "Iteration 24, loss = 0.92335840\n",
      "Iteration 25, loss = 0.88620490\n",
      "Iteration 26, loss = 0.85238216\n",
      "Iteration 27, loss = 0.82151551\n",
      "Iteration 28, loss = 0.79312317\n",
      "Iteration 29, loss = 0.76643906\n",
      "Iteration 30, loss = 0.74156111\n",
      "Iteration 31, loss = 0.71827854\n",
      "Iteration 32, loss = 0.69656427\n",
      "Iteration 33, loss = 0.67626976\n",
      "Iteration 34, loss = 0.65761645\n",
      "Iteration 35, loss = 0.64014871\n",
      "Iteration 36, loss = 0.62410226\n",
      "Iteration 37, loss = 0.60911291\n",
      "Iteration 38, loss = 0.59536606\n",
      "Iteration 39, loss = 0.58254926\n",
      "Iteration 40, loss = 0.57067444\n",
      "Iteration 41, loss = 0.55952021\n",
      "Iteration 42, loss = 0.54932735\n",
      "Iteration 43, loss = 0.53957565\n",
      "Iteration 44, loss = 0.53076646\n",
      "Iteration 45, loss = 0.52227804\n",
      "Iteration 46, loss = 0.51446214\n",
      "Iteration 47, loss = 0.50690342\n",
      "Iteration 48, loss = 0.49982296\n",
      "Iteration 49, loss = 0.49325007\n",
      "Iteration 50, loss = 0.48699173\n",
      "Iteration 51, loss = 0.48088068\n",
      "Iteration 52, loss = 0.47554668\n",
      "Iteration 53, loss = 0.47003867\n",
      "Iteration 54, loss = 0.46517319\n",
      "Iteration 55, loss = 0.46036009\n",
      "Iteration 56, loss = 0.45580858\n",
      "Iteration 57, loss = 0.45157284\n",
      "Iteration 58, loss = 0.44746899\n",
      "Iteration 59, loss = 0.44354937\n",
      "Iteration 60, loss = 0.43986377\n",
      "Iteration 61, loss = 0.43615191\n",
      "Iteration 62, loss = 0.43283716\n",
      "Iteration 63, loss = 0.42961185\n",
      "Iteration 64, loss = 0.42645774\n",
      "Iteration 65, loss = 0.42347428\n",
      "Iteration 66, loss = 0.42070508\n",
      "Iteration 67, loss = 0.41795305\n",
      "Iteration 68, loss = 0.41528809\n",
      "Iteration 69, loss = 0.41276895\n",
      "Iteration 70, loss = 0.41024059\n",
      "Iteration 71, loss = 0.40799984\n",
      "Iteration 72, loss = 0.40561573\n",
      "Iteration 73, loss = 0.40351194\n",
      "Iteration 74, loss = 0.40138825\n",
      "Iteration 75, loss = 0.39931881\n",
      "Iteration 76, loss = 0.39727026\n",
      "Iteration 77, loss = 0.39533883\n",
      "Iteration 78, loss = 0.39357209\n",
      "Iteration 79, loss = 0.39178320\n",
      "Iteration 80, loss = 0.39004240\n",
      "Iteration 81, loss = 0.38835960\n",
      "Iteration 82, loss = 0.38657931\n",
      "Iteration 83, loss = 0.38506281\n",
      "Iteration 84, loss = 0.38344148\n",
      "Iteration 85, loss = 0.38203130\n",
      "Iteration 86, loss = 0.38059796\n",
      "Iteration 87, loss = 0.37898518\n",
      "Iteration 88, loss = 0.37762409\n",
      "Iteration 89, loss = 0.37622989\n",
      "Iteration 90, loss = 0.37490277\n",
      "Iteration 91, loss = 0.37361157\n",
      "Iteration 92, loss = 0.37224618\n",
      "Iteration 93, loss = 0.37102051\n",
      "Iteration 94, loss = 0.36979435\n",
      "Iteration 95, loss = 0.36845227\n",
      "Iteration 96, loss = 0.36732064\n",
      "Iteration 97, loss = 0.36612854\n",
      "Iteration 98, loss = 0.36495391\n",
      "Iteration 99, loss = 0.36375113\n",
      "Iteration 100, loss = 0.36274459\n",
      "Iteration 101, loss = 0.36166721\n",
      "Iteration 102, loss = 0.36050664\n",
      "Iteration 103, loss = 0.35954398\n",
      "Iteration 104, loss = 0.35847388\n",
      "Iteration 105, loss = 0.35731322\n",
      "Iteration 106, loss = 0.35639432\n",
      "Iteration 107, loss = 0.35529209\n",
      "Iteration 108, loss = 0.35436345\n",
      "Iteration 109, loss = 0.35341335\n",
      "Iteration 110, loss = 0.35249623\n",
      "Iteration 111, loss = 0.35155489\n",
      "Iteration 112, loss = 0.35057197\n",
      "Iteration 113, loss = 0.34979016\n",
      "Iteration 114, loss = 0.34870034\n",
      "Iteration 115, loss = 0.34786235\n",
      "Iteration 116, loss = 0.34698232\n",
      "Iteration 117, loss = 0.34614747\n",
      "Iteration 118, loss = 0.34518190\n",
      "Iteration 119, loss = 0.34427515\n",
      "Iteration 120, loss = 0.34346324\n",
      "Iteration 121, loss = 0.34270290\n",
      "Iteration 122, loss = 0.34179389\n",
      "Iteration 123, loss = 0.34098824\n",
      "Iteration 124, loss = 0.34021871\n",
      "Iteration 125, loss = 0.33938451\n",
      "Iteration 126, loss = 0.33847780\n",
      "Iteration 127, loss = 0.33775140\n",
      "Iteration 128, loss = 0.33700818\n",
      "Iteration 129, loss = 0.33639866\n",
      "Iteration 130, loss = 0.33547221\n",
      "Iteration 131, loss = 0.33477413\n",
      "Iteration 132, loss = 0.33400557\n",
      "Iteration 133, loss = 0.33325627\n",
      "Iteration 134, loss = 0.33246435\n",
      "Iteration 135, loss = 0.33186097\n",
      "Iteration 136, loss = 0.33105784\n",
      "Iteration 137, loss = 0.33035458\n",
      "Iteration 138, loss = 0.32948359\n",
      "Iteration 139, loss = 0.32891665\n",
      "Iteration 140, loss = 0.32821900\n",
      "Iteration 141, loss = 0.32750023\n",
      "Iteration 142, loss = 0.32677726\n",
      "Iteration 143, loss = 0.32608320\n",
      "Iteration 144, loss = 0.32534401\n",
      "Iteration 145, loss = 0.32480758\n",
      "Iteration 146, loss = 0.32412071\n",
      "Iteration 147, loss = 0.32330784\n",
      "Iteration 148, loss = 0.32261873\n",
      "Iteration 149, loss = 0.32209701\n",
      "Iteration 150, loss = 0.32144525\n",
      "Iteration 151, loss = 0.32074235\n",
      "Iteration 152, loss = 0.32017512\n",
      "Iteration 153, loss = 0.31949112\n",
      "Iteration 154, loss = 0.31891404\n",
      "Iteration 155, loss = 0.31821403\n",
      "Iteration 156, loss = 0.31763599\n",
      "Iteration 157, loss = 0.31693918\n",
      "Iteration 158, loss = 0.31647608\n",
      "Iteration 159, loss = 0.31582231\n",
      "Iteration 160, loss = 0.31520760\n",
      "Iteration 161, loss = 0.31464806\n",
      "Iteration 162, loss = 0.31394349\n",
      "Iteration 163, loss = 0.31342017\n",
      "Iteration 164, loss = 0.31272915\n",
      "Iteration 165, loss = 0.31239836\n",
      "Iteration 166, loss = 0.31167058\n",
      "Iteration 167, loss = 0.31108432\n",
      "Iteration 168, loss = 0.31055374\n",
      "Iteration 169, loss = 0.31000515\n",
      "Iteration 170, loss = 0.30938089\n",
      "Iteration 171, loss = 0.30874583\n",
      "Iteration 172, loss = 0.30828207\n",
      "Iteration 173, loss = 0.30770905\n",
      "Iteration 174, loss = 0.30713975\n",
      "Iteration 175, loss = 0.30665646\n",
      "Iteration 176, loss = 0.30603364\n",
      "Iteration 177, loss = 0.30553006\n",
      "Iteration 178, loss = 0.30487140\n",
      "Iteration 179, loss = 0.30439822\n",
      "Iteration 180, loss = 0.30383207\n",
      "Iteration 181, loss = 0.30349717\n",
      "Iteration 182, loss = 0.30289372\n",
      "Iteration 183, loss = 0.30230909\n",
      "Iteration 184, loss = 0.30179662\n",
      "Iteration 185, loss = 0.30123158\n",
      "Iteration 186, loss = 0.30081161\n",
      "Iteration 187, loss = 0.30036117\n",
      "Iteration 188, loss = 0.29968665\n",
      "Iteration 189, loss = 0.29928885\n",
      "Iteration 190, loss = 0.29875702\n",
      "Iteration 191, loss = 0.29825310\n",
      "Iteration 192, loss = 0.29767271\n",
      "Iteration 193, loss = 0.29704573\n",
      "Iteration 194, loss = 0.29673897\n",
      "Iteration 195, loss = 0.29626879\n",
      "Iteration 196, loss = 0.29579862\n",
      "Iteration 197, loss = 0.29526459\n",
      "Iteration 198, loss = 0.29474559\n",
      "Iteration 199, loss = 0.29425271\n",
      "Iteration 200, loss = 0.29386706\n",
      "[CV] END activation=logistic, alpha=0.01, batch_size=100, hidden_layer_sizes=(400, 100), solver=sgd; total time= 7.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.42525052\n",
      "Iteration 2, loss = 0.55113040\n",
      "Iteration 3, loss = 0.39393307\n",
      "Iteration 4, loss = 0.31996898\n",
      "Iteration 5, loss = 0.27260887\n",
      "Iteration 6, loss = 0.23865363\n",
      "Iteration 7, loss = 0.21523885\n",
      "Iteration 8, loss = 0.20015346\n",
      "Iteration 9, loss = 0.18524227\n",
      "Iteration 10, loss = 0.17337787\n",
      "Iteration 11, loss = 0.16331579\n",
      "Iteration 12, loss = 0.15675262\n",
      "Iteration 13, loss = 0.15083467\n",
      "Iteration 14, loss = 0.14468271\n",
      "Iteration 15, loss = 0.13906850\n",
      "Iteration 16, loss = 0.13477823\n",
      "Iteration 17, loss = 0.13217561\n",
      "Iteration 18, loss = 0.12713517\n",
      "Iteration 19, loss = 0.12512876\n",
      "Iteration 20, loss = 0.12309216\n",
      "Iteration 21, loss = 0.11918274\n",
      "Iteration 22, loss = 0.11711126\n",
      "Iteration 23, loss = 0.11649268\n",
      "Iteration 24, loss = 0.11409614\n",
      "Iteration 25, loss = 0.11103477\n",
      "Iteration 26, loss = 0.10887851\n",
      "Iteration 27, loss = 0.10764219\n",
      "Iteration 28, loss = 0.10628981\n",
      "Iteration 29, loss = 0.10540102\n",
      "Iteration 30, loss = 0.10402597\n",
      "Iteration 31, loss = 0.10336345\n",
      "Iteration 32, loss = 0.10483436\n",
      "Iteration 33, loss = 0.10234033\n",
      "Iteration 34, loss = 0.10003172\n",
      "Iteration 35, loss = 0.10040278\n",
      "Iteration 36, loss = 0.09998278\n",
      "Iteration 37, loss = 0.09945048\n",
      "Iteration 38, loss = 0.09784877\n",
      "Iteration 39, loss = 0.09616969\n",
      "Iteration 40, loss = 0.09760495\n",
      "Iteration 41, loss = 0.10074259\n",
      "Iteration 42, loss = 0.09688823\n",
      "Iteration 43, loss = 0.09443165\n",
      "Iteration 44, loss = 0.09441467\n",
      "Iteration 45, loss = 0.09511750\n",
      "Iteration 46, loss = 0.09673041\n",
      "Iteration 47, loss = 0.09637598\n",
      "Iteration 48, loss = 0.09271754\n",
      "Iteration 49, loss = 0.09331562\n",
      "Iteration 50, loss = 0.09460652\n",
      "Iteration 51, loss = 0.09418542\n",
      "Iteration 52, loss = 0.09483505\n",
      "Iteration 53, loss = 0.09228053\n",
      "Iteration 54, loss = 0.09128208\n",
      "Iteration 55, loss = 0.09332350\n",
      "Iteration 56, loss = 0.09715359\n",
      "Iteration 57, loss = 0.09239281\n",
      "Iteration 58, loss = 0.09176839\n",
      "Iteration 59, loss = 0.09131531\n",
      "Iteration 60, loss = 0.09249645\n",
      "Iteration 61, loss = 0.09099858\n",
      "Iteration 62, loss = 0.08991217\n",
      "Iteration 63, loss = 0.08983100\n",
      "Iteration 64, loss = 0.09306407\n",
      "Iteration 65, loss = 0.09232172\n",
      "Iteration 66, loss = 0.08925401\n",
      "Iteration 67, loss = 0.08835838\n",
      "Iteration 68, loss = 0.09145025\n",
      "Iteration 69, loss = 0.09203606\n",
      "Iteration 70, loss = 0.09026129\n",
      "Iteration 71, loss = 0.08762117\n",
      "Iteration 72, loss = 0.08994716\n",
      "Iteration 73, loss = 0.08978021\n",
      "Iteration 74, loss = 0.09122042\n",
      "Iteration 75, loss = 0.09031964\n",
      "Iteration 76, loss = 0.08807860\n",
      "Iteration 77, loss = 0.08815363\n",
      "Iteration 78, loss = 0.09077782\n",
      "Iteration 79, loss = 0.08752005\n",
      "Iteration 80, loss = 0.08786368\n",
      "Iteration 81, loss = 0.08973046\n",
      "Iteration 82, loss = 0.08946202\n",
      "Iteration 83, loss = 0.08781607\n",
      "Iteration 84, loss = 0.08846365\n",
      "Iteration 85, loss = 0.08881139\n",
      "Iteration 86, loss = 0.08728387\n",
      "Iteration 87, loss = 0.08888036\n",
      "Iteration 88, loss = 0.08947470\n",
      "Iteration 89, loss = 0.08715506\n",
      "Iteration 90, loss = 0.08873626\n",
      "Iteration 91, loss = 0.08657324\n",
      "Iteration 92, loss = 0.08527255\n",
      "Iteration 93, loss = 0.08877804\n",
      "Iteration 94, loss = 0.08949404\n",
      "Iteration 95, loss = 0.08586395\n",
      "Iteration 96, loss = 0.08481740\n",
      "Iteration 97, loss = 0.08789692\n",
      "Iteration 98, loss = 0.09023379\n",
      "Iteration 99, loss = 0.08659518\n",
      "Iteration 100, loss = 0.08593982\n",
      "Iteration 101, loss = 0.08468543\n",
      "Iteration 102, loss = 0.08612287\n",
      "Iteration 103, loss = 0.08537390\n",
      "Iteration 104, loss = 0.08869358\n",
      "Iteration 105, loss = 0.08631103\n",
      "Iteration 106, loss = 0.08869514\n",
      "Iteration 107, loss = 0.08600080\n",
      "Iteration 108, loss = 0.08543853\n",
      "Iteration 109, loss = 0.08614209\n",
      "Iteration 110, loss = 0.08726807\n",
      "Iteration 111, loss = 0.08897257\n",
      "Iteration 112, loss = 0.08662414\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.01, batch_size=100, hidden_layer_sizes=(100, 100, 100), solver=adam; total time= 2.6min\n",
      "Iteration 1, loss = 1.38534582\n",
      "Iteration 2, loss = 0.51518817\n",
      "Iteration 3, loss = 0.36899322\n",
      "Iteration 4, loss = 0.30149479\n",
      "Iteration 5, loss = 0.25988326\n",
      "Iteration 6, loss = 0.22975649\n",
      "Iteration 7, loss = 0.20760975\n",
      "Iteration 8, loss = 0.19118364\n",
      "Iteration 9, loss = 0.17800378\n",
      "Iteration 10, loss = 0.16693811\n",
      "Iteration 11, loss = 0.15802110\n",
      "Iteration 12, loss = 0.15088147\n",
      "Iteration 13, loss = 0.14347992\n",
      "Iteration 14, loss = 0.13883850\n",
      "Iteration 15, loss = 0.13374693\n",
      "Iteration 16, loss = 0.12900981\n",
      "Iteration 17, loss = 0.12553196\n",
      "Iteration 18, loss = 0.12193017\n",
      "Iteration 19, loss = 0.11902235\n",
      "Iteration 20, loss = 0.11597734\n",
      "Iteration 21, loss = 0.11396493\n",
      "Iteration 22, loss = 0.11242733\n",
      "Iteration 23, loss = 0.11071566\n",
      "Iteration 24, loss = 0.10716002\n",
      "Iteration 25, loss = 0.10817922\n",
      "Iteration 26, loss = 0.10562838\n",
      "Iteration 27, loss = 0.10354797\n",
      "Iteration 28, loss = 0.10245281\n",
      "Iteration 29, loss = 0.10132117\n",
      "Iteration 30, loss = 0.09905624\n",
      "Iteration 31, loss = 0.10003851\n",
      "Iteration 32, loss = 0.09899313\n",
      "Iteration 33, loss = 0.09949729\n",
      "Iteration 34, loss = 0.09982213\n",
      "Iteration 35, loss = 0.09777007\n",
      "Iteration 36, loss = 0.09750743\n",
      "Iteration 37, loss = 0.09714431\n",
      "Iteration 38, loss = 0.09677288\n",
      "Iteration 39, loss = 0.09636163\n",
      "Iteration 40, loss = 0.09664779\n",
      "Iteration 41, loss = 0.09450205\n",
      "Iteration 42, loss = 0.09271754\n",
      "Iteration 43, loss = 0.09425742\n",
      "Iteration 44, loss = 0.09735457\n",
      "Iteration 45, loss = 0.09637403\n",
      "Iteration 46, loss = 0.09356437\n",
      "Iteration 47, loss = 0.09108624\n",
      "Iteration 48, loss = 0.09147181\n",
      "Iteration 49, loss = 0.09337660\n",
      "Iteration 50, loss = 0.09413004\n",
      "Iteration 51, loss = 0.09174048\n",
      "Iteration 52, loss = 0.09247852\n",
      "Iteration 53, loss = 0.09127463\n",
      "Iteration 54, loss = 0.08992801\n",
      "Iteration 55, loss = 0.09125261\n",
      "Iteration 56, loss = 0.09254355\n",
      "Iteration 57, loss = 0.09235699\n",
      "Iteration 58, loss = 0.08982703\n",
      "Iteration 59, loss = 0.09203126\n",
      "Iteration 60, loss = 0.08958817\n",
      "Iteration 61, loss = 0.09154584\n",
      "Iteration 62, loss = 0.08868785\n",
      "Iteration 63, loss = 0.09105122\n",
      "Iteration 64, loss = 0.09214542\n",
      "Iteration 65, loss = 0.08981709\n",
      "Iteration 66, loss = 0.09152309\n",
      "Iteration 67, loss = 0.08898954\n",
      "Iteration 68, loss = 0.08735191\n",
      "Iteration 69, loss = 0.09115663\n",
      "Iteration 70, loss = 0.09123016\n",
      "Iteration 71, loss = 0.08879200\n",
      "Iteration 72, loss = 0.08973009\n",
      "Iteration 73, loss = 0.08787784\n",
      "Iteration 74, loss = 0.09148346\n",
      "Iteration 75, loss = 0.09113600\n",
      "Iteration 76, loss = 0.08765471\n",
      "Iteration 77, loss = 0.08779395\n",
      "Iteration 78, loss = 0.08763446\n",
      "Iteration 79, loss = 0.08809040\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.01, batch_size=100, hidden_layer_sizes=(100, 100, 100), solver=adam; total time= 1.8min\n",
      "Iteration 1, loss = 1.42256856\n",
      "Iteration 2, loss = 0.52492548\n",
      "Iteration 3, loss = 0.36334253\n",
      "Iteration 4, loss = 0.29373862\n",
      "Iteration 5, loss = 0.25511556\n",
      "Iteration 6, loss = 0.22726497\n",
      "Iteration 7, loss = 0.20776949\n",
      "Iteration 8, loss = 0.19056964\n",
      "Iteration 9, loss = 0.17834130\n",
      "Iteration 10, loss = 0.16732903\n",
      "Iteration 11, loss = 0.16006217\n",
      "Iteration 12, loss = 0.15341672\n",
      "Iteration 13, loss = 0.14546540\n",
      "Iteration 14, loss = 0.14053006\n",
      "Iteration 15, loss = 0.13572031\n",
      "Iteration 16, loss = 0.13090132\n",
      "Iteration 17, loss = 0.12706956\n",
      "Iteration 18, loss = 0.12353264\n",
      "Iteration 19, loss = 0.12077808\n",
      "Iteration 20, loss = 0.11778508\n",
      "Iteration 21, loss = 0.11609327\n",
      "Iteration 22, loss = 0.11331278\n",
      "Iteration 23, loss = 0.11383398\n",
      "Iteration 24, loss = 0.10865264\n",
      "Iteration 25, loss = 0.10849060\n",
      "Iteration 26, loss = 0.10658572\n",
      "Iteration 27, loss = 0.10659144\n",
      "Iteration 28, loss = 0.10597208\n",
      "Iteration 29, loss = 0.10453446\n",
      "Iteration 30, loss = 0.10211600\n",
      "Iteration 31, loss = 0.10199838\n",
      "Iteration 32, loss = 0.09965909\n",
      "Iteration 33, loss = 0.10042496\n",
      "Iteration 34, loss = 0.10061852\n",
      "Iteration 35, loss = 0.09955680\n",
      "Iteration 36, loss = 0.09908673\n",
      "Iteration 37, loss = 0.09836116\n",
      "Iteration 38, loss = 0.09874304\n",
      "Iteration 39, loss = 0.09637960\n",
      "Iteration 40, loss = 0.09640249\n",
      "Iteration 41, loss = 0.09666752\n",
      "Iteration 42, loss = 0.09614883\n",
      "Iteration 43, loss = 0.09578402\n",
      "Iteration 44, loss = 0.09583522\n",
      "Iteration 45, loss = 0.09523160\n",
      "Iteration 46, loss = 0.09329389\n",
      "Iteration 47, loss = 0.09492431\n",
      "Iteration 48, loss = 0.09471579\n",
      "Iteration 49, loss = 0.09417435\n",
      "Iteration 50, loss = 0.09384919\n",
      "Iteration 51, loss = 0.09310880\n",
      "Iteration 52, loss = 0.09132869\n",
      "Iteration 53, loss = 0.09494912\n",
      "Iteration 54, loss = 0.09413332\n",
      "Iteration 55, loss = 0.09412175\n",
      "Iteration 56, loss = 0.08966725\n",
      "Iteration 57, loss = 0.09417882\n",
      "Iteration 58, loss = 0.09425952\n",
      "Iteration 59, loss = 0.09190042\n",
      "Iteration 60, loss = 0.08989873\n",
      "Iteration 61, loss = 0.09153050\n",
      "Iteration 62, loss = 0.09189781\n",
      "Iteration 63, loss = 0.09066355\n",
      "Iteration 64, loss = 0.09305906\n",
      "Iteration 65, loss = 0.09310472\n",
      "Iteration 66, loss = 0.09104948\n",
      "Iteration 67, loss = 0.08857096\n",
      "Iteration 68, loss = 0.08967915\n",
      "Iteration 69, loss = 0.08977969\n",
      "Iteration 70, loss = 0.09098222\n",
      "Iteration 71, loss = 0.08945683\n",
      "Iteration 72, loss = 0.09068677\n",
      "Iteration 73, loss = 0.08780458\n",
      "Iteration 74, loss = 0.08993276\n",
      "Iteration 75, loss = 0.08990908\n",
      "Iteration 76, loss = 0.08868192\n",
      "Iteration 77, loss = 0.08755820\n",
      "Iteration 78, loss = 0.08956490\n",
      "Iteration 79, loss = 0.09041484\n",
      "Iteration 80, loss = 0.08854298\n",
      "Iteration 81, loss = 0.08798051\n",
      "Iteration 82, loss = 0.08981114\n",
      "Iteration 83, loss = 0.08717908\n",
      "Iteration 84, loss = 0.08670137\n",
      "Iteration 85, loss = 0.08871934\n",
      "Iteration 86, loss = 0.08764133\n",
      "Iteration 87, loss = 0.08705837\n",
      "Iteration 88, loss = 0.08955596\n",
      "Iteration 89, loss = 0.08999972\n",
      "Iteration 90, loss = 0.08575682\n",
      "Iteration 91, loss = 0.08954411\n",
      "Iteration 92, loss = 0.09053152\n",
      "Iteration 93, loss = 0.08503036\n",
      "Iteration 94, loss = 0.08809461\n",
      "Iteration 95, loss = 0.08934933\n",
      "Iteration 96, loss = 0.08751020\n",
      "Iteration 97, loss = 0.08719612\n",
      "Iteration 98, loss = 0.08734064\n",
      "Iteration 99, loss = 0.08767164\n",
      "Iteration 100, loss = 0.08661613\n",
      "Iteration 101, loss = 0.08690391\n",
      "Iteration 102, loss = 0.08509916\n",
      "Iteration 103, loss = 0.08993512\n",
      "Iteration 104, loss = 0.08512631\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.01, batch_size=100, hidden_layer_sizes=(100, 100, 100), solver=adam; total time= 2.4min\n",
      "Iteration 1, loss = 1.35868530\n",
      "Iteration 2, loss = 0.47951153\n",
      "Iteration 3, loss = 0.34305568\n",
      "Iteration 4, loss = 0.28607796\n",
      "Iteration 5, loss = 0.24870169\n",
      "Iteration 6, loss = 0.22311919\n",
      "Iteration 7, loss = 0.20221908\n",
      "Iteration 8, loss = 0.18695698\n",
      "Iteration 9, loss = 0.17434761\n",
      "Iteration 10, loss = 0.16470196\n",
      "Iteration 11, loss = 0.15405409\n",
      "Iteration 12, loss = 0.14886747\n",
      "Iteration 13, loss = 0.14294514\n",
      "Iteration 14, loss = 0.13725476\n",
      "Iteration 15, loss = 0.13546082\n",
      "Iteration 16, loss = 0.12681407\n",
      "Iteration 17, loss = 0.12539326\n",
      "Iteration 18, loss = 0.12010210\n",
      "Iteration 19, loss = 0.11863950\n",
      "Iteration 20, loss = 0.11622971\n",
      "Iteration 21, loss = 0.11255906\n",
      "Iteration 22, loss = 0.11181531\n",
      "Iteration 23, loss = 0.11056824\n",
      "Iteration 24, loss = 0.10779500\n",
      "Iteration 25, loss = 0.10605870\n",
      "Iteration 26, loss = 0.10499103\n",
      "Iteration 27, loss = 0.10539426\n",
      "Iteration 28, loss = 0.10171388\n",
      "Iteration 29, loss = 0.10206757\n",
      "Iteration 30, loss = 0.10050937\n",
      "Iteration 31, loss = 0.10013722\n",
      "Iteration 32, loss = 0.09928441\n",
      "Iteration 33, loss = 0.09968708\n",
      "Iteration 34, loss = 0.09702033\n",
      "Iteration 35, loss = 0.09717737\n",
      "Iteration 36, loss = 0.09765856\n",
      "Iteration 37, loss = 0.09954114\n",
      "Iteration 38, loss = 0.09573895\n",
      "Iteration 39, loss = 0.09381108\n",
      "Iteration 40, loss = 0.09464728\n",
      "Iteration 41, loss = 0.09521573\n",
      "Iteration 42, loss = 0.09427220\n",
      "Iteration 43, loss = 0.09410196\n",
      "Iteration 44, loss = 0.09262520\n",
      "Iteration 45, loss = 0.09619535\n",
      "Iteration 46, loss = 0.09415355\n",
      "Iteration 47, loss = 0.09347770\n",
      "Iteration 48, loss = 0.09386335\n",
      "Iteration 49, loss = 0.09069005\n",
      "Iteration 50, loss = 0.09241101\n",
      "Iteration 51, loss = 0.09447095\n",
      "Iteration 52, loss = 0.09310176\n",
      "Iteration 53, loss = 0.09137797\n",
      "Iteration 54, loss = 0.09080888\n",
      "Iteration 55, loss = 0.09077565\n",
      "Iteration 56, loss = 0.09112277\n",
      "Iteration 57, loss = 0.09182266\n",
      "Iteration 58, loss = 0.08966418\n",
      "Iteration 59, loss = 0.09184207\n",
      "Iteration 60, loss = 0.09168343\n",
      "Iteration 61, loss = 0.08906676\n",
      "Iteration 62, loss = 0.08908183\n",
      "Iteration 63, loss = 0.08921758\n",
      "Iteration 64, loss = 0.09067483\n",
      "Iteration 65, loss = 0.09007599\n",
      "Iteration 66, loss = 0.09147401\n",
      "Iteration 67, loss = 0.09133408\n",
      "Iteration 68, loss = 0.09136095\n",
      "Iteration 69, loss = 0.08699778\n",
      "Iteration 70, loss = 0.08976702\n",
      "Iteration 71, loss = 0.08921595\n",
      "Iteration 72, loss = 0.08812991\n",
      "Iteration 73, loss = 0.09246922\n",
      "Iteration 74, loss = 0.08857528\n",
      "Iteration 75, loss = 0.08845626\n",
      "Iteration 76, loss = 0.08964192\n",
      "Iteration 77, loss = 0.08876612\n",
      "Iteration 78, loss = 0.08580358\n",
      "Iteration 79, loss = 0.08742086\n",
      "Iteration 80, loss = 0.08903400\n",
      "Iteration 81, loss = 0.08815647\n",
      "Iteration 82, loss = 0.08870542\n",
      "Iteration 83, loss = 0.08852213\n",
      "Iteration 84, loss = 0.08797030\n",
      "Iteration 85, loss = 0.08601489\n",
      "Iteration 86, loss = 0.09059141\n",
      "Iteration 87, loss = 0.08591918\n",
      "Iteration 88, loss = 0.08549103\n",
      "Iteration 89, loss = 0.08843046\n",
      "Iteration 90, loss = 0.08999016\n",
      "Iteration 91, loss = 0.08675675\n",
      "Iteration 92, loss = 0.09042628\n",
      "Iteration 93, loss = 0.08873495\n",
      "Iteration 94, loss = 0.08509114\n",
      "Iteration 95, loss = 0.08591287\n",
      "Iteration 96, loss = 0.08564991\n",
      "Iteration 97, loss = 0.09136956\n",
      "Iteration 98, loss = 0.08747115\n",
      "Iteration 99, loss = 0.08511994\n",
      "Iteration 100, loss = 0.08617406\n",
      "Iteration 101, loss = 0.08694477\n",
      "Iteration 102, loss = 0.08506703\n",
      "Iteration 103, loss = 0.08667735\n",
      "Iteration 104, loss = 0.08778564\n",
      "Iteration 105, loss = 0.08756659\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.01, batch_size=100, hidden_layer_sizes=(100, 100, 100), solver=adam; total time= 2.5min\n",
      "Iteration 1, loss = 1.35521312\n",
      "Iteration 2, loss = 0.50630017\n",
      "Iteration 3, loss = 0.36325704\n",
      "Iteration 4, loss = 0.29797900\n",
      "Iteration 5, loss = 0.25680143\n",
      "Iteration 6, loss = 0.23060848\n",
      "Iteration 7, loss = 0.20987860\n",
      "Iteration 8, loss = 0.19324928\n",
      "Iteration 9, loss = 0.18062252\n",
      "Iteration 10, loss = 0.16979193\n",
      "Iteration 11, loss = 0.16047168\n",
      "Iteration 12, loss = 0.15252599\n",
      "Iteration 13, loss = 0.14801454\n",
      "Iteration 14, loss = 0.14371164\n",
      "Iteration 15, loss = 0.13702493\n",
      "Iteration 16, loss = 0.13436947\n",
      "Iteration 17, loss = 0.12817879\n",
      "Iteration 18, loss = 0.12509318\n",
      "Iteration 19, loss = 0.12382428\n",
      "Iteration 20, loss = 0.12110544\n",
      "Iteration 21, loss = 0.11948873\n",
      "Iteration 22, loss = 0.11538982\n",
      "Iteration 23, loss = 0.11309636\n",
      "Iteration 24, loss = 0.11172748\n",
      "Iteration 25, loss = 0.11033806\n",
      "Iteration 26, loss = 0.10789894\n",
      "Iteration 27, loss = 0.10715440\n",
      "Iteration 28, loss = 0.10630525\n",
      "Iteration 29, loss = 0.10759818\n",
      "Iteration 30, loss = 0.10495701\n",
      "Iteration 31, loss = 0.10259422\n",
      "Iteration 32, loss = 0.10203108\n",
      "Iteration 33, loss = 0.10437604\n",
      "Iteration 34, loss = 0.10010971\n",
      "Iteration 35, loss = 0.09827709\n",
      "Iteration 36, loss = 0.09904890\n",
      "Iteration 37, loss = 0.09965616\n",
      "Iteration 38, loss = 0.09993643\n",
      "Iteration 39, loss = 0.09706527\n",
      "Iteration 40, loss = 0.09830025\n",
      "Iteration 41, loss = 0.09745286\n",
      "Iteration 42, loss = 0.09450504\n",
      "Iteration 43, loss = 0.09581472\n",
      "Iteration 44, loss = 0.09808223\n",
      "Iteration 45, loss = 0.09494840\n",
      "Iteration 46, loss = 0.09764891\n",
      "Iteration 47, loss = 0.09417436\n",
      "Iteration 48, loss = 0.09370838\n",
      "Iteration 49, loss = 0.09317071\n",
      "Iteration 50, loss = 0.09367163\n",
      "Iteration 51, loss = 0.09575766\n",
      "Iteration 52, loss = 0.09539003\n",
      "Iteration 53, loss = 0.09141208\n",
      "Iteration 54, loss = 0.09362470\n",
      "Iteration 55, loss = 0.09483593\n",
      "Iteration 56, loss = 0.09616958\n",
      "Iteration 57, loss = 0.09112138\n",
      "Iteration 58, loss = 0.09109959\n",
      "Iteration 59, loss = 0.09394040\n",
      "Iteration 60, loss = 0.09157878\n",
      "Iteration 61, loss = 0.09020642\n",
      "Iteration 62, loss = 0.09240618\n",
      "Iteration 63, loss = 0.09163430\n",
      "Iteration 64, loss = 0.09159658\n",
      "Iteration 65, loss = 0.09020616\n",
      "Iteration 66, loss = 0.08968436\n",
      "Iteration 67, loss = 0.09029270\n",
      "Iteration 68, loss = 0.09159312\n",
      "Iteration 69, loss = 0.09771980\n",
      "Iteration 70, loss = 0.09038955\n",
      "Iteration 71, loss = 0.08802730\n",
      "Iteration 72, loss = 0.08852727\n",
      "Iteration 73, loss = 0.08905651\n",
      "Iteration 74, loss = 0.09199715\n",
      "Iteration 75, loss = 0.08912091\n",
      "Iteration 76, loss = 0.09085355\n",
      "Iteration 77, loss = 0.09100108\n",
      "Iteration 78, loss = 0.08822499\n",
      "Iteration 79, loss = 0.08915536\n",
      "Iteration 80, loss = 0.08713677\n",
      "Iteration 81, loss = 0.08944367\n",
      "Iteration 82, loss = 0.09094373\n",
      "Iteration 83, loss = 0.09081591\n",
      "Iteration 84, loss = 0.08651266\n",
      "Iteration 85, loss = 0.08876631\n",
      "Iteration 86, loss = 0.08823000\n",
      "Iteration 87, loss = 0.09034529\n",
      "Iteration 88, loss = 0.08969795\n",
      "Iteration 89, loss = 0.08856588\n",
      "Iteration 90, loss = 0.08682392\n",
      "Iteration 91, loss = 0.09023309\n",
      "Iteration 92, loss = 0.08673941\n",
      "Iteration 93, loss = 0.08696899\n",
      "Iteration 94, loss = 0.08975944\n",
      "Iteration 95, loss = 0.08747048\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.01, batch_size=100, hidden_layer_sizes=(100, 100, 100), solver=adam; total time= 2.2min\n",
      "Iteration 1, loss = 2.31128713\n",
      "Iteration 2, loss = 2.30815973\n",
      "Iteration 3, loss = 2.30822834\n",
      "Iteration 4, loss = 2.30810422\n",
      "Iteration 5, loss = 2.30784562\n",
      "Iteration 6, loss = 2.30785558\n",
      "Iteration 7, loss = 2.30773633\n",
      "Iteration 8, loss = 2.30773302\n",
      "Iteration 9, loss = 2.30756279\n",
      "Iteration 10, loss = 2.30744196\n",
      "Iteration 11, loss = 2.30737660\n",
      "Iteration 12, loss = 2.30728691\n",
      "Iteration 13, loss = 2.30719768\n",
      "Iteration 14, loss = 2.30723338\n",
      "Iteration 15, loss = 2.30689412\n",
      "Iteration 16, loss = 2.30677547\n",
      "Iteration 17, loss = 2.30663675\n",
      "Iteration 18, loss = 2.30673070\n",
      "Iteration 19, loss = 2.30650242\n",
      "Iteration 20, loss = 2.30635467\n",
      "Iteration 21, loss = 2.30633909\n",
      "Iteration 22, loss = 2.30608728\n",
      "Iteration 23, loss = 2.30621515\n",
      "Iteration 24, loss = 2.30608728\n",
      "Iteration 25, loss = 2.30579715\n",
      "Iteration 26, loss = 2.30568371\n",
      "Iteration 27, loss = 2.30563439\n",
      "Iteration 28, loss = 2.30518966\n",
      "Iteration 29, loss = 2.30501044\n",
      "Iteration 30, loss = 2.30506872\n",
      "Iteration 31, loss = 2.30455632\n",
      "Iteration 32, loss = 2.30469426\n",
      "Iteration 33, loss = 2.30428361\n",
      "Iteration 34, loss = 2.30398194\n",
      "Iteration 35, loss = 2.30381920\n",
      "Iteration 36, loss = 2.30361480\n",
      "Iteration 37, loss = 2.30327755\n",
      "Iteration 38, loss = 2.30313378\n",
      "Iteration 39, loss = 2.30271163\n",
      "Iteration 40, loss = 2.30227953\n",
      "Iteration 41, loss = 2.30196762\n",
      "Iteration 42, loss = 2.30169932\n",
      "Iteration 43, loss = 2.30131858\n",
      "Iteration 44, loss = 2.30066845\n",
      "Iteration 45, loss = 2.30022973\n",
      "Iteration 46, loss = 2.29968849\n",
      "Iteration 47, loss = 2.29892462\n",
      "Iteration 48, loss = 2.29819327\n",
      "Iteration 49, loss = 2.29741637\n",
      "Iteration 50, loss = 2.29665903\n",
      "Iteration 51, loss = 2.29563470\n",
      "Iteration 52, loss = 2.29453980\n",
      "Iteration 53, loss = 2.29315073\n",
      "Iteration 54, loss = 2.29181396\n",
      "Iteration 55, loss = 2.29016843\n",
      "Iteration 56, loss = 2.28838045\n",
      "Iteration 57, loss = 2.28602448\n",
      "Iteration 58, loss = 2.28351161\n",
      "Iteration 59, loss = 2.28020573\n",
      "Iteration 60, loss = 2.27656757\n",
      "Iteration 61, loss = 2.27218203\n",
      "Iteration 62, loss = 2.26653579\n",
      "Iteration 63, loss = 2.25972895\n",
      "Iteration 64, loss = 2.25093794\n",
      "Iteration 65, loss = 2.23981998\n",
      "Iteration 66, loss = 2.22542291\n",
      "Iteration 67, loss = 2.20653961\n",
      "Iteration 68, loss = 2.18173620\n",
      "Iteration 69, loss = 2.14933397\n",
      "Iteration 70, loss = 2.10798525\n",
      "Iteration 71, loss = 2.05772794\n",
      "Iteration 72, loss = 2.00043885\n",
      "Iteration 73, loss = 1.93941383\n",
      "Iteration 74, loss = 1.87679970\n",
      "Iteration 75, loss = 1.81343507\n",
      "Iteration 76, loss = 1.74821807\n",
      "Iteration 77, loss = 1.68151148\n",
      "Iteration 78, loss = 1.61581496\n",
      "Iteration 79, loss = 1.55467075\n",
      "Iteration 80, loss = 1.50121222\n",
      "Iteration 81, loss = 1.45635926\n",
      "Iteration 82, loss = 1.41936938\n",
      "Iteration 83, loss = 1.38933996\n",
      "Iteration 84, loss = 1.36402663\n",
      "Iteration 85, loss = 1.34252795\n",
      "Iteration 86, loss = 1.32366770\n",
      "Iteration 87, loss = 1.30666991\n",
      "Iteration 88, loss = 1.29132134\n",
      "Iteration 89, loss = 1.27712555\n",
      "Iteration 90, loss = 1.26341292\n",
      "Iteration 91, loss = 1.25051488\n",
      "Iteration 92, loss = 1.23776421\n",
      "Iteration 93, loss = 1.22547104\n",
      "Iteration 94, loss = 1.21300209\n",
      "Iteration 95, loss = 1.20038548\n",
      "Iteration 96, loss = 1.18759835\n",
      "Iteration 97, loss = 1.17462476\n",
      "Iteration 98, loss = 1.16098453\n",
      "Iteration 99, loss = 1.14695747\n",
      "Iteration 100, loss = 1.13206181\n",
      "Iteration 101, loss = 1.11648229\n",
      "Iteration 102, loss = 1.09974849\n",
      "Iteration 103, loss = 1.08177161\n",
      "Iteration 104, loss = 1.06230723\n",
      "Iteration 105, loss = 1.04172539\n",
      "Iteration 106, loss = 1.01908897\n",
      "Iteration 107, loss = 0.99534511\n",
      "Iteration 108, loss = 0.97032811\n",
      "Iteration 109, loss = 0.94500889\n",
      "Iteration 110, loss = 0.92016768\n",
      "Iteration 111, loss = 0.89655821\n",
      "Iteration 112, loss = 0.87481042\n",
      "Iteration 113, loss = 0.85501017\n",
      "Iteration 114, loss = 0.83740123\n",
      "Iteration 115, loss = 0.82157598\n",
      "Iteration 116, loss = 0.80753815\n",
      "Iteration 117, loss = 0.79483471\n",
      "Iteration 118, loss = 0.78313474\n",
      "Iteration 119, loss = 0.77265347\n",
      "Iteration 120, loss = 0.76270859\n",
      "Iteration 121, loss = 0.75350216\n",
      "Iteration 122, loss = 0.74486364\n",
      "Iteration 123, loss = 0.73648822\n",
      "Iteration 124, loss = 0.72873539\n",
      "Iteration 125, loss = 0.72102659\n",
      "Iteration 126, loss = 0.71401046\n",
      "Iteration 127, loss = 0.70694634\n",
      "Iteration 128, loss = 0.70019055\n",
      "Iteration 129, loss = 0.69351696\n",
      "Iteration 130, loss = 0.68730073\n",
      "Iteration 131, loss = 0.68105630\n",
      "Iteration 132, loss = 0.67492606\n",
      "Iteration 133, loss = 0.66879417\n",
      "Iteration 134, loss = 0.66322530\n",
      "Iteration 135, loss = 0.65739664\n",
      "Iteration 136, loss = 0.65185900\n",
      "Iteration 137, loss = 0.64631134\n",
      "Iteration 138, loss = 0.64104268\n",
      "Iteration 139, loss = 0.63577934\n",
      "Iteration 140, loss = 0.63054517\n",
      "Iteration 141, loss = 0.62555363\n",
      "Iteration 142, loss = 0.62047214\n",
      "Iteration 143, loss = 0.61560069\n",
      "Iteration 144, loss = 0.61076889\n",
      "Iteration 145, loss = 0.60617852\n",
      "Iteration 146, loss = 0.60146899\n",
      "Iteration 147, loss = 0.59690508\n",
      "Iteration 148, loss = 0.59229673\n",
      "Iteration 149, loss = 0.58805827\n",
      "Iteration 150, loss = 0.58382619\n",
      "Iteration 151, loss = 0.57960788\n",
      "Iteration 152, loss = 0.57523200\n",
      "Iteration 153, loss = 0.57136859\n",
      "Iteration 154, loss = 0.56743527\n",
      "Iteration 155, loss = 0.56342459\n",
      "Iteration 156, loss = 0.55949371\n",
      "Iteration 157, loss = 0.55578314\n",
      "Iteration 158, loss = 0.55192628\n",
      "Iteration 159, loss = 0.54838807\n",
      "Iteration 160, loss = 0.54488028\n",
      "Iteration 161, loss = 0.54132435\n",
      "Iteration 162, loss = 0.53785929\n",
      "Iteration 163, loss = 0.53447177\n",
      "Iteration 164, loss = 0.53116520\n",
      "Iteration 165, loss = 0.52786936\n",
      "Iteration 166, loss = 0.52464066\n",
      "Iteration 167, loss = 0.52136730\n",
      "Iteration 168, loss = 0.51832363\n",
      "Iteration 169, loss = 0.51528458\n",
      "Iteration 170, loss = 0.51227501\n",
      "Iteration 171, loss = 0.50925498\n",
      "Iteration 172, loss = 0.50621580\n",
      "Iteration 173, loss = 0.50332067\n",
      "Iteration 174, loss = 0.50054438\n",
      "Iteration 175, loss = 0.49776017\n",
      "Iteration 176, loss = 0.49489279\n",
      "Iteration 177, loss = 0.49214934\n",
      "Iteration 178, loss = 0.48961872\n",
      "Iteration 179, loss = 0.48701011\n",
      "Iteration 180, loss = 0.48428229\n",
      "Iteration 181, loss = 0.48168201\n",
      "Iteration 182, loss = 0.47908423\n",
      "Iteration 183, loss = 0.47668380\n",
      "Iteration 184, loss = 0.47420841\n",
      "Iteration 185, loss = 0.47164162\n",
      "Iteration 186, loss = 0.46921363\n",
      "Iteration 187, loss = 0.46676701\n",
      "Iteration 188, loss = 0.46446443\n",
      "Iteration 189, loss = 0.46201054\n",
      "Iteration 190, loss = 0.45970066\n",
      "Iteration 191, loss = 0.45741186\n",
      "Iteration 192, loss = 0.45516093\n",
      "Iteration 193, loss = 0.45294761\n",
      "Iteration 194, loss = 0.45065873\n",
      "Iteration 195, loss = 0.44842026\n",
      "Iteration 196, loss = 0.44627367\n",
      "Iteration 197, loss = 0.44408638\n",
      "Iteration 198, loss = 0.44193318\n",
      "Iteration 199, loss = 0.43971873\n",
      "Iteration 200, loss = 0.43767432\n",
      "[CV] END activation=logistic, alpha=0.01, batch_size=100, hidden_layer_sizes=(100, 100, 100), solver=sgd; total time= 3.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.31764411\n",
      "Iteration 2, loss = 2.30851210\n",
      "Iteration 3, loss = 2.30837497\n",
      "Iteration 4, loss = 2.30833797\n",
      "Iteration 5, loss = 2.30826195\n",
      "Iteration 6, loss = 2.30822876\n",
      "Iteration 7, loss = 2.30809882\n",
      "Iteration 8, loss = 2.30793605\n",
      "Iteration 9, loss = 2.30797619\n",
      "Iteration 10, loss = 2.30779565\n",
      "Iteration 11, loss = 2.30785056\n",
      "Iteration 12, loss = 2.30775433\n",
      "Iteration 13, loss = 2.30760608\n",
      "Iteration 14, loss = 2.30756325\n",
      "Iteration 15, loss = 2.30758557\n",
      "Iteration 16, loss = 2.30726713\n",
      "Iteration 17, loss = 2.30724403\n",
      "Iteration 18, loss = 2.30725200\n",
      "Iteration 19, loss = 2.30720303\n",
      "Iteration 20, loss = 2.30699281\n",
      "Iteration 21, loss = 2.30691673\n",
      "Iteration 22, loss = 2.30678087\n",
      "Iteration 23, loss = 2.30665586\n",
      "Iteration 24, loss = 2.30673742\n",
      "Iteration 25, loss = 2.30661649\n",
      "Iteration 26, loss = 2.30645413\n",
      "Iteration 27, loss = 2.30635977\n",
      "Iteration 28, loss = 2.30623083\n",
      "Iteration 29, loss = 2.30620901\n",
      "Iteration 30, loss = 2.30597127\n",
      "Iteration 31, loss = 2.30591003\n",
      "Iteration 32, loss = 2.30562825\n",
      "Iteration 33, loss = 2.30560289\n",
      "Iteration 34, loss = 2.30525525\n",
      "Iteration 35, loss = 2.30507264\n",
      "Iteration 36, loss = 2.30497971\n",
      "Iteration 37, loss = 2.30462573\n",
      "Iteration 38, loss = 2.30455577\n",
      "Iteration 39, loss = 2.30439808\n",
      "Iteration 40, loss = 2.30405235\n",
      "Iteration 41, loss = 2.30371028\n",
      "Iteration 42, loss = 2.30367239\n",
      "Iteration 43, loss = 2.30315249\n",
      "Iteration 44, loss = 2.30283476\n",
      "Iteration 45, loss = 2.30255023\n",
      "Iteration 46, loss = 2.30222449\n",
      "Iteration 47, loss = 2.30181520\n",
      "Iteration 48, loss = 2.30131175\n",
      "Iteration 49, loss = 2.30073451\n",
      "Iteration 50, loss = 2.30008172\n",
      "Iteration 51, loss = 2.29947579\n",
      "Iteration 52, loss = 2.29885753\n",
      "Iteration 53, loss = 2.29803292\n",
      "Iteration 54, loss = 2.29711894\n",
      "Iteration 55, loss = 2.29610792\n",
      "Iteration 56, loss = 2.29497726\n",
      "Iteration 57, loss = 2.29348653\n",
      "Iteration 58, loss = 2.29193697\n",
      "Iteration 59, loss = 2.29009202\n",
      "Iteration 60, loss = 2.28787016\n",
      "Iteration 61, loss = 2.28523924\n",
      "Iteration 62, loss = 2.28207379\n",
      "Iteration 63, loss = 2.27809501\n",
      "Iteration 64, loss = 2.27303069\n",
      "Iteration 65, loss = 2.26665676\n",
      "Iteration 66, loss = 2.25885299\n",
      "Iteration 67, loss = 2.24814477\n",
      "Iteration 68, loss = 2.23427859\n",
      "Iteration 69, loss = 2.21577841\n",
      "Iteration 70, loss = 2.19068910\n",
      "Iteration 71, loss = 2.15830234\n",
      "Iteration 72, loss = 2.11802513\n",
      "Iteration 73, loss = 2.07170387\n",
      "Iteration 74, loss = 2.02350224\n",
      "Iteration 75, loss = 1.97756577\n",
      "Iteration 76, loss = 1.93610128\n",
      "Iteration 77, loss = 1.89939129\n",
      "Iteration 78, loss = 1.86671067\n",
      "Iteration 79, loss = 1.83711006\n",
      "Iteration 80, loss = 1.80988476\n",
      "Iteration 81, loss = 1.78482691\n",
      "Iteration 82, loss = 1.76105495\n",
      "Iteration 83, loss = 1.73818210\n",
      "Iteration 84, loss = 1.71538712\n",
      "Iteration 85, loss = 1.69228029\n",
      "Iteration 86, loss = 1.66788752\n",
      "Iteration 87, loss = 1.64144117\n",
      "Iteration 88, loss = 1.61251157\n",
      "Iteration 89, loss = 1.58029224\n",
      "Iteration 90, loss = 1.54436970\n",
      "Iteration 91, loss = 1.50559291\n",
      "Iteration 92, loss = 1.46549751\n",
      "Iteration 93, loss = 1.42581204\n",
      "Iteration 94, loss = 1.38888550\n",
      "Iteration 95, loss = 1.35621127\n",
      "Iteration 96, loss = 1.32803552\n",
      "Iteration 97, loss = 1.30355120\n",
      "Iteration 98, loss = 1.28271366\n",
      "Iteration 99, loss = 1.26392355\n",
      "Iteration 100, loss = 1.24701076\n",
      "Iteration 101, loss = 1.23151331\n",
      "Iteration 102, loss = 1.21718998\n",
      "Iteration 103, loss = 1.20357463\n",
      "Iteration 104, loss = 1.19048935\n",
      "Iteration 105, loss = 1.17805755\n",
      "Iteration 106, loss = 1.16590203\n",
      "Iteration 107, loss = 1.15401122\n",
      "Iteration 108, loss = 1.14267796\n",
      "Iteration 109, loss = 1.13126440\n",
      "Iteration 110, loss = 1.12026873\n",
      "Iteration 111, loss = 1.10947273\n",
      "Iteration 112, loss = 1.09865474\n",
      "Iteration 113, loss = 1.08796398\n",
      "Iteration 114, loss = 1.07706757\n",
      "Iteration 115, loss = 1.06621526\n",
      "Iteration 116, loss = 1.05508801\n",
      "Iteration 117, loss = 1.04352154\n",
      "Iteration 118, loss = 1.03182427\n",
      "Iteration 119, loss = 1.01964161\n",
      "Iteration 120, loss = 1.00685386\n",
      "Iteration 121, loss = 0.99351033\n",
      "Iteration 122, loss = 0.97926219\n",
      "Iteration 123, loss = 0.96406707\n",
      "Iteration 124, loss = 0.94767232\n",
      "Iteration 125, loss = 0.93005134\n",
      "Iteration 126, loss = 0.91124763\n",
      "Iteration 127, loss = 0.89117371\n",
      "Iteration 128, loss = 0.87013931\n",
      "Iteration 129, loss = 0.84802949\n",
      "Iteration 130, loss = 0.82596689\n",
      "Iteration 131, loss = 0.80430457\n",
      "Iteration 132, loss = 0.78338492\n",
      "Iteration 133, loss = 0.76414913\n",
      "Iteration 134, loss = 0.74609951\n",
      "Iteration 135, loss = 0.72984517\n",
      "Iteration 136, loss = 0.71513731\n",
      "Iteration 137, loss = 0.70180983\n",
      "Iteration 138, loss = 0.68966946\n",
      "Iteration 139, loss = 0.67854585\n",
      "Iteration 140, loss = 0.66863788\n",
      "Iteration 141, loss = 0.65894837\n",
      "Iteration 142, loss = 0.65031673\n",
      "Iteration 143, loss = 0.64206423\n",
      "Iteration 144, loss = 0.63431968\n",
      "Iteration 145, loss = 0.62717231\n",
      "Iteration 146, loss = 0.62007513\n",
      "Iteration 147, loss = 0.61344916\n",
      "Iteration 148, loss = 0.60726386\n",
      "Iteration 149, loss = 0.60127064\n",
      "Iteration 150, loss = 0.59547419\n",
      "Iteration 151, loss = 0.58995186\n",
      "Iteration 152, loss = 0.58451577\n",
      "Iteration 153, loss = 0.57941813\n",
      "Iteration 154, loss = 0.57435026\n",
      "Iteration 155, loss = 0.56960718\n",
      "Iteration 156, loss = 0.56486888\n",
      "Iteration 157, loss = 0.56027388\n",
      "Iteration 158, loss = 0.55578587\n",
      "Iteration 159, loss = 0.55140814\n",
      "Iteration 160, loss = 0.54721588\n",
      "Iteration 161, loss = 0.54328721\n",
      "Iteration 162, loss = 0.53930596\n",
      "Iteration 163, loss = 0.53532944\n",
      "Iteration 164, loss = 0.53161612\n",
      "Iteration 165, loss = 0.52783807\n",
      "Iteration 166, loss = 0.52427803\n",
      "Iteration 167, loss = 0.52068929\n",
      "Iteration 168, loss = 0.51715428\n",
      "Iteration 169, loss = 0.51378005\n",
      "Iteration 170, loss = 0.51048941\n",
      "Iteration 171, loss = 0.50713275\n",
      "Iteration 172, loss = 0.50396054\n",
      "Iteration 173, loss = 0.50082134\n",
      "Iteration 174, loss = 0.49779197\n",
      "Iteration 175, loss = 0.49468323\n",
      "Iteration 176, loss = 0.49179773\n",
      "Iteration 177, loss = 0.48894861\n",
      "Iteration 178, loss = 0.48582580\n",
      "Iteration 179, loss = 0.48299974\n",
      "Iteration 180, loss = 0.48009031\n",
      "Iteration 181, loss = 0.47748298\n",
      "Iteration 182, loss = 0.47470414\n",
      "Iteration 183, loss = 0.47196868\n",
      "Iteration 184, loss = 0.46944228\n",
      "Iteration 185, loss = 0.46680013\n",
      "Iteration 186, loss = 0.46406834\n",
      "Iteration 187, loss = 0.46148529\n",
      "Iteration 188, loss = 0.45883831\n",
      "Iteration 189, loss = 0.45625200\n",
      "Iteration 190, loss = 0.45404947\n",
      "Iteration 191, loss = 0.45142799\n",
      "Iteration 192, loss = 0.44889863\n",
      "Iteration 193, loss = 0.44648951\n",
      "Iteration 194, loss = 0.44434302\n",
      "Iteration 195, loss = 0.44190501\n",
      "Iteration 196, loss = 0.43965431\n",
      "Iteration 197, loss = 0.43715891\n",
      "Iteration 198, loss = 0.43482515\n",
      "Iteration 199, loss = 0.43259921\n",
      "Iteration 200, loss = 0.43024948\n",
      "[CV] END activation=logistic, alpha=0.01, batch_size=100, hidden_layer_sizes=(100, 100, 100), solver=sgd; total time= 3.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.31627697\n",
      "Iteration 2, loss = 2.30837851\n",
      "Iteration 3, loss = 2.30835837\n",
      "Iteration 4, loss = 2.30818457\n",
      "Iteration 5, loss = 2.30809648\n",
      "Iteration 6, loss = 2.30796382\n",
      "Iteration 7, loss = 2.30793224\n",
      "Iteration 8, loss = 2.30778386\n",
      "Iteration 9, loss = 2.30771308\n",
      "Iteration 10, loss = 2.30749243\n",
      "Iteration 11, loss = 2.30746265\n",
      "Iteration 12, loss = 2.30746736\n",
      "Iteration 13, loss = 2.30723367\n",
      "Iteration 14, loss = 2.30716004\n",
      "Iteration 15, loss = 2.30715517\n",
      "Iteration 16, loss = 2.30704362\n",
      "Iteration 17, loss = 2.30677682\n",
      "Iteration 18, loss = 2.30684607\n",
      "Iteration 19, loss = 2.30659257\n",
      "Iteration 20, loss = 2.30640513\n",
      "Iteration 21, loss = 2.30635582\n",
      "Iteration 22, loss = 2.30614226\n",
      "Iteration 23, loss = 2.30614496\n",
      "Iteration 24, loss = 2.30593962\n",
      "Iteration 25, loss = 2.30576316\n",
      "Iteration 26, loss = 2.30558626\n",
      "Iteration 27, loss = 2.30533350\n",
      "Iteration 28, loss = 2.30541636\n",
      "Iteration 29, loss = 2.30499799\n",
      "Iteration 30, loss = 2.30484346\n",
      "Iteration 31, loss = 2.30454753\n",
      "Iteration 32, loss = 2.30449944\n",
      "Iteration 33, loss = 2.30418206\n",
      "Iteration 34, loss = 2.30373006\n",
      "Iteration 35, loss = 2.30353140\n",
      "Iteration 36, loss = 2.30340020\n",
      "Iteration 37, loss = 2.30302792\n",
      "Iteration 38, loss = 2.30277000\n",
      "Iteration 39, loss = 2.30221303\n",
      "Iteration 40, loss = 2.30192157\n",
      "Iteration 41, loss = 2.30137112\n",
      "Iteration 42, loss = 2.30104667\n",
      "Iteration 43, loss = 2.30028066\n",
      "Iteration 44, loss = 2.29963626\n",
      "Iteration 45, loss = 2.29893897\n",
      "Iteration 46, loss = 2.29817229\n",
      "Iteration 47, loss = 2.29733850\n",
      "Iteration 48, loss = 2.29657199\n",
      "Iteration 49, loss = 2.29527275\n",
      "Iteration 50, loss = 2.29386658\n",
      "Iteration 51, loss = 2.29246464\n",
      "Iteration 52, loss = 2.29056162\n",
      "Iteration 53, loss = 2.28870826\n",
      "Iteration 54, loss = 2.28646512\n",
      "Iteration 55, loss = 2.28335883\n",
      "Iteration 56, loss = 2.28000209\n",
      "Iteration 57, loss = 2.27554051\n",
      "Iteration 58, loss = 2.27022809\n",
      "Iteration 59, loss = 2.26344206\n",
      "Iteration 60, loss = 2.25435731\n",
      "Iteration 61, loss = 2.24273330\n",
      "Iteration 62, loss = 2.22714420\n",
      "Iteration 63, loss = 2.20642179\n",
      "Iteration 64, loss = 2.17876642\n",
      "Iteration 65, loss = 2.14244420\n",
      "Iteration 66, loss = 2.09808497\n",
      "Iteration 67, loss = 2.04782438\n",
      "Iteration 68, loss = 1.99669786\n",
      "Iteration 69, loss = 1.94857632\n",
      "Iteration 70, loss = 1.90594272\n",
      "Iteration 71, loss = 1.86934922\n",
      "Iteration 72, loss = 1.83769598\n",
      "Iteration 73, loss = 1.80998369\n",
      "Iteration 74, loss = 1.78495547\n",
      "Iteration 75, loss = 1.76189690\n",
      "Iteration 76, loss = 1.74012098\n",
      "Iteration 77, loss = 1.71793780\n",
      "Iteration 78, loss = 1.69486642\n",
      "Iteration 79, loss = 1.66967253\n",
      "Iteration 80, loss = 1.64172823\n",
      "Iteration 81, loss = 1.61005521\n",
      "Iteration 82, loss = 1.57448227\n",
      "Iteration 83, loss = 1.53534983\n",
      "Iteration 84, loss = 1.49497872\n",
      "Iteration 85, loss = 1.45529645\n",
      "Iteration 86, loss = 1.41861271\n",
      "Iteration 87, loss = 1.38599664\n",
      "Iteration 88, loss = 1.35693441\n",
      "Iteration 89, loss = 1.33196319\n",
      "Iteration 90, loss = 1.30938831\n",
      "Iteration 91, loss = 1.28932507\n",
      "Iteration 92, loss = 1.27040529\n",
      "Iteration 93, loss = 1.25303328\n",
      "Iteration 94, loss = 1.23595505\n",
      "Iteration 95, loss = 1.21957822\n",
      "Iteration 96, loss = 1.20326712\n",
      "Iteration 97, loss = 1.18719341\n",
      "Iteration 98, loss = 1.17061801\n",
      "Iteration 99, loss = 1.15387330\n",
      "Iteration 100, loss = 1.13650640\n",
      "Iteration 101, loss = 1.11842396\n",
      "Iteration 102, loss = 1.09987947\n",
      "Iteration 103, loss = 1.08026076\n",
      "Iteration 104, loss = 1.06009400\n",
      "Iteration 105, loss = 1.03891184\n",
      "Iteration 106, loss = 1.01712080\n",
      "Iteration 107, loss = 0.99479336\n",
      "Iteration 108, loss = 0.97196574\n",
      "Iteration 109, loss = 0.94934047\n",
      "Iteration 110, loss = 0.92669046\n",
      "Iteration 111, loss = 0.90485092\n",
      "Iteration 112, loss = 0.88370662\n",
      "Iteration 113, loss = 0.86368966\n",
      "Iteration 114, loss = 0.84492406\n",
      "Iteration 115, loss = 0.82744161\n",
      "Iteration 116, loss = 0.81129505\n",
      "Iteration 117, loss = 0.79624688\n",
      "Iteration 118, loss = 0.78244068\n",
      "Iteration 119, loss = 0.76996756\n",
      "Iteration 120, loss = 0.75797218\n",
      "Iteration 121, loss = 0.74713218\n",
      "Iteration 122, loss = 0.73664058\n",
      "Iteration 123, loss = 0.72716578\n",
      "Iteration 124, loss = 0.71796142\n",
      "Iteration 125, loss = 0.70942548\n",
      "Iteration 126, loss = 0.70114398\n",
      "Iteration 127, loss = 0.69328446\n",
      "Iteration 128, loss = 0.68567000\n",
      "Iteration 129, loss = 0.67840471\n",
      "Iteration 130, loss = 0.67140544\n",
      "Iteration 131, loss = 0.66458655\n",
      "Iteration 132, loss = 0.65798622\n",
      "Iteration 133, loss = 0.65165184\n",
      "Iteration 134, loss = 0.64558297\n",
      "Iteration 135, loss = 0.63950069\n",
      "Iteration 136, loss = 0.63358849\n",
      "Iteration 137, loss = 0.62804366\n",
      "Iteration 138, loss = 0.62233562\n",
      "Iteration 139, loss = 0.61694399\n",
      "Iteration 140, loss = 0.61155594\n",
      "Iteration 141, loss = 0.60649895\n",
      "Iteration 142, loss = 0.60152557\n",
      "Iteration 143, loss = 0.59655420\n",
      "Iteration 144, loss = 0.59187529\n",
      "Iteration 145, loss = 0.58717747\n",
      "Iteration 146, loss = 0.58266640\n",
      "Iteration 147, loss = 0.57832395\n",
      "Iteration 148, loss = 0.57389754\n",
      "Iteration 149, loss = 0.56965685\n",
      "Iteration 150, loss = 0.56543027\n",
      "Iteration 151, loss = 0.56150212\n",
      "Iteration 152, loss = 0.55757955\n",
      "Iteration 153, loss = 0.55371081\n",
      "Iteration 154, loss = 0.55008374\n",
      "Iteration 155, loss = 0.54623258\n",
      "Iteration 156, loss = 0.54280990\n",
      "Iteration 157, loss = 0.53918040\n",
      "Iteration 158, loss = 0.53579822\n",
      "Iteration 159, loss = 0.53234263\n",
      "Iteration 160, loss = 0.52912620\n",
      "Iteration 161, loss = 0.52584967\n",
      "Iteration 162, loss = 0.52252311\n",
      "Iteration 163, loss = 0.51954659\n",
      "Iteration 164, loss = 0.51651574\n",
      "Iteration 165, loss = 0.51329341\n",
      "Iteration 166, loss = 0.51051381\n",
      "Iteration 167, loss = 0.50772652\n",
      "Iteration 168, loss = 0.50484356\n",
      "Iteration 169, loss = 0.50192359\n",
      "Iteration 170, loss = 0.49920645\n",
      "Iteration 171, loss = 0.49666359\n",
      "Iteration 172, loss = 0.49380665\n",
      "Iteration 173, loss = 0.49109459\n",
      "Iteration 174, loss = 0.48858047\n",
      "Iteration 175, loss = 0.48615098\n",
      "Iteration 176, loss = 0.48343793\n",
      "Iteration 177, loss = 0.48101126\n",
      "Iteration 178, loss = 0.47856649\n",
      "Iteration 179, loss = 0.47615124\n",
      "Iteration 180, loss = 0.47376326\n",
      "Iteration 181, loss = 0.47154964\n",
      "Iteration 182, loss = 0.46909989\n",
      "Iteration 183, loss = 0.46690343\n",
      "Iteration 184, loss = 0.46435577\n",
      "Iteration 185, loss = 0.46219844\n",
      "Iteration 186, loss = 0.45999325\n",
      "Iteration 187, loss = 0.45774678\n",
      "Iteration 188, loss = 0.45555549\n",
      "Iteration 189, loss = 0.45348639\n",
      "Iteration 190, loss = 0.45134718\n",
      "Iteration 191, loss = 0.44911272\n",
      "Iteration 192, loss = 0.44699989\n",
      "Iteration 193, loss = 0.44495452\n",
      "Iteration 194, loss = 0.44289366\n",
      "Iteration 195, loss = 0.44079660\n",
      "Iteration 196, loss = 0.43874816\n",
      "Iteration 197, loss = 0.43673885\n",
      "Iteration 198, loss = 0.43452331\n",
      "Iteration 199, loss = 0.43259382\n",
      "Iteration 200, loss = 0.43073230\n",
      "[CV] END activation=logistic, alpha=0.01, batch_size=100, hidden_layer_sizes=(100, 100, 100), solver=sgd; total time= 3.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.31372143\n",
      "Iteration 2, loss = 2.30829107\n",
      "Iteration 3, loss = 2.30845991\n",
      "Iteration 4, loss = 2.30822130\n",
      "Iteration 5, loss = 2.30827535\n",
      "Iteration 6, loss = 2.30815935\n",
      "Iteration 7, loss = 2.30793964\n",
      "Iteration 8, loss = 2.30806132\n",
      "Iteration 9, loss = 2.30790399\n",
      "Iteration 10, loss = 2.30785804\n",
      "Iteration 11, loss = 2.30798660\n",
      "Iteration 12, loss = 2.30766990\n",
      "Iteration 13, loss = 2.30754646\n",
      "Iteration 14, loss = 2.30746617\n",
      "Iteration 15, loss = 2.30732942\n",
      "Iteration 16, loss = 2.30723671\n",
      "Iteration 17, loss = 2.30723268\n",
      "Iteration 18, loss = 2.30705092\n",
      "Iteration 19, loss = 2.30686492\n",
      "Iteration 20, loss = 2.30690740\n",
      "Iteration 21, loss = 2.30658717\n",
      "Iteration 22, loss = 2.30682345\n",
      "Iteration 23, loss = 2.30637903\n",
      "Iteration 24, loss = 2.30649548\n",
      "Iteration 25, loss = 2.30630149\n",
      "Iteration 26, loss = 2.30604413\n",
      "Iteration 27, loss = 2.30610091\n",
      "Iteration 28, loss = 2.30605126\n",
      "Iteration 29, loss = 2.30572089\n",
      "Iteration 30, loss = 2.30542485\n",
      "Iteration 31, loss = 2.30539865\n",
      "Iteration 32, loss = 2.30528608\n",
      "Iteration 33, loss = 2.30503402\n",
      "Iteration 34, loss = 2.30490928\n",
      "Iteration 35, loss = 2.30478578\n",
      "Iteration 36, loss = 2.30442397\n",
      "Iteration 37, loss = 2.30416227\n",
      "Iteration 38, loss = 2.30391469\n",
      "Iteration 39, loss = 2.30355568\n",
      "Iteration 40, loss = 2.30346037\n",
      "Iteration 41, loss = 2.30312054\n",
      "Iteration 42, loss = 2.30280209\n",
      "Iteration 43, loss = 2.30246495\n",
      "Iteration 44, loss = 2.30197909\n",
      "Iteration 45, loss = 2.30152715\n",
      "Iteration 46, loss = 2.30098056\n",
      "Iteration 47, loss = 2.30053057\n",
      "Iteration 48, loss = 2.30000893\n",
      "Iteration 49, loss = 2.29926319\n",
      "Iteration 50, loss = 2.29869668\n",
      "Iteration 51, loss = 2.29802739\n",
      "Iteration 52, loss = 2.29727539\n",
      "Iteration 53, loss = 2.29606899\n",
      "Iteration 54, loss = 2.29502374\n",
      "Iteration 55, loss = 2.29365077\n",
      "Iteration 56, loss = 2.29228812\n",
      "Iteration 57, loss = 2.29061399\n",
      "Iteration 58, loss = 2.28863681\n",
      "Iteration 59, loss = 2.28628523\n",
      "Iteration 60, loss = 2.28338540\n",
      "Iteration 61, loss = 2.28013556\n",
      "Iteration 62, loss = 2.27612272\n",
      "Iteration 63, loss = 2.27103263\n",
      "Iteration 64, loss = 2.26459262\n",
      "Iteration 65, loss = 2.25622224\n",
      "Iteration 66, loss = 2.24569340\n",
      "Iteration 67, loss = 2.23153060\n",
      "Iteration 68, loss = 2.21228557\n",
      "Iteration 69, loss = 2.18586919\n",
      "Iteration 70, loss = 2.15001331\n",
      "Iteration 71, loss = 2.10288748\n",
      "Iteration 72, loss = 2.04453180\n",
      "Iteration 73, loss = 1.98092200\n",
      "Iteration 74, loss = 1.92006773\n",
      "Iteration 75, loss = 1.86789446\n",
      "Iteration 76, loss = 1.82545241\n",
      "Iteration 77, loss = 1.79067380\n",
      "Iteration 78, loss = 1.75978328\n",
      "Iteration 79, loss = 1.73007021\n",
      "Iteration 80, loss = 1.69909067\n",
      "Iteration 81, loss = 1.66513500\n",
      "Iteration 82, loss = 1.62823767\n",
      "Iteration 83, loss = 1.58870069\n",
      "Iteration 84, loss = 1.54838692\n",
      "Iteration 85, loss = 1.50887562\n",
      "Iteration 86, loss = 1.47207328\n",
      "Iteration 87, loss = 1.43861946\n",
      "Iteration 88, loss = 1.40899004\n",
      "Iteration 89, loss = 1.38221428\n",
      "Iteration 90, loss = 1.35819882\n",
      "Iteration 91, loss = 1.33657036\n",
      "Iteration 92, loss = 1.31657705\n",
      "Iteration 93, loss = 1.29792133\n",
      "Iteration 94, loss = 1.28020806\n",
      "Iteration 95, loss = 1.26336478\n",
      "Iteration 96, loss = 1.24685155\n",
      "Iteration 97, loss = 1.23081828\n",
      "Iteration 98, loss = 1.21497599\n",
      "Iteration 99, loss = 1.19954463\n",
      "Iteration 100, loss = 1.18414750\n",
      "Iteration 101, loss = 1.16880595\n",
      "Iteration 102, loss = 1.15353023\n",
      "Iteration 103, loss = 1.13857738\n",
      "Iteration 104, loss = 1.12373311\n",
      "Iteration 105, loss = 1.10920756\n",
      "Iteration 106, loss = 1.09482985\n",
      "Iteration 107, loss = 1.08082175\n",
      "Iteration 108, loss = 1.06716971\n",
      "Iteration 109, loss = 1.05388484\n",
      "Iteration 110, loss = 1.04098149\n",
      "Iteration 111, loss = 1.02811627\n",
      "Iteration 112, loss = 1.01558509\n",
      "Iteration 113, loss = 1.00305760\n",
      "Iteration 114, loss = 0.99072145\n",
      "Iteration 115, loss = 0.97825711\n",
      "Iteration 116, loss = 0.96582138\n",
      "Iteration 117, loss = 0.95308504\n",
      "Iteration 118, loss = 0.93979713\n",
      "Iteration 119, loss = 0.92672969\n",
      "Iteration 120, loss = 0.91294236\n",
      "Iteration 121, loss = 0.89865038\n",
      "Iteration 122, loss = 0.88381523\n",
      "Iteration 123, loss = 0.86834992\n",
      "Iteration 124, loss = 0.85239931\n",
      "Iteration 125, loss = 0.83604463\n",
      "Iteration 126, loss = 0.81894751\n",
      "Iteration 127, loss = 0.80196010\n",
      "Iteration 128, loss = 0.78483956\n",
      "Iteration 129, loss = 0.76822420\n",
      "Iteration 130, loss = 0.75212185\n",
      "Iteration 131, loss = 0.73690224\n",
      "Iteration 132, loss = 0.72281450\n",
      "Iteration 133, loss = 0.70961739\n",
      "Iteration 134, loss = 0.69728226\n",
      "Iteration 135, loss = 0.68594244\n",
      "Iteration 136, loss = 0.67562795\n",
      "Iteration 137, loss = 0.66586338\n",
      "Iteration 138, loss = 0.65661022\n",
      "Iteration 139, loss = 0.64826860\n",
      "Iteration 140, loss = 0.64037267\n",
      "Iteration 141, loss = 0.63261375\n",
      "Iteration 142, loss = 0.62551449\n",
      "Iteration 143, loss = 0.61857418\n",
      "Iteration 144, loss = 0.61212648\n",
      "Iteration 145, loss = 0.60594241\n",
      "Iteration 146, loss = 0.59984904\n",
      "Iteration 147, loss = 0.59427139\n",
      "Iteration 148, loss = 0.58854183\n",
      "Iteration 149, loss = 0.58311464\n",
      "Iteration 150, loss = 0.57795521\n",
      "Iteration 151, loss = 0.57288129\n",
      "Iteration 152, loss = 0.56803797\n",
      "Iteration 153, loss = 0.56327063\n",
      "Iteration 154, loss = 0.55855277\n",
      "Iteration 155, loss = 0.55413914\n",
      "Iteration 156, loss = 0.54960345\n",
      "Iteration 157, loss = 0.54527633\n",
      "Iteration 158, loss = 0.54108928\n",
      "Iteration 159, loss = 0.53688740\n",
      "Iteration 160, loss = 0.53301956\n",
      "Iteration 161, loss = 0.52909513\n",
      "Iteration 162, loss = 0.52512188\n",
      "Iteration 163, loss = 0.52142311\n",
      "Iteration 164, loss = 0.51777770\n",
      "Iteration 165, loss = 0.51412999\n",
      "Iteration 166, loss = 0.51069430\n",
      "Iteration 167, loss = 0.50711546\n",
      "Iteration 168, loss = 0.50367953\n",
      "Iteration 169, loss = 0.50033730\n",
      "Iteration 170, loss = 0.49701526\n",
      "Iteration 171, loss = 0.49378388\n",
      "Iteration 172, loss = 0.49048052\n",
      "Iteration 173, loss = 0.48750064\n",
      "Iteration 174, loss = 0.48438083\n",
      "Iteration 175, loss = 0.48144008\n",
      "Iteration 176, loss = 0.47841241\n",
      "Iteration 177, loss = 0.47541769\n",
      "Iteration 178, loss = 0.47258939\n",
      "Iteration 179, loss = 0.46970121\n",
      "Iteration 180, loss = 0.46693768\n",
      "Iteration 181, loss = 0.46424729\n",
      "Iteration 182, loss = 0.46134696\n",
      "Iteration 183, loss = 0.45882479\n",
      "Iteration 184, loss = 0.45614390\n",
      "Iteration 185, loss = 0.45374086\n",
      "Iteration 186, loss = 0.45103829\n",
      "Iteration 187, loss = 0.44856736\n",
      "Iteration 188, loss = 0.44612535\n",
      "Iteration 189, loss = 0.44365601\n",
      "Iteration 190, loss = 0.44123977\n",
      "Iteration 191, loss = 0.43899405\n",
      "Iteration 192, loss = 0.43653497\n",
      "Iteration 193, loss = 0.43438128\n",
      "Iteration 194, loss = 0.43185184\n",
      "Iteration 195, loss = 0.42966186\n",
      "Iteration 196, loss = 0.42760477\n",
      "Iteration 197, loss = 0.42534233\n",
      "Iteration 198, loss = 0.42315335\n",
      "Iteration 199, loss = 0.42114354\n",
      "Iteration 200, loss = 0.41898927\n",
      "[CV] END activation=logistic, alpha=0.01, batch_size=100, hidden_layer_sizes=(100, 100, 100), solver=sgd; total time= 3.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.31505239\n",
      "Iteration 2, loss = 2.30838065\n",
      "Iteration 3, loss = 2.30814452\n",
      "Iteration 4, loss = 2.30822324\n",
      "Iteration 5, loss = 2.30796936\n",
      "Iteration 6, loss = 2.30800315\n",
      "Iteration 7, loss = 2.30796775\n",
      "Iteration 8, loss = 2.30786786\n",
      "Iteration 9, loss = 2.30771075\n",
      "Iteration 10, loss = 2.30759124\n",
      "Iteration 11, loss = 2.30756500\n",
      "Iteration 12, loss = 2.30741913\n",
      "Iteration 13, loss = 2.30743020\n",
      "Iteration 14, loss = 2.30721043\n",
      "Iteration 15, loss = 2.30701328\n",
      "Iteration 16, loss = 2.30695966\n",
      "Iteration 17, loss = 2.30686771\n",
      "Iteration 18, loss = 2.30673895\n",
      "Iteration 19, loss = 2.30663453\n",
      "Iteration 20, loss = 2.30666410\n",
      "Iteration 21, loss = 2.30638870\n",
      "Iteration 22, loss = 2.30636747\n",
      "Iteration 23, loss = 2.30619243\n",
      "Iteration 24, loss = 2.30604112\n",
      "Iteration 25, loss = 2.30587420\n",
      "Iteration 26, loss = 2.30574656\n",
      "Iteration 27, loss = 2.30531334\n",
      "Iteration 28, loss = 2.30546366\n",
      "Iteration 29, loss = 2.30525312\n",
      "Iteration 30, loss = 2.30505965\n",
      "Iteration 31, loss = 2.30478506\n",
      "Iteration 32, loss = 2.30466449\n",
      "Iteration 33, loss = 2.30433151\n",
      "Iteration 34, loss = 2.30412293\n",
      "Iteration 35, loss = 2.30404498\n",
      "Iteration 36, loss = 2.30380411\n",
      "Iteration 37, loss = 2.30340271\n",
      "Iteration 38, loss = 2.30298846\n",
      "Iteration 39, loss = 2.30291328\n",
      "Iteration 40, loss = 2.30233633\n",
      "Iteration 41, loss = 2.30203286\n",
      "Iteration 42, loss = 2.30163755\n",
      "Iteration 43, loss = 2.30102800\n",
      "Iteration 44, loss = 2.30074955\n",
      "Iteration 45, loss = 2.30007757\n",
      "Iteration 46, loss = 2.29959401\n",
      "Iteration 47, loss = 2.29896961\n",
      "Iteration 48, loss = 2.29825876\n",
      "Iteration 49, loss = 2.29744077\n",
      "Iteration 50, loss = 2.29646523\n",
      "Iteration 51, loss = 2.29543574\n",
      "Iteration 52, loss = 2.29433164\n",
      "Iteration 53, loss = 2.29291308\n",
      "Iteration 54, loss = 2.29136794\n",
      "Iteration 55, loss = 2.28961337\n",
      "Iteration 56, loss = 2.28750648\n",
      "Iteration 57, loss = 2.28509044\n",
      "Iteration 58, loss = 2.28193181\n",
      "Iteration 59, loss = 2.27848527\n",
      "Iteration 60, loss = 2.27409298\n",
      "Iteration 61, loss = 2.26891801\n",
      "Iteration 62, loss = 2.26192025\n",
      "Iteration 63, loss = 2.25348165\n",
      "Iteration 64, loss = 2.24207470\n",
      "Iteration 65, loss = 2.22757867\n",
      "Iteration 66, loss = 2.20835074\n",
      "Iteration 67, loss = 2.18298179\n",
      "Iteration 68, loss = 2.15013664\n",
      "Iteration 69, loss = 2.10903861\n",
      "Iteration 70, loss = 2.06112573\n",
      "Iteration 71, loss = 2.00955460\n",
      "Iteration 72, loss = 1.95842869\n",
      "Iteration 73, loss = 1.91051136\n",
      "Iteration 74, loss = 1.86674077\n",
      "Iteration 75, loss = 1.82672655\n",
      "Iteration 76, loss = 1.78911876\n",
      "Iteration 77, loss = 1.75269757\n",
      "Iteration 78, loss = 1.71608371\n",
      "Iteration 79, loss = 1.67766111\n",
      "Iteration 80, loss = 1.63617936\n",
      "Iteration 81, loss = 1.59139663\n",
      "Iteration 82, loss = 1.54184670\n",
      "Iteration 83, loss = 1.48999938\n",
      "Iteration 84, loss = 1.43836816\n",
      "Iteration 85, loss = 1.39032191\n",
      "Iteration 86, loss = 1.34858452\n",
      "Iteration 87, loss = 1.31316761\n",
      "Iteration 88, loss = 1.28378049\n",
      "Iteration 89, loss = 1.25899434\n",
      "Iteration 90, loss = 1.23756606\n",
      "Iteration 91, loss = 1.21859433\n",
      "Iteration 92, loss = 1.20138030\n",
      "Iteration 93, loss = 1.18549471\n",
      "Iteration 94, loss = 1.17007882\n",
      "Iteration 95, loss = 1.15512166\n",
      "Iteration 96, loss = 1.14041678\n",
      "Iteration 97, loss = 1.12558511\n",
      "Iteration 98, loss = 1.11058587\n",
      "Iteration 99, loss = 1.09512927\n",
      "Iteration 100, loss = 1.07898284\n",
      "Iteration 101, loss = 1.06225945\n",
      "Iteration 102, loss = 1.04465665\n",
      "Iteration 103, loss = 1.02598600\n",
      "Iteration 104, loss = 1.00660258\n",
      "Iteration 105, loss = 0.98615284\n",
      "Iteration 106, loss = 0.96474298\n",
      "Iteration 107, loss = 0.94245450\n",
      "Iteration 108, loss = 0.91970399\n",
      "Iteration 109, loss = 0.89657479\n",
      "Iteration 110, loss = 0.87367075\n",
      "Iteration 111, loss = 0.85147763\n",
      "Iteration 112, loss = 0.83043358\n",
      "Iteration 113, loss = 0.81083283\n",
      "Iteration 114, loss = 0.79320457\n",
      "Iteration 115, loss = 0.77689090\n",
      "Iteration 116, loss = 0.76218512\n",
      "Iteration 117, loss = 0.74876287\n",
      "Iteration 118, loss = 0.73648730\n",
      "Iteration 119, loss = 0.72516503\n",
      "Iteration 120, loss = 0.71489887\n",
      "Iteration 121, loss = 0.70520185\n",
      "Iteration 122, loss = 0.69613284\n",
      "Iteration 123, loss = 0.68758332\n",
      "Iteration 124, loss = 0.67959454\n",
      "Iteration 125, loss = 0.67193307\n",
      "Iteration 126, loss = 0.66453541\n",
      "Iteration 127, loss = 0.65780648\n",
      "Iteration 128, loss = 0.65104241\n",
      "Iteration 129, loss = 0.64481320\n",
      "Iteration 130, loss = 0.63867725\n",
      "Iteration 131, loss = 0.63274858\n",
      "Iteration 132, loss = 0.62705783\n",
      "Iteration 133, loss = 0.62145807\n",
      "Iteration 134, loss = 0.61602167\n",
      "Iteration 135, loss = 0.61090361\n",
      "Iteration 136, loss = 0.60586571\n",
      "Iteration 137, loss = 0.60100523\n",
      "Iteration 138, loss = 0.59640605\n",
      "Iteration 139, loss = 0.59172283\n",
      "Iteration 140, loss = 0.58730180\n",
      "Iteration 141, loss = 0.58295591\n",
      "Iteration 142, loss = 0.57865253\n",
      "Iteration 143, loss = 0.57452870\n",
      "Iteration 144, loss = 0.57055452\n",
      "Iteration 145, loss = 0.56673679\n",
      "Iteration 146, loss = 0.56271126\n",
      "Iteration 147, loss = 0.55909899\n",
      "Iteration 148, loss = 0.55552457\n",
      "Iteration 149, loss = 0.55188999\n",
      "Iteration 150, loss = 0.54849684\n",
      "Iteration 151, loss = 0.54497519\n",
      "Iteration 152, loss = 0.54176860\n",
      "Iteration 153, loss = 0.53850636\n",
      "Iteration 154, loss = 0.53519968\n",
      "Iteration 155, loss = 0.53216137\n",
      "Iteration 156, loss = 0.52923738\n",
      "Iteration 157, loss = 0.52624779\n",
      "Iteration 158, loss = 0.52320572\n",
      "Iteration 159, loss = 0.52021643\n",
      "Iteration 160, loss = 0.51751580\n",
      "Iteration 161, loss = 0.51472995\n",
      "Iteration 162, loss = 0.51191642\n",
      "Iteration 163, loss = 0.50929008\n",
      "Iteration 164, loss = 0.50649021\n",
      "Iteration 165, loss = 0.50395051\n",
      "Iteration 166, loss = 0.50146479\n",
      "Iteration 167, loss = 0.49901729\n",
      "Iteration 168, loss = 0.49631817\n",
      "Iteration 169, loss = 0.49395616\n",
      "Iteration 170, loss = 0.49142125\n",
      "Iteration 171, loss = 0.48902887\n",
      "Iteration 172, loss = 0.48670725\n",
      "Iteration 173, loss = 0.48429191\n",
      "Iteration 174, loss = 0.48211850\n",
      "Iteration 175, loss = 0.47969702\n",
      "Iteration 176, loss = 0.47732963\n",
      "Iteration 177, loss = 0.47513030\n",
      "Iteration 178, loss = 0.47279831\n",
      "Iteration 179, loss = 0.47075201\n",
      "Iteration 180, loss = 0.46840131\n",
      "Iteration 181, loss = 0.46634080\n",
      "Iteration 182, loss = 0.46407828\n",
      "Iteration 183, loss = 0.46207172\n",
      "Iteration 184, loss = 0.45988781\n",
      "Iteration 185, loss = 0.45791127\n",
      "Iteration 186, loss = 0.45575819\n",
      "Iteration 187, loss = 0.45355476\n",
      "Iteration 188, loss = 0.45161323\n",
      "Iteration 189, loss = 0.44952376\n",
      "Iteration 190, loss = 0.44751140\n",
      "Iteration 191, loss = 0.44555097\n",
      "Iteration 192, loss = 0.44347281\n",
      "Iteration 193, loss = 0.44136052\n",
      "Iteration 194, loss = 0.43943423\n",
      "Iteration 195, loss = 0.43755864\n",
      "Iteration 196, loss = 0.43547884\n",
      "Iteration 197, loss = 0.43362836\n",
      "Iteration 198, loss = 0.43168370\n",
      "Iteration 199, loss = 0.42969369\n",
      "Iteration 200, loss = 0.42774681\n",
      "[CV] END activation=logistic, alpha=0.01, batch_size=100, hidden_layer_sizes=(100, 100, 100), solver=sgd; total time= 3.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.12926055\n",
      "Iteration 2, loss = 0.45499429\n",
      "Iteration 3, loss = 0.34228003\n",
      "Iteration 4, loss = 0.29517387\n",
      "Iteration 5, loss = 0.26576644\n",
      "Iteration 6, loss = 0.24472187\n",
      "Iteration 7, loss = 0.22798901\n",
      "Iteration 8, loss = 0.21390460\n",
      "Iteration 9, loss = 0.20235520\n",
      "Iteration 10, loss = 0.19134507\n",
      "Iteration 11, loss = 0.18292095\n",
      "Iteration 12, loss = 0.17481272\n",
      "Iteration 13, loss = 0.16735003\n",
      "Iteration 14, loss = 0.16083751\n",
      "Iteration 15, loss = 0.15499101\n",
      "Iteration 16, loss = 0.14924327\n",
      "Iteration 17, loss = 0.14388297\n",
      "Iteration 18, loss = 0.13895113\n",
      "Iteration 19, loss = 0.13469130\n",
      "Iteration 20, loss = 0.13039613\n",
      "Iteration 21, loss = 0.12662173\n",
      "Iteration 22, loss = 0.12260087\n",
      "Iteration 23, loss = 0.11925203\n",
      "Iteration 24, loss = 0.11628372\n",
      "Iteration 25, loss = 0.11328220\n",
      "Iteration 26, loss = 0.11053456\n",
      "Iteration 27, loss = 0.10789699\n",
      "Iteration 28, loss = 0.10543450\n",
      "Iteration 29, loss = 0.10261350\n",
      "Iteration 30, loss = 0.10087381\n",
      "Iteration 31, loss = 0.09837714\n",
      "Iteration 32, loss = 0.09648101\n",
      "Iteration 33, loss = 0.09486282\n",
      "Iteration 34, loss = 0.09313384\n",
      "Iteration 35, loss = 0.09145408\n",
      "Iteration 36, loss = 0.08999564\n",
      "Iteration 37, loss = 0.08843570\n",
      "Iteration 38, loss = 0.08689735\n",
      "Iteration 39, loss = 0.08587949\n",
      "Iteration 40, loss = 0.08450753\n",
      "Iteration 41, loss = 0.08326308\n",
      "Iteration 42, loss = 0.08202606\n",
      "Iteration 43, loss = 0.08104933\n",
      "Iteration 44, loss = 0.08023283\n",
      "Iteration 45, loss = 0.07898861\n",
      "Iteration 46, loss = 0.07818971\n",
      "Iteration 47, loss = 0.07723424\n",
      "Iteration 48, loss = 0.07632326\n",
      "Iteration 49, loss = 0.07609309\n",
      "Iteration 50, loss = 0.07530186\n",
      "Iteration 51, loss = 0.07453362\n",
      "Iteration 52, loss = 0.07344686\n",
      "Iteration 53, loss = 0.07290836\n",
      "Iteration 54, loss = 0.07222850\n",
      "Iteration 55, loss = 0.07160234\n",
      "Iteration 56, loss = 0.07106009\n",
      "Iteration 57, loss = 0.07076753\n",
      "Iteration 58, loss = 0.07035817\n",
      "Iteration 59, loss = 0.06978745\n",
      "Iteration 60, loss = 0.06898361\n",
      "Iteration 61, loss = 0.06919303\n",
      "Iteration 62, loss = 0.06804981\n",
      "Iteration 63, loss = 0.06808128\n",
      "Iteration 64, loss = 0.06816634\n",
      "Iteration 65, loss = 0.06725816\n",
      "Iteration 66, loss = 0.06665039\n",
      "Iteration 67, loss = 0.06631641\n",
      "Iteration 68, loss = 0.06626505\n",
      "Iteration 69, loss = 0.06566998\n",
      "Iteration 70, loss = 0.06567294\n",
      "Iteration 71, loss = 0.06547430\n",
      "Iteration 72, loss = 0.06503849\n",
      "Iteration 73, loss = 0.06465419\n",
      "Iteration 74, loss = 0.06432321\n",
      "Iteration 75, loss = 0.06445995\n",
      "Iteration 76, loss = 0.06445676\n",
      "Iteration 77, loss = 0.06393355\n",
      "Iteration 78, loss = 0.06347010\n",
      "Iteration 79, loss = 0.06381020\n",
      "Iteration 80, loss = 0.06317059\n",
      "Iteration 81, loss = 0.06312968\n",
      "Iteration 82, loss = 0.06271655\n",
      "Iteration 83, loss = 0.06252390\n",
      "Iteration 84, loss = 0.06257315\n",
      "Iteration 85, loss = 0.06235415\n",
      "Iteration 86, loss = 0.06238202\n",
      "Iteration 87, loss = 0.06208641\n",
      "Iteration 88, loss = 0.06176975\n",
      "Iteration 89, loss = 0.06180423\n",
      "Iteration 90, loss = 0.06153625\n",
      "Iteration 91, loss = 0.06151966\n",
      "Iteration 92, loss = 0.06161496\n",
      "Iteration 93, loss = 0.06152305\n",
      "Iteration 94, loss = 0.06124946\n",
      "Iteration 95, loss = 0.06115985\n",
      "Iteration 96, loss = 0.06093442\n",
      "Iteration 97, loss = 0.06087175\n",
      "Iteration 98, loss = 0.06070377\n",
      "Iteration 99, loss = 0.06066075\n",
      "Iteration 100, loss = 0.06047187\n",
      "Iteration 101, loss = 0.06056068\n",
      "Iteration 102, loss = 0.06036455\n",
      "Iteration 103, loss = 0.06040745\n",
      "Iteration 104, loss = 0.06023683\n",
      "Iteration 105, loss = 0.06014337\n",
      "Iteration 106, loss = 0.06058592\n",
      "Iteration 107, loss = 0.06005906\n",
      "Iteration 108, loss = 0.05992770\n",
      "Iteration 109, loss = 0.05985825\n",
      "Iteration 110, loss = 0.05966038\n",
      "Iteration 111, loss = 0.05986631\n",
      "Iteration 112, loss = 0.05967742\n",
      "Iteration 113, loss = 0.05951809\n",
      "Iteration 114, loss = 0.05969566\n",
      "Iteration 115, loss = 0.05953906\n",
      "Iteration 116, loss = 0.05942602\n",
      "Iteration 117, loss = 0.05939299\n",
      "Iteration 118, loss = 0.05939291\n",
      "Iteration 119, loss = 0.05929246\n",
      "Iteration 120, loss = 0.05919598\n",
      "Iteration 121, loss = 0.05920239\n",
      "Iteration 122, loss = 0.05916330\n",
      "Iteration 123, loss = 0.05922284\n",
      "Iteration 124, loss = 0.05876752\n",
      "Iteration 125, loss = 0.05907617\n",
      "Iteration 126, loss = 0.05886438\n",
      "Iteration 127, loss = 0.05878094\n",
      "Iteration 128, loss = 0.05862543\n",
      "Iteration 129, loss = 0.05867356\n",
      "Iteration 130, loss = 0.05872408\n",
      "Iteration 131, loss = 0.05890075\n",
      "Iteration 132, loss = 0.05880062\n",
      "Iteration 133, loss = 0.05866322\n",
      "Iteration 134, loss = 0.05838437\n",
      "Iteration 135, loss = 0.05840723\n",
      "Iteration 136, loss = 0.05853632\n",
      "Iteration 137, loss = 0.05839395\n",
      "Iteration 138, loss = 0.05850118\n",
      "Iteration 139, loss = 0.05827797\n",
      "Iteration 140, loss = 0.05828546\n",
      "Iteration 141, loss = 0.05837712\n",
      "Iteration 142, loss = 0.05805259\n",
      "Iteration 143, loss = 0.05838785\n",
      "Iteration 144, loss = 0.05824652\n",
      "Iteration 145, loss = 0.05814351\n",
      "Iteration 146, loss = 0.05805170\n",
      "Iteration 147, loss = 0.05791811\n",
      "Iteration 148, loss = 0.05781032\n",
      "Iteration 149, loss = 0.05821050\n",
      "Iteration 150, loss = 0.05793410\n",
      "Iteration 151, loss = 0.05787357\n",
      "Iteration 152, loss = 0.05783293\n",
      "Iteration 153, loss = 0.05802792\n",
      "Iteration 154, loss = 0.05789741\n",
      "Iteration 155, loss = 0.05770132\n",
      "Iteration 156, loss = 0.05772904\n",
      "Iteration 157, loss = 0.05788325\n",
      "Iteration 158, loss = 0.05790142\n",
      "Iteration 159, loss = 0.05777740\n",
      "Iteration 160, loss = 0.05796803\n",
      "Iteration 161, loss = 0.05776018\n",
      "Iteration 162, loss = 0.05785538\n",
      "Iteration 163, loss = 0.05777273\n",
      "Iteration 164, loss = 0.05756941\n",
      "Iteration 165, loss = 0.05754816\n",
      "Iteration 166, loss = 0.05757321\n",
      "Iteration 167, loss = 0.05753723\n",
      "Iteration 168, loss = 0.05731172\n",
      "Iteration 169, loss = 0.05746668\n",
      "Iteration 170, loss = 0.05744349\n",
      "Iteration 171, loss = 0.05745534\n",
      "Iteration 172, loss = 0.05737402\n",
      "Iteration 173, loss = 0.05717217\n",
      "Iteration 174, loss = 0.05734960\n",
      "Iteration 175, loss = 0.05742185\n",
      "Iteration 176, loss = 0.05742352\n",
      "Iteration 177, loss = 0.05712251\n",
      "Iteration 178, loss = 0.05728256\n",
      "Iteration 179, loss = 0.05724856\n",
      "Iteration 180, loss = 0.05747205\n",
      "Iteration 181, loss = 0.05691479\n",
      "Iteration 182, loss = 0.05731915\n",
      "Iteration 183, loss = 0.05739911\n",
      "Iteration 184, loss = 0.05743498\n",
      "Iteration 185, loss = 0.05754675\n",
      "Iteration 186, loss = 0.05713738\n",
      "Iteration 187, loss = 0.05696061\n",
      "Iteration 188, loss = 0.05725733\n",
      "Iteration 189, loss = 0.05696236\n",
      "Iteration 190, loss = 0.05718045\n",
      "Iteration 191, loss = 0.05677313\n",
      "Iteration 192, loss = 0.05683920\n",
      "Iteration 193, loss = 0.05692927\n",
      "Iteration 194, loss = 0.05709713\n",
      "Iteration 195, loss = 0.05746555\n",
      "Iteration 196, loss = 0.05701726\n",
      "Iteration 197, loss = 0.05683660\n",
      "Iteration 198, loss = 0.05684821\n",
      "Iteration 199, loss = 0.05697328\n",
      "Iteration 200, loss = 0.05687365\n",
      "[CV] END activation=logistic, alpha=0.01, batch_size=200, hidden_layer_sizes=(100,), solver=adam; total time= 1.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.11016226\n",
      "Iteration 2, loss = 0.45185362\n",
      "Iteration 3, loss = 0.34396750\n",
      "Iteration 4, loss = 0.29864647\n",
      "Iteration 5, loss = 0.27023612\n",
      "Iteration 6, loss = 0.24939975\n",
      "Iteration 7, loss = 0.23212105\n",
      "Iteration 8, loss = 0.21767236\n",
      "Iteration 9, loss = 0.20517758\n",
      "Iteration 10, loss = 0.19476737\n",
      "Iteration 11, loss = 0.18528832\n",
      "Iteration 12, loss = 0.17685185\n",
      "Iteration 13, loss = 0.16908875\n",
      "Iteration 14, loss = 0.16208890\n",
      "Iteration 15, loss = 0.15632107\n",
      "Iteration 16, loss = 0.15035194\n",
      "Iteration 17, loss = 0.14518303\n",
      "Iteration 18, loss = 0.13970455\n",
      "Iteration 19, loss = 0.13538675\n",
      "Iteration 20, loss = 0.13098689\n",
      "Iteration 21, loss = 0.12753013\n",
      "Iteration 22, loss = 0.12352261\n",
      "Iteration 23, loss = 0.12010046\n",
      "Iteration 24, loss = 0.11727758\n",
      "Iteration 25, loss = 0.11366097\n",
      "Iteration 26, loss = 0.11139523\n",
      "Iteration 27, loss = 0.10851203\n",
      "Iteration 28, loss = 0.10599786\n",
      "Iteration 29, loss = 0.10384867\n",
      "Iteration 30, loss = 0.10129974\n",
      "Iteration 31, loss = 0.09943733\n",
      "Iteration 32, loss = 0.09709371\n",
      "Iteration 33, loss = 0.09578104\n",
      "Iteration 34, loss = 0.09356287\n",
      "Iteration 35, loss = 0.09210701\n",
      "Iteration 36, loss = 0.09082036\n",
      "Iteration 37, loss = 0.08866734\n",
      "Iteration 38, loss = 0.08759011\n",
      "Iteration 39, loss = 0.08631075\n",
      "Iteration 40, loss = 0.08490849\n",
      "Iteration 41, loss = 0.08330167\n",
      "Iteration 42, loss = 0.08200064\n",
      "Iteration 43, loss = 0.08152261\n",
      "Iteration 44, loss = 0.07986632\n",
      "Iteration 45, loss = 0.07927577\n",
      "Iteration 46, loss = 0.07832136\n",
      "Iteration 47, loss = 0.07800511\n",
      "Iteration 48, loss = 0.07682528\n",
      "Iteration 49, loss = 0.07577256\n",
      "Iteration 50, loss = 0.07494836\n",
      "Iteration 51, loss = 0.07440822\n",
      "Iteration 52, loss = 0.07386092\n",
      "Iteration 53, loss = 0.07292430\n",
      "Iteration 54, loss = 0.07245596\n",
      "Iteration 55, loss = 0.07217214\n",
      "Iteration 56, loss = 0.07147007\n",
      "Iteration 57, loss = 0.07091441\n",
      "Iteration 58, loss = 0.07019257\n",
      "Iteration 59, loss = 0.06986851\n",
      "Iteration 60, loss = 0.06924297\n",
      "Iteration 61, loss = 0.06893759\n",
      "Iteration 62, loss = 0.06872794\n",
      "Iteration 63, loss = 0.06798369\n",
      "Iteration 64, loss = 0.06774146\n",
      "Iteration 65, loss = 0.06728499\n",
      "Iteration 66, loss = 0.06702552\n",
      "Iteration 67, loss = 0.06664282\n",
      "Iteration 68, loss = 0.06636172\n",
      "Iteration 69, loss = 0.06599420\n",
      "Iteration 70, loss = 0.06562343\n",
      "Iteration 71, loss = 0.06548189\n",
      "Iteration 72, loss = 0.06539187\n",
      "Iteration 73, loss = 0.06502268\n",
      "Iteration 74, loss = 0.06463195\n",
      "Iteration 75, loss = 0.06471234\n",
      "Iteration 76, loss = 0.06437362\n",
      "Iteration 77, loss = 0.06391532\n",
      "Iteration 78, loss = 0.06394720\n",
      "Iteration 79, loss = 0.06367376\n",
      "Iteration 80, loss = 0.06347569\n",
      "Iteration 81, loss = 0.06349293\n",
      "Iteration 82, loss = 0.06327826\n",
      "Iteration 83, loss = 0.06312105\n",
      "Iteration 84, loss = 0.06332017\n",
      "Iteration 85, loss = 0.06255892\n",
      "Iteration 86, loss = 0.06252389\n",
      "Iteration 87, loss = 0.06255616\n",
      "Iteration 88, loss = 0.06243964\n",
      "Iteration 89, loss = 0.06212946\n",
      "Iteration 90, loss = 0.06214304\n",
      "Iteration 91, loss = 0.06216031\n",
      "Iteration 92, loss = 0.06194717\n",
      "Iteration 93, loss = 0.06185020\n",
      "Iteration 94, loss = 0.06162647\n",
      "Iteration 95, loss = 0.06176031\n",
      "Iteration 96, loss = 0.06138874\n",
      "Iteration 97, loss = 0.06180279\n",
      "Iteration 98, loss = 0.06127095\n",
      "Iteration 99, loss = 0.06118716\n",
      "Iteration 100, loss = 0.06092014\n",
      "Iteration 101, loss = 0.06095942\n",
      "Iteration 102, loss = 0.06085431\n",
      "Iteration 103, loss = 0.06087388\n",
      "Iteration 104, loss = 0.06116222\n",
      "Iteration 105, loss = 0.06073135\n",
      "Iteration 106, loss = 0.06075414\n",
      "Iteration 107, loss = 0.06043135\n",
      "Iteration 108, loss = 0.06051504\n",
      "Iteration 109, loss = 0.06036903\n",
      "Iteration 110, loss = 0.06031561\n",
      "Iteration 111, loss = 0.06006367\n",
      "Iteration 112, loss = 0.06005616\n",
      "Iteration 113, loss = 0.06018595\n",
      "Iteration 114, loss = 0.06034278\n",
      "Iteration 115, loss = 0.05997408\n",
      "Iteration 116, loss = 0.06001101\n",
      "Iteration 117, loss = 0.05982230\n",
      "Iteration 118, loss = 0.05978258\n",
      "Iteration 119, loss = 0.05983975\n",
      "Iteration 120, loss = 0.05986857\n",
      "Iteration 121, loss = 0.05966399\n",
      "Iteration 122, loss = 0.05948014\n",
      "Iteration 123, loss = 0.05969182\n",
      "Iteration 124, loss = 0.05957390\n",
      "Iteration 125, loss = 0.05942690\n",
      "Iteration 126, loss = 0.05937673\n",
      "Iteration 127, loss = 0.05959338\n",
      "Iteration 128, loss = 0.05934730\n",
      "Iteration 129, loss = 0.05899470\n",
      "Iteration 130, loss = 0.05945237\n",
      "Iteration 131, loss = 0.05917574\n",
      "Iteration 132, loss = 0.05968510\n",
      "Iteration 133, loss = 0.05898905\n",
      "Iteration 134, loss = 0.05903639\n",
      "Iteration 135, loss = 0.05896481\n",
      "Iteration 136, loss = 0.05889032\n",
      "Iteration 137, loss = 0.05889583\n",
      "Iteration 138, loss = 0.05876749\n",
      "Iteration 139, loss = 0.05889643\n",
      "Iteration 140, loss = 0.05884730\n",
      "Iteration 141, loss = 0.05880209\n",
      "Iteration 142, loss = 0.05885358\n",
      "Iteration 143, loss = 0.05892212\n",
      "Iteration 144, loss = 0.05877504\n",
      "Iteration 145, loss = 0.05868570\n",
      "Iteration 146, loss = 0.05879007\n",
      "Iteration 147, loss = 0.05855260\n",
      "Iteration 148, loss = 0.05884016\n",
      "Iteration 149, loss = 0.05870920\n",
      "Iteration 150, loss = 0.05847348\n",
      "Iteration 151, loss = 0.05874107\n",
      "Iteration 152, loss = 0.05825907\n",
      "Iteration 153, loss = 0.05843809\n",
      "Iteration 154, loss = 0.05838344\n",
      "Iteration 155, loss = 0.05828877\n",
      "Iteration 156, loss = 0.05824981\n",
      "Iteration 157, loss = 0.05833517\n",
      "Iteration 158, loss = 0.05811868\n",
      "Iteration 159, loss = 0.05827226\n",
      "Iteration 160, loss = 0.05833993\n",
      "Iteration 161, loss = 0.05853623\n",
      "Iteration 162, loss = 0.05837008\n",
      "Iteration 163, loss = 0.05788158\n",
      "Iteration 164, loss = 0.05803365\n",
      "Iteration 165, loss = 0.05809215\n",
      "Iteration 166, loss = 0.05819575\n",
      "Iteration 167, loss = 0.05815039\n",
      "Iteration 168, loss = 0.05821796\n",
      "Iteration 169, loss = 0.05804867\n",
      "Iteration 170, loss = 0.05795449\n",
      "Iteration 171, loss = 0.05769866\n",
      "Iteration 172, loss = 0.05780575\n",
      "Iteration 173, loss = 0.05775089\n",
      "Iteration 174, loss = 0.05777526\n",
      "Iteration 175, loss = 0.05791457\n",
      "Iteration 176, loss = 0.05784864\n",
      "Iteration 177, loss = 0.05767376\n",
      "Iteration 178, loss = 0.05782373\n",
      "Iteration 179, loss = 0.05776237\n",
      "Iteration 180, loss = 0.05757797\n",
      "Iteration 181, loss = 0.05763791\n",
      "Iteration 182, loss = 0.05760305\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.01, batch_size=200, hidden_layer_sizes=(100,), solver=adam; total time= 1.6min\n",
      "Iteration 1, loss = 1.14810849\n",
      "Iteration 2, loss = 0.46138371\n",
      "Iteration 3, loss = 0.34713252\n",
      "Iteration 4, loss = 0.29842260\n",
      "Iteration 5, loss = 0.26913952\n",
      "Iteration 6, loss = 0.24712980\n",
      "Iteration 7, loss = 0.22995694\n",
      "Iteration 8, loss = 0.21552769\n",
      "Iteration 9, loss = 0.20360507\n",
      "Iteration 10, loss = 0.19300778\n",
      "Iteration 11, loss = 0.18319680\n",
      "Iteration 12, loss = 0.17454584\n",
      "Iteration 13, loss = 0.16705114\n",
      "Iteration 14, loss = 0.16021583\n",
      "Iteration 15, loss = 0.15439330\n",
      "Iteration 16, loss = 0.14854238\n",
      "Iteration 17, loss = 0.14331782\n",
      "Iteration 18, loss = 0.13903216\n",
      "Iteration 19, loss = 0.13388149\n",
      "Iteration 20, loss = 0.12958744\n",
      "Iteration 21, loss = 0.12581614\n",
      "Iteration 22, loss = 0.12239603\n",
      "Iteration 23, loss = 0.11917526\n",
      "Iteration 24, loss = 0.11625096\n",
      "Iteration 25, loss = 0.11288007\n",
      "Iteration 26, loss = 0.10992388\n",
      "Iteration 27, loss = 0.10723296\n",
      "Iteration 28, loss = 0.10544995\n",
      "Iteration 29, loss = 0.10289785\n",
      "Iteration 30, loss = 0.10045079\n",
      "Iteration 31, loss = 0.09878374\n",
      "Iteration 32, loss = 0.09671388\n",
      "Iteration 33, loss = 0.09492920\n",
      "Iteration 34, loss = 0.09318953\n",
      "Iteration 35, loss = 0.09144919\n",
      "Iteration 36, loss = 0.08987361\n",
      "Iteration 37, loss = 0.08855734\n",
      "Iteration 38, loss = 0.08703518\n",
      "Iteration 39, loss = 0.08546066\n",
      "Iteration 40, loss = 0.08429187\n",
      "Iteration 41, loss = 0.08292336\n",
      "Iteration 42, loss = 0.08175702\n",
      "Iteration 43, loss = 0.08111546\n",
      "Iteration 44, loss = 0.08018935\n",
      "Iteration 45, loss = 0.07926288\n",
      "Iteration 46, loss = 0.07795858\n",
      "Iteration 47, loss = 0.07715563\n",
      "Iteration 48, loss = 0.07665221\n",
      "Iteration 49, loss = 0.07559392\n",
      "Iteration 50, loss = 0.07484803\n",
      "Iteration 51, loss = 0.07401368\n",
      "Iteration 52, loss = 0.07326899\n",
      "Iteration 53, loss = 0.07271814\n",
      "Iteration 54, loss = 0.07223363\n",
      "Iteration 55, loss = 0.07145200\n",
      "Iteration 56, loss = 0.07081830\n",
      "Iteration 57, loss = 0.07056283\n",
      "Iteration 58, loss = 0.06979355\n",
      "Iteration 59, loss = 0.06899533\n",
      "Iteration 60, loss = 0.06877966\n",
      "Iteration 61, loss = 0.06861516\n",
      "Iteration 62, loss = 0.06812398\n",
      "Iteration 63, loss = 0.06782078\n",
      "Iteration 64, loss = 0.06698235\n",
      "Iteration 65, loss = 0.06683586\n",
      "Iteration 66, loss = 0.06664535\n",
      "Iteration 67, loss = 0.06605366\n",
      "Iteration 68, loss = 0.06598904\n",
      "Iteration 69, loss = 0.06580911\n",
      "Iteration 70, loss = 0.06535770\n",
      "Iteration 71, loss = 0.06510463\n",
      "Iteration 72, loss = 0.06477344\n",
      "Iteration 73, loss = 0.06444880\n",
      "Iteration 74, loss = 0.06442256\n",
      "Iteration 75, loss = 0.06388207\n",
      "Iteration 76, loss = 0.06362761\n",
      "Iteration 77, loss = 0.06389922\n",
      "Iteration 78, loss = 0.06346697\n",
      "Iteration 79, loss = 0.06305476\n",
      "Iteration 80, loss = 0.06320925\n",
      "Iteration 81, loss = 0.06290701\n",
      "Iteration 82, loss = 0.06242456\n",
      "Iteration 83, loss = 0.06198386\n",
      "Iteration 84, loss = 0.06259881\n",
      "Iteration 85, loss = 0.06202635\n",
      "Iteration 86, loss = 0.06227735\n",
      "Iteration 87, loss = 0.06204673\n",
      "Iteration 88, loss = 0.06195150\n",
      "Iteration 89, loss = 0.06134338\n",
      "Iteration 90, loss = 0.06148703\n",
      "Iteration 91, loss = 0.06143078\n",
      "Iteration 92, loss = 0.06124304\n",
      "Iteration 93, loss = 0.06103630\n",
      "Iteration 94, loss = 0.06095419\n",
      "Iteration 95, loss = 0.06086661\n",
      "Iteration 96, loss = 0.06079232\n",
      "Iteration 97, loss = 0.06058801\n",
      "Iteration 98, loss = 0.06050365\n",
      "Iteration 99, loss = 0.06058535\n",
      "Iteration 100, loss = 0.06025047\n",
      "Iteration 101, loss = 0.05992622\n",
      "Iteration 102, loss = 0.06012166\n",
      "Iteration 103, loss = 0.06024342\n",
      "Iteration 104, loss = 0.05999948\n",
      "Iteration 105, loss = 0.06032234\n",
      "Iteration 106, loss = 0.06000129\n",
      "Iteration 107, loss = 0.05969201\n",
      "Iteration 108, loss = 0.05977848\n",
      "Iteration 109, loss = 0.05941761\n",
      "Iteration 110, loss = 0.05926770\n",
      "Iteration 111, loss = 0.05955124\n",
      "Iteration 112, loss = 0.05964101\n",
      "Iteration 113, loss = 0.05933107\n",
      "Iteration 114, loss = 0.05908483\n",
      "Iteration 115, loss = 0.05931679\n",
      "Iteration 116, loss = 0.05921000\n",
      "Iteration 117, loss = 0.05899307\n",
      "Iteration 118, loss = 0.05909691\n",
      "Iteration 119, loss = 0.05891581\n",
      "Iteration 120, loss = 0.05858543\n",
      "Iteration 121, loss = 0.05876043\n",
      "Iteration 122, loss = 0.05870105\n",
      "Iteration 123, loss = 0.05873117\n",
      "Iteration 124, loss = 0.05869805\n",
      "Iteration 125, loss = 0.05866699\n",
      "Iteration 126, loss = 0.05858562\n",
      "Iteration 127, loss = 0.05847482\n",
      "Iteration 128, loss = 0.05842702\n",
      "Iteration 129, loss = 0.05842337\n",
      "Iteration 130, loss = 0.05878294\n",
      "Iteration 131, loss = 0.05822681\n",
      "Iteration 132, loss = 0.05839103\n",
      "Iteration 133, loss = 0.05856176\n",
      "Iteration 134, loss = 0.05800522\n",
      "Iteration 135, loss = 0.05802223\n",
      "Iteration 136, loss = 0.05804796\n",
      "Iteration 137, loss = 0.05802796\n",
      "Iteration 138, loss = 0.05801555\n",
      "Iteration 139, loss = 0.05801247\n",
      "Iteration 140, loss = 0.05809808\n",
      "Iteration 141, loss = 0.05803822\n",
      "Iteration 142, loss = 0.05784911\n",
      "Iteration 143, loss = 0.05787945\n",
      "Iteration 144, loss = 0.05784948\n",
      "Iteration 145, loss = 0.05778231\n",
      "Iteration 146, loss = 0.05798014\n",
      "Iteration 147, loss = 0.05759236\n",
      "Iteration 148, loss = 0.05750833\n",
      "Iteration 149, loss = 0.05775041\n",
      "Iteration 150, loss = 0.05751661\n",
      "Iteration 151, loss = 0.05757716\n",
      "Iteration 152, loss = 0.05765665\n",
      "Iteration 153, loss = 0.05772635\n",
      "Iteration 154, loss = 0.05750943\n",
      "Iteration 155, loss = 0.05736538\n",
      "Iteration 156, loss = 0.05752146\n",
      "Iteration 157, loss = 0.05729793\n",
      "Iteration 158, loss = 0.05742124\n",
      "Iteration 159, loss = 0.05726132\n",
      "Iteration 160, loss = 0.05698824\n",
      "Iteration 161, loss = 0.05724986\n",
      "Iteration 162, loss = 0.05743748\n",
      "Iteration 163, loss = 0.05717927\n",
      "Iteration 164, loss = 0.05715914\n",
      "Iteration 165, loss = 0.05710536\n",
      "Iteration 166, loss = 0.05697917\n",
      "Iteration 167, loss = 0.05695066\n",
      "Iteration 168, loss = 0.05703882\n",
      "Iteration 169, loss = 0.05699910\n",
      "Iteration 170, loss = 0.05696025\n",
      "Iteration 171, loss = 0.05716687\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.01, batch_size=200, hidden_layer_sizes=(100,), solver=adam; total time= 1.5min\n",
      "Iteration 1, loss = 1.12721836\n",
      "Iteration 2, loss = 0.45572506\n",
      "Iteration 3, loss = 0.34261538\n",
      "Iteration 4, loss = 0.29558331\n",
      "Iteration 5, loss = 0.26652163\n",
      "Iteration 6, loss = 0.24476276\n",
      "Iteration 7, loss = 0.22722459\n",
      "Iteration 8, loss = 0.21314546\n",
      "Iteration 9, loss = 0.20105095\n",
      "Iteration 10, loss = 0.19056579\n",
      "Iteration 11, loss = 0.18201095\n",
      "Iteration 12, loss = 0.17357033\n",
      "Iteration 13, loss = 0.16610855\n",
      "Iteration 14, loss = 0.15980902\n",
      "Iteration 15, loss = 0.15368693\n",
      "Iteration 16, loss = 0.14820701\n",
      "Iteration 17, loss = 0.14311634\n",
      "Iteration 18, loss = 0.13790158\n",
      "Iteration 19, loss = 0.13366690\n",
      "Iteration 20, loss = 0.12975653\n",
      "Iteration 21, loss = 0.12587649\n",
      "Iteration 22, loss = 0.12228422\n",
      "Iteration 23, loss = 0.11883139\n",
      "Iteration 24, loss = 0.11547262\n",
      "Iteration 25, loss = 0.11289512\n",
      "Iteration 26, loss = 0.10997129\n",
      "Iteration 27, loss = 0.10706223\n",
      "Iteration 28, loss = 0.10453272\n",
      "Iteration 29, loss = 0.10261813\n",
      "Iteration 30, loss = 0.10044492\n",
      "Iteration 31, loss = 0.09841187\n",
      "Iteration 32, loss = 0.09618784\n",
      "Iteration 33, loss = 0.09440266\n",
      "Iteration 34, loss = 0.09321575\n",
      "Iteration 35, loss = 0.09129712\n",
      "Iteration 36, loss = 0.08957065\n",
      "Iteration 37, loss = 0.08820878\n",
      "Iteration 38, loss = 0.08690258\n",
      "Iteration 39, loss = 0.08585918\n",
      "Iteration 40, loss = 0.08421640\n",
      "Iteration 41, loss = 0.08308238\n",
      "Iteration 42, loss = 0.08229811\n",
      "Iteration 43, loss = 0.08103587\n",
      "Iteration 44, loss = 0.07987494\n",
      "Iteration 45, loss = 0.07881094\n",
      "Iteration 46, loss = 0.07791218\n",
      "Iteration 47, loss = 0.07684491\n",
      "Iteration 48, loss = 0.07612204\n",
      "Iteration 49, loss = 0.07520697\n",
      "Iteration 50, loss = 0.07498324\n",
      "Iteration 51, loss = 0.07409306\n",
      "Iteration 52, loss = 0.07370632\n",
      "Iteration 53, loss = 0.07267079\n",
      "Iteration 54, loss = 0.07209790\n",
      "Iteration 55, loss = 0.07145042\n",
      "Iteration 56, loss = 0.07093191\n",
      "Iteration 57, loss = 0.07048936\n",
      "Iteration 58, loss = 0.06968516\n",
      "Iteration 59, loss = 0.06951894\n",
      "Iteration 60, loss = 0.06902940\n",
      "Iteration 61, loss = 0.06836068\n",
      "Iteration 62, loss = 0.06790859\n",
      "Iteration 63, loss = 0.06762757\n",
      "Iteration 64, loss = 0.06778847\n",
      "Iteration 65, loss = 0.06691188\n",
      "Iteration 66, loss = 0.06649160\n",
      "Iteration 67, loss = 0.06617667\n",
      "Iteration 68, loss = 0.06595400\n",
      "Iteration 69, loss = 0.06572235\n",
      "Iteration 70, loss = 0.06575613\n",
      "Iteration 71, loss = 0.06516727\n",
      "Iteration 72, loss = 0.06483529\n",
      "Iteration 73, loss = 0.06470675\n",
      "Iteration 74, loss = 0.06470416\n",
      "Iteration 75, loss = 0.06397051\n",
      "Iteration 76, loss = 0.06394120\n",
      "Iteration 77, loss = 0.06416492\n",
      "Iteration 78, loss = 0.06344603\n",
      "Iteration 79, loss = 0.06325034\n",
      "Iteration 80, loss = 0.06327046\n",
      "Iteration 81, loss = 0.06303501\n",
      "Iteration 82, loss = 0.06305759\n",
      "Iteration 83, loss = 0.06248751\n",
      "Iteration 84, loss = 0.06265253\n",
      "Iteration 85, loss = 0.06220414\n",
      "Iteration 86, loss = 0.06235306\n",
      "Iteration 87, loss = 0.06213045\n",
      "Iteration 88, loss = 0.06204375\n",
      "Iteration 89, loss = 0.06154034\n",
      "Iteration 90, loss = 0.06199737\n",
      "Iteration 91, loss = 0.06157902\n",
      "Iteration 92, loss = 0.06155028\n",
      "Iteration 93, loss = 0.06114167\n",
      "Iteration 94, loss = 0.06114858\n",
      "Iteration 95, loss = 0.06148881\n",
      "Iteration 96, loss = 0.06082894\n",
      "Iteration 97, loss = 0.06107947\n",
      "Iteration 98, loss = 0.06092693\n",
      "Iteration 99, loss = 0.06063071\n",
      "Iteration 100, loss = 0.06068206\n",
      "Iteration 101, loss = 0.06078793\n",
      "Iteration 102, loss = 0.06062738\n",
      "Iteration 103, loss = 0.06048376\n",
      "Iteration 104, loss = 0.06063889\n",
      "Iteration 105, loss = 0.06038339\n",
      "Iteration 106, loss = 0.06026614\n",
      "Iteration 107, loss = 0.06000829\n",
      "Iteration 108, loss = 0.06023446\n",
      "Iteration 109, loss = 0.06014167\n",
      "Iteration 110, loss = 0.06018887\n",
      "Iteration 111, loss = 0.05996962\n",
      "Iteration 112, loss = 0.06006225\n",
      "Iteration 113, loss = 0.05963937\n",
      "Iteration 114, loss = 0.05979408\n",
      "Iteration 115, loss = 0.05965881\n",
      "Iteration 116, loss = 0.05976809\n",
      "Iteration 117, loss = 0.05967313\n",
      "Iteration 118, loss = 0.05954049\n",
      "Iteration 119, loss = 0.05950104\n",
      "Iteration 120, loss = 0.05935686\n",
      "Iteration 121, loss = 0.05926756\n",
      "Iteration 122, loss = 0.05942490\n",
      "Iteration 123, loss = 0.05916277\n",
      "Iteration 124, loss = 0.05919335\n",
      "Iteration 125, loss = 0.05905867\n",
      "Iteration 126, loss = 0.05890937\n",
      "Iteration 127, loss = 0.05913030\n",
      "Iteration 128, loss = 0.05894997\n",
      "Iteration 129, loss = 0.05921252\n",
      "Iteration 130, loss = 0.05921360\n",
      "Iteration 131, loss = 0.05869475\n",
      "Iteration 132, loss = 0.05889723\n",
      "Iteration 133, loss = 0.05871766\n",
      "Iteration 134, loss = 0.05855620\n",
      "Iteration 135, loss = 0.05874347\n",
      "Iteration 136, loss = 0.05859118\n",
      "Iteration 137, loss = 0.05867491\n",
      "Iteration 138, loss = 0.05842202\n",
      "Iteration 139, loss = 0.05864639\n",
      "Iteration 140, loss = 0.05829231\n",
      "Iteration 141, loss = 0.05827404\n",
      "Iteration 142, loss = 0.05836031\n",
      "Iteration 143, loss = 0.05846089\n",
      "Iteration 144, loss = 0.05835370\n",
      "Iteration 145, loss = 0.05845090\n",
      "Iteration 146, loss = 0.05811585\n",
      "Iteration 147, loss = 0.05833549\n",
      "Iteration 148, loss = 0.05849952\n",
      "Iteration 149, loss = 0.05826506\n",
      "Iteration 150, loss = 0.05844139\n",
      "Iteration 151, loss = 0.05807808\n",
      "Iteration 152, loss = 0.05803173\n",
      "Iteration 153, loss = 0.05826471\n",
      "Iteration 154, loss = 0.05809468\n",
      "Iteration 155, loss = 0.05814640\n",
      "Iteration 156, loss = 0.05800077\n",
      "Iteration 157, loss = 0.05812594\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.01, batch_size=200, hidden_layer_sizes=(100,), solver=adam; total time= 1.4min\n",
      "Iteration 1, loss = 1.11059230\n",
      "Iteration 2, loss = 0.45102064\n",
      "Iteration 3, loss = 0.34289402\n",
      "Iteration 4, loss = 0.29688301\n",
      "Iteration 5, loss = 0.26827926\n",
      "Iteration 6, loss = 0.24695183\n",
      "Iteration 7, loss = 0.22990169\n",
      "Iteration 8, loss = 0.21589807\n",
      "Iteration 9, loss = 0.20344604\n",
      "Iteration 10, loss = 0.19315048\n",
      "Iteration 11, loss = 0.18397550\n",
      "Iteration 12, loss = 0.17532921\n",
      "Iteration 13, loss = 0.16778953\n",
      "Iteration 14, loss = 0.16134265\n",
      "Iteration 15, loss = 0.15482535\n",
      "Iteration 16, loss = 0.14907248\n",
      "Iteration 17, loss = 0.14411740\n",
      "Iteration 18, loss = 0.13942227\n",
      "Iteration 19, loss = 0.13495299\n",
      "Iteration 20, loss = 0.13054528\n",
      "Iteration 21, loss = 0.12694121\n",
      "Iteration 22, loss = 0.12261559\n",
      "Iteration 23, loss = 0.11937922\n",
      "Iteration 24, loss = 0.11645428\n",
      "Iteration 25, loss = 0.11343185\n",
      "Iteration 26, loss = 0.11045082\n",
      "Iteration 27, loss = 0.10770782\n",
      "Iteration 28, loss = 0.10519995\n",
      "Iteration 29, loss = 0.10269095\n",
      "Iteration 30, loss = 0.10053709\n",
      "Iteration 31, loss = 0.09851807\n",
      "Iteration 32, loss = 0.09663817\n",
      "Iteration 33, loss = 0.09435784\n",
      "Iteration 34, loss = 0.09286412\n",
      "Iteration 35, loss = 0.09101825\n",
      "Iteration 36, loss = 0.08942046\n",
      "Iteration 37, loss = 0.08831199\n",
      "Iteration 38, loss = 0.08662228\n",
      "Iteration 39, loss = 0.08527321\n",
      "Iteration 40, loss = 0.08365849\n",
      "Iteration 41, loss = 0.08267216\n",
      "Iteration 42, loss = 0.08129460\n",
      "Iteration 43, loss = 0.08049796\n",
      "Iteration 44, loss = 0.07930794\n",
      "Iteration 45, loss = 0.07874513\n",
      "Iteration 46, loss = 0.07751132\n",
      "Iteration 47, loss = 0.07682734\n",
      "Iteration 48, loss = 0.07546879\n",
      "Iteration 49, loss = 0.07502675\n",
      "Iteration 50, loss = 0.07432489\n",
      "Iteration 51, loss = 0.07359626\n",
      "Iteration 52, loss = 0.07280562\n",
      "Iteration 53, loss = 0.07188511\n",
      "Iteration 54, loss = 0.07172081\n",
      "Iteration 55, loss = 0.07091051\n",
      "Iteration 56, loss = 0.07051313\n",
      "Iteration 57, loss = 0.06984040\n",
      "Iteration 58, loss = 0.06953206\n",
      "Iteration 59, loss = 0.06864898\n",
      "Iteration 60, loss = 0.06855549\n",
      "Iteration 61, loss = 0.06802069\n",
      "Iteration 62, loss = 0.06745437\n",
      "Iteration 63, loss = 0.06729211\n",
      "Iteration 64, loss = 0.06686176\n",
      "Iteration 65, loss = 0.06658486\n",
      "Iteration 66, loss = 0.06599773\n",
      "Iteration 67, loss = 0.06594937\n",
      "Iteration 68, loss = 0.06534725\n",
      "Iteration 69, loss = 0.06532980\n",
      "Iteration 70, loss = 0.06499266\n",
      "Iteration 71, loss = 0.06467196\n",
      "Iteration 72, loss = 0.06445846\n",
      "Iteration 73, loss = 0.06417743\n",
      "Iteration 74, loss = 0.06413684\n",
      "Iteration 75, loss = 0.06375900\n",
      "Iteration 76, loss = 0.06345846\n",
      "Iteration 77, loss = 0.06338533\n",
      "Iteration 78, loss = 0.06305857\n",
      "Iteration 79, loss = 0.06316816\n",
      "Iteration 80, loss = 0.06260858\n",
      "Iteration 81, loss = 0.06240456\n",
      "Iteration 82, loss = 0.06249223\n",
      "Iteration 83, loss = 0.06252148\n",
      "Iteration 84, loss = 0.06190998\n",
      "Iteration 85, loss = 0.06214825\n",
      "Iteration 86, loss = 0.06183147\n",
      "Iteration 87, loss = 0.06164333\n",
      "Iteration 88, loss = 0.06180628\n",
      "Iteration 89, loss = 0.06133952\n",
      "Iteration 90, loss = 0.06144112\n",
      "Iteration 91, loss = 0.06129453\n",
      "Iteration 92, loss = 0.06097005\n",
      "Iteration 93, loss = 0.06084184\n",
      "Iteration 94, loss = 0.06087972\n",
      "Iteration 95, loss = 0.06084450\n",
      "Iteration 96, loss = 0.06069927\n",
      "Iteration 97, loss = 0.06076540\n",
      "Iteration 98, loss = 0.06066014\n",
      "Iteration 99, loss = 0.06069156\n",
      "Iteration 100, loss = 0.06049548\n",
      "Iteration 101, loss = 0.06015726\n",
      "Iteration 102, loss = 0.06024009\n",
      "Iteration 103, loss = 0.06011318\n",
      "Iteration 104, loss = 0.05998624\n",
      "Iteration 105, loss = 0.06041349\n",
      "Iteration 106, loss = 0.06009967\n",
      "Iteration 107, loss = 0.05968958\n",
      "Iteration 108, loss = 0.05979626\n",
      "Iteration 109, loss = 0.05962935\n",
      "Iteration 110, loss = 0.05951585\n",
      "Iteration 111, loss = 0.05948896\n",
      "Iteration 112, loss = 0.05942995\n",
      "Iteration 113, loss = 0.05937952\n",
      "Iteration 114, loss = 0.05932734\n",
      "Iteration 115, loss = 0.05954232\n",
      "Iteration 116, loss = 0.05947030\n",
      "Iteration 117, loss = 0.05955197\n",
      "Iteration 118, loss = 0.05928355\n",
      "Iteration 119, loss = 0.05923267\n",
      "Iteration 120, loss = 0.05894202\n",
      "Iteration 121, loss = 0.05893988\n",
      "Iteration 122, loss = 0.05908178\n",
      "Iteration 123, loss = 0.05900359\n",
      "Iteration 124, loss = 0.05885619\n",
      "Iteration 125, loss = 0.05874224\n",
      "Iteration 126, loss = 0.05872787\n",
      "Iteration 127, loss = 0.05864134\n",
      "Iteration 128, loss = 0.05881145\n",
      "Iteration 129, loss = 0.05880537\n",
      "Iteration 130, loss = 0.05855466\n",
      "Iteration 131, loss = 0.05849843\n",
      "Iteration 132, loss = 0.05846804\n",
      "Iteration 133, loss = 0.05839624\n",
      "Iteration 134, loss = 0.05875063\n",
      "Iteration 135, loss = 0.05858293\n",
      "Iteration 136, loss = 0.05850984\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.01, batch_size=200, hidden_layer_sizes=(100,), solver=adam; total time= 1.2min\n",
      "Iteration 1, loss = 2.28214207\n",
      "Iteration 2, loss = 2.19948769\n",
      "Iteration 3, loss = 2.11799068\n",
      "Iteration 4, loss = 2.02606138\n",
      "Iteration 5, loss = 1.92249895\n",
      "Iteration 6, loss = 1.80949516\n",
      "Iteration 7, loss = 1.69142208\n",
      "Iteration 8, loss = 1.57366501\n",
      "Iteration 9, loss = 1.46111586\n",
      "Iteration 10, loss = 1.35671354\n",
      "Iteration 11, loss = 1.26240267\n",
      "Iteration 12, loss = 1.17815436\n",
      "Iteration 13, loss = 1.10359796\n",
      "Iteration 14, loss = 1.03790949\n",
      "Iteration 15, loss = 0.98012565\n",
      "Iteration 16, loss = 0.92919498\n",
      "Iteration 17, loss = 0.88420205\n",
      "Iteration 18, loss = 0.84440444\n",
      "Iteration 19, loss = 0.80879995\n",
      "Iteration 20, loss = 0.77706759\n",
      "Iteration 21, loss = 0.74859718\n",
      "Iteration 22, loss = 0.72291249\n",
      "Iteration 23, loss = 0.69972530\n",
      "Iteration 24, loss = 0.67862456\n",
      "Iteration 25, loss = 0.65932602\n",
      "Iteration 26, loss = 0.64163281\n",
      "Iteration 27, loss = 0.62538322\n",
      "Iteration 28, loss = 0.61032454\n",
      "Iteration 29, loss = 0.59644992\n",
      "Iteration 30, loss = 0.58361904\n",
      "Iteration 31, loss = 0.57161754\n",
      "Iteration 32, loss = 0.56045024\n",
      "Iteration 33, loss = 0.54995497\n",
      "Iteration 34, loss = 0.54010975\n",
      "Iteration 35, loss = 0.53091649\n",
      "Iteration 36, loss = 0.52225738\n",
      "Iteration 37, loss = 0.51407871\n",
      "Iteration 38, loss = 0.50638631\n",
      "Iteration 39, loss = 0.49909138\n",
      "Iteration 40, loss = 0.49217249\n",
      "Iteration 41, loss = 0.48565667\n",
      "Iteration 42, loss = 0.47942104\n",
      "Iteration 43, loss = 0.47350651\n",
      "Iteration 44, loss = 0.46793063\n",
      "Iteration 45, loss = 0.46258555\n",
      "Iteration 46, loss = 0.45746923\n",
      "Iteration 47, loss = 0.45258946\n",
      "Iteration 48, loss = 0.44793407\n",
      "Iteration 49, loss = 0.44346918\n",
      "Iteration 50, loss = 0.43917460\n",
      "Iteration 51, loss = 0.43512923\n",
      "Iteration 52, loss = 0.43120584\n",
      "Iteration 53, loss = 0.42744979\n",
      "Iteration 54, loss = 0.42387114\n",
      "Iteration 55, loss = 0.42034445\n",
      "Iteration 56, loss = 0.41706616\n",
      "Iteration 57, loss = 0.41380251\n",
      "Iteration 58, loss = 0.41069971\n",
      "Iteration 59, loss = 0.40772967\n",
      "Iteration 60, loss = 0.40483977\n",
      "Iteration 61, loss = 0.40203306\n",
      "Iteration 62, loss = 0.39934897\n",
      "Iteration 63, loss = 0.39676638\n",
      "Iteration 64, loss = 0.39423628\n",
      "Iteration 65, loss = 0.39177926\n",
      "Iteration 66, loss = 0.38939022\n",
      "Iteration 67, loss = 0.38713858\n",
      "Iteration 68, loss = 0.38489614\n",
      "Iteration 69, loss = 0.38271497\n",
      "Iteration 70, loss = 0.38063138\n",
      "Iteration 71, loss = 0.37859610\n",
      "Iteration 72, loss = 0.37663950\n",
      "Iteration 73, loss = 0.37471939\n",
      "Iteration 74, loss = 0.37280281\n",
      "Iteration 75, loss = 0.37100594\n",
      "Iteration 76, loss = 0.36922487\n",
      "Iteration 77, loss = 0.36746868\n",
      "Iteration 78, loss = 0.36577973\n",
      "Iteration 79, loss = 0.36412917\n",
      "Iteration 80, loss = 0.36252875\n",
      "Iteration 81, loss = 0.36095845\n",
      "Iteration 82, loss = 0.35941871\n",
      "Iteration 83, loss = 0.35791883\n",
      "Iteration 84, loss = 0.35646549\n",
      "Iteration 85, loss = 0.35501970\n",
      "Iteration 86, loss = 0.35361447\n",
      "Iteration 87, loss = 0.35224514\n",
      "Iteration 88, loss = 0.35091136\n",
      "Iteration 89, loss = 0.34960479\n",
      "Iteration 90, loss = 0.34830919\n",
      "Iteration 91, loss = 0.34704395\n",
      "Iteration 92, loss = 0.34580675\n",
      "Iteration 93, loss = 0.34457827\n",
      "Iteration 94, loss = 0.34340360\n",
      "Iteration 95, loss = 0.34223108\n",
      "Iteration 96, loss = 0.34109340\n",
      "Iteration 97, loss = 0.33998351\n",
      "Iteration 98, loss = 0.33887080\n",
      "Iteration 99, loss = 0.33779148\n",
      "Iteration 100, loss = 0.33670826\n",
      "Iteration 101, loss = 0.33567415\n",
      "Iteration 102, loss = 0.33464669\n",
      "Iteration 103, loss = 0.33363885\n",
      "Iteration 104, loss = 0.33263001\n",
      "Iteration 105, loss = 0.33165991\n",
      "Iteration 106, loss = 0.33069692\n",
      "Iteration 107, loss = 0.32973635\n",
      "Iteration 108, loss = 0.32881921\n",
      "Iteration 109, loss = 0.32787927\n",
      "Iteration 110, loss = 0.32698464\n",
      "Iteration 111, loss = 0.32607713\n",
      "Iteration 112, loss = 0.32522940\n",
      "Iteration 113, loss = 0.32435668\n",
      "Iteration 114, loss = 0.32352096\n",
      "Iteration 115, loss = 0.32267656\n",
      "Iteration 116, loss = 0.32181948\n",
      "Iteration 117, loss = 0.32100478\n",
      "Iteration 118, loss = 0.32019642\n",
      "Iteration 119, loss = 0.31938716\n",
      "Iteration 120, loss = 0.31863017\n",
      "Iteration 121, loss = 0.31785561\n",
      "Iteration 122, loss = 0.31706058\n",
      "Iteration 123, loss = 0.31633179\n",
      "Iteration 124, loss = 0.31560868\n",
      "Iteration 125, loss = 0.31482355\n",
      "Iteration 126, loss = 0.31409689\n",
      "Iteration 127, loss = 0.31338255\n",
      "Iteration 128, loss = 0.31269514\n",
      "Iteration 129, loss = 0.31196385\n",
      "Iteration 130, loss = 0.31129905\n",
      "Iteration 131, loss = 0.31056329\n",
      "Iteration 132, loss = 0.30991064\n",
      "Iteration 133, loss = 0.30924156\n",
      "Iteration 134, loss = 0.30857965\n",
      "Iteration 135, loss = 0.30789484\n",
      "Iteration 136, loss = 0.30728438\n",
      "Iteration 137, loss = 0.30662234\n",
      "Iteration 138, loss = 0.30597686\n",
      "Iteration 139, loss = 0.30537981\n",
      "Iteration 140, loss = 0.30473842\n",
      "Iteration 141, loss = 0.30410846\n",
      "Iteration 142, loss = 0.30348509\n",
      "Iteration 143, loss = 0.30288554\n",
      "Iteration 144, loss = 0.30226549\n",
      "Iteration 145, loss = 0.30169810\n",
      "Iteration 146, loss = 0.30108867\n",
      "Iteration 147, loss = 0.30049830\n",
      "Iteration 148, loss = 0.29993966\n",
      "Iteration 149, loss = 0.29932608\n",
      "Iteration 150, loss = 0.29878885\n",
      "Iteration 151, loss = 0.29823534\n",
      "Iteration 152, loss = 0.29766390\n",
      "Iteration 153, loss = 0.29710195\n",
      "Iteration 154, loss = 0.29656657\n",
      "Iteration 155, loss = 0.29601498\n",
      "Iteration 156, loss = 0.29546515\n",
      "Iteration 157, loss = 0.29494540\n",
      "Iteration 158, loss = 0.29440798\n",
      "Iteration 159, loss = 0.29386715\n",
      "Iteration 160, loss = 0.29333092\n",
      "Iteration 161, loss = 0.29285127\n",
      "Iteration 162, loss = 0.29231946\n",
      "Iteration 163, loss = 0.29179294\n",
      "Iteration 164, loss = 0.29130017\n",
      "Iteration 165, loss = 0.29079563\n",
      "Iteration 166, loss = 0.29025895\n",
      "Iteration 167, loss = 0.28976056\n",
      "Iteration 168, loss = 0.28931184\n",
      "Iteration 169, loss = 0.28878613\n",
      "Iteration 170, loss = 0.28833758\n",
      "Iteration 171, loss = 0.28780805\n",
      "Iteration 172, loss = 0.28733090\n",
      "Iteration 173, loss = 0.28687669\n",
      "Iteration 174, loss = 0.28637468\n",
      "Iteration 175, loss = 0.28591116\n",
      "Iteration 176, loss = 0.28544544\n",
      "Iteration 177, loss = 0.28498828\n",
      "Iteration 178, loss = 0.28451571\n",
      "Iteration 179, loss = 0.28404990\n",
      "Iteration 180, loss = 0.28358852\n",
      "Iteration 181, loss = 0.28314750\n",
      "Iteration 182, loss = 0.28270743\n",
      "Iteration 183, loss = 0.28222479\n",
      "Iteration 184, loss = 0.28181984\n",
      "Iteration 185, loss = 0.28131216\n",
      "Iteration 186, loss = 0.28089262\n",
      "Iteration 187, loss = 0.28045255\n",
      "Iteration 188, loss = 0.28003172\n",
      "Iteration 189, loss = 0.27959759\n",
      "Iteration 190, loss = 0.27914358\n",
      "Iteration 191, loss = 0.27871954\n",
      "Iteration 192, loss = 0.27830265\n",
      "Iteration 193, loss = 0.27787209\n",
      "Iteration 194, loss = 0.27744516\n",
      "Iteration 195, loss = 0.27703363\n",
      "Iteration 196, loss = 0.27660400\n",
      "Iteration 197, loss = 0.27617600\n",
      "Iteration 198, loss = 0.27577917\n",
      "Iteration 199, loss = 0.27534966\n",
      "Iteration 200, loss = 0.27495021\n",
      "[CV] END activation=logistic, alpha=0.01, batch_size=200, hidden_layer_sizes=(100,), solver=sgd; total time= 1.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.26418540\n",
      "Iteration 2, loss = 2.18344442\n",
      "Iteration 3, loss = 2.09919963\n",
      "Iteration 4, loss = 2.00393491\n",
      "Iteration 5, loss = 1.89675801\n",
      "Iteration 6, loss = 1.78016440\n",
      "Iteration 7, loss = 1.65955389\n",
      "Iteration 8, loss = 1.54054688\n",
      "Iteration 9, loss = 1.42812697\n",
      "Iteration 10, loss = 1.32551619\n",
      "Iteration 11, loss = 1.23352693\n",
      "Iteration 12, loss = 1.15245642\n",
      "Iteration 13, loss = 1.08109896\n",
      "Iteration 14, loss = 1.01849220\n",
      "Iteration 15, loss = 0.96346434\n",
      "Iteration 16, loss = 0.91499181\n",
      "Iteration 17, loss = 0.87205761\n",
      "Iteration 18, loss = 0.83394870\n",
      "Iteration 19, loss = 0.79993015\n",
      "Iteration 20, loss = 0.76944828\n",
      "Iteration 21, loss = 0.74188956\n",
      "Iteration 22, loss = 0.71716930\n",
      "Iteration 23, loss = 0.69462951\n",
      "Iteration 24, loss = 0.67415819\n",
      "Iteration 25, loss = 0.65534846\n",
      "Iteration 26, loss = 0.63819289\n",
      "Iteration 27, loss = 0.62231654\n",
      "Iteration 28, loss = 0.60765877\n",
      "Iteration 29, loss = 0.59413249\n",
      "Iteration 30, loss = 0.58150889\n",
      "Iteration 31, loss = 0.56976278\n",
      "Iteration 32, loss = 0.55878825\n",
      "Iteration 33, loss = 0.54854968\n",
      "Iteration 34, loss = 0.53897460\n",
      "Iteration 35, loss = 0.52991345\n",
      "Iteration 36, loss = 0.52145583\n",
      "Iteration 37, loss = 0.51346124\n",
      "Iteration 38, loss = 0.50588827\n",
      "Iteration 39, loss = 0.49877815\n",
      "Iteration 40, loss = 0.49198234\n",
      "Iteration 41, loss = 0.48559618\n",
      "Iteration 42, loss = 0.47954208\n",
      "Iteration 43, loss = 0.47372854\n",
      "Iteration 44, loss = 0.46824108\n",
      "Iteration 45, loss = 0.46300014\n",
      "Iteration 46, loss = 0.45800142\n",
      "Iteration 47, loss = 0.45327079\n",
      "Iteration 48, loss = 0.44869701\n",
      "Iteration 49, loss = 0.44434306\n",
      "Iteration 50, loss = 0.44016391\n",
      "Iteration 51, loss = 0.43621611\n",
      "Iteration 52, loss = 0.43233945\n",
      "Iteration 53, loss = 0.42867847\n",
      "Iteration 54, loss = 0.42513026\n",
      "Iteration 55, loss = 0.42173852\n",
      "Iteration 56, loss = 0.41847708\n",
      "Iteration 57, loss = 0.41534953\n",
      "Iteration 58, loss = 0.41228492\n",
      "Iteration 59, loss = 0.40934230\n",
      "Iteration 60, loss = 0.40650981\n",
      "Iteration 61, loss = 0.40380987\n",
      "Iteration 62, loss = 0.40117626\n",
      "Iteration 63, loss = 0.39863359\n",
      "Iteration 64, loss = 0.39615083\n",
      "Iteration 65, loss = 0.39376303\n",
      "Iteration 66, loss = 0.39147913\n",
      "Iteration 67, loss = 0.38920351\n",
      "Iteration 68, loss = 0.38704445\n",
      "Iteration 69, loss = 0.38492866\n",
      "Iteration 70, loss = 0.38287190\n",
      "Iteration 71, loss = 0.38089019\n",
      "Iteration 72, loss = 0.37893048\n",
      "Iteration 73, loss = 0.37705639\n",
      "Iteration 74, loss = 0.37522425\n",
      "Iteration 75, loss = 0.37344542\n",
      "Iteration 76, loss = 0.37169558\n",
      "Iteration 77, loss = 0.37001616\n",
      "Iteration 78, loss = 0.36832039\n",
      "Iteration 79, loss = 0.36672359\n",
      "Iteration 80, loss = 0.36516263\n",
      "Iteration 81, loss = 0.36364245\n",
      "Iteration 82, loss = 0.36213645\n",
      "Iteration 83, loss = 0.36062799\n",
      "Iteration 84, loss = 0.35919491\n",
      "Iteration 85, loss = 0.35780545\n",
      "Iteration 86, loss = 0.35641300\n",
      "Iteration 87, loss = 0.35511151\n",
      "Iteration 88, loss = 0.35376290\n",
      "Iteration 89, loss = 0.35245679\n",
      "Iteration 90, loss = 0.35118566\n",
      "Iteration 91, loss = 0.34997223\n",
      "Iteration 92, loss = 0.34875005\n",
      "Iteration 93, loss = 0.34758357\n",
      "Iteration 94, loss = 0.34640945\n",
      "Iteration 95, loss = 0.34525520\n",
      "Iteration 96, loss = 0.34412713\n",
      "Iteration 97, loss = 0.34302786\n",
      "Iteration 98, loss = 0.34193465\n",
      "Iteration 99, loss = 0.34090265\n",
      "Iteration 100, loss = 0.33982024\n",
      "Iteration 101, loss = 0.33877250\n",
      "Iteration 102, loss = 0.33779423\n",
      "Iteration 103, loss = 0.33678726\n",
      "Iteration 104, loss = 0.33578681\n",
      "Iteration 105, loss = 0.33484584\n",
      "Iteration 106, loss = 0.33389032\n",
      "Iteration 107, loss = 0.33296089\n",
      "Iteration 108, loss = 0.33205605\n",
      "Iteration 109, loss = 0.33116301\n",
      "Iteration 110, loss = 0.33026030\n",
      "Iteration 111, loss = 0.32939171\n",
      "Iteration 112, loss = 0.32850483\n",
      "Iteration 113, loss = 0.32766720\n",
      "Iteration 114, loss = 0.32679400\n",
      "Iteration 115, loss = 0.32600632\n",
      "Iteration 116, loss = 0.32516476\n",
      "Iteration 117, loss = 0.32437226\n",
      "Iteration 118, loss = 0.32355341\n",
      "Iteration 119, loss = 0.32280365\n",
      "Iteration 120, loss = 0.32198580\n",
      "Iteration 121, loss = 0.32123996\n",
      "Iteration 122, loss = 0.32049718\n",
      "Iteration 123, loss = 0.31974887\n",
      "Iteration 124, loss = 0.31900598\n",
      "Iteration 125, loss = 0.31830053\n",
      "Iteration 126, loss = 0.31755418\n",
      "Iteration 127, loss = 0.31685918\n",
      "Iteration 128, loss = 0.31615767\n",
      "Iteration 129, loss = 0.31547641\n",
      "Iteration 130, loss = 0.31478810\n",
      "Iteration 131, loss = 0.31408903\n",
      "Iteration 132, loss = 0.31344418\n",
      "Iteration 133, loss = 0.31273054\n",
      "Iteration 134, loss = 0.31212088\n",
      "Iteration 135, loss = 0.31145932\n",
      "Iteration 136, loss = 0.31082971\n",
      "Iteration 137, loss = 0.31020641\n",
      "Iteration 138, loss = 0.30955458\n",
      "Iteration 139, loss = 0.30892364\n",
      "Iteration 140, loss = 0.30831641\n",
      "Iteration 141, loss = 0.30770081\n",
      "Iteration 142, loss = 0.30710443\n",
      "Iteration 143, loss = 0.30653062\n",
      "Iteration 144, loss = 0.30590687\n",
      "Iteration 145, loss = 0.30530379\n",
      "Iteration 146, loss = 0.30474934\n",
      "Iteration 147, loss = 0.30417771\n",
      "Iteration 148, loss = 0.30360443\n",
      "Iteration 149, loss = 0.30304329\n",
      "Iteration 150, loss = 0.30248244\n",
      "Iteration 151, loss = 0.30189634\n",
      "Iteration 152, loss = 0.30134476\n",
      "Iteration 153, loss = 0.30080176\n",
      "Iteration 154, loss = 0.30027745\n",
      "Iteration 155, loss = 0.29973059\n",
      "Iteration 156, loss = 0.29919952\n",
      "Iteration 157, loss = 0.29868340\n",
      "Iteration 158, loss = 0.29817211\n",
      "Iteration 159, loss = 0.29762745\n",
      "Iteration 160, loss = 0.29711949\n",
      "Iteration 161, loss = 0.29658068\n",
      "Iteration 162, loss = 0.29608159\n",
      "Iteration 163, loss = 0.29557302\n",
      "Iteration 164, loss = 0.29510277\n",
      "Iteration 165, loss = 0.29457251\n",
      "Iteration 166, loss = 0.29406823\n",
      "Iteration 167, loss = 0.29361544\n",
      "Iteration 168, loss = 0.29310260\n",
      "Iteration 169, loss = 0.29262258\n",
      "Iteration 170, loss = 0.29213634\n",
      "Iteration 171, loss = 0.29165977\n",
      "Iteration 172, loss = 0.29118900\n",
      "Iteration 173, loss = 0.29070008\n",
      "Iteration 174, loss = 0.29022346\n",
      "Iteration 175, loss = 0.28977631\n",
      "Iteration 176, loss = 0.28929636\n",
      "Iteration 177, loss = 0.28883346\n",
      "Iteration 178, loss = 0.28838731\n",
      "Iteration 179, loss = 0.28790626\n",
      "Iteration 180, loss = 0.28748706\n",
      "Iteration 181, loss = 0.28703207\n",
      "Iteration 182, loss = 0.28658313\n",
      "Iteration 183, loss = 0.28613385\n",
      "Iteration 184, loss = 0.28569674\n",
      "Iteration 185, loss = 0.28527882\n",
      "Iteration 186, loss = 0.28479515\n",
      "Iteration 187, loss = 0.28437113\n",
      "Iteration 188, loss = 0.28394891\n",
      "Iteration 189, loss = 0.28350921\n",
      "Iteration 190, loss = 0.28310750\n",
      "Iteration 191, loss = 0.28266300\n",
      "Iteration 192, loss = 0.28223360\n",
      "Iteration 193, loss = 0.28181738\n",
      "Iteration 194, loss = 0.28137982\n",
      "Iteration 195, loss = 0.28098697\n",
      "Iteration 196, loss = 0.28057158\n",
      "Iteration 197, loss = 0.28011974\n",
      "Iteration 198, loss = 0.27975688\n",
      "Iteration 199, loss = 0.27934703\n",
      "Iteration 200, loss = 0.27892311\n",
      "[CV] END activation=logistic, alpha=0.01, batch_size=200, hidden_layer_sizes=(100,), solver=sgd; total time= 1.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.28439265\n",
      "Iteration 2, loss = 2.19411208\n",
      "Iteration 3, loss = 2.11159387\n",
      "Iteration 4, loss = 2.01833255\n",
      "Iteration 5, loss = 1.91307447\n",
      "Iteration 6, loss = 1.79800329\n",
      "Iteration 7, loss = 1.67761898\n",
      "Iteration 8, loss = 1.55749364\n",
      "Iteration 9, loss = 1.44308764\n",
      "Iteration 10, loss = 1.33784157\n",
      "Iteration 11, loss = 1.24351753\n",
      "Iteration 12, loss = 1.16030030\n",
      "Iteration 13, loss = 1.08735217\n",
      "Iteration 14, loss = 1.02366334\n",
      "Iteration 15, loss = 0.96793003\n",
      "Iteration 16, loss = 0.91896728\n",
      "Iteration 17, loss = 0.87594302\n",
      "Iteration 18, loss = 0.83778053\n",
      "Iteration 19, loss = 0.80377160\n",
      "Iteration 20, loss = 0.77331401\n",
      "Iteration 21, loss = 0.74591331\n",
      "Iteration 22, loss = 0.72120361\n",
      "Iteration 23, loss = 0.69863696\n",
      "Iteration 24, loss = 0.67807668\n",
      "Iteration 25, loss = 0.65928912\n",
      "Iteration 26, loss = 0.64203094\n",
      "Iteration 27, loss = 0.62607729\n",
      "Iteration 28, loss = 0.61131419\n",
      "Iteration 29, loss = 0.59762158\n",
      "Iteration 30, loss = 0.58490847\n",
      "Iteration 31, loss = 0.57303957\n",
      "Iteration 32, loss = 0.56194404\n",
      "Iteration 33, loss = 0.55152176\n",
      "Iteration 34, loss = 0.54178964\n",
      "Iteration 35, loss = 0.53259872\n",
      "Iteration 36, loss = 0.52393761\n",
      "Iteration 37, loss = 0.51586652\n",
      "Iteration 38, loss = 0.50813578\n",
      "Iteration 39, loss = 0.50090980\n",
      "Iteration 40, loss = 0.49397526\n",
      "Iteration 41, loss = 0.48749520\n",
      "Iteration 42, loss = 0.48123401\n",
      "Iteration 43, loss = 0.47534541\n",
      "Iteration 44, loss = 0.46975297\n",
      "Iteration 45, loss = 0.46439168\n",
      "Iteration 46, loss = 0.45927087\n",
      "Iteration 47, loss = 0.45439903\n",
      "Iteration 48, loss = 0.44977930\n",
      "Iteration 49, loss = 0.44530074\n",
      "Iteration 50, loss = 0.44101959\n",
      "Iteration 51, loss = 0.43696445\n",
      "Iteration 52, loss = 0.43303549\n",
      "Iteration 53, loss = 0.42924423\n",
      "Iteration 54, loss = 0.42561287\n",
      "Iteration 55, loss = 0.42215919\n",
      "Iteration 56, loss = 0.41883230\n",
      "Iteration 57, loss = 0.41558504\n",
      "Iteration 58, loss = 0.41247797\n",
      "Iteration 59, loss = 0.40948849\n",
      "Iteration 60, loss = 0.40654213\n",
      "Iteration 61, loss = 0.40379965\n",
      "Iteration 62, loss = 0.40104852\n",
      "Iteration 63, loss = 0.39845398\n",
      "Iteration 64, loss = 0.39590051\n",
      "Iteration 65, loss = 0.39348297\n",
      "Iteration 66, loss = 0.39108862\n",
      "Iteration 67, loss = 0.38880407\n",
      "Iteration 68, loss = 0.38660964\n",
      "Iteration 69, loss = 0.38439194\n",
      "Iteration 70, loss = 0.38230391\n",
      "Iteration 71, loss = 0.38022879\n",
      "Iteration 72, loss = 0.37824271\n",
      "Iteration 73, loss = 0.37631657\n",
      "Iteration 74, loss = 0.37442128\n",
      "Iteration 75, loss = 0.37258356\n",
      "Iteration 76, loss = 0.37081957\n",
      "Iteration 77, loss = 0.36906014\n",
      "Iteration 78, loss = 0.36733937\n",
      "Iteration 79, loss = 0.36568303\n",
      "Iteration 80, loss = 0.36409898\n",
      "Iteration 81, loss = 0.36251609\n",
      "Iteration 82, loss = 0.36097778\n",
      "Iteration 83, loss = 0.35944388\n",
      "Iteration 84, loss = 0.35796904\n",
      "Iteration 85, loss = 0.35655981\n",
      "Iteration 86, loss = 0.35516928\n",
      "Iteration 87, loss = 0.35376841\n",
      "Iteration 88, loss = 0.35236367\n",
      "Iteration 89, loss = 0.35110627\n",
      "Iteration 90, loss = 0.34977865\n",
      "Iteration 91, loss = 0.34851025\n",
      "Iteration 92, loss = 0.34728542\n",
      "Iteration 93, loss = 0.34605105\n",
      "Iteration 94, loss = 0.34488609\n",
      "Iteration 95, loss = 0.34370766\n",
      "Iteration 96, loss = 0.34254448\n",
      "Iteration 97, loss = 0.34140608\n",
      "Iteration 98, loss = 0.34031721\n",
      "Iteration 99, loss = 0.33922023\n",
      "Iteration 100, loss = 0.33816745\n",
      "Iteration 101, loss = 0.33712809\n",
      "Iteration 102, loss = 0.33608980\n",
      "Iteration 103, loss = 0.33503377\n",
      "Iteration 104, loss = 0.33407459\n",
      "Iteration 105, loss = 0.33305261\n",
      "Iteration 106, loss = 0.33212314\n",
      "Iteration 107, loss = 0.33115121\n",
      "Iteration 108, loss = 0.33022566\n",
      "Iteration 109, loss = 0.32928973\n",
      "Iteration 110, loss = 0.32838158\n",
      "Iteration 111, loss = 0.32745447\n",
      "Iteration 112, loss = 0.32661058\n",
      "Iteration 113, loss = 0.32571330\n",
      "Iteration 114, loss = 0.32485873\n",
      "Iteration 115, loss = 0.32401086\n",
      "Iteration 116, loss = 0.32318924\n",
      "Iteration 117, loss = 0.32239021\n",
      "Iteration 118, loss = 0.32155526\n",
      "Iteration 119, loss = 0.32075949\n",
      "Iteration 120, loss = 0.31997700\n",
      "Iteration 121, loss = 0.31919799\n",
      "Iteration 122, loss = 0.31843848\n",
      "Iteration 123, loss = 0.31769288\n",
      "Iteration 124, loss = 0.31692231\n",
      "Iteration 125, loss = 0.31618829\n",
      "Iteration 126, loss = 0.31545165\n",
      "Iteration 127, loss = 0.31471472\n",
      "Iteration 128, loss = 0.31399954\n",
      "Iteration 129, loss = 0.31332004\n",
      "Iteration 130, loss = 0.31260308\n",
      "Iteration 131, loss = 0.31190381\n",
      "Iteration 132, loss = 0.31124600\n",
      "Iteration 133, loss = 0.31054971\n",
      "Iteration 134, loss = 0.30986896\n",
      "Iteration 135, loss = 0.30926348\n",
      "Iteration 136, loss = 0.30858675\n",
      "Iteration 137, loss = 0.30791194\n",
      "Iteration 138, loss = 0.30729640\n",
      "Iteration 139, loss = 0.30663645\n",
      "Iteration 140, loss = 0.30600675\n",
      "Iteration 141, loss = 0.30540758\n",
      "Iteration 142, loss = 0.30480360\n",
      "Iteration 143, loss = 0.30415784\n",
      "Iteration 144, loss = 0.30358813\n",
      "Iteration 145, loss = 0.30297831\n",
      "Iteration 146, loss = 0.30240447\n",
      "Iteration 147, loss = 0.30179080\n",
      "Iteration 148, loss = 0.30121078\n",
      "Iteration 149, loss = 0.30065888\n",
      "Iteration 150, loss = 0.30008341\n",
      "Iteration 151, loss = 0.29951998\n",
      "Iteration 152, loss = 0.29894807\n",
      "Iteration 153, loss = 0.29840797\n",
      "Iteration 154, loss = 0.29784605\n",
      "Iteration 155, loss = 0.29730513\n",
      "Iteration 156, loss = 0.29676997\n",
      "Iteration 157, loss = 0.29623934\n",
      "Iteration 158, loss = 0.29570309\n",
      "Iteration 159, loss = 0.29517026\n",
      "Iteration 160, loss = 0.29463320\n",
      "Iteration 161, loss = 0.29414432\n",
      "Iteration 162, loss = 0.29360722\n",
      "Iteration 163, loss = 0.29310313\n",
      "Iteration 164, loss = 0.29258499\n",
      "Iteration 165, loss = 0.29205146\n",
      "Iteration 166, loss = 0.29157920\n",
      "Iteration 167, loss = 0.29107688\n",
      "Iteration 168, loss = 0.29058687\n",
      "Iteration 169, loss = 0.29011197\n",
      "Iteration 170, loss = 0.28961720\n",
      "Iteration 171, loss = 0.28912983\n",
      "Iteration 172, loss = 0.28862447\n",
      "Iteration 173, loss = 0.28813440\n",
      "Iteration 174, loss = 0.28767986\n",
      "Iteration 175, loss = 0.28719935\n",
      "Iteration 176, loss = 0.28674974\n",
      "Iteration 177, loss = 0.28627392\n",
      "Iteration 178, loss = 0.28582066\n",
      "Iteration 179, loss = 0.28534863\n",
      "Iteration 180, loss = 0.28488477\n",
      "Iteration 181, loss = 0.28442891\n",
      "Iteration 182, loss = 0.28400112\n",
      "Iteration 183, loss = 0.28354446\n",
      "Iteration 184, loss = 0.28312633\n",
      "Iteration 185, loss = 0.28264518\n",
      "Iteration 186, loss = 0.28222954\n",
      "Iteration 187, loss = 0.28177039\n",
      "Iteration 188, loss = 0.28132536\n",
      "Iteration 189, loss = 0.28090545\n",
      "Iteration 190, loss = 0.28047523\n",
      "Iteration 191, loss = 0.28003122\n",
      "Iteration 192, loss = 0.27961691\n",
      "Iteration 193, loss = 0.27917536\n",
      "Iteration 194, loss = 0.27877190\n",
      "Iteration 195, loss = 0.27837302\n",
      "Iteration 196, loss = 0.27793238\n",
      "Iteration 197, loss = 0.27750587\n",
      "Iteration 198, loss = 0.27710051\n",
      "Iteration 199, loss = 0.27668849\n",
      "Iteration 200, loss = 0.27627300\n",
      "[CV] END activation=logistic, alpha=0.01, batch_size=200, hidden_layer_sizes=(100,), solver=sgd; total time= 1.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.28222781\n",
      "Iteration 2, loss = 2.19692866\n",
      "Iteration 3, loss = 2.11477500\n",
      "Iteration 4, loss = 2.02192413\n",
      "Iteration 5, loss = 1.91612847\n",
      "Iteration 6, loss = 1.79854768\n",
      "Iteration 7, loss = 1.67391866\n",
      "Iteration 8, loss = 1.54878228\n",
      "Iteration 9, loss = 1.42974043\n",
      "Iteration 10, loss = 1.32099286\n",
      "Iteration 11, loss = 1.22419943\n",
      "Iteration 12, loss = 1.13961422\n",
      "Iteration 13, loss = 1.06612384\n",
      "Iteration 14, loss = 1.00229156\n",
      "Iteration 15, loss = 0.94677868\n",
      "Iteration 16, loss = 0.89812787\n",
      "Iteration 17, loss = 0.85549759\n",
      "Iteration 18, loss = 0.81787799\n",
      "Iteration 19, loss = 0.78441374\n",
      "Iteration 20, loss = 0.75459190\n",
      "Iteration 21, loss = 0.72772466\n",
      "Iteration 22, loss = 0.70357255\n",
      "Iteration 23, loss = 0.68165127\n",
      "Iteration 24, loss = 0.66172715\n",
      "Iteration 25, loss = 0.64350009\n",
      "Iteration 26, loss = 0.62678367\n",
      "Iteration 27, loss = 0.61140724\n",
      "Iteration 28, loss = 0.59717251\n",
      "Iteration 29, loss = 0.58404072\n",
      "Iteration 30, loss = 0.57181615\n",
      "Iteration 31, loss = 0.56036875\n",
      "Iteration 32, loss = 0.54975955\n",
      "Iteration 33, loss = 0.53985638\n",
      "Iteration 34, loss = 0.53050433\n",
      "Iteration 35, loss = 0.52175421\n",
      "Iteration 36, loss = 0.51350609\n",
      "Iteration 37, loss = 0.50575155\n",
      "Iteration 38, loss = 0.49844055\n",
      "Iteration 39, loss = 0.49146743\n",
      "Iteration 40, loss = 0.48495874\n",
      "Iteration 41, loss = 0.47870828\n",
      "Iteration 42, loss = 0.47283617\n",
      "Iteration 43, loss = 0.46721156\n",
      "Iteration 44, loss = 0.46189279\n",
      "Iteration 45, loss = 0.45682292\n",
      "Iteration 46, loss = 0.45196279\n",
      "Iteration 47, loss = 0.44734099\n",
      "Iteration 48, loss = 0.44294193\n",
      "Iteration 49, loss = 0.43872023\n",
      "Iteration 50, loss = 0.43465009\n",
      "Iteration 51, loss = 0.43076621\n",
      "Iteration 52, loss = 0.42703324\n",
      "Iteration 53, loss = 0.42349381\n",
      "Iteration 54, loss = 0.42004520\n",
      "Iteration 55, loss = 0.41675918\n",
      "Iteration 56, loss = 0.41355661\n",
      "Iteration 57, loss = 0.41054501\n",
      "Iteration 58, loss = 0.40755432\n",
      "Iteration 59, loss = 0.40472963\n",
      "Iteration 60, loss = 0.40202616\n",
      "Iteration 61, loss = 0.39934888\n",
      "Iteration 62, loss = 0.39675955\n",
      "Iteration 63, loss = 0.39428525\n",
      "Iteration 64, loss = 0.39189740\n",
      "Iteration 65, loss = 0.38953429\n",
      "Iteration 66, loss = 0.38733165\n",
      "Iteration 67, loss = 0.38511877\n",
      "Iteration 68, loss = 0.38298936\n",
      "Iteration 69, loss = 0.38095531\n",
      "Iteration 70, loss = 0.37894646\n",
      "Iteration 71, loss = 0.37700658\n",
      "Iteration 72, loss = 0.37513047\n",
      "Iteration 73, loss = 0.37326806\n",
      "Iteration 74, loss = 0.37148048\n",
      "Iteration 75, loss = 0.36975476\n",
      "Iteration 76, loss = 0.36804281\n",
      "Iteration 77, loss = 0.36640117\n",
      "Iteration 78, loss = 0.36476026\n",
      "Iteration 79, loss = 0.36318601\n",
      "Iteration 80, loss = 0.36165889\n",
      "Iteration 81, loss = 0.36013754\n",
      "Iteration 82, loss = 0.35868671\n",
      "Iteration 83, loss = 0.35724990\n",
      "Iteration 84, loss = 0.35586598\n",
      "Iteration 85, loss = 0.35446192\n",
      "Iteration 86, loss = 0.35314339\n",
      "Iteration 87, loss = 0.35179440\n",
      "Iteration 88, loss = 0.35051717\n",
      "Iteration 89, loss = 0.34925853\n",
      "Iteration 90, loss = 0.34804156\n",
      "Iteration 91, loss = 0.34683106\n",
      "Iteration 92, loss = 0.34563910\n",
      "Iteration 93, loss = 0.34448021\n",
      "Iteration 94, loss = 0.34333983\n",
      "Iteration 95, loss = 0.34222123\n",
      "Iteration 96, loss = 0.34109571\n",
      "Iteration 97, loss = 0.34002153\n",
      "Iteration 98, loss = 0.33897717\n",
      "Iteration 99, loss = 0.33792271\n",
      "Iteration 100, loss = 0.33690413\n",
      "Iteration 101, loss = 0.33585010\n",
      "Iteration 102, loss = 0.33489862\n",
      "Iteration 103, loss = 0.33392408\n",
      "Iteration 104, loss = 0.33297217\n",
      "Iteration 105, loss = 0.33204169\n",
      "Iteration 106, loss = 0.33109216\n",
      "Iteration 107, loss = 0.33015769\n",
      "Iteration 108, loss = 0.32928428\n",
      "Iteration 109, loss = 0.32837561\n",
      "Iteration 110, loss = 0.32751636\n",
      "Iteration 111, loss = 0.32667329\n",
      "Iteration 112, loss = 0.32579447\n",
      "Iteration 113, loss = 0.32496943\n",
      "Iteration 114, loss = 0.32412472\n",
      "Iteration 115, loss = 0.32329584\n",
      "Iteration 116, loss = 0.32251843\n",
      "Iteration 117, loss = 0.32171908\n",
      "Iteration 118, loss = 0.32095551\n",
      "Iteration 119, loss = 0.32015722\n",
      "Iteration 120, loss = 0.31940315\n",
      "Iteration 121, loss = 0.31867600\n",
      "Iteration 122, loss = 0.31791634\n",
      "Iteration 123, loss = 0.31717137\n",
      "Iteration 124, loss = 0.31647021\n",
      "Iteration 125, loss = 0.31573477\n",
      "Iteration 126, loss = 0.31504516\n",
      "Iteration 127, loss = 0.31433213\n",
      "Iteration 128, loss = 0.31363706\n",
      "Iteration 129, loss = 0.31294791\n",
      "Iteration 130, loss = 0.31225353\n",
      "Iteration 131, loss = 0.31159178\n",
      "Iteration 132, loss = 0.31090895\n",
      "Iteration 133, loss = 0.31027842\n",
      "Iteration 134, loss = 0.30965351\n",
      "Iteration 135, loss = 0.30900758\n",
      "Iteration 136, loss = 0.30835558\n",
      "Iteration 137, loss = 0.30772982\n",
      "Iteration 138, loss = 0.30711149\n",
      "Iteration 139, loss = 0.30649901\n",
      "Iteration 140, loss = 0.30587769\n",
      "Iteration 141, loss = 0.30526975\n",
      "Iteration 142, loss = 0.30467656\n",
      "Iteration 143, loss = 0.30408286\n",
      "Iteration 144, loss = 0.30349980\n",
      "Iteration 145, loss = 0.30290166\n",
      "Iteration 146, loss = 0.30232515\n",
      "Iteration 147, loss = 0.30177953\n",
      "Iteration 148, loss = 0.30119478\n",
      "Iteration 149, loss = 0.30064347\n",
      "Iteration 150, loss = 0.30006268\n",
      "Iteration 151, loss = 0.29950580\n",
      "Iteration 152, loss = 0.29897119\n",
      "Iteration 153, loss = 0.29843077\n",
      "Iteration 154, loss = 0.29790504\n",
      "Iteration 155, loss = 0.29734058\n",
      "Iteration 156, loss = 0.29681888\n",
      "Iteration 157, loss = 0.29632662\n",
      "Iteration 158, loss = 0.29577840\n",
      "Iteration 159, loss = 0.29523986\n",
      "Iteration 160, loss = 0.29472344\n",
      "Iteration 161, loss = 0.29421241\n",
      "Iteration 162, loss = 0.29372621\n",
      "Iteration 163, loss = 0.29320795\n",
      "Iteration 164, loss = 0.29272391\n",
      "Iteration 165, loss = 0.29223242\n",
      "Iteration 166, loss = 0.29173123\n",
      "Iteration 167, loss = 0.29121336\n",
      "Iteration 168, loss = 0.29074856\n",
      "Iteration 169, loss = 0.29027238\n",
      "Iteration 170, loss = 0.28978952\n",
      "Iteration 171, loss = 0.28929241\n",
      "Iteration 172, loss = 0.28883624\n",
      "Iteration 173, loss = 0.28836935\n",
      "Iteration 174, loss = 0.28787102\n",
      "Iteration 175, loss = 0.28741403\n",
      "Iteration 176, loss = 0.28695547\n",
      "Iteration 177, loss = 0.28649486\n",
      "Iteration 178, loss = 0.28601837\n",
      "Iteration 179, loss = 0.28558615\n",
      "Iteration 180, loss = 0.28513133\n",
      "Iteration 181, loss = 0.28468466\n",
      "Iteration 182, loss = 0.28424861\n",
      "Iteration 183, loss = 0.28377346\n",
      "Iteration 184, loss = 0.28337770\n",
      "Iteration 185, loss = 0.28285506\n",
      "Iteration 186, loss = 0.28250059\n",
      "Iteration 187, loss = 0.28204997\n",
      "Iteration 188, loss = 0.28162078\n",
      "Iteration 189, loss = 0.28119076\n",
      "Iteration 190, loss = 0.28073684\n",
      "Iteration 191, loss = 0.28032766\n",
      "Iteration 192, loss = 0.27988246\n",
      "Iteration 193, loss = 0.27950890\n",
      "Iteration 194, loss = 0.27906671\n",
      "Iteration 195, loss = 0.27862939\n",
      "Iteration 196, loss = 0.27824330\n",
      "Iteration 197, loss = 0.27783492\n",
      "Iteration 198, loss = 0.27743737\n",
      "Iteration 199, loss = 0.27702139\n",
      "Iteration 200, loss = 0.27659216\n",
      "[CV] END activation=logistic, alpha=0.01, batch_size=200, hidden_layer_sizes=(100,), solver=sgd; total time= 1.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.28030868\n",
      "Iteration 2, loss = 2.18374918\n",
      "Iteration 3, loss = 2.09673340\n",
      "Iteration 4, loss = 1.99939311\n",
      "Iteration 5, loss = 1.89076398\n",
      "Iteration 6, loss = 1.77295004\n",
      "Iteration 7, loss = 1.65086661\n",
      "Iteration 8, loss = 1.53049626\n",
      "Iteration 9, loss = 1.41663551\n",
      "Iteration 10, loss = 1.31252788\n",
      "Iteration 11, loss = 1.21959058\n",
      "Iteration 12, loss = 1.13790302\n",
      "Iteration 13, loss = 1.06628986\n",
      "Iteration 14, loss = 1.00367976\n",
      "Iteration 15, loss = 0.94892009\n",
      "Iteration 16, loss = 0.90086274\n",
      "Iteration 17, loss = 0.85853126\n",
      "Iteration 18, loss = 0.82096352\n",
      "Iteration 19, loss = 0.78767728\n",
      "Iteration 20, loss = 0.75790458\n",
      "Iteration 21, loss = 0.73117072\n",
      "Iteration 22, loss = 0.70698901\n",
      "Iteration 23, loss = 0.68518559\n",
      "Iteration 24, loss = 0.66530929\n",
      "Iteration 25, loss = 0.64711838\n",
      "Iteration 26, loss = 0.63053987\n",
      "Iteration 27, loss = 0.61525215\n",
      "Iteration 28, loss = 0.60107396\n",
      "Iteration 29, loss = 0.58800583\n",
      "Iteration 30, loss = 0.57586856\n",
      "Iteration 31, loss = 0.56455388\n",
      "Iteration 32, loss = 0.55397073\n",
      "Iteration 33, loss = 0.54408657\n",
      "Iteration 34, loss = 0.53483598\n",
      "Iteration 35, loss = 0.52614326\n",
      "Iteration 36, loss = 0.51789789\n",
      "Iteration 37, loss = 0.51022845\n",
      "Iteration 38, loss = 0.50294363\n",
      "Iteration 39, loss = 0.49605386\n",
      "Iteration 40, loss = 0.48949133\n",
      "Iteration 41, loss = 0.48325916\n",
      "Iteration 42, loss = 0.47737208\n",
      "Iteration 43, loss = 0.47177839\n",
      "Iteration 44, loss = 0.46645942\n",
      "Iteration 45, loss = 0.46136085\n",
      "Iteration 46, loss = 0.45651298\n",
      "Iteration 47, loss = 0.45183923\n",
      "Iteration 48, loss = 0.44737304\n",
      "Iteration 49, loss = 0.44316776\n",
      "Iteration 50, loss = 0.43910020\n",
      "Iteration 51, loss = 0.43519080\n",
      "Iteration 52, loss = 0.43141765\n",
      "Iteration 53, loss = 0.42782278\n",
      "Iteration 54, loss = 0.42436329\n",
      "Iteration 55, loss = 0.42098037\n",
      "Iteration 56, loss = 0.41778558\n",
      "Iteration 57, loss = 0.41468033\n",
      "Iteration 58, loss = 0.41170418\n",
      "Iteration 59, loss = 0.40878289\n",
      "Iteration 60, loss = 0.40599838\n",
      "Iteration 61, loss = 0.40331046\n",
      "Iteration 62, loss = 0.40070124\n",
      "Iteration 63, loss = 0.39814634\n",
      "Iteration 64, loss = 0.39571024\n",
      "Iteration 65, loss = 0.39333362\n",
      "Iteration 66, loss = 0.39099002\n",
      "Iteration 67, loss = 0.38877929\n",
      "Iteration 68, loss = 0.38660929\n",
      "Iteration 69, loss = 0.38446860\n",
      "Iteration 70, loss = 0.38246457\n",
      "Iteration 71, loss = 0.38046820\n",
      "Iteration 72, loss = 0.37847747\n",
      "Iteration 73, loss = 0.37660941\n",
      "Iteration 74, loss = 0.37475894\n",
      "Iteration 75, loss = 0.37298957\n",
      "Iteration 76, loss = 0.37123019\n",
      "Iteration 77, loss = 0.36951595\n",
      "Iteration 78, loss = 0.36781622\n",
      "Iteration 79, loss = 0.36623829\n",
      "Iteration 80, loss = 0.36462714\n",
      "Iteration 81, loss = 0.36306474\n",
      "Iteration 82, loss = 0.36157369\n",
      "Iteration 83, loss = 0.36008021\n",
      "Iteration 84, loss = 0.35864725\n",
      "Iteration 85, loss = 0.35720604\n",
      "Iteration 86, loss = 0.35578270\n",
      "Iteration 87, loss = 0.35447601\n",
      "Iteration 88, loss = 0.35312607\n",
      "Iteration 89, loss = 0.35179810\n",
      "Iteration 90, loss = 0.35053619\n",
      "Iteration 91, loss = 0.34925794\n",
      "Iteration 92, loss = 0.34800581\n",
      "Iteration 93, loss = 0.34681189\n",
      "Iteration 94, loss = 0.34562065\n",
      "Iteration 95, loss = 0.34448627\n",
      "Iteration 96, loss = 0.34331642\n",
      "Iteration 97, loss = 0.34219006\n",
      "Iteration 98, loss = 0.34108695\n",
      "Iteration 99, loss = 0.34002105\n",
      "Iteration 100, loss = 0.33893056\n",
      "Iteration 101, loss = 0.33788586\n",
      "Iteration 102, loss = 0.33689370\n",
      "Iteration 103, loss = 0.33585411\n",
      "Iteration 104, loss = 0.33483716\n",
      "Iteration 105, loss = 0.33383733\n",
      "Iteration 106, loss = 0.33288621\n",
      "Iteration 107, loss = 0.33196655\n",
      "Iteration 108, loss = 0.33100953\n",
      "Iteration 109, loss = 0.33007368\n",
      "Iteration 110, loss = 0.32917861\n",
      "Iteration 111, loss = 0.32824244\n",
      "Iteration 112, loss = 0.32736375\n",
      "Iteration 113, loss = 0.32650393\n",
      "Iteration 114, loss = 0.32567224\n",
      "Iteration 115, loss = 0.32478482\n",
      "Iteration 116, loss = 0.32394014\n",
      "Iteration 117, loss = 0.32314116\n",
      "Iteration 118, loss = 0.32231740\n",
      "Iteration 119, loss = 0.32153889\n",
      "Iteration 120, loss = 0.32071431\n",
      "Iteration 121, loss = 0.31993938\n",
      "Iteration 122, loss = 0.31917331\n",
      "Iteration 123, loss = 0.31840318\n",
      "Iteration 124, loss = 0.31762274\n",
      "Iteration 125, loss = 0.31688513\n",
      "Iteration 126, loss = 0.31617695\n",
      "Iteration 127, loss = 0.31541941\n",
      "Iteration 128, loss = 0.31473929\n",
      "Iteration 129, loss = 0.31398250\n",
      "Iteration 130, loss = 0.31329417\n",
      "Iteration 131, loss = 0.31260216\n",
      "Iteration 132, loss = 0.31191215\n",
      "Iteration 133, loss = 0.31122568\n",
      "Iteration 134, loss = 0.31053301\n",
      "Iteration 135, loss = 0.30987331\n",
      "Iteration 136, loss = 0.30922491\n",
      "Iteration 137, loss = 0.30859339\n",
      "Iteration 138, loss = 0.30792632\n",
      "Iteration 139, loss = 0.30728517\n",
      "Iteration 140, loss = 0.30666441\n",
      "Iteration 141, loss = 0.30600149\n",
      "Iteration 142, loss = 0.30540576\n",
      "Iteration 143, loss = 0.30479554\n",
      "Iteration 144, loss = 0.30418758\n",
      "Iteration 145, loss = 0.30356287\n",
      "Iteration 146, loss = 0.30297773\n",
      "Iteration 147, loss = 0.30239764\n",
      "Iteration 148, loss = 0.30182655\n",
      "Iteration 149, loss = 0.30118716\n",
      "Iteration 150, loss = 0.30063335\n",
      "Iteration 151, loss = 0.30007819\n",
      "Iteration 152, loss = 0.29950282\n",
      "Iteration 153, loss = 0.29896180\n",
      "Iteration 154, loss = 0.29840629\n",
      "Iteration 155, loss = 0.29781213\n",
      "Iteration 156, loss = 0.29727871\n",
      "Iteration 157, loss = 0.29672873\n",
      "Iteration 158, loss = 0.29620165\n",
      "Iteration 159, loss = 0.29565896\n",
      "Iteration 160, loss = 0.29514052\n",
      "Iteration 161, loss = 0.29460650\n",
      "Iteration 162, loss = 0.29403883\n",
      "Iteration 163, loss = 0.29355370\n",
      "Iteration 164, loss = 0.29303072\n",
      "Iteration 165, loss = 0.29251050\n",
      "Iteration 166, loss = 0.29201101\n",
      "Iteration 167, loss = 0.29151449\n",
      "Iteration 168, loss = 0.29099748\n",
      "Iteration 169, loss = 0.29052038\n",
      "Iteration 170, loss = 0.29000976\n",
      "Iteration 171, loss = 0.28953332\n",
      "Iteration 172, loss = 0.28905922\n",
      "Iteration 173, loss = 0.28857245\n",
      "Iteration 174, loss = 0.28806844\n",
      "Iteration 175, loss = 0.28759365\n",
      "Iteration 176, loss = 0.28711450\n",
      "Iteration 177, loss = 0.28664379\n",
      "Iteration 178, loss = 0.28620068\n",
      "Iteration 179, loss = 0.28573413\n",
      "Iteration 180, loss = 0.28524944\n",
      "Iteration 181, loss = 0.28479269\n",
      "Iteration 182, loss = 0.28433277\n",
      "Iteration 183, loss = 0.28389201\n",
      "Iteration 184, loss = 0.28342990\n",
      "Iteration 185, loss = 0.28298381\n",
      "Iteration 186, loss = 0.28253324\n",
      "Iteration 187, loss = 0.28208370\n",
      "Iteration 188, loss = 0.28166177\n",
      "Iteration 189, loss = 0.28124119\n",
      "Iteration 190, loss = 0.28076896\n",
      "Iteration 191, loss = 0.28034385\n",
      "Iteration 192, loss = 0.27989517\n",
      "Iteration 193, loss = 0.27949236\n",
      "Iteration 194, loss = 0.27907210\n",
      "Iteration 195, loss = 0.27862682\n",
      "Iteration 196, loss = 0.27820532\n",
      "Iteration 197, loss = 0.27777922\n",
      "Iteration 198, loss = 0.27736337\n",
      "Iteration 199, loss = 0.27696206\n",
      "Iteration 200, loss = 0.27653076\n",
      "[CV] END activation=logistic, alpha=0.01, batch_size=200, hidden_layer_sizes=(100,), solver=sgd; total time= 1.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.16397224\n",
      "Iteration 2, loss = 0.40224504\n",
      "Iteration 3, loss = 0.30669229\n",
      "Iteration 4, loss = 0.26237083\n",
      "Iteration 5, loss = 0.23231602\n",
      "Iteration 6, loss = 0.20816357\n",
      "Iteration 7, loss = 0.19073038\n",
      "Iteration 8, loss = 0.17457871\n",
      "Iteration 9, loss = 0.16144518\n",
      "Iteration 10, loss = 0.14988252\n",
      "Iteration 11, loss = 0.13980780\n",
      "Iteration 12, loss = 0.13286905\n",
      "Iteration 13, loss = 0.12517034\n",
      "Iteration 14, loss = 0.11830992\n",
      "Iteration 15, loss = 0.11394872\n",
      "Iteration 16, loss = 0.10947768\n",
      "Iteration 17, loss = 0.10513149\n",
      "Iteration 18, loss = 0.10058495\n",
      "Iteration 19, loss = 0.09698691\n",
      "Iteration 20, loss = 0.09414250\n",
      "Iteration 21, loss = 0.09226444\n",
      "Iteration 22, loss = 0.08878611\n",
      "Iteration 23, loss = 0.08609292\n",
      "Iteration 24, loss = 0.08441749\n",
      "Iteration 25, loss = 0.08265527\n",
      "Iteration 26, loss = 0.08140234\n",
      "Iteration 27, loss = 0.07959498\n",
      "Iteration 28, loss = 0.07748282\n",
      "Iteration 29, loss = 0.07636483\n",
      "Iteration 30, loss = 0.07424846\n",
      "Iteration 31, loss = 0.07429484\n",
      "Iteration 32, loss = 0.07443859\n",
      "Iteration 33, loss = 0.07293160\n",
      "Iteration 34, loss = 0.07188478\n",
      "Iteration 35, loss = 0.06972101\n",
      "Iteration 36, loss = 0.06932992\n",
      "Iteration 37, loss = 0.06883260\n",
      "Iteration 38, loss = 0.06788333\n",
      "Iteration 39, loss = 0.06777323\n",
      "Iteration 40, loss = 0.06763490\n",
      "Iteration 41, loss = 0.06548516\n",
      "Iteration 42, loss = 0.06745468\n",
      "Iteration 43, loss = 0.06692838\n",
      "Iteration 44, loss = 0.06558465\n",
      "Iteration 45, loss = 0.06448077\n",
      "Iteration 46, loss = 0.06299074\n",
      "Iteration 47, loss = 0.06492564\n",
      "Iteration 48, loss = 0.06295703\n",
      "Iteration 49, loss = 0.06282127\n",
      "Iteration 50, loss = 0.06697048\n",
      "Iteration 51, loss = 0.06157988\n",
      "Iteration 52, loss = 0.06237642\n",
      "Iteration 53, loss = 0.06156671\n",
      "Iteration 54, loss = 0.05987187\n",
      "Iteration 55, loss = 0.06056152\n",
      "Iteration 56, loss = 0.06290326\n",
      "Iteration 57, loss = 0.06170415\n",
      "Iteration 58, loss = 0.06104474\n",
      "Iteration 59, loss = 0.05936217\n",
      "Iteration 60, loss = 0.05981730\n",
      "Iteration 61, loss = 0.06224567\n",
      "Iteration 62, loss = 0.05983074\n",
      "Iteration 63, loss = 0.06171412\n",
      "Iteration 64, loss = 0.05893092\n",
      "Iteration 65, loss = 0.05919393\n",
      "Iteration 66, loss = 0.05938030\n",
      "Iteration 67, loss = 0.05849840\n",
      "Iteration 68, loss = 0.05820525\n",
      "Iteration 69, loss = 0.06115531\n",
      "Iteration 70, loss = 0.05976839\n",
      "Iteration 71, loss = 0.05806146\n",
      "Iteration 72, loss = 0.05736290\n",
      "Iteration 73, loss = 0.05957745\n",
      "Iteration 74, loss = 0.05899510\n",
      "Iteration 75, loss = 0.05974492\n",
      "Iteration 76, loss = 0.05682093\n",
      "Iteration 77, loss = 0.05726403\n",
      "Iteration 78, loss = 0.05534858\n",
      "Iteration 79, loss = 0.05590613\n",
      "Iteration 80, loss = 0.05856070\n",
      "Iteration 81, loss = 0.05718052\n",
      "Iteration 82, loss = 0.05680116\n",
      "Iteration 83, loss = 0.05519668\n",
      "Iteration 84, loss = 0.05505077\n",
      "Iteration 85, loss = 0.05561762\n",
      "Iteration 86, loss = 0.05742980\n",
      "Iteration 87, loss = 0.05644608\n",
      "Iteration 88, loss = 0.05510732\n",
      "Iteration 89, loss = 0.05560550\n",
      "Iteration 90, loss = 0.05605092\n",
      "Iteration 91, loss = 0.05639228\n",
      "Iteration 92, loss = 0.05667427\n",
      "Iteration 93, loss = 0.05484711\n",
      "Iteration 94, loss = 0.05374510\n",
      "Iteration 95, loss = 0.05502562\n",
      "Iteration 96, loss = 0.05557317\n",
      "Iteration 97, loss = 0.05637340\n",
      "Iteration 98, loss = 0.05555217\n",
      "Iteration 99, loss = 0.05438677\n",
      "Iteration 100, loss = 0.05410308\n",
      "Iteration 101, loss = 0.05358332\n",
      "Iteration 102, loss = 0.05355029\n",
      "Iteration 103, loss = 0.05616604\n",
      "Iteration 104, loss = 0.05689659\n",
      "Iteration 105, loss = 0.05458167\n",
      "Iteration 106, loss = 0.05358735\n",
      "Iteration 107, loss = 0.05339168\n",
      "Iteration 108, loss = 0.05373967\n",
      "Iteration 109, loss = 0.05535215\n",
      "Iteration 110, loss = 0.06018422\n",
      "Iteration 111, loss = 0.05270177\n",
      "Iteration 112, loss = 0.05280267\n",
      "Iteration 113, loss = 0.05349836\n",
      "Iteration 114, loss = 0.05302234\n",
      "Iteration 115, loss = 0.05294314\n",
      "Iteration 116, loss = 0.05367118\n",
      "Iteration 117, loss = 0.05526064\n",
      "Iteration 118, loss = 0.05554201\n",
      "Iteration 119, loss = 0.05215701\n",
      "Iteration 120, loss = 0.05214868\n",
      "Iteration 121, loss = 0.05252944\n",
      "Iteration 122, loss = 0.05506934\n",
      "Iteration 123, loss = 0.05490137\n",
      "Iteration 124, loss = 0.05230488\n",
      "Iteration 125, loss = 0.05253419\n",
      "Iteration 126, loss = 0.05214617\n",
      "Iteration 127, loss = 0.05200396\n",
      "Iteration 128, loss = 0.05175490\n",
      "Iteration 129, loss = 0.05189933\n",
      "Iteration 130, loss = 0.05806756\n",
      "Iteration 131, loss = 0.05606117\n",
      "Iteration 132, loss = 0.05323832\n",
      "Iteration 133, loss = 0.05198153\n",
      "Iteration 134, loss = 0.05083814\n",
      "Iteration 135, loss = 0.05206892\n",
      "Iteration 136, loss = 0.05175570\n",
      "Iteration 137, loss = 0.05126182\n",
      "Iteration 138, loss = 0.05376997\n",
      "Iteration 139, loss = 0.05225468\n",
      "Iteration 140, loss = 0.05089489\n",
      "Iteration 141, loss = 0.05267073\n",
      "Iteration 142, loss = 0.05251167\n",
      "Iteration 143, loss = 0.05151242\n",
      "Iteration 144, loss = 0.05366574\n",
      "Iteration 145, loss = 0.05186307\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.01, batch_size=200, hidden_layer_sizes=(400, 100), solver=adam; total time= 5.0min\n",
      "Iteration 1, loss = 1.12466314\n",
      "Iteration 2, loss = 0.38883331\n",
      "Iteration 3, loss = 0.29996603\n",
      "Iteration 4, loss = 0.25794224\n",
      "Iteration 5, loss = 0.22866387\n",
      "Iteration 6, loss = 0.20727749\n",
      "Iteration 7, loss = 0.19089970\n",
      "Iteration 8, loss = 0.17424754\n",
      "Iteration 9, loss = 0.16080017\n",
      "Iteration 10, loss = 0.15077294\n",
      "Iteration 11, loss = 0.14083742\n",
      "Iteration 12, loss = 0.13319199\n",
      "Iteration 13, loss = 0.12669302\n",
      "Iteration 14, loss = 0.12211352\n",
      "Iteration 15, loss = 0.11496426\n",
      "Iteration 16, loss = 0.11108435\n",
      "Iteration 17, loss = 0.10551246\n",
      "Iteration 18, loss = 0.10135026\n",
      "Iteration 19, loss = 0.09829511\n",
      "Iteration 20, loss = 0.09524794\n",
      "Iteration 21, loss = 0.09201116\n",
      "Iteration 22, loss = 0.09031282\n",
      "Iteration 23, loss = 0.08729536\n",
      "Iteration 24, loss = 0.08469080\n",
      "Iteration 25, loss = 0.08390992\n",
      "Iteration 26, loss = 0.08316691\n",
      "Iteration 27, loss = 0.08124112\n",
      "Iteration 28, loss = 0.07934821\n",
      "Iteration 29, loss = 0.07763529\n",
      "Iteration 30, loss = 0.07679748\n",
      "Iteration 31, loss = 0.07427600\n",
      "Iteration 32, loss = 0.07296110\n",
      "Iteration 33, loss = 0.07243694\n",
      "Iteration 34, loss = 0.07236700\n",
      "Iteration 35, loss = 0.07094540\n",
      "Iteration 36, loss = 0.07001780\n",
      "Iteration 37, loss = 0.07281800\n",
      "Iteration 38, loss = 0.07005632\n",
      "Iteration 39, loss = 0.06827679\n",
      "Iteration 40, loss = 0.06604926\n",
      "Iteration 41, loss = 0.06706680\n",
      "Iteration 42, loss = 0.06574973\n",
      "Iteration 43, loss = 0.06627289\n",
      "Iteration 44, loss = 0.06706987\n",
      "Iteration 45, loss = 0.06533159\n",
      "Iteration 46, loss = 0.06364871\n",
      "Iteration 47, loss = 0.06405577\n",
      "Iteration 48, loss = 0.06338131\n",
      "Iteration 49, loss = 0.06373422\n",
      "Iteration 50, loss = 0.06290696\n",
      "Iteration 51, loss = 0.06397620\n",
      "Iteration 52, loss = 0.06161349\n",
      "Iteration 53, loss = 0.06317072\n",
      "Iteration 54, loss = 0.06391488\n",
      "Iteration 55, loss = 0.06251490\n",
      "Iteration 56, loss = 0.06080896\n",
      "Iteration 57, loss = 0.06062838\n",
      "Iteration 58, loss = 0.06086561\n",
      "Iteration 59, loss = 0.06028934\n",
      "Iteration 60, loss = 0.06021021\n",
      "Iteration 61, loss = 0.06201323\n",
      "Iteration 62, loss = 0.06086899\n",
      "Iteration 63, loss = 0.06108663\n",
      "Iteration 64, loss = 0.06203759\n",
      "Iteration 65, loss = 0.06146320\n",
      "Iteration 66, loss = 0.05985829\n",
      "Iteration 67, loss = 0.05754708\n",
      "Iteration 68, loss = 0.06062073\n",
      "Iteration 69, loss = 0.05890841\n",
      "Iteration 70, loss = 0.05816309\n",
      "Iteration 71, loss = 0.05874663\n",
      "Iteration 72, loss = 0.05812275\n",
      "Iteration 73, loss = 0.05855408\n",
      "Iteration 74, loss = 0.05806327\n",
      "Iteration 75, loss = 0.06243480\n",
      "Iteration 76, loss = 0.05848662\n",
      "Iteration 77, loss = 0.05634610\n",
      "Iteration 78, loss = 0.05624029\n",
      "Iteration 79, loss = 0.05544819\n",
      "Iteration 80, loss = 0.06179825\n",
      "Iteration 81, loss = 0.05945312\n",
      "Iteration 82, loss = 0.05675945\n",
      "Iteration 83, loss = 0.05759357\n",
      "Iteration 84, loss = 0.05644820\n",
      "Iteration 85, loss = 0.05574314\n",
      "Iteration 86, loss = 0.05774518\n",
      "Iteration 87, loss = 0.05693805\n",
      "Iteration 88, loss = 0.05743493\n",
      "Iteration 89, loss = 0.05811376\n",
      "Iteration 90, loss = 0.06124701\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.01, batch_size=200, hidden_layer_sizes=(400, 100), solver=adam; total time= 3.0min\n",
      "Iteration 1, loss = 1.16021202\n",
      "Iteration 2, loss = 0.39640148\n",
      "Iteration 3, loss = 0.30447253\n",
      "Iteration 4, loss = 0.26153512\n",
      "Iteration 5, loss = 0.23223506\n",
      "Iteration 6, loss = 0.20953588\n",
      "Iteration 7, loss = 0.18911319\n",
      "Iteration 8, loss = 0.17409847\n",
      "Iteration 9, loss = 0.16169295\n",
      "Iteration 10, loss = 0.14937455\n",
      "Iteration 11, loss = 0.14086758\n",
      "Iteration 12, loss = 0.13384290\n",
      "Iteration 13, loss = 0.12505082\n",
      "Iteration 14, loss = 0.11852670\n",
      "Iteration 15, loss = 0.11283971\n",
      "Iteration 16, loss = 0.10962978\n",
      "Iteration 17, loss = 0.10449703\n",
      "Iteration 18, loss = 0.10132159\n",
      "Iteration 19, loss = 0.09666092\n",
      "Iteration 20, loss = 0.09497009\n",
      "Iteration 21, loss = 0.09130513\n",
      "Iteration 22, loss = 0.08877487\n",
      "Iteration 23, loss = 0.08695356\n",
      "Iteration 24, loss = 0.08465793\n",
      "Iteration 25, loss = 0.08284762\n",
      "Iteration 26, loss = 0.08211384\n",
      "Iteration 27, loss = 0.07873749\n",
      "Iteration 28, loss = 0.07724229\n",
      "Iteration 29, loss = 0.07603409\n",
      "Iteration 30, loss = 0.07677050\n",
      "Iteration 31, loss = 0.07516010\n",
      "Iteration 32, loss = 0.07572041\n",
      "Iteration 33, loss = 0.07502938\n",
      "Iteration 34, loss = 0.07116526\n",
      "Iteration 35, loss = 0.06931244\n",
      "Iteration 36, loss = 0.06877485\n",
      "Iteration 37, loss = 0.06930934\n",
      "Iteration 38, loss = 0.06766687\n",
      "Iteration 39, loss = 0.06617533\n",
      "Iteration 40, loss = 0.06708742\n",
      "Iteration 41, loss = 0.06576590\n",
      "Iteration 42, loss = 0.06577140\n",
      "Iteration 43, loss = 0.06515931\n",
      "Iteration 44, loss = 0.06531331\n",
      "Iteration 45, loss = 0.06411257\n",
      "Iteration 46, loss = 0.06740831\n",
      "Iteration 47, loss = 0.06352583\n",
      "Iteration 48, loss = 0.06313545\n",
      "Iteration 49, loss = 0.06372457\n",
      "Iteration 50, loss = 0.06293398\n",
      "Iteration 51, loss = 0.06171626\n",
      "Iteration 52, loss = 0.06129620\n",
      "Iteration 53, loss = 0.06094211\n",
      "Iteration 54, loss = 0.06284763\n",
      "Iteration 55, loss = 0.05995587\n",
      "Iteration 56, loss = 0.06164646\n",
      "Iteration 57, loss = 0.06062273\n",
      "Iteration 58, loss = 0.06028705\n",
      "Iteration 59, loss = 0.05994649\n",
      "Iteration 60, loss = 0.06055983\n",
      "Iteration 61, loss = 0.05829640\n",
      "Iteration 62, loss = 0.06206400\n",
      "Iteration 63, loss = 0.05927448\n",
      "Iteration 64, loss = 0.06024856\n",
      "Iteration 65, loss = 0.05847985\n",
      "Iteration 66, loss = 0.05832723\n",
      "Iteration 67, loss = 0.06271721\n",
      "Iteration 68, loss = 0.05798051\n",
      "Iteration 69, loss = 0.05736044\n",
      "Iteration 70, loss = 0.05687200\n",
      "Iteration 71, loss = 0.05642708\n",
      "Iteration 72, loss = 0.05827528\n",
      "Iteration 73, loss = 0.05830213\n",
      "Iteration 74, loss = 0.05926741\n",
      "Iteration 75, loss = 0.05820722\n",
      "Iteration 76, loss = 0.05776727\n",
      "Iteration 77, loss = 0.05645089\n",
      "Iteration 78, loss = 0.05703666\n",
      "Iteration 79, loss = 0.05659309\n",
      "Iteration 80, loss = 0.05426405\n",
      "Iteration 81, loss = 0.05737757\n",
      "Iteration 82, loss = 0.05759061\n",
      "Iteration 83, loss = 0.05515881\n",
      "Iteration 84, loss = 0.05540107\n",
      "Iteration 85, loss = 0.05656900\n",
      "Iteration 86, loss = 0.05733115\n",
      "Iteration 87, loss = 0.05522230\n",
      "Iteration 88, loss = 0.05532856\n",
      "Iteration 89, loss = 0.05531405\n",
      "Iteration 90, loss = 0.05555044\n",
      "Iteration 91, loss = 0.05419721\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.01, batch_size=200, hidden_layer_sizes=(400, 100), solver=adam; total time= 3.1min\n",
      "Iteration 1, loss = 1.16190619\n",
      "Iteration 2, loss = 0.38984792\n",
      "Iteration 3, loss = 0.30255898\n",
      "Iteration 4, loss = 0.25941517\n",
      "Iteration 5, loss = 0.22874678\n",
      "Iteration 6, loss = 0.20699148\n",
      "Iteration 7, loss = 0.18995178\n",
      "Iteration 8, loss = 0.17304358\n",
      "Iteration 9, loss = 0.16111738\n",
      "Iteration 10, loss = 0.14848253\n",
      "Iteration 11, loss = 0.13990492\n",
      "Iteration 12, loss = 0.13168923\n",
      "Iteration 13, loss = 0.12535872\n",
      "Iteration 14, loss = 0.11777170\n",
      "Iteration 15, loss = 0.11321347\n",
      "Iteration 16, loss = 0.10800712\n",
      "Iteration 17, loss = 0.10258764\n",
      "Iteration 18, loss = 0.10061026\n",
      "Iteration 19, loss = 0.09598541\n",
      "Iteration 20, loss = 0.09291298\n",
      "Iteration 21, loss = 0.09008717\n",
      "Iteration 22, loss = 0.08770713\n",
      "Iteration 23, loss = 0.08589320\n",
      "Iteration 24, loss = 0.08339564\n",
      "Iteration 25, loss = 0.08109167\n",
      "Iteration 26, loss = 0.08052995\n",
      "Iteration 27, loss = 0.07881970\n",
      "Iteration 28, loss = 0.07678531\n",
      "Iteration 29, loss = 0.07551407\n",
      "Iteration 30, loss = 0.07483385\n",
      "Iteration 31, loss = 0.07489354\n",
      "Iteration 32, loss = 0.07258092\n",
      "Iteration 33, loss = 0.07162830\n",
      "Iteration 34, loss = 0.07052905\n",
      "Iteration 35, loss = 0.07040374\n",
      "Iteration 36, loss = 0.06826055\n",
      "Iteration 37, loss = 0.06767589\n",
      "Iteration 38, loss = 0.06947072\n",
      "Iteration 39, loss = 0.06702688\n",
      "Iteration 40, loss = 0.06635095\n",
      "Iteration 41, loss = 0.06737535\n",
      "Iteration 42, loss = 0.06658415\n",
      "Iteration 43, loss = 0.06548923\n",
      "Iteration 44, loss = 0.06522435\n",
      "Iteration 45, loss = 0.06400619\n",
      "Iteration 46, loss = 0.06205000\n",
      "Iteration 47, loss = 0.06286543\n",
      "Iteration 48, loss = 0.06238890\n",
      "Iteration 49, loss = 0.06267424\n",
      "Iteration 50, loss = 0.06318244\n",
      "Iteration 51, loss = 0.06329371\n",
      "Iteration 52, loss = 0.06289601\n",
      "Iteration 53, loss = 0.06150294\n",
      "Iteration 54, loss = 0.06216710\n",
      "Iteration 55, loss = 0.06262078\n",
      "Iteration 56, loss = 0.06099221\n",
      "Iteration 57, loss = 0.05960289\n",
      "Iteration 58, loss = 0.06055173\n",
      "Iteration 59, loss = 0.05954686\n",
      "Iteration 60, loss = 0.06003987\n",
      "Iteration 61, loss = 0.06145899\n",
      "Iteration 62, loss = 0.06127338\n",
      "Iteration 63, loss = 0.05955165\n",
      "Iteration 64, loss = 0.05710416\n",
      "Iteration 65, loss = 0.05852582\n",
      "Iteration 66, loss = 0.05936921\n",
      "Iteration 67, loss = 0.05951097\n",
      "Iteration 68, loss = 0.05979030\n",
      "Iteration 69, loss = 0.05681007\n",
      "Iteration 70, loss = 0.05894514\n",
      "Iteration 71, loss = 0.05872989\n",
      "Iteration 72, loss = 0.06001867\n",
      "Iteration 73, loss = 0.05658769\n",
      "Iteration 74, loss = 0.05630843\n",
      "Iteration 75, loss = 0.05707539\n",
      "Iteration 76, loss = 0.05673304\n",
      "Iteration 77, loss = 0.05757085\n",
      "Iteration 78, loss = 0.05629070\n",
      "Iteration 79, loss = 0.05646531\n",
      "Iteration 80, loss = 0.05592354\n",
      "Iteration 81, loss = 0.05783378\n",
      "Iteration 82, loss = 0.05731991\n",
      "Iteration 83, loss = 0.05637562\n",
      "Iteration 84, loss = 0.05637879\n",
      "Iteration 85, loss = 0.05627662\n",
      "Iteration 86, loss = 0.05537137\n",
      "Iteration 87, loss = 0.05565954\n",
      "Iteration 88, loss = 0.06047103\n",
      "Iteration 89, loss = 0.05811096\n",
      "Iteration 90, loss = 0.05517067\n",
      "Iteration 91, loss = 0.05380030\n",
      "Iteration 92, loss = 0.05455904\n",
      "Iteration 93, loss = 0.05554794\n",
      "Iteration 94, loss = 0.05797768\n",
      "Iteration 95, loss = 0.05683602\n",
      "Iteration 96, loss = 0.05499038\n",
      "Iteration 97, loss = 0.05436780\n",
      "Iteration 98, loss = 0.05489907\n",
      "Iteration 99, loss = 0.05569594\n",
      "Iteration 100, loss = 0.05499156\n",
      "Iteration 101, loss = 0.05395496\n",
      "Iteration 102, loss = 0.05362766\n",
      "Iteration 103, loss = 0.05443908\n",
      "Iteration 104, loss = 0.05487282\n",
      "Iteration 105, loss = 0.05328503\n",
      "Iteration 106, loss = 0.05459329\n",
      "Iteration 107, loss = 0.05656381\n",
      "Iteration 108, loss = 0.05466128\n",
      "Iteration 109, loss = 0.05404407\n",
      "Iteration 110, loss = 0.05468073\n",
      "Iteration 111, loss = 0.05550436\n",
      "Iteration 112, loss = 0.05475277\n",
      "Iteration 113, loss = 0.05333008\n",
      "Iteration 114, loss = 0.05361115\n",
      "Iteration 115, loss = 0.05324401\n",
      "Iteration 116, loss = 0.05574593\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.01, batch_size=200, hidden_layer_sizes=(400, 100), solver=adam; total time= 4.0min\n",
      "Iteration 1, loss = 1.14408779\n",
      "Iteration 2, loss = 0.39303742\n",
      "Iteration 3, loss = 0.30389733\n",
      "Iteration 4, loss = 0.26119153\n",
      "Iteration 5, loss = 0.23186334\n",
      "Iteration 6, loss = 0.20935523\n",
      "Iteration 7, loss = 0.19142318\n",
      "Iteration 8, loss = 0.17577545\n",
      "Iteration 9, loss = 0.16149931\n",
      "Iteration 10, loss = 0.15027451\n",
      "Iteration 11, loss = 0.14091408\n",
      "Iteration 12, loss = 0.13209279\n",
      "Iteration 13, loss = 0.12547164\n",
      "Iteration 14, loss = 0.12069828\n",
      "Iteration 15, loss = 0.11337780\n",
      "Iteration 16, loss = 0.10870785\n",
      "Iteration 17, loss = 0.10570932\n",
      "Iteration 18, loss = 0.09981664\n",
      "Iteration 19, loss = 0.09766091\n",
      "Iteration 20, loss = 0.09423674\n",
      "Iteration 21, loss = 0.09193651\n",
      "Iteration 22, loss = 0.08876794\n",
      "Iteration 23, loss = 0.08618191\n",
      "Iteration 24, loss = 0.08445476\n",
      "Iteration 25, loss = 0.08301403\n",
      "Iteration 26, loss = 0.08079904\n",
      "Iteration 27, loss = 0.07925681\n",
      "Iteration 28, loss = 0.07689973\n",
      "Iteration 29, loss = 0.07584324\n",
      "Iteration 30, loss = 0.07441412\n",
      "Iteration 31, loss = 0.07554228\n",
      "Iteration 32, loss = 0.07331791\n",
      "Iteration 33, loss = 0.07173567\n",
      "Iteration 34, loss = 0.06978501\n",
      "Iteration 35, loss = 0.07111764\n",
      "Iteration 36, loss = 0.07082509\n",
      "Iteration 37, loss = 0.07010290\n",
      "Iteration 38, loss = 0.06868962\n",
      "Iteration 39, loss = 0.06875973\n",
      "Iteration 40, loss = 0.06676131\n",
      "Iteration 41, loss = 0.06867342\n",
      "Iteration 42, loss = 0.06555517\n",
      "Iteration 43, loss = 0.06603271\n",
      "Iteration 44, loss = 0.06645205\n",
      "Iteration 45, loss = 0.06321934\n",
      "Iteration 46, loss = 0.06275345\n",
      "Iteration 47, loss = 0.06398799\n",
      "Iteration 48, loss = 0.06251721\n",
      "Iteration 49, loss = 0.06517355\n",
      "Iteration 50, loss = 0.06416197\n",
      "Iteration 51, loss = 0.06314439\n",
      "Iteration 52, loss = 0.06224731\n",
      "Iteration 53, loss = 0.06036778\n",
      "Iteration 54, loss = 0.05987683\n",
      "Iteration 55, loss = 0.06080690\n",
      "Iteration 56, loss = 0.06045616\n",
      "Iteration 57, loss = 0.06121003\n",
      "Iteration 58, loss = 0.06060940\n",
      "Iteration 59, loss = 0.05973603\n",
      "Iteration 60, loss = 0.06258456\n",
      "Iteration 61, loss = 0.06125774\n",
      "Iteration 62, loss = 0.06186145\n",
      "Iteration 63, loss = 0.05932529\n",
      "Iteration 64, loss = 0.05785748\n",
      "Iteration 65, loss = 0.05830726\n",
      "Iteration 66, loss = 0.05755563\n",
      "Iteration 67, loss = 0.06096114\n",
      "Iteration 68, loss = 0.05747035\n",
      "Iteration 69, loss = 0.05884562\n",
      "Iteration 70, loss = 0.06284579\n",
      "Iteration 71, loss = 0.05787571\n",
      "Iteration 72, loss = 0.05797962\n",
      "Iteration 73, loss = 0.05590477\n",
      "Iteration 74, loss = 0.05756658\n",
      "Iteration 75, loss = 0.05923685\n",
      "Iteration 76, loss = 0.05755556\n",
      "Iteration 77, loss = 0.05614746\n",
      "Iteration 78, loss = 0.05680678\n",
      "Iteration 79, loss = 0.05721500\n",
      "Iteration 80, loss = 0.05652960\n",
      "Iteration 81, loss = 0.05858873\n",
      "Iteration 82, loss = 0.06012554\n",
      "Iteration 83, loss = 0.05632535\n",
      "Iteration 84, loss = 0.05577298\n",
      "Iteration 85, loss = 0.05563993\n",
      "Iteration 86, loss = 0.05641133\n",
      "Iteration 87, loss = 0.06056188\n",
      "Iteration 88, loss = 0.05695852\n",
      "Iteration 89, loss = 0.05528575\n",
      "Iteration 90, loss = 0.05387495\n",
      "Iteration 91, loss = 0.05487882\n",
      "Iteration 92, loss = 0.05483016\n",
      "Iteration 93, loss = 0.05956371\n",
      "Iteration 94, loss = 0.05690161\n",
      "Iteration 95, loss = 0.05533700\n",
      "Iteration 96, loss = 0.05378212\n",
      "Iteration 97, loss = 0.05337281\n",
      "Iteration 98, loss = 0.05562981\n",
      "Iteration 99, loss = 0.05526717\n",
      "Iteration 100, loss = 0.05686721\n",
      "Iteration 101, loss = 0.05546541\n",
      "Iteration 102, loss = 0.05653157\n",
      "Iteration 103, loss = 0.05404271\n",
      "Iteration 104, loss = 0.05371590\n",
      "Iteration 105, loss = 0.05410242\n",
      "Iteration 106, loss = 0.05349344\n",
      "Iteration 107, loss = 0.05780499\n",
      "Iteration 108, loss = 0.05461583\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.01, batch_size=200, hidden_layer_sizes=(400, 100), solver=adam; total time= 3.7min\n",
      "Iteration 1, loss = 2.31718550\n",
      "Iteration 2, loss = 2.30338812\n",
      "Iteration 3, loss = 2.30008894\n",
      "Iteration 4, loss = 2.29682952\n",
      "Iteration 5, loss = 2.29334137\n",
      "Iteration 6, loss = 2.28987105\n",
      "Iteration 7, loss = 2.28606329\n",
      "Iteration 8, loss = 2.28194990\n",
      "Iteration 9, loss = 2.27746945\n",
      "Iteration 10, loss = 2.27259897\n",
      "Iteration 11, loss = 2.26705339\n",
      "Iteration 12, loss = 2.26114677\n",
      "Iteration 13, loss = 2.25429397\n",
      "Iteration 14, loss = 2.24655077\n",
      "Iteration 15, loss = 2.23779517\n",
      "Iteration 16, loss = 2.22743801\n",
      "Iteration 17, loss = 2.21556284\n",
      "Iteration 18, loss = 2.20167398\n",
      "Iteration 19, loss = 2.18526598\n",
      "Iteration 20, loss = 2.16586858\n",
      "Iteration 21, loss = 2.14300925\n",
      "Iteration 22, loss = 2.11597226\n",
      "Iteration 23, loss = 2.08419696\n",
      "Iteration 24, loss = 2.04731606\n",
      "Iteration 25, loss = 2.00484440\n",
      "Iteration 26, loss = 1.95705982\n",
      "Iteration 27, loss = 1.90416029\n",
      "Iteration 28, loss = 1.84736175\n",
      "Iteration 29, loss = 1.78767416\n",
      "Iteration 30, loss = 1.72635765\n",
      "Iteration 31, loss = 1.66485358\n",
      "Iteration 32, loss = 1.60437716\n",
      "Iteration 33, loss = 1.54570925\n",
      "Iteration 34, loss = 1.48968930\n",
      "Iteration 35, loss = 1.43668977\n",
      "Iteration 36, loss = 1.38668002\n",
      "Iteration 37, loss = 1.33971486\n",
      "Iteration 38, loss = 1.29550954\n",
      "Iteration 39, loss = 1.25389567\n",
      "Iteration 40, loss = 1.21445761\n",
      "Iteration 41, loss = 1.17728456\n",
      "Iteration 42, loss = 1.14213571\n",
      "Iteration 43, loss = 1.10891281\n",
      "Iteration 44, loss = 1.07781712\n",
      "Iteration 45, loss = 1.04830043\n",
      "Iteration 46, loss = 1.02041124\n",
      "Iteration 47, loss = 0.99396651\n",
      "Iteration 48, loss = 0.96891440\n",
      "Iteration 49, loss = 0.94532522\n",
      "Iteration 50, loss = 0.92266736\n",
      "Iteration 51, loss = 0.90123046\n",
      "Iteration 52, loss = 0.88083045\n",
      "Iteration 53, loss = 0.86120365\n",
      "Iteration 54, loss = 0.84246337\n",
      "Iteration 55, loss = 0.82453707\n",
      "Iteration 56, loss = 0.80741689\n",
      "Iteration 57, loss = 0.79091247\n",
      "Iteration 58, loss = 0.77519515\n",
      "Iteration 59, loss = 0.75996072\n",
      "Iteration 60, loss = 0.74549625\n",
      "Iteration 61, loss = 0.73146554\n",
      "Iteration 62, loss = 0.71802107\n",
      "Iteration 63, loss = 0.70512128\n",
      "Iteration 64, loss = 0.69276852\n",
      "Iteration 65, loss = 0.68079620\n",
      "Iteration 66, loss = 0.66949443\n",
      "Iteration 67, loss = 0.65843772\n",
      "Iteration 68, loss = 0.64802113\n",
      "Iteration 69, loss = 0.63794279\n",
      "Iteration 70, loss = 0.62829546\n",
      "Iteration 71, loss = 0.61897909\n",
      "Iteration 72, loss = 0.61021558\n",
      "Iteration 73, loss = 0.60164778\n",
      "Iteration 74, loss = 0.59344818\n",
      "Iteration 75, loss = 0.58566450\n",
      "Iteration 76, loss = 0.57812688\n",
      "Iteration 77, loss = 0.57102563\n",
      "Iteration 78, loss = 0.56405447\n",
      "Iteration 79, loss = 0.55735637\n",
      "Iteration 80, loss = 0.55092942\n",
      "Iteration 81, loss = 0.54477228\n",
      "Iteration 82, loss = 0.53886367\n",
      "Iteration 83, loss = 0.53318586\n",
      "Iteration 84, loss = 0.52770783\n",
      "Iteration 85, loss = 0.52236504\n",
      "Iteration 86, loss = 0.51724335\n",
      "Iteration 87, loss = 0.51229879\n",
      "Iteration 88, loss = 0.50759541\n",
      "Iteration 89, loss = 0.50297693\n",
      "Iteration 90, loss = 0.49854126\n",
      "Iteration 91, loss = 0.49419089\n",
      "Iteration 92, loss = 0.49003308\n",
      "Iteration 93, loss = 0.48600925\n",
      "Iteration 94, loss = 0.48215919\n",
      "Iteration 95, loss = 0.47831829\n",
      "Iteration 96, loss = 0.47463253\n",
      "Iteration 97, loss = 0.47116790\n",
      "Iteration 98, loss = 0.46774705\n",
      "Iteration 99, loss = 0.46443553\n",
      "Iteration 100, loss = 0.46114827\n",
      "Iteration 101, loss = 0.45789253\n",
      "Iteration 102, loss = 0.45493198\n",
      "Iteration 103, loss = 0.45213049\n",
      "Iteration 104, loss = 0.44916880\n",
      "Iteration 105, loss = 0.44641290\n",
      "Iteration 106, loss = 0.44379889\n",
      "Iteration 107, loss = 0.44103463\n",
      "Iteration 108, loss = 0.43854581\n",
      "Iteration 109, loss = 0.43605710\n",
      "Iteration 110, loss = 0.43367299\n",
      "Iteration 111, loss = 0.43129970\n",
      "Iteration 112, loss = 0.42906924\n",
      "Iteration 113, loss = 0.42681115\n",
      "Iteration 114, loss = 0.42463583\n",
      "Iteration 115, loss = 0.42254685\n",
      "Iteration 116, loss = 0.42049670\n",
      "Iteration 117, loss = 0.41850615\n",
      "Iteration 118, loss = 0.41653849\n",
      "Iteration 119, loss = 0.41464614\n",
      "Iteration 120, loss = 0.41282375\n",
      "Iteration 121, loss = 0.41103589\n",
      "Iteration 122, loss = 0.40923113\n",
      "Iteration 123, loss = 0.40753146\n",
      "Iteration 124, loss = 0.40581871\n",
      "Iteration 125, loss = 0.40419046\n",
      "Iteration 126, loss = 0.40255302\n",
      "Iteration 127, loss = 0.40092930\n",
      "Iteration 128, loss = 0.39955845\n",
      "Iteration 129, loss = 0.39800833\n",
      "Iteration 130, loss = 0.39647302\n",
      "Iteration 131, loss = 0.39506882\n",
      "Iteration 132, loss = 0.39362947\n",
      "Iteration 133, loss = 0.39225164\n",
      "Iteration 134, loss = 0.39102485\n",
      "Iteration 135, loss = 0.38959703\n",
      "Iteration 136, loss = 0.38826542\n",
      "Iteration 137, loss = 0.38698283\n",
      "Iteration 138, loss = 0.38578165\n",
      "Iteration 139, loss = 0.38457869\n",
      "Iteration 140, loss = 0.38330536\n",
      "Iteration 141, loss = 0.38219057\n",
      "Iteration 142, loss = 0.38099461\n",
      "Iteration 143, loss = 0.37993264\n",
      "Iteration 144, loss = 0.37874392\n",
      "Iteration 145, loss = 0.37769319\n",
      "Iteration 146, loss = 0.37660198\n",
      "Iteration 147, loss = 0.37555382\n",
      "Iteration 148, loss = 0.37445954\n",
      "Iteration 149, loss = 0.37346857\n",
      "Iteration 150, loss = 0.37245338\n",
      "Iteration 151, loss = 0.37146897\n",
      "Iteration 152, loss = 0.37045653\n",
      "Iteration 153, loss = 0.36950787\n",
      "Iteration 154, loss = 0.36852741\n",
      "Iteration 155, loss = 0.36765808\n",
      "Iteration 156, loss = 0.36664913\n",
      "Iteration 157, loss = 0.36579415\n",
      "Iteration 158, loss = 0.36495866\n",
      "Iteration 159, loss = 0.36398551\n",
      "Iteration 160, loss = 0.36303030\n",
      "Iteration 161, loss = 0.36224980\n",
      "Iteration 162, loss = 0.36141049\n",
      "Iteration 163, loss = 0.36057287\n",
      "Iteration 164, loss = 0.35972473\n",
      "Iteration 165, loss = 0.35899362\n",
      "Iteration 166, loss = 0.35810312\n",
      "Iteration 167, loss = 0.35729758\n",
      "Iteration 168, loss = 0.35647542\n",
      "Iteration 169, loss = 0.35579483\n",
      "Iteration 170, loss = 0.35502505\n",
      "Iteration 171, loss = 0.35427267\n",
      "Iteration 172, loss = 0.35349021\n",
      "Iteration 173, loss = 0.35275219\n",
      "Iteration 174, loss = 0.35195314\n",
      "Iteration 175, loss = 0.35131459\n",
      "Iteration 176, loss = 0.35057472\n",
      "Iteration 177, loss = 0.34980822\n",
      "Iteration 178, loss = 0.34913702\n",
      "Iteration 179, loss = 0.34846505\n",
      "Iteration 180, loss = 0.34774535\n",
      "Iteration 181, loss = 0.34706612\n",
      "Iteration 182, loss = 0.34640488\n",
      "Iteration 183, loss = 0.34569069\n",
      "Iteration 184, loss = 0.34510646\n",
      "Iteration 185, loss = 0.34436812\n",
      "Iteration 186, loss = 0.34375903\n",
      "Iteration 187, loss = 0.34319286\n",
      "Iteration 188, loss = 0.34247245\n",
      "Iteration 189, loss = 0.34186100\n",
      "Iteration 190, loss = 0.34117344\n",
      "Iteration 191, loss = 0.34065295\n",
      "Iteration 192, loss = 0.33995523\n",
      "Iteration 193, loss = 0.33933238\n",
      "Iteration 194, loss = 0.33876326\n",
      "Iteration 195, loss = 0.33819067\n",
      "Iteration 196, loss = 0.33758079\n",
      "Iteration 197, loss = 0.33692309\n",
      "Iteration 198, loss = 0.33644507\n",
      "Iteration 199, loss = 0.33583122\n",
      "Iteration 200, loss = 0.33524714\n",
      "[CV] END activation=logistic, alpha=0.01, batch_size=200, hidden_layer_sizes=(400, 100), solver=sgd; total time= 5.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.31232014\n",
      "Iteration 2, loss = 2.30126871\n",
      "Iteration 3, loss = 2.29768174\n",
      "Iteration 4, loss = 2.29385817\n",
      "Iteration 5, loss = 2.28988869\n",
      "Iteration 6, loss = 2.28564791\n",
      "Iteration 7, loss = 2.28112417\n",
      "Iteration 8, loss = 2.27616966\n",
      "Iteration 9, loss = 2.27058515\n",
      "Iteration 10, loss = 2.26438611\n",
      "Iteration 11, loss = 2.25753717\n",
      "Iteration 12, loss = 2.24970664\n",
      "Iteration 13, loss = 2.24033437\n",
      "Iteration 14, loss = 2.22984700\n",
      "Iteration 15, loss = 2.21750472\n",
      "Iteration 16, loss = 2.20302475\n",
      "Iteration 17, loss = 2.18583264\n",
      "Iteration 18, loss = 2.16553779\n",
      "Iteration 19, loss = 2.14151716\n",
      "Iteration 20, loss = 2.11309580\n",
      "Iteration 21, loss = 2.07981757\n",
      "Iteration 22, loss = 2.04091175\n",
      "Iteration 23, loss = 1.99679727\n",
      "Iteration 24, loss = 1.94713648\n",
      "Iteration 25, loss = 1.89291542\n",
      "Iteration 26, loss = 1.83506151\n",
      "Iteration 27, loss = 1.77521965\n",
      "Iteration 28, loss = 1.71422091\n",
      "Iteration 29, loss = 1.65384323\n",
      "Iteration 30, loss = 1.59481906\n",
      "Iteration 31, loss = 1.53828315\n",
      "Iteration 32, loss = 1.48429894\n",
      "Iteration 33, loss = 1.43337929\n",
      "Iteration 34, loss = 1.38524683\n",
      "Iteration 35, loss = 1.33963407\n",
      "Iteration 36, loss = 1.29659012\n",
      "Iteration 37, loss = 1.25587907\n",
      "Iteration 38, loss = 1.21736180\n",
      "Iteration 39, loss = 1.18102650\n",
      "Iteration 40, loss = 1.14680191\n",
      "Iteration 41, loss = 1.11457379\n",
      "Iteration 42, loss = 1.08429677\n",
      "Iteration 43, loss = 1.05589023\n",
      "Iteration 44, loss = 1.02935330\n",
      "Iteration 45, loss = 1.00428717\n",
      "Iteration 46, loss = 0.98067037\n",
      "Iteration 47, loss = 0.95844257\n",
      "Iteration 48, loss = 0.93742033\n",
      "Iteration 49, loss = 0.91744444\n",
      "Iteration 50, loss = 0.89851331\n",
      "Iteration 51, loss = 0.88035571\n",
      "Iteration 52, loss = 0.86302326\n",
      "Iteration 53, loss = 0.84644460\n",
      "Iteration 54, loss = 0.83055679\n",
      "Iteration 55, loss = 0.81526338\n",
      "Iteration 56, loss = 0.80077688\n",
      "Iteration 57, loss = 0.78654895\n",
      "Iteration 58, loss = 0.77294443\n",
      "Iteration 59, loss = 0.75995744\n",
      "Iteration 60, loss = 0.74736992\n",
      "Iteration 61, loss = 0.73528739\n",
      "Iteration 62, loss = 0.72367006\n",
      "Iteration 63, loss = 0.71232889\n",
      "Iteration 64, loss = 0.70161241\n",
      "Iteration 65, loss = 0.69107612\n",
      "Iteration 66, loss = 0.68108367\n",
      "Iteration 67, loss = 0.67145615\n",
      "Iteration 68, loss = 0.66219595\n",
      "Iteration 69, loss = 0.65321457\n",
      "Iteration 70, loss = 0.64465835\n",
      "Iteration 71, loss = 0.63634522\n",
      "Iteration 72, loss = 0.62836795\n",
      "Iteration 73, loss = 0.62079324\n",
      "Iteration 74, loss = 0.61342626\n",
      "Iteration 75, loss = 0.60622254\n",
      "Iteration 76, loss = 0.59950660\n",
      "Iteration 77, loss = 0.59288780\n",
      "Iteration 78, loss = 0.58643334\n",
      "Iteration 79, loss = 0.58030779\n",
      "Iteration 80, loss = 0.57437368\n",
      "Iteration 81, loss = 0.56869280\n",
      "Iteration 82, loss = 0.56317604\n",
      "Iteration 83, loss = 0.55778903\n",
      "Iteration 84, loss = 0.55257893\n",
      "Iteration 85, loss = 0.54756106\n",
      "Iteration 86, loss = 0.54274869\n",
      "Iteration 87, loss = 0.53792877\n",
      "Iteration 88, loss = 0.53344456\n",
      "Iteration 89, loss = 0.52893040\n",
      "Iteration 90, loss = 0.52464800\n",
      "Iteration 91, loss = 0.52044053\n",
      "Iteration 92, loss = 0.51641432\n",
      "Iteration 93, loss = 0.51244228\n",
      "Iteration 94, loss = 0.50857408\n",
      "Iteration 95, loss = 0.50486356\n",
      "Iteration 96, loss = 0.50110754\n",
      "Iteration 97, loss = 0.49762671\n",
      "Iteration 98, loss = 0.49413966\n",
      "Iteration 99, loss = 0.49071896\n",
      "Iteration 100, loss = 0.48744781\n",
      "Iteration 101, loss = 0.48429382\n",
      "Iteration 102, loss = 0.48109288\n",
      "Iteration 103, loss = 0.47806795\n",
      "Iteration 104, loss = 0.47510563\n",
      "Iteration 105, loss = 0.47215973\n",
      "Iteration 106, loss = 0.46932147\n",
      "Iteration 107, loss = 0.46658019\n",
      "Iteration 108, loss = 0.46394781\n",
      "Iteration 109, loss = 0.46122298\n",
      "Iteration 110, loss = 0.45862549\n",
      "Iteration 111, loss = 0.45618134\n",
      "Iteration 112, loss = 0.45368004\n",
      "Iteration 113, loss = 0.45128167\n",
      "Iteration 114, loss = 0.44893242\n",
      "Iteration 115, loss = 0.44660786\n",
      "Iteration 116, loss = 0.44433374\n",
      "Iteration 117, loss = 0.44221586\n",
      "Iteration 118, loss = 0.44003661\n",
      "Iteration 119, loss = 0.43786760\n",
      "Iteration 120, loss = 0.43588667\n",
      "Iteration 121, loss = 0.43381437\n",
      "Iteration 122, loss = 0.43190676\n",
      "Iteration 123, loss = 0.42997272\n",
      "Iteration 124, loss = 0.42801146\n",
      "Iteration 125, loss = 0.42620465\n",
      "Iteration 126, loss = 0.42438227\n",
      "Iteration 127, loss = 0.42265743\n",
      "Iteration 128, loss = 0.42090058\n",
      "Iteration 129, loss = 0.41914608\n",
      "Iteration 130, loss = 0.41754615\n",
      "Iteration 131, loss = 0.41588953\n",
      "Iteration 132, loss = 0.41421818\n",
      "Iteration 133, loss = 0.41280500\n",
      "Iteration 134, loss = 0.41117333\n",
      "Iteration 135, loss = 0.40968039\n",
      "Iteration 136, loss = 0.40817990\n",
      "Iteration 137, loss = 0.40667278\n",
      "Iteration 138, loss = 0.40535351\n",
      "Iteration 139, loss = 0.40388487\n",
      "Iteration 140, loss = 0.40250939\n",
      "Iteration 141, loss = 0.40119176\n",
      "Iteration 142, loss = 0.39978919\n",
      "Iteration 143, loss = 0.39847853\n",
      "Iteration 144, loss = 0.39717707\n",
      "Iteration 145, loss = 0.39597118\n",
      "Iteration 146, loss = 0.39469560\n",
      "Iteration 147, loss = 0.39349991\n",
      "Iteration 148, loss = 0.39228295\n",
      "Iteration 149, loss = 0.39109923\n",
      "Iteration 150, loss = 0.38993986\n",
      "Iteration 151, loss = 0.38878515\n",
      "Iteration 152, loss = 0.38769140\n",
      "Iteration 153, loss = 0.38662487\n",
      "Iteration 154, loss = 0.38547989\n",
      "Iteration 155, loss = 0.38442074\n",
      "Iteration 156, loss = 0.38330487\n",
      "Iteration 157, loss = 0.38222271\n",
      "Iteration 158, loss = 0.38128538\n",
      "Iteration 159, loss = 0.38022398\n",
      "Iteration 160, loss = 0.37921049\n",
      "Iteration 161, loss = 0.37828118\n",
      "Iteration 162, loss = 0.37734024\n",
      "Iteration 163, loss = 0.37633009\n",
      "Iteration 164, loss = 0.37537159\n",
      "Iteration 165, loss = 0.37446390\n",
      "Iteration 166, loss = 0.37353716\n",
      "Iteration 167, loss = 0.37264090\n",
      "Iteration 168, loss = 0.37173152\n",
      "Iteration 169, loss = 0.37085628\n",
      "Iteration 170, loss = 0.37001890\n",
      "Iteration 171, loss = 0.36910535\n",
      "Iteration 172, loss = 0.36828989\n",
      "Iteration 173, loss = 0.36746130\n",
      "Iteration 174, loss = 0.36659827\n",
      "Iteration 175, loss = 0.36578075\n",
      "Iteration 176, loss = 0.36503813\n",
      "Iteration 177, loss = 0.36416240\n",
      "Iteration 178, loss = 0.36344909\n",
      "Iteration 179, loss = 0.36264709\n",
      "Iteration 180, loss = 0.36182598\n",
      "Iteration 181, loss = 0.36108959\n",
      "Iteration 182, loss = 0.36038359\n",
      "Iteration 183, loss = 0.35957329\n",
      "Iteration 184, loss = 0.35886587\n",
      "Iteration 185, loss = 0.35815548\n",
      "Iteration 186, loss = 0.35740064\n",
      "Iteration 187, loss = 0.35670350\n",
      "Iteration 188, loss = 0.35598363\n",
      "Iteration 189, loss = 0.35530753\n",
      "Iteration 190, loss = 0.35467953\n",
      "Iteration 191, loss = 0.35388530\n",
      "Iteration 192, loss = 0.35324510\n",
      "Iteration 193, loss = 0.35259938\n",
      "Iteration 194, loss = 0.35183031\n",
      "Iteration 195, loss = 0.35124663\n",
      "Iteration 196, loss = 0.35064600\n",
      "Iteration 197, loss = 0.34994344\n",
      "Iteration 198, loss = 0.34925367\n",
      "Iteration 199, loss = 0.34873492\n",
      "Iteration 200, loss = 0.34798021\n",
      "[CV] END activation=logistic, alpha=0.01, batch_size=200, hidden_layer_sizes=(400, 100), solver=sgd; total time= 4.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.31536758\n",
      "Iteration 2, loss = 2.30431902\n",
      "Iteration 3, loss = 2.30137797\n",
      "Iteration 4, loss = 2.29822108\n",
      "Iteration 5, loss = 2.29490150\n",
      "Iteration 6, loss = 2.29155667\n",
      "Iteration 7, loss = 2.28793761\n",
      "Iteration 8, loss = 2.28408054\n",
      "Iteration 9, loss = 2.27999283\n",
      "Iteration 10, loss = 2.27549011\n",
      "Iteration 11, loss = 2.27054980\n",
      "Iteration 12, loss = 2.26493451\n",
      "Iteration 13, loss = 2.25876643\n",
      "Iteration 14, loss = 2.25191348\n",
      "Iteration 15, loss = 2.24378913\n",
      "Iteration 16, loss = 2.23480338\n",
      "Iteration 17, loss = 2.22425730\n",
      "Iteration 18, loss = 2.21212793\n",
      "Iteration 19, loss = 2.19774463\n",
      "Iteration 20, loss = 2.18082102\n",
      "Iteration 21, loss = 2.16094767\n",
      "Iteration 22, loss = 2.13737481\n",
      "Iteration 23, loss = 2.10935876\n",
      "Iteration 24, loss = 2.07640662\n",
      "Iteration 25, loss = 2.03765707\n",
      "Iteration 26, loss = 1.99299697\n",
      "Iteration 27, loss = 1.94196462\n",
      "Iteration 28, loss = 1.88553190\n",
      "Iteration 29, loss = 1.82452365\n",
      "Iteration 30, loss = 1.76117448\n",
      "Iteration 31, loss = 1.69684236\n",
      "Iteration 32, loss = 1.63347762\n",
      "Iteration 33, loss = 1.57229583\n",
      "Iteration 34, loss = 1.51420704\n",
      "Iteration 35, loss = 1.45917443\n",
      "Iteration 36, loss = 1.40735902\n",
      "Iteration 37, loss = 1.35830465\n",
      "Iteration 38, loss = 1.31227711\n",
      "Iteration 39, loss = 1.26850194\n",
      "Iteration 40, loss = 1.22689096\n",
      "Iteration 41, loss = 1.18738665\n",
      "Iteration 42, loss = 1.14989173\n",
      "Iteration 43, loss = 1.11420058\n",
      "Iteration 44, loss = 1.08048727\n",
      "Iteration 45, loss = 1.04850186\n",
      "Iteration 46, loss = 1.01836317\n",
      "Iteration 47, loss = 0.98983507\n",
      "Iteration 48, loss = 0.96288577\n",
      "Iteration 49, loss = 0.93742721\n",
      "Iteration 50, loss = 0.91347701\n",
      "Iteration 51, loss = 0.89072378\n",
      "Iteration 52, loss = 0.86925096\n",
      "Iteration 53, loss = 0.84898150\n",
      "Iteration 54, loss = 0.82976110\n",
      "Iteration 55, loss = 0.81142660\n",
      "Iteration 56, loss = 0.79420763\n",
      "Iteration 57, loss = 0.77781539\n",
      "Iteration 58, loss = 0.76219112\n",
      "Iteration 59, loss = 0.74743313\n",
      "Iteration 60, loss = 0.73324951\n",
      "Iteration 61, loss = 0.71985107\n",
      "Iteration 62, loss = 0.70709457\n",
      "Iteration 63, loss = 0.69486489\n",
      "Iteration 64, loss = 0.68343453\n",
      "Iteration 65, loss = 0.67241757\n",
      "Iteration 66, loss = 0.66191812\n",
      "Iteration 67, loss = 0.65194790\n",
      "Iteration 68, loss = 0.64230242\n",
      "Iteration 69, loss = 0.63339849\n",
      "Iteration 70, loss = 0.62459127\n",
      "Iteration 71, loss = 0.61637187\n",
      "Iteration 72, loss = 0.60839791\n",
      "Iteration 73, loss = 0.60090195\n",
      "Iteration 74, loss = 0.59362013\n",
      "Iteration 75, loss = 0.58656445\n",
      "Iteration 76, loss = 0.57985296\n",
      "Iteration 77, loss = 0.57339945\n",
      "Iteration 78, loss = 0.56724249\n",
      "Iteration 79, loss = 0.56128458\n",
      "Iteration 80, loss = 0.55547647\n",
      "Iteration 81, loss = 0.55007866\n",
      "Iteration 82, loss = 0.54470714\n",
      "Iteration 83, loss = 0.53938983\n",
      "Iteration 84, loss = 0.53450559\n",
      "Iteration 85, loss = 0.52968169\n",
      "Iteration 86, loss = 0.52500809\n",
      "Iteration 87, loss = 0.52043423\n",
      "Iteration 88, loss = 0.51610336\n",
      "Iteration 89, loss = 0.51184396\n",
      "Iteration 90, loss = 0.50766060\n",
      "Iteration 91, loss = 0.50364156\n",
      "Iteration 92, loss = 0.49980899\n",
      "Iteration 93, loss = 0.49597505\n",
      "Iteration 94, loss = 0.49231329\n",
      "Iteration 95, loss = 0.48864486\n",
      "Iteration 96, loss = 0.48519793\n",
      "Iteration 97, loss = 0.48184301\n",
      "Iteration 98, loss = 0.47839354\n",
      "Iteration 99, loss = 0.47525825\n",
      "Iteration 100, loss = 0.47209840\n",
      "Iteration 101, loss = 0.46898669\n",
      "Iteration 102, loss = 0.46598641\n",
      "Iteration 103, loss = 0.46310031\n",
      "Iteration 104, loss = 0.46017784\n",
      "Iteration 105, loss = 0.45745523\n",
      "Iteration 106, loss = 0.45477953\n",
      "Iteration 107, loss = 0.45213178\n",
      "Iteration 108, loss = 0.44953278\n",
      "Iteration 109, loss = 0.44700562\n",
      "Iteration 110, loss = 0.44452317\n",
      "Iteration 111, loss = 0.44213822\n",
      "Iteration 112, loss = 0.43971526\n",
      "Iteration 113, loss = 0.43749424\n",
      "Iteration 114, loss = 0.43522353\n",
      "Iteration 115, loss = 0.43300516\n",
      "Iteration 116, loss = 0.43092246\n",
      "Iteration 117, loss = 0.42876636\n",
      "Iteration 118, loss = 0.42673662\n",
      "Iteration 119, loss = 0.42473662\n",
      "Iteration 120, loss = 0.42280037\n",
      "Iteration 121, loss = 0.42083195\n",
      "Iteration 122, loss = 0.41905387\n",
      "Iteration 123, loss = 0.41713553\n",
      "Iteration 124, loss = 0.41530751\n",
      "Iteration 125, loss = 0.41363104\n",
      "Iteration 126, loss = 0.41182049\n",
      "Iteration 127, loss = 0.41020615\n",
      "Iteration 128, loss = 0.40853445\n",
      "Iteration 129, loss = 0.40692055\n",
      "Iteration 130, loss = 0.40534659\n",
      "Iteration 131, loss = 0.40382807\n",
      "Iteration 132, loss = 0.40224328\n",
      "Iteration 133, loss = 0.40077156\n",
      "Iteration 134, loss = 0.39928375\n",
      "Iteration 135, loss = 0.39785066\n",
      "Iteration 136, loss = 0.39649412\n",
      "Iteration 137, loss = 0.39509685\n",
      "Iteration 138, loss = 0.39374070\n",
      "Iteration 139, loss = 0.39240523\n",
      "Iteration 140, loss = 0.39107083\n",
      "Iteration 141, loss = 0.38986078\n",
      "Iteration 142, loss = 0.38855828\n",
      "Iteration 143, loss = 0.38725442\n",
      "Iteration 144, loss = 0.38610615\n",
      "Iteration 145, loss = 0.38491921\n",
      "Iteration 146, loss = 0.38375836\n",
      "Iteration 147, loss = 0.38257925\n",
      "Iteration 148, loss = 0.38146219\n",
      "Iteration 149, loss = 0.38033023\n",
      "Iteration 150, loss = 0.37917036\n",
      "Iteration 151, loss = 0.37823978\n",
      "Iteration 152, loss = 0.37713332\n",
      "Iteration 153, loss = 0.37607628\n",
      "Iteration 154, loss = 0.37503342\n",
      "Iteration 155, loss = 0.37401061\n",
      "Iteration 156, loss = 0.37294640\n",
      "Iteration 157, loss = 0.37210923\n",
      "Iteration 158, loss = 0.37107237\n",
      "Iteration 159, loss = 0.37015509\n",
      "Iteration 160, loss = 0.36913934\n",
      "Iteration 161, loss = 0.36832412\n",
      "Iteration 162, loss = 0.36737165\n",
      "Iteration 163, loss = 0.36648476\n",
      "Iteration 164, loss = 0.36561985\n",
      "Iteration 165, loss = 0.36479219\n",
      "Iteration 166, loss = 0.36384415\n",
      "Iteration 167, loss = 0.36302022\n",
      "Iteration 168, loss = 0.36217935\n",
      "Iteration 169, loss = 0.36130395\n",
      "Iteration 170, loss = 0.36049827\n",
      "Iteration 171, loss = 0.35965818\n",
      "Iteration 172, loss = 0.35897239\n",
      "Iteration 173, loss = 0.35818416\n",
      "Iteration 174, loss = 0.35741320\n",
      "Iteration 175, loss = 0.35670783\n",
      "Iteration 176, loss = 0.35596796\n",
      "Iteration 177, loss = 0.35504382\n",
      "Iteration 178, loss = 0.35438470\n",
      "Iteration 179, loss = 0.35370773\n",
      "Iteration 180, loss = 0.35296938\n",
      "Iteration 181, loss = 0.35222771\n",
      "Iteration 182, loss = 0.35154858\n",
      "Iteration 183, loss = 0.35084861\n",
      "Iteration 184, loss = 0.35016644\n",
      "Iteration 185, loss = 0.34946447\n",
      "Iteration 186, loss = 0.34878599\n",
      "Iteration 187, loss = 0.34812256\n",
      "Iteration 188, loss = 0.34748093\n",
      "Iteration 189, loss = 0.34684472\n",
      "Iteration 190, loss = 0.34614604\n",
      "Iteration 191, loss = 0.34554360\n",
      "Iteration 192, loss = 0.34486352\n",
      "Iteration 193, loss = 0.34433811\n",
      "Iteration 194, loss = 0.34362293\n",
      "Iteration 195, loss = 0.34304737\n",
      "Iteration 196, loss = 0.34244857\n",
      "Iteration 197, loss = 0.34179485\n",
      "Iteration 198, loss = 0.34122247\n",
      "Iteration 199, loss = 0.34066649\n",
      "Iteration 200, loss = 0.33998164\n",
      "[CV] END activation=logistic, alpha=0.01, batch_size=200, hidden_layer_sizes=(400, 100), solver=sgd; total time= 4.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.31365892\n",
      "Iteration 2, loss = 2.30298389\n",
      "Iteration 3, loss = 2.29967491\n",
      "Iteration 4, loss = 2.29636460\n",
      "Iteration 5, loss = 2.29283808\n",
      "Iteration 6, loss = 2.28908191\n",
      "Iteration 7, loss = 2.28508392\n",
      "Iteration 8, loss = 2.28072224\n",
      "Iteration 9, loss = 2.27602210\n",
      "Iteration 10, loss = 2.27087054\n",
      "Iteration 11, loss = 2.26506468\n",
      "Iteration 12, loss = 2.25859873\n",
      "Iteration 13, loss = 2.25113977\n",
      "Iteration 14, loss = 2.24275183\n",
      "Iteration 15, loss = 2.23292360\n",
      "Iteration 16, loss = 2.22181339\n",
      "Iteration 17, loss = 2.20836588\n",
      "Iteration 18, loss = 2.19295789\n",
      "Iteration 19, loss = 2.17458379\n",
      "Iteration 20, loss = 2.15288636\n",
      "Iteration 21, loss = 2.12724346\n",
      "Iteration 22, loss = 2.09746869\n",
      "Iteration 23, loss = 2.06250958\n",
      "Iteration 24, loss = 2.02287914\n",
      "Iteration 25, loss = 1.97806463\n",
      "Iteration 26, loss = 1.92900679\n",
      "Iteration 27, loss = 1.87637134\n",
      "Iteration 28, loss = 1.82074585\n",
      "Iteration 29, loss = 1.76303753\n",
      "Iteration 30, loss = 1.70386558\n",
      "Iteration 31, loss = 1.64380530\n",
      "Iteration 32, loss = 1.58333313\n",
      "Iteration 33, loss = 1.52293617\n",
      "Iteration 34, loss = 1.46347345\n",
      "Iteration 35, loss = 1.40568042\n",
      "Iteration 36, loss = 1.35046647\n",
      "Iteration 37, loss = 1.29839394\n",
      "Iteration 38, loss = 1.24985617\n",
      "Iteration 39, loss = 1.20505721\n",
      "Iteration 40, loss = 1.16387543\n",
      "Iteration 41, loss = 1.12620220\n",
      "Iteration 42, loss = 1.09170601\n",
      "Iteration 43, loss = 1.06007450\n",
      "Iteration 44, loss = 1.03097800\n",
      "Iteration 45, loss = 1.00387630\n",
      "Iteration 46, loss = 0.97862365\n",
      "Iteration 47, loss = 0.95506506\n",
      "Iteration 48, loss = 0.93286061\n",
      "Iteration 49, loss = 0.91180897\n",
      "Iteration 50, loss = 0.89175117\n",
      "Iteration 51, loss = 0.87276961\n",
      "Iteration 52, loss = 0.85439653\n",
      "Iteration 53, loss = 0.83701034\n",
      "Iteration 54, loss = 0.82010065\n",
      "Iteration 55, loss = 0.80391294\n",
      "Iteration 56, loss = 0.78838599\n",
      "Iteration 57, loss = 0.77334197\n",
      "Iteration 58, loss = 0.75897233\n",
      "Iteration 59, loss = 0.74511728\n",
      "Iteration 60, loss = 0.73168041\n",
      "Iteration 61, loss = 0.71883259\n",
      "Iteration 62, loss = 0.70641338\n",
      "Iteration 63, loss = 0.69453612\n",
      "Iteration 64, loss = 0.68326055\n",
      "Iteration 65, loss = 0.67228722\n",
      "Iteration 66, loss = 0.66184440\n",
      "Iteration 67, loss = 0.65188742\n",
      "Iteration 68, loss = 0.64228022\n",
      "Iteration 69, loss = 0.63297355\n",
      "Iteration 70, loss = 0.62425091\n",
      "Iteration 71, loss = 0.61585931\n",
      "Iteration 72, loss = 0.60779480\n",
      "Iteration 73, loss = 0.60004333\n",
      "Iteration 74, loss = 0.59263221\n",
      "Iteration 75, loss = 0.58555894\n",
      "Iteration 76, loss = 0.57879589\n",
      "Iteration 77, loss = 0.57222598\n",
      "Iteration 78, loss = 0.56590985\n",
      "Iteration 79, loss = 0.55987886\n",
      "Iteration 80, loss = 0.55413465\n",
      "Iteration 81, loss = 0.54859815\n",
      "Iteration 82, loss = 0.54319272\n",
      "Iteration 83, loss = 0.53798795\n",
      "Iteration 84, loss = 0.53312733\n",
      "Iteration 85, loss = 0.52824982\n",
      "Iteration 86, loss = 0.52357040\n",
      "Iteration 87, loss = 0.51907572\n",
      "Iteration 88, loss = 0.51478178\n",
      "Iteration 89, loss = 0.51055175\n",
      "Iteration 90, loss = 0.50652594\n",
      "Iteration 91, loss = 0.50257340\n",
      "Iteration 92, loss = 0.49871050\n",
      "Iteration 93, loss = 0.49500639\n",
      "Iteration 94, loss = 0.49145727\n",
      "Iteration 95, loss = 0.48796265\n",
      "Iteration 96, loss = 0.48459562\n",
      "Iteration 97, loss = 0.48129788\n",
      "Iteration 98, loss = 0.47813154\n",
      "Iteration 99, loss = 0.47497780\n",
      "Iteration 100, loss = 0.47193230\n",
      "Iteration 101, loss = 0.46906319\n",
      "Iteration 102, loss = 0.46614684\n",
      "Iteration 103, loss = 0.46341081\n",
      "Iteration 104, loss = 0.46061052\n",
      "Iteration 105, loss = 0.45792823\n",
      "Iteration 106, loss = 0.45545442\n",
      "Iteration 107, loss = 0.45288958\n",
      "Iteration 108, loss = 0.45046035\n",
      "Iteration 109, loss = 0.44798951\n",
      "Iteration 110, loss = 0.44564780\n",
      "Iteration 111, loss = 0.44331894\n",
      "Iteration 112, loss = 0.44114343\n",
      "Iteration 113, loss = 0.43896445\n",
      "Iteration 114, loss = 0.43683261\n",
      "Iteration 115, loss = 0.43476387\n",
      "Iteration 116, loss = 0.43274338\n",
      "Iteration 117, loss = 0.43069387\n",
      "Iteration 118, loss = 0.42874807\n",
      "Iteration 119, loss = 0.42683848\n",
      "Iteration 120, loss = 0.42506313\n",
      "Iteration 121, loss = 0.42315922\n",
      "Iteration 122, loss = 0.42130399\n",
      "Iteration 123, loss = 0.41967182\n",
      "Iteration 124, loss = 0.41790482\n",
      "Iteration 125, loss = 0.41617477\n",
      "Iteration 126, loss = 0.41460098\n",
      "Iteration 127, loss = 0.41293688\n",
      "Iteration 128, loss = 0.41137786\n",
      "Iteration 129, loss = 0.40987945\n",
      "Iteration 130, loss = 0.40834098\n",
      "Iteration 131, loss = 0.40692714\n",
      "Iteration 132, loss = 0.40538400\n",
      "Iteration 133, loss = 0.40396422\n",
      "Iteration 134, loss = 0.40258385\n",
      "Iteration 135, loss = 0.40118827\n",
      "Iteration 136, loss = 0.39986536\n",
      "Iteration 137, loss = 0.39849281\n",
      "Iteration 138, loss = 0.39732233\n",
      "Iteration 139, loss = 0.39589806\n",
      "Iteration 140, loss = 0.39473130\n",
      "Iteration 141, loss = 0.39343981\n",
      "Iteration 142, loss = 0.39225436\n",
      "Iteration 143, loss = 0.39101905\n",
      "Iteration 144, loss = 0.38986849\n",
      "Iteration 145, loss = 0.38876902\n",
      "Iteration 146, loss = 0.38758583\n",
      "Iteration 147, loss = 0.38648527\n",
      "Iteration 148, loss = 0.38540600\n",
      "Iteration 149, loss = 0.38429931\n",
      "Iteration 150, loss = 0.38321900\n",
      "Iteration 151, loss = 0.38210532\n",
      "Iteration 152, loss = 0.38120197\n",
      "Iteration 153, loss = 0.38017066\n",
      "Iteration 154, loss = 0.37914352\n",
      "Iteration 155, loss = 0.37815175\n",
      "Iteration 156, loss = 0.37720324\n",
      "Iteration 157, loss = 0.37624523\n",
      "Iteration 158, loss = 0.37527597\n",
      "Iteration 159, loss = 0.37436536\n",
      "Iteration 160, loss = 0.37342479\n",
      "Iteration 161, loss = 0.37254803\n",
      "Iteration 162, loss = 0.37165166\n",
      "Iteration 163, loss = 0.37078174\n",
      "Iteration 164, loss = 0.36987925\n",
      "Iteration 165, loss = 0.36903070\n",
      "Iteration 166, loss = 0.36819346\n",
      "Iteration 167, loss = 0.36736259\n",
      "Iteration 168, loss = 0.36652232\n",
      "Iteration 169, loss = 0.36579185\n",
      "Iteration 170, loss = 0.36485301\n",
      "Iteration 171, loss = 0.36413423\n",
      "Iteration 172, loss = 0.36330599\n",
      "Iteration 173, loss = 0.36260319\n",
      "Iteration 174, loss = 0.36183076\n",
      "Iteration 175, loss = 0.36108732\n",
      "Iteration 176, loss = 0.36032591\n",
      "Iteration 177, loss = 0.35949083\n",
      "Iteration 178, loss = 0.35888175\n",
      "Iteration 179, loss = 0.35808044\n",
      "Iteration 180, loss = 0.35734091\n",
      "Iteration 181, loss = 0.35663833\n",
      "Iteration 182, loss = 0.35596858\n",
      "Iteration 183, loss = 0.35521540\n",
      "Iteration 184, loss = 0.35457955\n",
      "Iteration 185, loss = 0.35386922\n",
      "Iteration 186, loss = 0.35324452\n",
      "Iteration 187, loss = 0.35248501\n",
      "Iteration 188, loss = 0.35188178\n",
      "Iteration 189, loss = 0.35122982\n",
      "Iteration 190, loss = 0.35052983\n",
      "Iteration 191, loss = 0.34991341\n",
      "Iteration 192, loss = 0.34931605\n",
      "Iteration 193, loss = 0.34861235\n",
      "Iteration 194, loss = 0.34797440\n",
      "Iteration 195, loss = 0.34738016\n",
      "Iteration 196, loss = 0.34679204\n",
      "Iteration 197, loss = 0.34617349\n",
      "Iteration 198, loss = 0.34554028\n",
      "Iteration 199, loss = 0.34493667\n",
      "Iteration 200, loss = 0.34435685\n",
      "[CV] END activation=logistic, alpha=0.01, batch_size=200, hidden_layer_sizes=(400, 100), solver=sgd; total time= 4.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.30948078\n",
      "Iteration 2, loss = 2.30341585\n",
      "Iteration 3, loss = 2.30015196\n",
      "Iteration 4, loss = 2.29662966\n",
      "Iteration 5, loss = 2.29297310\n",
      "Iteration 6, loss = 2.28910695\n",
      "Iteration 7, loss = 2.28504489\n",
      "Iteration 8, loss = 2.28059737\n",
      "Iteration 9, loss = 2.27582629\n",
      "Iteration 10, loss = 2.27043419\n",
      "Iteration 11, loss = 2.26463304\n",
      "Iteration 12, loss = 2.25790559\n",
      "Iteration 13, loss = 2.25035293\n",
      "Iteration 14, loss = 2.24182830\n",
      "Iteration 15, loss = 2.23177513\n",
      "Iteration 16, loss = 2.22005806\n",
      "Iteration 17, loss = 2.20661774\n",
      "Iteration 18, loss = 2.19063771\n",
      "Iteration 19, loss = 2.17168651\n",
      "Iteration 20, loss = 2.14950956\n",
      "Iteration 21, loss = 2.12341200\n",
      "Iteration 22, loss = 2.09272757\n",
      "Iteration 23, loss = 2.05711841\n",
      "Iteration 24, loss = 2.01651712\n",
      "Iteration 25, loss = 1.97078009\n",
      "Iteration 26, loss = 1.92050009\n",
      "Iteration 27, loss = 1.86639303\n",
      "Iteration 28, loss = 1.80916423\n",
      "Iteration 29, loss = 1.74973912\n",
      "Iteration 30, loss = 1.68920399\n",
      "Iteration 31, loss = 1.62873513\n",
      "Iteration 32, loss = 1.56903107\n",
      "Iteration 33, loss = 1.51132732\n",
      "Iteration 34, loss = 1.45609608\n",
      "Iteration 35, loss = 1.40341092\n",
      "Iteration 36, loss = 1.35347206\n",
      "Iteration 37, loss = 1.30649429\n",
      "Iteration 38, loss = 1.26214428\n",
      "Iteration 39, loss = 1.22024838\n",
      "Iteration 40, loss = 1.18110513\n",
      "Iteration 41, loss = 1.14458108\n",
      "Iteration 42, loss = 1.11041721\n",
      "Iteration 43, loss = 1.07847350\n",
      "Iteration 44, loss = 1.04888903\n",
      "Iteration 45, loss = 1.02129553\n",
      "Iteration 46, loss = 0.99558805\n",
      "Iteration 47, loss = 0.97153905\n",
      "Iteration 48, loss = 0.94901434\n",
      "Iteration 49, loss = 0.92790205\n",
      "Iteration 50, loss = 0.90780555\n",
      "Iteration 51, loss = 0.88889656\n",
      "Iteration 52, loss = 0.87082133\n",
      "Iteration 53, loss = 0.85367520\n",
      "Iteration 54, loss = 0.83724734\n",
      "Iteration 55, loss = 0.82147525\n",
      "Iteration 56, loss = 0.80628946\n",
      "Iteration 57, loss = 0.79170212\n",
      "Iteration 58, loss = 0.77765281\n",
      "Iteration 59, loss = 0.76401010\n",
      "Iteration 60, loss = 0.75077178\n",
      "Iteration 61, loss = 0.73810233\n",
      "Iteration 62, loss = 0.72568786\n",
      "Iteration 63, loss = 0.71384909\n",
      "Iteration 64, loss = 0.70223045\n",
      "Iteration 65, loss = 0.69102106\n",
      "Iteration 66, loss = 0.68021767\n",
      "Iteration 67, loss = 0.66973698\n",
      "Iteration 68, loss = 0.65966833\n",
      "Iteration 69, loss = 0.64977747\n",
      "Iteration 70, loss = 0.64038521\n",
      "Iteration 71, loss = 0.63134802\n",
      "Iteration 72, loss = 0.62255434\n",
      "Iteration 73, loss = 0.61416146\n",
      "Iteration 74, loss = 0.60602910\n",
      "Iteration 75, loss = 0.59822941\n",
      "Iteration 76, loss = 0.59062295\n",
      "Iteration 77, loss = 0.58329596\n",
      "Iteration 78, loss = 0.57641865\n",
      "Iteration 79, loss = 0.56971681\n",
      "Iteration 80, loss = 0.56327249\n",
      "Iteration 81, loss = 0.55699108\n",
      "Iteration 82, loss = 0.55104215\n",
      "Iteration 83, loss = 0.54522777\n",
      "Iteration 84, loss = 0.53963963\n",
      "Iteration 85, loss = 0.53425574\n",
      "Iteration 86, loss = 0.52907767\n",
      "Iteration 87, loss = 0.52400039\n",
      "Iteration 88, loss = 0.51918450\n",
      "Iteration 89, loss = 0.51448520\n",
      "Iteration 90, loss = 0.51001554\n",
      "Iteration 91, loss = 0.50556896\n",
      "Iteration 92, loss = 0.50132485\n",
      "Iteration 93, loss = 0.49725699\n",
      "Iteration 94, loss = 0.49320403\n",
      "Iteration 95, loss = 0.48941038\n",
      "Iteration 96, loss = 0.48570549\n",
      "Iteration 97, loss = 0.48195084\n",
      "Iteration 98, loss = 0.47854946\n",
      "Iteration 99, loss = 0.47503105\n",
      "Iteration 100, loss = 0.47175828\n",
      "Iteration 101, loss = 0.46850422\n",
      "Iteration 102, loss = 0.46538436\n",
      "Iteration 103, loss = 0.46229862\n",
      "Iteration 104, loss = 0.45933784\n",
      "Iteration 105, loss = 0.45647121\n",
      "Iteration 106, loss = 0.45364523\n",
      "Iteration 107, loss = 0.45094706\n",
      "Iteration 108, loss = 0.44827648\n",
      "Iteration 109, loss = 0.44563935\n",
      "Iteration 110, loss = 0.44312692\n",
      "Iteration 111, loss = 0.44074293\n",
      "Iteration 112, loss = 0.43831447\n",
      "Iteration 113, loss = 0.43595364\n",
      "Iteration 114, loss = 0.43369331\n",
      "Iteration 115, loss = 0.43153290\n",
      "Iteration 116, loss = 0.42925317\n",
      "Iteration 117, loss = 0.42720278\n",
      "Iteration 118, loss = 0.42520556\n",
      "Iteration 119, loss = 0.42312126\n",
      "Iteration 120, loss = 0.42117293\n",
      "Iteration 121, loss = 0.41918369\n",
      "Iteration 122, loss = 0.41736964\n",
      "Iteration 123, loss = 0.41549839\n",
      "Iteration 124, loss = 0.41374500\n",
      "Iteration 125, loss = 0.41195939\n",
      "Iteration 126, loss = 0.41027733\n",
      "Iteration 127, loss = 0.40856828\n",
      "Iteration 128, loss = 0.40692424\n",
      "Iteration 129, loss = 0.40527009\n",
      "Iteration 130, loss = 0.40376954\n",
      "Iteration 131, loss = 0.40223292\n",
      "Iteration 132, loss = 0.40070256\n",
      "Iteration 133, loss = 0.39921431\n",
      "Iteration 134, loss = 0.39773583\n",
      "Iteration 135, loss = 0.39639283\n",
      "Iteration 136, loss = 0.39497920\n",
      "Iteration 137, loss = 0.39358182\n",
      "Iteration 138, loss = 0.39234191\n",
      "Iteration 139, loss = 0.39095975\n",
      "Iteration 140, loss = 0.38962008\n",
      "Iteration 141, loss = 0.38836247\n",
      "Iteration 142, loss = 0.38716129\n",
      "Iteration 143, loss = 0.38590663\n",
      "Iteration 144, loss = 0.38477788\n",
      "Iteration 145, loss = 0.38359430\n",
      "Iteration 146, loss = 0.38247420\n",
      "Iteration 147, loss = 0.38129397\n",
      "Iteration 148, loss = 0.38016505\n",
      "Iteration 149, loss = 0.37907301\n",
      "Iteration 150, loss = 0.37799389\n",
      "Iteration 151, loss = 0.37697638\n",
      "Iteration 152, loss = 0.37592178\n",
      "Iteration 153, loss = 0.37492014\n",
      "Iteration 154, loss = 0.37386298\n",
      "Iteration 155, loss = 0.37285692\n",
      "Iteration 156, loss = 0.37179630\n",
      "Iteration 157, loss = 0.37089274\n",
      "Iteration 158, loss = 0.36994185\n",
      "Iteration 159, loss = 0.36903321\n",
      "Iteration 160, loss = 0.36809545\n",
      "Iteration 161, loss = 0.36718225\n",
      "Iteration 162, loss = 0.36627997\n",
      "Iteration 163, loss = 0.36539723\n",
      "Iteration 164, loss = 0.36452610\n",
      "Iteration 165, loss = 0.36366041\n",
      "Iteration 166, loss = 0.36284306\n",
      "Iteration 167, loss = 0.36196412\n",
      "Iteration 168, loss = 0.36116671\n",
      "Iteration 169, loss = 0.36032777\n",
      "Iteration 170, loss = 0.35955709\n",
      "Iteration 171, loss = 0.35870774\n",
      "Iteration 172, loss = 0.35795959\n",
      "Iteration 173, loss = 0.35720651\n",
      "Iteration 174, loss = 0.35641744\n",
      "Iteration 175, loss = 0.35565139\n",
      "Iteration 176, loss = 0.35496566\n",
      "Iteration 177, loss = 0.35415251\n",
      "Iteration 178, loss = 0.35338410\n",
      "Iteration 179, loss = 0.35272511\n",
      "Iteration 180, loss = 0.35195217\n",
      "Iteration 181, loss = 0.35130600\n",
      "Iteration 182, loss = 0.35061678\n",
      "Iteration 183, loss = 0.34990090\n",
      "Iteration 184, loss = 0.34922339\n",
      "Iteration 185, loss = 0.34858268\n",
      "Iteration 186, loss = 0.34791809\n",
      "Iteration 187, loss = 0.34722671\n",
      "Iteration 188, loss = 0.34661064\n",
      "Iteration 189, loss = 0.34595633\n",
      "Iteration 190, loss = 0.34520605\n",
      "Iteration 191, loss = 0.34467771\n",
      "Iteration 192, loss = 0.34403778\n",
      "Iteration 193, loss = 0.34344985\n",
      "Iteration 194, loss = 0.34277441\n",
      "Iteration 195, loss = 0.34226039\n",
      "Iteration 196, loss = 0.34155067\n",
      "Iteration 197, loss = 0.34105811\n",
      "Iteration 198, loss = 0.34046478\n",
      "Iteration 199, loss = 0.33974382\n",
      "Iteration 200, loss = 0.33920262\n",
      "[CV] END activation=logistic, alpha=0.01, batch_size=200, hidden_layer_sizes=(400, 100), solver=sgd; total time= 4.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.76618846\n",
      "Iteration 2, loss = 0.67620600\n",
      "Iteration 3, loss = 0.42114640\n",
      "Iteration 4, loss = 0.33734569\n",
      "Iteration 5, loss = 0.28683630\n",
      "Iteration 6, loss = 0.24815776\n",
      "Iteration 7, loss = 0.21872083\n",
      "Iteration 8, loss = 0.19619413\n",
      "Iteration 9, loss = 0.17744062\n",
      "Iteration 10, loss = 0.16214720\n",
      "Iteration 11, loss = 0.15073370\n",
      "Iteration 12, loss = 0.13995901\n",
      "Iteration 13, loss = 0.13051408\n",
      "Iteration 14, loss = 0.12380413\n",
      "Iteration 15, loss = 0.11691415\n",
      "Iteration 16, loss = 0.11067198\n",
      "Iteration 17, loss = 0.10468208\n",
      "Iteration 18, loss = 0.10067153\n",
      "Iteration 19, loss = 0.09720710\n",
      "Iteration 20, loss = 0.09111119\n",
      "Iteration 21, loss = 0.08813972\n",
      "Iteration 22, loss = 0.08470367\n",
      "Iteration 23, loss = 0.08171970\n",
      "Iteration 24, loss = 0.07970494\n",
      "Iteration 25, loss = 0.07785089\n",
      "Iteration 26, loss = 0.07526588\n",
      "Iteration 27, loss = 0.07396400\n",
      "Iteration 28, loss = 0.07194833\n",
      "Iteration 29, loss = 0.06932700\n",
      "Iteration 30, loss = 0.06854326\n",
      "Iteration 31, loss = 0.06634894\n",
      "Iteration 32, loss = 0.06510866\n",
      "Iteration 33, loss = 0.06461183\n",
      "Iteration 34, loss = 0.06299662\n",
      "Iteration 35, loss = 0.06172916\n",
      "Iteration 36, loss = 0.06237628\n",
      "Iteration 37, loss = 0.06047461\n",
      "Iteration 38, loss = 0.05992742\n",
      "Iteration 39, loss = 0.05835187\n",
      "Iteration 40, loss = 0.05781599\n",
      "Iteration 41, loss = 0.05684937\n",
      "Iteration 42, loss = 0.05617708\n",
      "Iteration 43, loss = 0.05700251\n",
      "Iteration 44, loss = 0.05735312\n",
      "Iteration 45, loss = 0.05598356\n",
      "Iteration 46, loss = 0.05529116\n",
      "Iteration 47, loss = 0.05516226\n",
      "Iteration 48, loss = 0.05634898\n",
      "Iteration 49, loss = 0.05344790\n",
      "Iteration 50, loss = 0.05296382\n",
      "Iteration 51, loss = 0.05427538\n",
      "Iteration 52, loss = 0.05736488\n",
      "Iteration 53, loss = 0.05298501\n",
      "Iteration 54, loss = 0.05137402\n",
      "Iteration 55, loss = 0.05178853\n",
      "Iteration 56, loss = 0.05091125\n",
      "Iteration 57, loss = 0.05077342\n",
      "Iteration 58, loss = 0.05120108\n",
      "Iteration 59, loss = 0.05110768\n",
      "Iteration 60, loss = 0.05256614\n",
      "Iteration 61, loss = 0.05141663\n",
      "Iteration 62, loss = 0.05057769\n",
      "Iteration 63, loss = 0.05306986\n",
      "Iteration 64, loss = 0.05299891\n",
      "Iteration 65, loss = 0.05057508\n",
      "Iteration 66, loss = 0.05129454\n",
      "Iteration 67, loss = 0.04904608\n",
      "Iteration 68, loss = 0.04972479\n",
      "Iteration 69, loss = 0.04909833\n",
      "Iteration 70, loss = 0.04974672\n",
      "Iteration 71, loss = 0.05107705\n",
      "Iteration 72, loss = 0.05127703\n",
      "Iteration 73, loss = 0.04951317\n",
      "Iteration 74, loss = 0.04833313\n",
      "Iteration 75, loss = 0.04839858\n",
      "Iteration 76, loss = 0.04933165\n",
      "Iteration 77, loss = 0.04923494\n",
      "Iteration 78, loss = 0.05027116\n",
      "Iteration 79, loss = 0.05067244\n",
      "Iteration 80, loss = 0.04970895\n",
      "Iteration 81, loss = 0.04898868\n",
      "Iteration 82, loss = 0.04865449\n",
      "Iteration 83, loss = 0.04737687\n",
      "Iteration 84, loss = 0.04711955\n",
      "Iteration 85, loss = 0.05014226\n",
      "Iteration 86, loss = 0.05626927\n",
      "Iteration 87, loss = 0.04954218\n",
      "Iteration 88, loss = 0.04740557\n",
      "Iteration 89, loss = 0.04672949\n",
      "Iteration 90, loss = 0.04611754\n",
      "Iteration 91, loss = 0.04610934\n",
      "Iteration 92, loss = 0.04682072\n",
      "Iteration 93, loss = 0.04983791\n",
      "Iteration 94, loss = 0.05142813\n",
      "Iteration 95, loss = 0.04989924\n",
      "Iteration 96, loss = 0.04675265\n",
      "Iteration 97, loss = 0.04613457\n",
      "Iteration 98, loss = 0.04559673\n",
      "Iteration 99, loss = 0.04660095\n",
      "Iteration 100, loss = 0.04930254\n",
      "Iteration 101, loss = 0.05223103\n",
      "Iteration 102, loss = 0.04893954\n",
      "Iteration 103, loss = 0.04713171\n",
      "Iteration 104, loss = 0.04601695\n",
      "Iteration 105, loss = 0.04533728\n",
      "Iteration 106, loss = 0.04503415\n",
      "Iteration 107, loss = 0.04555609\n",
      "Iteration 108, loss = 0.04587269\n",
      "Iteration 109, loss = 0.04708152\n",
      "Iteration 110, loss = 0.05478894\n",
      "Iteration 111, loss = 0.04770268\n",
      "Iteration 112, loss = 0.04557221\n",
      "Iteration 113, loss = 0.04585325\n",
      "Iteration 114, loss = 0.04530310\n",
      "Iteration 115, loss = 0.04567786\n",
      "Iteration 116, loss = 0.04505251\n",
      "Iteration 117, loss = 0.04566403\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.01, batch_size=200, hidden_layer_sizes=(100, 100, 100), solver=adam; total time= 1.6min\n",
      "Iteration 1, loss = 1.77696832\n",
      "Iteration 2, loss = 0.65288193\n",
      "Iteration 3, loss = 0.41015690\n",
      "Iteration 4, loss = 0.32944778\n",
      "Iteration 5, loss = 0.27985163\n",
      "Iteration 6, loss = 0.24212811\n",
      "Iteration 7, loss = 0.21390501\n",
      "Iteration 8, loss = 0.19234878\n",
      "Iteration 9, loss = 0.17494613\n",
      "Iteration 10, loss = 0.16108368\n",
      "Iteration 11, loss = 0.14860105\n",
      "Iteration 12, loss = 0.13844865\n",
      "Iteration 13, loss = 0.13045206\n",
      "Iteration 14, loss = 0.12179460\n",
      "Iteration 15, loss = 0.11614900\n",
      "Iteration 16, loss = 0.10871902\n",
      "Iteration 17, loss = 0.10428069\n",
      "Iteration 18, loss = 0.10015009\n",
      "Iteration 19, loss = 0.09519882\n",
      "Iteration 20, loss = 0.09158929\n",
      "Iteration 21, loss = 0.08796754\n",
      "Iteration 22, loss = 0.08569376\n",
      "Iteration 23, loss = 0.08308036\n",
      "Iteration 24, loss = 0.07869231\n",
      "Iteration 25, loss = 0.07673216\n",
      "Iteration 26, loss = 0.07612785\n",
      "Iteration 27, loss = 0.07399495\n",
      "Iteration 28, loss = 0.07087287\n",
      "Iteration 29, loss = 0.06938082\n",
      "Iteration 30, loss = 0.06845501\n",
      "Iteration 31, loss = 0.06606471\n",
      "Iteration 32, loss = 0.06438351\n",
      "Iteration 33, loss = 0.06350312\n",
      "Iteration 34, loss = 0.06235823\n",
      "Iteration 35, loss = 0.06203863\n",
      "Iteration 36, loss = 0.06096588\n",
      "Iteration 37, loss = 0.06036896\n",
      "Iteration 38, loss = 0.05997611\n",
      "Iteration 39, loss = 0.05894689\n",
      "Iteration 40, loss = 0.05807581\n",
      "Iteration 41, loss = 0.05794841\n",
      "Iteration 42, loss = 0.05748974\n",
      "Iteration 43, loss = 0.05772622\n",
      "Iteration 44, loss = 0.05654441\n",
      "Iteration 45, loss = 0.05527420\n",
      "Iteration 46, loss = 0.05865904\n",
      "Iteration 47, loss = 0.05532907\n",
      "Iteration 48, loss = 0.05560393\n",
      "Iteration 49, loss = 0.05424689\n",
      "Iteration 50, loss = 0.05292299\n",
      "Iteration 51, loss = 0.05268327\n",
      "Iteration 52, loss = 0.05428818\n",
      "Iteration 53, loss = 0.05755203\n",
      "Iteration 54, loss = 0.05246737\n",
      "Iteration 55, loss = 0.05200131\n",
      "Iteration 56, loss = 0.05208003\n",
      "Iteration 57, loss = 0.05229910\n",
      "Iteration 58, loss = 0.05245891\n",
      "Iteration 59, loss = 0.05138216\n",
      "Iteration 60, loss = 0.05085520\n",
      "Iteration 61, loss = 0.05184804\n",
      "Iteration 62, loss = 0.05103457\n",
      "Iteration 63, loss = 0.05121674\n",
      "Iteration 64, loss = 0.05567782\n",
      "Iteration 65, loss = 0.05320637\n",
      "Iteration 66, loss = 0.05019289\n",
      "Iteration 67, loss = 0.04865905\n",
      "Iteration 68, loss = 0.04936272\n",
      "Iteration 69, loss = 0.04897704\n",
      "Iteration 70, loss = 0.04975205\n",
      "Iteration 71, loss = 0.04995820\n",
      "Iteration 72, loss = 0.05100884\n",
      "Iteration 73, loss = 0.05203976\n",
      "Iteration 74, loss = 0.04874441\n",
      "Iteration 75, loss = 0.04856786\n",
      "Iteration 76, loss = 0.04923628\n",
      "Iteration 77, loss = 0.04953133\n",
      "Iteration 78, loss = 0.05250886\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.01, batch_size=200, hidden_layer_sizes=(100, 100, 100), solver=adam; total time= 1.1min\n",
      "Iteration 1, loss = 1.81734355\n",
      "Iteration 2, loss = 0.76857890\n",
      "Iteration 3, loss = 0.45703281\n",
      "Iteration 4, loss = 0.33454433\n",
      "Iteration 5, loss = 0.27845260\n",
      "Iteration 6, loss = 0.24396710\n",
      "Iteration 7, loss = 0.21830077\n",
      "Iteration 8, loss = 0.19954699\n",
      "Iteration 9, loss = 0.18131621\n",
      "Iteration 10, loss = 0.16653695\n",
      "Iteration 11, loss = 0.15474441\n",
      "Iteration 12, loss = 0.14477440\n",
      "Iteration 13, loss = 0.13540164\n",
      "Iteration 14, loss = 0.12759954\n",
      "Iteration 15, loss = 0.11994929\n",
      "Iteration 16, loss = 0.11397174\n",
      "Iteration 17, loss = 0.10768540\n",
      "Iteration 18, loss = 0.10309534\n",
      "Iteration 19, loss = 0.09885251\n",
      "Iteration 20, loss = 0.09436516\n",
      "Iteration 21, loss = 0.09164669\n",
      "Iteration 22, loss = 0.08784366\n",
      "Iteration 23, loss = 0.08578615\n",
      "Iteration 24, loss = 0.08325028\n",
      "Iteration 25, loss = 0.07876874\n",
      "Iteration 26, loss = 0.07766325\n",
      "Iteration 27, loss = 0.07591075\n",
      "Iteration 28, loss = 0.07355976\n",
      "Iteration 29, loss = 0.07123114\n",
      "Iteration 30, loss = 0.07008472\n",
      "Iteration 31, loss = 0.06907511\n",
      "Iteration 32, loss = 0.06820885\n",
      "Iteration 33, loss = 0.06561688\n",
      "Iteration 34, loss = 0.06431681\n",
      "Iteration 35, loss = 0.06415948\n",
      "Iteration 36, loss = 0.06377539\n",
      "Iteration 37, loss = 0.06261782\n",
      "Iteration 38, loss = 0.06205543\n",
      "Iteration 39, loss = 0.06076347\n",
      "Iteration 40, loss = 0.06017807\n",
      "Iteration 41, loss = 0.05957381\n",
      "Iteration 42, loss = 0.06003697\n",
      "Iteration 43, loss = 0.05788151\n",
      "Iteration 44, loss = 0.05800829\n",
      "Iteration 45, loss = 0.05702069\n",
      "Iteration 46, loss = 0.05760065\n",
      "Iteration 47, loss = 0.05582179\n",
      "Iteration 48, loss = 0.05539516\n",
      "Iteration 49, loss = 0.05487229\n",
      "Iteration 50, loss = 0.05476956\n",
      "Iteration 51, loss = 0.05753056\n",
      "Iteration 52, loss = 0.05659410\n",
      "Iteration 53, loss = 0.05344182\n",
      "Iteration 54, loss = 0.05253299\n",
      "Iteration 55, loss = 0.05250296\n",
      "Iteration 56, loss = 0.05372534\n",
      "Iteration 57, loss = 0.05637271\n",
      "Iteration 58, loss = 0.05825896\n",
      "Iteration 59, loss = 0.05471113\n",
      "Iteration 60, loss = 0.05152925\n",
      "Iteration 61, loss = 0.05078051\n",
      "Iteration 62, loss = 0.05046137\n",
      "Iteration 63, loss = 0.05027338\n",
      "Iteration 64, loss = 0.05205567\n",
      "Iteration 65, loss = 0.05547678\n",
      "Iteration 66, loss = 0.05140613\n",
      "Iteration 67, loss = 0.05021942\n",
      "Iteration 68, loss = 0.05088369\n",
      "Iteration 69, loss = 0.05036660\n",
      "Iteration 70, loss = 0.05046737\n",
      "Iteration 71, loss = 0.05177288\n",
      "Iteration 72, loss = 0.05460162\n",
      "Iteration 73, loss = 0.05370772\n",
      "Iteration 74, loss = 0.05010208\n",
      "Iteration 75, loss = 0.04878572\n",
      "Iteration 76, loss = 0.04893551\n",
      "Iteration 77, loss = 0.04835257\n",
      "Iteration 78, loss = 0.04906153\n",
      "Iteration 79, loss = 0.05397533\n",
      "Iteration 80, loss = 0.05378141\n",
      "Iteration 81, loss = 0.05069190\n",
      "Iteration 82, loss = 0.04852016\n",
      "Iteration 83, loss = 0.04825090\n",
      "Iteration 84, loss = 0.04772447\n",
      "Iteration 85, loss = 0.04795883\n",
      "Iteration 86, loss = 0.04817062\n",
      "Iteration 87, loss = 0.05939860\n",
      "Iteration 88, loss = 0.05266935\n",
      "Iteration 89, loss = 0.04855276\n",
      "Iteration 90, loss = 0.04832159\n",
      "Iteration 91, loss = 0.04783425\n",
      "Iteration 92, loss = 0.04728624\n",
      "Iteration 93, loss = 0.04718295\n",
      "Iteration 94, loss = 0.04848143\n",
      "Iteration 95, loss = 0.04897588\n",
      "Iteration 96, loss = 0.04987311\n",
      "Iteration 97, loss = 0.05337965\n",
      "Iteration 98, loss = 0.04946534\n",
      "Iteration 99, loss = 0.04786191\n",
      "Iteration 100, loss = 0.04716943\n",
      "Iteration 101, loss = 0.04739412\n",
      "Iteration 102, loss = 0.05022807\n",
      "Iteration 103, loss = 0.06187876\n",
      "Iteration 104, loss = 0.04821635\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.01, batch_size=200, hidden_layer_sizes=(100, 100, 100), solver=adam; total time= 1.4min\n",
      "Iteration 1, loss = 1.84685544\n",
      "Iteration 2, loss = 0.75148977\n",
      "Iteration 3, loss = 0.43238949\n",
      "Iteration 4, loss = 0.33245370\n",
      "Iteration 5, loss = 0.27752350\n",
      "Iteration 6, loss = 0.24154704\n",
      "Iteration 7, loss = 0.21397269\n",
      "Iteration 8, loss = 0.19375650\n",
      "Iteration 9, loss = 0.17511924\n",
      "Iteration 10, loss = 0.16255321\n",
      "Iteration 11, loss = 0.14954860\n",
      "Iteration 12, loss = 0.13915570\n",
      "Iteration 13, loss = 0.13021327\n",
      "Iteration 14, loss = 0.12411063\n",
      "Iteration 15, loss = 0.11691338\n",
      "Iteration 16, loss = 0.11148906\n",
      "Iteration 17, loss = 0.10552346\n",
      "Iteration 18, loss = 0.10156386\n",
      "Iteration 19, loss = 0.09785911\n",
      "Iteration 20, loss = 0.09454904\n",
      "Iteration 21, loss = 0.08923412\n",
      "Iteration 22, loss = 0.08536447\n",
      "Iteration 23, loss = 0.08382046\n",
      "Iteration 24, loss = 0.07875934\n",
      "Iteration 25, loss = 0.07773975\n",
      "Iteration 26, loss = 0.07589868\n",
      "Iteration 27, loss = 0.07364322\n",
      "Iteration 28, loss = 0.07137931\n",
      "Iteration 29, loss = 0.06954189\n",
      "Iteration 30, loss = 0.06849828\n",
      "Iteration 31, loss = 0.06692429\n",
      "Iteration 32, loss = 0.06569831\n",
      "Iteration 33, loss = 0.06377973\n",
      "Iteration 34, loss = 0.06300459\n",
      "Iteration 35, loss = 0.06377675\n",
      "Iteration 36, loss = 0.06157188\n",
      "Iteration 37, loss = 0.06034767\n",
      "Iteration 38, loss = 0.06016029\n",
      "Iteration 39, loss = 0.05919899\n",
      "Iteration 40, loss = 0.05822721\n",
      "Iteration 41, loss = 0.05888929\n",
      "Iteration 42, loss = 0.05753349\n",
      "Iteration 43, loss = 0.05637632\n",
      "Iteration 44, loss = 0.05590299\n",
      "Iteration 45, loss = 0.05567228\n",
      "Iteration 46, loss = 0.05563821\n",
      "Iteration 47, loss = 0.05621831\n",
      "Iteration 48, loss = 0.05544897\n",
      "Iteration 49, loss = 0.05591285\n",
      "Iteration 50, loss = 0.05501306\n",
      "Iteration 51, loss = 0.05408848\n",
      "Iteration 52, loss = 0.05255683\n",
      "Iteration 53, loss = 0.05228745\n",
      "Iteration 54, loss = 0.05503880\n",
      "Iteration 55, loss = 0.05404661\n",
      "Iteration 56, loss = 0.05218925\n",
      "Iteration 57, loss = 0.05357668\n",
      "Iteration 58, loss = 0.05239948\n",
      "Iteration 59, loss = 0.05131752\n",
      "Iteration 60, loss = 0.05191686\n",
      "Iteration 61, loss = 0.05172375\n",
      "Iteration 62, loss = 0.05257169\n",
      "Iteration 63, loss = 0.05313421\n",
      "Iteration 64, loss = 0.05038449\n",
      "Iteration 65, loss = 0.05014010\n",
      "Iteration 66, loss = 0.05030813\n",
      "Iteration 67, loss = 0.05066842\n",
      "Iteration 68, loss = 0.05481215\n",
      "Iteration 69, loss = 0.05222528\n",
      "Iteration 70, loss = 0.04915035\n",
      "Iteration 71, loss = 0.04876887\n",
      "Iteration 72, loss = 0.04840233\n",
      "Iteration 73, loss = 0.04937009\n",
      "Iteration 74, loss = 0.05078802\n",
      "Iteration 75, loss = 0.04878221\n",
      "Iteration 76, loss = 0.04875777\n",
      "Iteration 77, loss = 0.05284017\n",
      "Iteration 78, loss = 0.05172449\n",
      "Iteration 79, loss = 0.04891884\n",
      "Iteration 80, loss = 0.04787695\n",
      "Iteration 81, loss = 0.04816970\n",
      "Iteration 82, loss = 0.04819680\n",
      "Iteration 83, loss = 0.04790544\n",
      "Iteration 84, loss = 0.04908468\n",
      "Iteration 85, loss = 0.05354239\n",
      "Iteration 86, loss = 0.05007547\n",
      "Iteration 87, loss = 0.04860813\n",
      "Iteration 88, loss = 0.04743264\n",
      "Iteration 89, loss = 0.04748563\n",
      "Iteration 90, loss = 0.04661450\n",
      "Iteration 91, loss = 0.04749469\n",
      "Iteration 92, loss = 0.04773837\n",
      "Iteration 93, loss = 0.05001027\n",
      "Iteration 94, loss = 0.05282048\n",
      "Iteration 95, loss = 0.04875673\n",
      "Iteration 96, loss = 0.04697551\n",
      "Iteration 97, loss = 0.04755042\n",
      "Iteration 98, loss = 0.04833958\n",
      "Iteration 99, loss = 0.04911266\n",
      "Iteration 100, loss = 0.04783701\n",
      "Iteration 101, loss = 0.04660528\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.01, batch_size=200, hidden_layer_sizes=(100, 100, 100), solver=adam; total time= 1.4min\n",
      "Iteration 1, loss = 1.81620135\n",
      "Iteration 2, loss = 0.71712392\n",
      "Iteration 3, loss = 0.42197352\n",
      "Iteration 4, loss = 0.33070699\n",
      "Iteration 5, loss = 0.27756908\n",
      "Iteration 6, loss = 0.23904665\n",
      "Iteration 7, loss = 0.21135568\n",
      "Iteration 8, loss = 0.18870805\n",
      "Iteration 9, loss = 0.17052618\n",
      "Iteration 10, loss = 0.15761084\n",
      "Iteration 11, loss = 0.14603540\n",
      "Iteration 12, loss = 0.13372124\n",
      "Iteration 13, loss = 0.12709822\n",
      "Iteration 14, loss = 0.11898470\n",
      "Iteration 15, loss = 0.11369896\n",
      "Iteration 16, loss = 0.10806790\n",
      "Iteration 17, loss = 0.10362874\n",
      "Iteration 18, loss = 0.09842109\n",
      "Iteration 19, loss = 0.09407440\n",
      "Iteration 20, loss = 0.09065367\n",
      "Iteration 21, loss = 0.08668836\n",
      "Iteration 22, loss = 0.08511562\n",
      "Iteration 23, loss = 0.08131789\n",
      "Iteration 24, loss = 0.07864248\n",
      "Iteration 25, loss = 0.07846104\n",
      "Iteration 26, loss = 0.07531236\n",
      "Iteration 27, loss = 0.07283930\n",
      "Iteration 28, loss = 0.07069152\n",
      "Iteration 29, loss = 0.07077894\n",
      "Iteration 30, loss = 0.06856612\n",
      "Iteration 31, loss = 0.06670277\n",
      "Iteration 32, loss = 0.06498361\n",
      "Iteration 33, loss = 0.06505843\n",
      "Iteration 34, loss = 0.06323417\n",
      "Iteration 35, loss = 0.06211849\n",
      "Iteration 36, loss = 0.06076094\n",
      "Iteration 37, loss = 0.06049155\n",
      "Iteration 38, loss = 0.06037132\n",
      "Iteration 39, loss = 0.05895897\n",
      "Iteration 40, loss = 0.05888169\n",
      "Iteration 41, loss = 0.05853151\n",
      "Iteration 42, loss = 0.05849288\n",
      "Iteration 43, loss = 0.05680506\n",
      "Iteration 44, loss = 0.05565298\n",
      "Iteration 45, loss = 0.05456645\n",
      "Iteration 46, loss = 0.05432937\n",
      "Iteration 47, loss = 0.05636026\n",
      "Iteration 48, loss = 0.05486457\n",
      "Iteration 49, loss = 0.05483226\n",
      "Iteration 50, loss = 0.05402118\n",
      "Iteration 51, loss = 0.05436419\n",
      "Iteration 52, loss = 0.05414366\n",
      "Iteration 53, loss = 0.05346575\n",
      "Iteration 54, loss = 0.05283890\n",
      "Iteration 55, loss = 0.05191007\n",
      "Iteration 56, loss = 0.05184715\n",
      "Iteration 57, loss = 0.05271411\n",
      "Iteration 58, loss = 0.05263838\n",
      "Iteration 59, loss = 0.05344655\n",
      "Iteration 60, loss = 0.05264873\n",
      "Iteration 61, loss = 0.05173693\n",
      "Iteration 62, loss = 0.05094549\n",
      "Iteration 63, loss = 0.05066973\n",
      "Iteration 64, loss = 0.05098786\n",
      "Iteration 65, loss = 0.05013070\n",
      "Iteration 66, loss = 0.05017995\n",
      "Iteration 67, loss = 0.05062642\n",
      "Iteration 68, loss = 0.05092959\n",
      "Iteration 69, loss = 0.05035984\n",
      "Iteration 70, loss = 0.04916494\n",
      "Iteration 71, loss = 0.04914968\n",
      "Iteration 72, loss = 0.04895097\n",
      "Iteration 73, loss = 0.04886516\n",
      "Iteration 74, loss = 0.05073093\n",
      "Iteration 75, loss = 0.04961374\n",
      "Iteration 76, loss = 0.05346032\n",
      "Iteration 77, loss = 0.05059094\n",
      "Iteration 78, loss = 0.04807750\n",
      "Iteration 79, loss = 0.04759778\n",
      "Iteration 80, loss = 0.04830844\n",
      "Iteration 81, loss = 0.04786025\n",
      "Iteration 82, loss = 0.04833422\n",
      "Iteration 83, loss = 0.04967182\n",
      "Iteration 84, loss = 0.04947838\n",
      "Iteration 85, loss = 0.04833531\n",
      "Iteration 86, loss = 0.04754300\n",
      "Iteration 87, loss = 0.04882457\n",
      "Iteration 88, loss = 0.05148318\n",
      "Iteration 89, loss = 0.04791224\n",
      "Iteration 90, loss = 0.04723828\n",
      "Iteration 91, loss = 0.04840349\n",
      "Iteration 92, loss = 0.05374861\n",
      "Iteration 93, loss = 0.04788115\n",
      "Iteration 94, loss = 0.04643212\n",
      "Iteration 95, loss = 0.04611970\n",
      "Iteration 96, loss = 0.04608656\n",
      "Iteration 97, loss = 0.04739510\n",
      "Iteration 98, loss = 0.05824276\n",
      "Iteration 99, loss = 0.05178815\n",
      "Iteration 100, loss = 0.04744981\n",
      "Iteration 101, loss = 0.04590680\n",
      "Iteration 102, loss = 0.04555542\n",
      "Iteration 103, loss = 0.04551474\n",
      "Iteration 104, loss = 0.04574247\n",
      "Iteration 105, loss = 0.04655702\n",
      "Iteration 106, loss = 0.04959164\n",
      "Iteration 107, loss = 0.04996121\n",
      "Iteration 108, loss = 0.04712728\n",
      "Iteration 109, loss = 0.04562411\n",
      "Iteration 110, loss = 0.04522582\n",
      "Iteration 111, loss = 0.04578146\n",
      "Iteration 112, loss = 0.04681857\n",
      "Iteration 113, loss = 0.05017725\n",
      "Iteration 114, loss = 0.05621029\n",
      "Iteration 115, loss = 0.04824870\n",
      "Iteration 116, loss = 0.04571936\n",
      "Iteration 117, loss = 0.04499234\n",
      "Iteration 118, loss = 0.04527729\n",
      "Iteration 119, loss = 0.04529998\n",
      "Iteration 120, loss = 0.04483850\n",
      "Iteration 121, loss = 0.04564874\n",
      "Iteration 122, loss = 0.04541521\n",
      "Iteration 123, loss = 0.05017337\n",
      "Iteration 124, loss = 0.05728006\n",
      "Iteration 125, loss = 0.04682210\n",
      "Iteration 126, loss = 0.04550642\n",
      "Iteration 127, loss = 0.04503132\n",
      "Iteration 128, loss = 0.04457633\n",
      "Iteration 129, loss = 0.04502957\n",
      "Iteration 130, loss = 0.04891770\n",
      "Iteration 131, loss = 0.04534390\n",
      "Iteration 132, loss = 0.04491654\n",
      "Iteration 133, loss = 0.04636233\n",
      "Iteration 134, loss = 0.04813337\n",
      "Iteration 135, loss = 0.04771516\n",
      "Iteration 136, loss = 0.04853472\n",
      "Iteration 137, loss = 0.04803227\n",
      "Iteration 138, loss = 0.04507967\n",
      "Iteration 139, loss = 0.04481630\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.01, batch_size=200, hidden_layer_sizes=(100, 100, 100), solver=adam; total time= 1.9min\n",
      "Iteration 1, loss = 2.32160346\n",
      "Iteration 2, loss = 2.30479031\n",
      "Iteration 3, loss = 2.30466092\n",
      "Iteration 4, loss = 2.30481520\n",
      "Iteration 5, loss = 2.30472904\n",
      "Iteration 6, loss = 2.30448959\n",
      "Iteration 7, loss = 2.30462201\n",
      "Iteration 8, loss = 2.30450559\n",
      "Iteration 9, loss = 2.30442958\n",
      "Iteration 10, loss = 2.30447905\n",
      "Iteration 11, loss = 2.30449590\n",
      "Iteration 12, loss = 2.30433374\n",
      "Iteration 13, loss = 2.30431854\n",
      "Iteration 14, loss = 2.30437609\n",
      "Iteration 15, loss = 2.30426114\n",
      "Iteration 16, loss = 2.30422720\n",
      "Iteration 17, loss = 2.30404811\n",
      "Iteration 18, loss = 2.30415885\n",
      "Iteration 19, loss = 2.30401310\n",
      "Iteration 20, loss = 2.30403162\n",
      "Iteration 21, loss = 2.30397988\n",
      "Iteration 22, loss = 2.30395678\n",
      "Iteration 23, loss = 2.30380783\n",
      "Iteration 24, loss = 2.30388486\n",
      "Iteration 25, loss = 2.30387242\n",
      "Iteration 26, loss = 2.30374655\n",
      "Iteration 27, loss = 2.30365882\n",
      "Iteration 28, loss = 2.30369528\n",
      "Iteration 29, loss = 2.30349913\n",
      "Iteration 30, loss = 2.30349436\n",
      "Iteration 31, loss = 2.30359541\n",
      "Iteration 32, loss = 2.30335786\n",
      "Iteration 33, loss = 2.30338116\n",
      "Iteration 34, loss = 2.30331855\n",
      "Iteration 35, loss = 2.30325400\n",
      "Iteration 36, loss = 2.30323394\n",
      "Iteration 37, loss = 2.30313689\n",
      "Iteration 38, loss = 2.30315826\n",
      "Iteration 39, loss = 2.30299658\n",
      "Iteration 40, loss = 2.30290093\n",
      "Iteration 41, loss = 2.30294496\n",
      "Iteration 42, loss = 2.30291936\n",
      "Iteration 43, loss = 2.30286031\n",
      "Iteration 44, loss = 2.30259421\n",
      "Iteration 45, loss = 2.30261635\n",
      "Iteration 46, loss = 2.30260654\n",
      "Iteration 47, loss = 2.30260686\n",
      "Iteration 48, loss = 2.30244658\n",
      "Iteration 49, loss = 2.30231265\n",
      "Iteration 50, loss = 2.30233022\n",
      "Iteration 51, loss = 2.30230582\n",
      "Iteration 52, loss = 2.30211666\n",
      "Iteration 53, loss = 2.30207467\n",
      "Iteration 54, loss = 2.30197157\n",
      "Iteration 55, loss = 2.30182736\n",
      "Iteration 56, loss = 2.30173912\n",
      "Iteration 57, loss = 2.30174173\n",
      "Iteration 58, loss = 2.30155628\n",
      "Iteration 59, loss = 2.30144183\n",
      "Iteration 60, loss = 2.30142382\n",
      "Iteration 61, loss = 2.30125687\n",
      "Iteration 62, loss = 2.30129526\n",
      "Iteration 63, loss = 2.30119547\n",
      "Iteration 64, loss = 2.30109324\n",
      "Iteration 65, loss = 2.30091025\n",
      "Iteration 66, loss = 2.30071009\n",
      "Iteration 67, loss = 2.30066232\n",
      "Iteration 68, loss = 2.30062890\n",
      "Iteration 69, loss = 2.30043746\n",
      "Iteration 70, loss = 2.30021494\n",
      "Iteration 71, loss = 2.30015825\n",
      "Iteration 72, loss = 2.30001876\n",
      "Iteration 73, loss = 2.29985379\n",
      "Iteration 74, loss = 2.29973398\n",
      "Iteration 75, loss = 2.29956066\n",
      "Iteration 76, loss = 2.29945827\n",
      "Iteration 77, loss = 2.29932336\n",
      "Iteration 78, loss = 2.29903963\n",
      "Iteration 79, loss = 2.29899256\n",
      "Iteration 80, loss = 2.29877743\n",
      "Iteration 81, loss = 2.29851314\n",
      "Iteration 82, loss = 2.29825980\n",
      "Iteration 83, loss = 2.29808846\n",
      "Iteration 84, loss = 2.29790448\n",
      "Iteration 85, loss = 2.29762794\n",
      "Iteration 86, loss = 2.29732720\n",
      "Iteration 87, loss = 2.29711749\n",
      "Iteration 88, loss = 2.29685138\n",
      "Iteration 89, loss = 2.29656447\n",
      "Iteration 90, loss = 2.29619964\n",
      "Iteration 91, loss = 2.29596908\n",
      "Iteration 92, loss = 2.29567781\n",
      "Iteration 93, loss = 2.29534643\n",
      "Iteration 94, loss = 2.29495719\n",
      "Iteration 95, loss = 2.29462734\n",
      "Iteration 96, loss = 2.29412574\n",
      "Iteration 97, loss = 2.29380796\n",
      "Iteration 98, loss = 2.29339094\n",
      "Iteration 99, loss = 2.29289367\n",
      "Iteration 100, loss = 2.29240577\n",
      "Iteration 101, loss = 2.29181460\n",
      "Iteration 102, loss = 2.29143997\n",
      "Iteration 103, loss = 2.29078047\n",
      "Iteration 104, loss = 2.29014767\n",
      "Iteration 105, loss = 2.28937184\n",
      "Iteration 106, loss = 2.28865548\n",
      "Iteration 107, loss = 2.28781716\n",
      "Iteration 108, loss = 2.28700315\n",
      "Iteration 109, loss = 2.28620428\n",
      "Iteration 110, loss = 2.28523368\n",
      "Iteration 111, loss = 2.28416823\n",
      "Iteration 112, loss = 2.28309118\n",
      "Iteration 113, loss = 2.28182743\n",
      "Iteration 114, loss = 2.28046935\n",
      "Iteration 115, loss = 2.27905241\n",
      "Iteration 116, loss = 2.27740250\n",
      "Iteration 117, loss = 2.27553679\n",
      "Iteration 118, loss = 2.27378065\n",
      "Iteration 119, loss = 2.27149056\n",
      "Iteration 120, loss = 2.26902767\n",
      "Iteration 121, loss = 2.26650004\n",
      "Iteration 122, loss = 2.26352484\n",
      "Iteration 123, loss = 2.26011679\n",
      "Iteration 124, loss = 2.25628532\n",
      "Iteration 125, loss = 2.25192099\n",
      "Iteration 126, loss = 2.24721996\n",
      "Iteration 127, loss = 2.24154352\n",
      "Iteration 128, loss = 2.23502845\n",
      "Iteration 129, loss = 2.22763060\n",
      "Iteration 130, loss = 2.21909359\n",
      "Iteration 131, loss = 2.20896877\n",
      "Iteration 132, loss = 2.19737924\n",
      "Iteration 133, loss = 2.18388040\n",
      "Iteration 134, loss = 2.16821189\n",
      "Iteration 135, loss = 2.15009469\n",
      "Iteration 136, loss = 2.12922973\n",
      "Iteration 137, loss = 2.10576753\n",
      "Iteration 138, loss = 2.07969076\n",
      "Iteration 139, loss = 2.05134113\n",
      "Iteration 140, loss = 2.02144243\n",
      "Iteration 141, loss = 1.99067149\n",
      "Iteration 142, loss = 1.96005494\n",
      "Iteration 143, loss = 1.93033526\n",
      "Iteration 144, loss = 1.90227115\n",
      "Iteration 145, loss = 1.87616624\n",
      "Iteration 146, loss = 1.85217774\n",
      "Iteration 147, loss = 1.83043037\n",
      "Iteration 148, loss = 1.81062063\n",
      "Iteration 149, loss = 1.79262533\n",
      "Iteration 150, loss = 1.77616341\n",
      "Iteration 151, loss = 1.76111765\n",
      "Iteration 152, loss = 1.74692545\n",
      "Iteration 153, loss = 1.73378470\n",
      "Iteration 154, loss = 1.72134151\n",
      "Iteration 155, loss = 1.70936234\n",
      "Iteration 156, loss = 1.69756518\n",
      "Iteration 157, loss = 1.68601958\n",
      "Iteration 158, loss = 1.67461223\n",
      "Iteration 159, loss = 1.66293258\n",
      "Iteration 160, loss = 1.65112985\n",
      "Iteration 161, loss = 1.63899341\n",
      "Iteration 162, loss = 1.62635873\n",
      "Iteration 163, loss = 1.61319147\n",
      "Iteration 164, loss = 1.59936766\n",
      "Iteration 165, loss = 1.58485349\n",
      "Iteration 166, loss = 1.56954184\n",
      "Iteration 167, loss = 1.55313636\n",
      "Iteration 168, loss = 1.53599388\n",
      "Iteration 169, loss = 1.51785087\n",
      "Iteration 170, loss = 1.49882110\n",
      "Iteration 171, loss = 1.47908229\n",
      "Iteration 172, loss = 1.45872414\n",
      "Iteration 173, loss = 1.43775794\n",
      "Iteration 174, loss = 1.41662405\n",
      "Iteration 175, loss = 1.39517185\n",
      "Iteration 176, loss = 1.37390401\n",
      "Iteration 177, loss = 1.35270364\n",
      "Iteration 178, loss = 1.33189892\n",
      "Iteration 179, loss = 1.31135397\n",
      "Iteration 180, loss = 1.29152556\n",
      "Iteration 181, loss = 1.27190906\n",
      "Iteration 182, loss = 1.25294822\n",
      "Iteration 183, loss = 1.23447321\n",
      "Iteration 184, loss = 1.21648251\n",
      "Iteration 185, loss = 1.19890405\n",
      "Iteration 186, loss = 1.18200246\n",
      "Iteration 187, loss = 1.16520984\n",
      "Iteration 188, loss = 1.14920457\n",
      "Iteration 189, loss = 1.13326232\n",
      "Iteration 190, loss = 1.11777300\n",
      "Iteration 191, loss = 1.10264859\n",
      "Iteration 192, loss = 1.08767892\n",
      "Iteration 193, loss = 1.07306063\n",
      "Iteration 194, loss = 1.05855764\n",
      "Iteration 195, loss = 1.04443895\n",
      "Iteration 196, loss = 1.03037395\n",
      "Iteration 197, loss = 1.01670385\n",
      "Iteration 198, loss = 1.00315705\n",
      "Iteration 199, loss = 0.98985100\n",
      "Iteration 200, loss = 0.97694619\n",
      "[CV] END activation=logistic, alpha=0.01, batch_size=200, hidden_layer_sizes=(100, 100, 100), solver=sgd; total time= 2.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.31353394\n",
      "Iteration 2, loss = 2.30460291\n",
      "Iteration 3, loss = 2.30450926\n",
      "Iteration 4, loss = 2.30454597\n",
      "Iteration 5, loss = 2.30450952\n",
      "Iteration 6, loss = 2.30436980\n",
      "Iteration 7, loss = 2.30436432\n",
      "Iteration 8, loss = 2.30434039\n",
      "Iteration 9, loss = 2.30432127\n",
      "Iteration 10, loss = 2.30428647\n",
      "Iteration 11, loss = 2.30418632\n",
      "Iteration 12, loss = 2.30406077\n",
      "Iteration 13, loss = 2.30402321\n",
      "Iteration 14, loss = 2.30395485\n",
      "Iteration 15, loss = 2.30393584\n",
      "Iteration 16, loss = 2.30382964\n",
      "Iteration 17, loss = 2.30382123\n",
      "Iteration 18, loss = 2.30376183\n",
      "Iteration 19, loss = 2.30364702\n",
      "Iteration 20, loss = 2.30359374\n",
      "Iteration 21, loss = 2.30356786\n",
      "Iteration 22, loss = 2.30348524\n",
      "Iteration 23, loss = 2.30347466\n",
      "Iteration 24, loss = 2.30338401\n",
      "Iteration 25, loss = 2.30341246\n",
      "Iteration 26, loss = 2.30325021\n",
      "Iteration 27, loss = 2.30307975\n",
      "Iteration 28, loss = 2.30295801\n",
      "Iteration 29, loss = 2.30303758\n",
      "Iteration 30, loss = 2.30291844\n",
      "Iteration 31, loss = 2.30286842\n",
      "Iteration 32, loss = 2.30286221\n",
      "Iteration 33, loss = 2.30270290\n",
      "Iteration 34, loss = 2.30257335\n",
      "Iteration 35, loss = 2.30259727\n",
      "Iteration 36, loss = 2.30236053\n",
      "Iteration 37, loss = 2.30234247\n",
      "Iteration 38, loss = 2.30229343\n",
      "Iteration 39, loss = 2.30212973\n",
      "Iteration 40, loss = 2.30201784\n",
      "Iteration 41, loss = 2.30195638\n",
      "Iteration 42, loss = 2.30194893\n",
      "Iteration 43, loss = 2.30180263\n",
      "Iteration 44, loss = 2.30172807\n",
      "Iteration 45, loss = 2.30160042\n",
      "Iteration 46, loss = 2.30144451\n",
      "Iteration 47, loss = 2.30128060\n",
      "Iteration 48, loss = 2.30128866\n",
      "Iteration 49, loss = 2.30105081\n",
      "Iteration 50, loss = 2.30093774\n",
      "Iteration 51, loss = 2.30081334\n",
      "Iteration 52, loss = 2.30059640\n",
      "Iteration 53, loss = 2.30037076\n",
      "Iteration 54, loss = 2.30028073\n",
      "Iteration 55, loss = 2.30015731\n",
      "Iteration 56, loss = 2.29992867\n",
      "Iteration 57, loss = 2.29979497\n",
      "Iteration 58, loss = 2.29954952\n",
      "Iteration 59, loss = 2.29945981\n",
      "Iteration 60, loss = 2.29919315\n",
      "Iteration 61, loss = 2.29894043\n",
      "Iteration 62, loss = 2.29869844\n",
      "Iteration 63, loss = 2.29850043\n",
      "Iteration 64, loss = 2.29827900\n",
      "Iteration 65, loss = 2.29801127\n",
      "Iteration 66, loss = 2.29764258\n",
      "Iteration 67, loss = 2.29734228\n",
      "Iteration 68, loss = 2.29710875\n",
      "Iteration 69, loss = 2.29667198\n",
      "Iteration 70, loss = 2.29639864\n",
      "Iteration 71, loss = 2.29594887\n",
      "Iteration 72, loss = 2.29560667\n",
      "Iteration 73, loss = 2.29512636\n",
      "Iteration 74, loss = 2.29482992\n",
      "Iteration 75, loss = 2.29429266\n",
      "Iteration 76, loss = 2.29374670\n",
      "Iteration 77, loss = 2.29321868\n",
      "Iteration 78, loss = 2.29253942\n",
      "Iteration 79, loss = 2.29182407\n",
      "Iteration 80, loss = 2.29110247\n",
      "Iteration 81, loss = 2.29044425\n",
      "Iteration 82, loss = 2.28956333\n",
      "Iteration 83, loss = 2.28869077\n",
      "Iteration 84, loss = 2.28777353\n",
      "Iteration 85, loss = 2.28663287\n",
      "Iteration 86, loss = 2.28546758\n",
      "Iteration 87, loss = 2.28407374\n",
      "Iteration 88, loss = 2.28262225\n",
      "Iteration 89, loss = 2.28094410\n",
      "Iteration 90, loss = 2.27932124\n",
      "Iteration 91, loss = 2.27728523\n",
      "Iteration 92, loss = 2.27506835\n",
      "Iteration 93, loss = 2.27250900\n",
      "Iteration 94, loss = 2.26978101\n",
      "Iteration 95, loss = 2.26656403\n",
      "Iteration 96, loss = 2.26301406\n",
      "Iteration 97, loss = 2.25874520\n",
      "Iteration 98, loss = 2.25405219\n",
      "Iteration 99, loss = 2.24866802\n",
      "Iteration 100, loss = 2.24242076\n",
      "Iteration 101, loss = 2.23497978\n",
      "Iteration 102, loss = 2.22650076\n",
      "Iteration 103, loss = 2.21672302\n",
      "Iteration 104, loss = 2.20510654\n",
      "Iteration 105, loss = 2.19168491\n",
      "Iteration 106, loss = 2.17611557\n",
      "Iteration 107, loss = 2.15822619\n",
      "Iteration 108, loss = 2.13781867\n",
      "Iteration 109, loss = 2.11472699\n",
      "Iteration 110, loss = 2.08942543\n",
      "Iteration 111, loss = 2.06208919\n",
      "Iteration 112, loss = 2.03346459\n",
      "Iteration 113, loss = 2.00418696\n",
      "Iteration 114, loss = 1.97514395\n",
      "Iteration 115, loss = 1.94682944\n",
      "Iteration 116, loss = 1.91986752\n",
      "Iteration 117, loss = 1.89461927\n",
      "Iteration 118, loss = 1.87118256\n",
      "Iteration 119, loss = 1.84974677\n",
      "Iteration 120, loss = 1.83007437\n",
      "Iteration 121, loss = 1.81234792\n",
      "Iteration 122, loss = 1.79625347\n",
      "Iteration 123, loss = 1.78152173\n",
      "Iteration 124, loss = 1.76822504\n",
      "Iteration 125, loss = 1.75608207\n",
      "Iteration 126, loss = 1.74487935\n",
      "Iteration 127, loss = 1.73467040\n",
      "Iteration 128, loss = 1.72503807\n",
      "Iteration 129, loss = 1.71610225\n",
      "Iteration 130, loss = 1.70771671\n",
      "Iteration 131, loss = 1.69977612\n",
      "Iteration 132, loss = 1.69224719\n",
      "Iteration 133, loss = 1.68497701\n",
      "Iteration 134, loss = 1.67817317\n",
      "Iteration 135, loss = 1.67144308\n",
      "Iteration 136, loss = 1.66499248\n",
      "Iteration 137, loss = 1.65850025\n",
      "Iteration 138, loss = 1.65255145\n",
      "Iteration 139, loss = 1.64645181\n",
      "Iteration 140, loss = 1.64063937\n",
      "Iteration 141, loss = 1.63480474\n",
      "Iteration 142, loss = 1.62911641\n",
      "Iteration 143, loss = 1.62342333\n",
      "Iteration 144, loss = 1.61813964\n",
      "Iteration 145, loss = 1.61264734\n",
      "Iteration 146, loss = 1.60722120\n",
      "Iteration 147, loss = 1.60180104\n",
      "Iteration 148, loss = 1.59653256\n",
      "Iteration 149, loss = 1.59139473\n",
      "Iteration 150, loss = 1.58623498\n",
      "Iteration 151, loss = 1.58104029\n",
      "Iteration 152, loss = 1.57584463\n",
      "Iteration 153, loss = 1.57068098\n",
      "Iteration 154, loss = 1.56569286\n",
      "Iteration 155, loss = 1.56040298\n",
      "Iteration 156, loss = 1.55539628\n",
      "Iteration 157, loss = 1.55008839\n",
      "Iteration 158, loss = 1.54500380\n",
      "Iteration 159, loss = 1.53983295\n",
      "Iteration 160, loss = 1.53461270\n",
      "Iteration 161, loss = 1.52912243\n",
      "Iteration 162, loss = 1.52397052\n",
      "Iteration 163, loss = 1.51849107\n",
      "Iteration 164, loss = 1.51294037\n",
      "Iteration 165, loss = 1.50731169\n",
      "Iteration 166, loss = 1.50154533\n",
      "Iteration 167, loss = 1.49570595\n",
      "Iteration 168, loss = 1.48974413\n",
      "Iteration 169, loss = 1.48352243\n",
      "Iteration 170, loss = 1.47715441\n",
      "Iteration 171, loss = 1.47049018\n",
      "Iteration 172, loss = 1.46371197\n",
      "Iteration 173, loss = 1.45658011\n",
      "Iteration 174, loss = 1.44927765\n",
      "Iteration 175, loss = 1.44139831\n",
      "Iteration 176, loss = 1.43332208\n",
      "Iteration 177, loss = 1.42480143\n",
      "Iteration 178, loss = 1.41584474\n",
      "Iteration 179, loss = 1.40647667\n",
      "Iteration 180, loss = 1.39648362\n",
      "Iteration 181, loss = 1.38602671\n",
      "Iteration 182, loss = 1.37491366\n",
      "Iteration 183, loss = 1.36335826\n",
      "Iteration 184, loss = 1.35126297\n",
      "Iteration 185, loss = 1.33881215\n",
      "Iteration 186, loss = 1.32569474\n",
      "Iteration 187, loss = 1.31220938\n",
      "Iteration 188, loss = 1.29833553\n",
      "Iteration 189, loss = 1.28438236\n",
      "Iteration 190, loss = 1.27014548\n",
      "Iteration 191, loss = 1.25588778\n",
      "Iteration 192, loss = 1.24151763\n",
      "Iteration 193, loss = 1.22731186\n",
      "Iteration 194, loss = 1.21313527\n",
      "Iteration 195, loss = 1.19913237\n",
      "Iteration 196, loss = 1.18527412\n",
      "Iteration 197, loss = 1.17175494\n",
      "Iteration 198, loss = 1.15853035\n",
      "Iteration 199, loss = 1.14557666\n",
      "Iteration 200, loss = 1.13301110\n",
      "[CV] END activation=logistic, alpha=0.01, batch_size=200, hidden_layer_sizes=(100, 100, 100), solver=sgd; total time= 2.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.31404772\n",
      "Iteration 2, loss = 2.30484650\n",
      "Iteration 3, loss = 2.30483122\n",
      "Iteration 4, loss = 2.30466561\n",
      "Iteration 5, loss = 2.30478212\n",
      "Iteration 6, loss = 2.30461473\n",
      "Iteration 7, loss = 2.30465935\n",
      "Iteration 8, loss = 2.30456062\n",
      "Iteration 9, loss = 2.30452884\n",
      "Iteration 10, loss = 2.30451352\n",
      "Iteration 11, loss = 2.30447687\n",
      "Iteration 12, loss = 2.30439027\n",
      "Iteration 13, loss = 2.30446470\n",
      "Iteration 14, loss = 2.30428807\n",
      "Iteration 15, loss = 2.30426379\n",
      "Iteration 16, loss = 2.30420300\n",
      "Iteration 17, loss = 2.30414916\n",
      "Iteration 18, loss = 2.30412372\n",
      "Iteration 19, loss = 2.30415896\n",
      "Iteration 20, loss = 2.30404614\n",
      "Iteration 21, loss = 2.30396952\n",
      "Iteration 22, loss = 2.30389400\n",
      "Iteration 23, loss = 2.30395419\n",
      "Iteration 24, loss = 2.30382801\n",
      "Iteration 25, loss = 2.30389311\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.01, batch_size=200, hidden_layer_sizes=(100, 100, 100), solver=sgd; total time=  17.7s\n",
      "Iteration 1, loss = 2.31088256\n",
      "Iteration 2, loss = 2.30476797\n",
      "Iteration 3, loss = 2.30472101\n",
      "Iteration 4, loss = 2.30460888\n",
      "Iteration 5, loss = 2.30461284\n",
      "Iteration 6, loss = 2.30466759\n",
      "Iteration 7, loss = 2.30456557\n",
      "Iteration 8, loss = 2.30452951\n",
      "Iteration 9, loss = 2.30449750\n",
      "Iteration 10, loss = 2.30446540\n",
      "Iteration 11, loss = 2.30438192\n",
      "Iteration 12, loss = 2.30421159\n",
      "Iteration 13, loss = 2.30423383\n",
      "Iteration 14, loss = 2.30425734\n",
      "Iteration 15, loss = 2.30427924\n",
      "Iteration 16, loss = 2.30411496\n",
      "Iteration 17, loss = 2.30411947\n",
      "Iteration 18, loss = 2.30399543\n",
      "Iteration 19, loss = 2.30403813\n",
      "Iteration 20, loss = 2.30389320\n",
      "Iteration 21, loss = 2.30389513\n",
      "Iteration 22, loss = 2.30371746\n",
      "Iteration 23, loss = 2.30383957\n",
      "Iteration 24, loss = 2.30369352\n",
      "Iteration 25, loss = 2.30373154\n",
      "Iteration 26, loss = 2.30365541\n",
      "Iteration 27, loss = 2.30360074\n",
      "Iteration 28, loss = 2.30355041\n",
      "Iteration 29, loss = 2.30341829\n",
      "Iteration 30, loss = 2.30348669\n",
      "Iteration 31, loss = 2.30326519\n",
      "Iteration 32, loss = 2.30333241\n",
      "Iteration 33, loss = 2.30336615\n",
      "Iteration 34, loss = 2.30316639\n",
      "Iteration 35, loss = 2.30318674\n",
      "Iteration 36, loss = 2.30307618\n",
      "Iteration 37, loss = 2.30301910\n",
      "Iteration 38, loss = 2.30292453\n",
      "Iteration 39, loss = 2.30290803\n",
      "Iteration 40, loss = 2.30286046\n",
      "Iteration 41, loss = 2.30282726\n",
      "Iteration 42, loss = 2.30262583\n",
      "Iteration 43, loss = 2.30254815\n",
      "Iteration 44, loss = 2.30255609\n",
      "Iteration 45, loss = 2.30235883\n",
      "Iteration 46, loss = 2.30239259\n",
      "Iteration 47, loss = 2.30229864\n",
      "Iteration 48, loss = 2.30222327\n",
      "Iteration 49, loss = 2.30216414\n",
      "Iteration 50, loss = 2.30199391\n",
      "Iteration 51, loss = 2.30197222\n",
      "Iteration 52, loss = 2.30190117\n",
      "Iteration 53, loss = 2.30181327\n",
      "Iteration 54, loss = 2.30171275\n",
      "Iteration 55, loss = 2.30157849\n",
      "Iteration 56, loss = 2.30147078\n",
      "Iteration 57, loss = 2.30134524\n",
      "Iteration 58, loss = 2.30115504\n",
      "Iteration 59, loss = 2.30120851\n",
      "Iteration 60, loss = 2.30110067\n",
      "Iteration 61, loss = 2.30088339\n",
      "Iteration 62, loss = 2.30082092\n",
      "Iteration 63, loss = 2.30069671\n",
      "Iteration 64, loss = 2.30057585\n",
      "Iteration 65, loss = 2.30038473\n",
      "Iteration 66, loss = 2.30026004\n",
      "Iteration 67, loss = 2.30012610\n",
      "Iteration 68, loss = 2.29996160\n",
      "Iteration 69, loss = 2.29989189\n",
      "Iteration 70, loss = 2.29968336\n",
      "Iteration 71, loss = 2.29948034\n",
      "Iteration 72, loss = 2.29924619\n",
      "Iteration 73, loss = 2.29909759\n",
      "Iteration 74, loss = 2.29893425\n",
      "Iteration 75, loss = 2.29867420\n",
      "Iteration 76, loss = 2.29853779\n",
      "Iteration 77, loss = 2.29830416\n",
      "Iteration 78, loss = 2.29799340\n",
      "Iteration 79, loss = 2.29780203\n",
      "Iteration 80, loss = 2.29763766\n",
      "Iteration 81, loss = 2.29740645\n",
      "Iteration 82, loss = 2.29717145\n",
      "Iteration 83, loss = 2.29684754\n",
      "Iteration 84, loss = 2.29654964\n",
      "Iteration 85, loss = 2.29613395\n",
      "Iteration 86, loss = 2.29584295\n",
      "Iteration 87, loss = 2.29555613\n",
      "Iteration 88, loss = 2.29514926\n",
      "Iteration 89, loss = 2.29479874\n",
      "Iteration 90, loss = 2.29441003\n",
      "Iteration 91, loss = 2.29396331\n",
      "Iteration 92, loss = 2.29348516\n",
      "Iteration 93, loss = 2.29305002\n",
      "Iteration 94, loss = 2.29253029\n",
      "Iteration 95, loss = 2.29202931\n",
      "Iteration 96, loss = 2.29135124\n",
      "Iteration 97, loss = 2.29093119\n",
      "Iteration 98, loss = 2.29008692\n",
      "Iteration 99, loss = 2.28940945\n",
      "Iteration 100, loss = 2.28878206\n",
      "Iteration 101, loss = 2.28783042\n",
      "Iteration 102, loss = 2.28695149\n",
      "Iteration 103, loss = 2.28607411\n",
      "Iteration 104, loss = 2.28506705\n",
      "Iteration 105, loss = 2.28403306\n",
      "Iteration 106, loss = 2.28286711\n",
      "Iteration 107, loss = 2.28159261\n",
      "Iteration 108, loss = 2.28010750\n",
      "Iteration 109, loss = 2.27857845\n",
      "Iteration 110, loss = 2.27702081\n",
      "Iteration 111, loss = 2.27511514\n",
      "Iteration 112, loss = 2.27297522\n",
      "Iteration 113, loss = 2.27086054\n",
      "Iteration 114, loss = 2.26826369\n",
      "Iteration 115, loss = 2.26551924\n",
      "Iteration 116, loss = 2.26251003\n",
      "Iteration 117, loss = 2.25897529\n",
      "Iteration 118, loss = 2.25518483\n",
      "Iteration 119, loss = 2.25080581\n",
      "Iteration 120, loss = 2.24596390\n",
      "Iteration 121, loss = 2.24026296\n",
      "Iteration 122, loss = 2.23375742\n",
      "Iteration 123, loss = 2.22669662\n",
      "Iteration 124, loss = 2.21824353\n",
      "Iteration 125, loss = 2.20858558\n",
      "Iteration 126, loss = 2.19762704\n",
      "Iteration 127, loss = 2.18497173\n",
      "Iteration 128, loss = 2.17045862\n",
      "Iteration 129, loss = 2.15398437\n",
      "Iteration 130, loss = 2.13523492\n",
      "Iteration 131, loss = 2.11418567\n",
      "Iteration 132, loss = 2.09116274\n",
      "Iteration 133, loss = 2.06584143\n",
      "Iteration 134, loss = 2.03890899\n",
      "Iteration 135, loss = 2.01088365\n",
      "Iteration 136, loss = 1.98188141\n",
      "Iteration 137, loss = 1.95256945\n",
      "Iteration 138, loss = 1.92344477\n",
      "Iteration 139, loss = 1.89435343\n",
      "Iteration 140, loss = 1.86560661\n",
      "Iteration 141, loss = 1.83714287\n",
      "Iteration 142, loss = 1.80880889\n",
      "Iteration 143, loss = 1.78050432\n",
      "Iteration 144, loss = 1.75196011\n",
      "Iteration 145, loss = 1.72340092\n",
      "Iteration 146, loss = 1.69456876\n",
      "Iteration 147, loss = 1.66554177\n",
      "Iteration 148, loss = 1.63637337\n",
      "Iteration 149, loss = 1.60700287\n",
      "Iteration 150, loss = 1.57793236\n",
      "Iteration 151, loss = 1.54935927\n",
      "Iteration 152, loss = 1.52150272\n",
      "Iteration 153, loss = 1.49461662\n",
      "Iteration 154, loss = 1.46899511\n",
      "Iteration 155, loss = 1.44478513\n",
      "Iteration 156, loss = 1.42190734\n",
      "Iteration 157, loss = 1.40072350\n",
      "Iteration 158, loss = 1.38095003\n",
      "Iteration 159, loss = 1.36258444\n",
      "Iteration 160, loss = 1.34557029\n",
      "Iteration 161, loss = 1.32972010\n",
      "Iteration 162, loss = 1.31502087\n",
      "Iteration 163, loss = 1.30134153\n",
      "Iteration 164, loss = 1.28849334\n",
      "Iteration 165, loss = 1.27643193\n",
      "Iteration 166, loss = 1.26501662\n",
      "Iteration 167, loss = 1.25429558\n",
      "Iteration 168, loss = 1.24406666\n",
      "Iteration 169, loss = 1.23436578\n",
      "Iteration 170, loss = 1.22502538\n",
      "Iteration 171, loss = 1.21613383\n",
      "Iteration 172, loss = 1.20758064\n",
      "Iteration 173, loss = 1.19922813\n",
      "Iteration 174, loss = 1.19127511\n",
      "Iteration 175, loss = 1.18345071\n",
      "Iteration 176, loss = 1.17581573\n",
      "Iteration 177, loss = 1.16849478\n",
      "Iteration 178, loss = 1.16116447\n",
      "Iteration 179, loss = 1.15406974\n",
      "Iteration 180, loss = 1.14719571\n",
      "Iteration 181, loss = 1.14033687\n",
      "Iteration 182, loss = 1.13353596\n",
      "Iteration 183, loss = 1.12688533\n",
      "Iteration 184, loss = 1.12028770\n",
      "Iteration 185, loss = 1.11382944\n",
      "Iteration 186, loss = 1.10734495\n",
      "Iteration 187, loss = 1.10095794\n",
      "Iteration 188, loss = 1.09459297\n",
      "Iteration 189, loss = 1.08832361\n",
      "Iteration 190, loss = 1.08212013\n",
      "Iteration 191, loss = 1.07594892\n",
      "Iteration 192, loss = 1.06977495\n",
      "Iteration 193, loss = 1.06373691\n",
      "Iteration 194, loss = 1.05762675\n",
      "Iteration 195, loss = 1.05161340\n",
      "Iteration 196, loss = 1.04584266\n",
      "Iteration 197, loss = 1.03980268\n",
      "Iteration 198, loss = 1.03409498\n",
      "Iteration 199, loss = 1.02833503\n",
      "Iteration 200, loss = 1.02253583\n",
      "[CV] END activation=logistic, alpha=0.01, batch_size=200, hidden_layer_sizes=(100, 100, 100), solver=sgd; total time= 2.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piero/miniconda3/envs/ML_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.32128771\n",
      "Iteration 2, loss = 2.30493951\n",
      "Iteration 3, loss = 2.30493592\n",
      "Iteration 4, loss = 2.30488962\n",
      "Iteration 5, loss = 2.30478412\n",
      "Iteration 6, loss = 2.30480909\n",
      "Iteration 7, loss = 2.30479514\n",
      "Iteration 8, loss = 2.30480116\n",
      "Iteration 9, loss = 2.30464054\n",
      "Iteration 10, loss = 2.30454392\n",
      "Iteration 11, loss = 2.30450169\n",
      "Iteration 12, loss = 2.30447608\n",
      "Iteration 13, loss = 2.30451086\n",
      "Iteration 14, loss = 2.30444110\n",
      "Iteration 15, loss = 2.30436316\n",
      "Iteration 16, loss = 2.30425117\n",
      "Iteration 17, loss = 2.30421171\n",
      "Iteration 18, loss = 2.30420494\n",
      "Iteration 19, loss = 2.30425020\n",
      "Iteration 20, loss = 2.30416064\n",
      "Iteration 21, loss = 2.30419395\n",
      "Iteration 22, loss = 2.30397900\n",
      "Iteration 23, loss = 2.30408665\n",
      "Iteration 24, loss = 2.30394243\n",
      "Iteration 25, loss = 2.30397414\n",
      "Iteration 26, loss = 2.30387391\n",
      "Iteration 27, loss = 2.30379119\n",
      "Iteration 28, loss = 2.30379143\n",
      "Iteration 29, loss = 2.30390901\n",
      "Iteration 30, loss = 2.30372943\n",
      "Iteration 31, loss = 2.30372193\n",
      "Iteration 32, loss = 2.30364323\n",
      "Iteration 33, loss = 2.30358115\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.01, batch_size=200, hidden_layer_sizes=(100, 100, 100), solver=sgd; total time=  23.2s\n",
      "Best Hyperparameters:  {'activation': 'relu', 'alpha': 0.0001, 'batch_size': 100, 'hidden_layer_sizes': (400, 100), 'solver': 'adam'}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "clf = MLPClassifier(max_iter=200,verbose=True)\n",
    "\n",
    "param_grid = {'hidden_layer_sizes': [(100,),(400,100),(100,100,100)],\n",
    "              'activation': ['relu','logistic'],\n",
    "              'solver': ['adam','sgd'],\n",
    "              'alpha': [0.0001,0.001,0.01],\n",
    "              'batch_size': [50,100,200]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=clf, param_grid=param_grid, cv=5, refit=False, scoring='accuracy',verbose=2)\n",
    "grid_search.fit(X,y)\n",
    "\n",
    "print(\"Best Hyperparameters: \", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters:  {'activation': 'relu', 'alpha': 0.0001, 'batch_size': 100, 'hidden_layer_sizes': (400, 100), 'solver': 'adam'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Hyperparameters: \", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.28414835\n",
      "Iteration 2, loss = 0.10662280\n",
      "Iteration 3, loss = 0.06677371\n",
      "Iteration 4, loss = 0.04719837\n",
      "Iteration 5, loss = 0.03248420\n",
      "Iteration 6, loss = 0.02556928\n",
      "Iteration 7, loss = 0.02202696\n",
      "Iteration 8, loss = 0.01731334\n",
      "Iteration 9, loss = 0.01585999\n",
      "Iteration 10, loss = 0.01275492\n",
      "Iteration 11, loss = 0.01348037\n",
      "Iteration 12, loss = 0.00923796\n",
      "Iteration 13, loss = 0.00989519\n",
      "Iteration 14, loss = 0.01126335\n",
      "Iteration 15, loss = 0.01211521\n",
      "Iteration 16, loss = 0.00812658\n",
      "Iteration 17, loss = 0.01083579\n",
      "Iteration 18, loss = 0.00829597\n",
      "Iteration 19, loss = 0.00745610\n",
      "Iteration 20, loss = 0.00659017\n",
      "Iteration 21, loss = 0.00664791\n",
      "Iteration 22, loss = 0.01052149\n",
      "Iteration 23, loss = 0.00866424\n",
      "Iteration 24, loss = 0.00635994\n",
      "Iteration 25, loss = 0.00746740\n",
      "Iteration 26, loss = 0.00876131\n",
      "Iteration 27, loss = 0.00761501\n",
      "Iteration 28, loss = 0.00417943\n",
      "Iteration 29, loss = 0.00396917\n",
      "Iteration 30, loss = 0.00560590\n",
      "Iteration 31, loss = 0.00941394\n",
      "Iteration 32, loss = 0.00861815\n",
      "Iteration 33, loss = 0.00563298\n",
      "Iteration 34, loss = 0.00633239\n",
      "Iteration 35, loss = 0.00573614\n",
      "Iteration 36, loss = 0.00224236\n",
      "Iteration 37, loss = 0.00175729\n",
      "Iteration 38, loss = 0.00163557\n",
      "Iteration 39, loss = 0.00160328\n",
      "Iteration 40, loss = 0.00157159\n",
      "Iteration 41, loss = 0.00153830\n",
      "Iteration 42, loss = 0.00150264\n",
      "Iteration 43, loss = 0.00146398\n",
      "Iteration 44, loss = 0.00142228\n",
      "Iteration 45, loss = 0.00137677\n",
      "Iteration 46, loss = 0.00132695\n",
      "Iteration 47, loss = 0.00127376\n",
      "Iteration 48, loss = 0.00121743\n",
      "Iteration 49, loss = 0.00115813\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(batch_size=100, hidden_layer_sizes=(400, 100), verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;MLPClassifier<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.neural_network.MLPClassifier.html\">?<span>Documentation for MLPClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>MLPClassifier(batch_size=100, hidden_layer_sizes=(400, 100), verbose=True)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "MLPClassifier(batch_size=100, hidden_layer_sizes=(400, 100), verbose=True)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "best_params_ = {'activation': 'relu', 'alpha': 0.0001, 'batch_size': 100, 'hidden_layer_sizes': (400, 100), 'solver': 'adam'}\n",
    "clf = MLPClassifier(**best_params_,verbose=True)\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval on train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score: 1.0\n",
      "precision score: 1.0\n",
      "precision score: 1.0\n",
      "precision score: 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(50.722222222222214, 0.5, 'True label')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "# Predict\n",
    "y_proba = clf.predict_proba(X)\n",
    "y_pred = clf.predict(X)\n",
    "\n",
    "# Print scores\n",
    "metric_names = ['accuracy_score','precision_score','recall_score','f1_score']\n",
    "print('accuracy score:', metrics.accuracy_score(y,y_pred))\n",
    "print('precision score:', metrics.precision_score(y,y_pred,average='weighted'))\n",
    "print('precision score:', metrics.recall_score(y,y_pred,average='weighted'))\n",
    "print('precision score:', metrics.f1_score(y,y_pred,average='weighted'))\n",
    "\n",
    "# Plot confusion matrix\n",
    "cm = metrics.confusion_matrix(y,y_pred)\n",
    "sns.heatmap(cm,annot=True)\n",
    "plt.title('Confusion matrix')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.ylabel('True label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict test set and submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageId</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ImageId  Label\n",
       "0        1      2\n",
       "1        2      0\n",
       "2        3      9\n",
       "3        4      9\n",
       "4        5      3"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = test_df.values.astype('float32')/255\n",
    "y_pred_test = clf.predict(X_test)\n",
    "\n",
    "ids = np.arange(1,len(y_pred_test)+1,dtype=int)\n",
    "df_submission = pd.DataFrame({'ImageId':ids,\n",
    "                              'Label':y_pred_test})\n",
    "df_submission.to_csv(\"MnistSubmissionPB.csv\",index=False)\n",
    "df_submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 480x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ex_idx = 0\n",
    "ex_label = y_pred_test[ex_idx]\n",
    "ex_image = X_test[ex_idx,:].reshape(28,28)\n",
    "plt.matshow(ex_image,cmap='binary')\n",
    "plt.title(ex_label)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit to kaggle\n",
    "import json\n",
    "import os\n",
    "\n",
    "with open(\"../Kaggle_Houses/kaggle.json\",\"rb\") as f:\n",
    "    credentials = json.load(f)\n",
    "credentials\n",
    "\n",
    "os.environ[\"KAGGLE_USERNAME\"]=credentials[\"username\"]\n",
    "os.environ[\"KAGGLE_KEY\"]=credentials[\"key\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 208k/208k [00:01<00:00, 209kB/s]\n",
      "Successfully submitted to Digit Recognizer"
     ]
    }
   ],
   "source": [
    "import kaggle\n",
    "\n",
    "!kaggle competitions submit -c digit-recognizer -f MnistSubmissionPB.csv -m \"MNIST digit recognition MLP\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
